Authors,Author full names,Author(s) ID,Title,Year,Source title,Volume,Issue,Art. No.,Page start,Page end,Page count,Cited by,DOI,Link,Abstract,Author Keywords,Index Keywords,Document Type,Publication Stage,Open Access,Source,EID,Domain
Kim S.; Seo M.-W.; Lee S.J.; Kang S.-J.,"Kim, Sanghyuk (57195959868); Seo, Min-Woo (57194181832); Lee, Seung Joon (57203597094); Kang, Suk-Ju (35088306400)",57195959868; 57194181832; 57203597094; 35088306400,Object tracking-based foveated super-resolution convolutional neural network for head mounted display,2018,"SIGGRAPH Asia 2018 Posters, SA 2018",,,3283325,,,,1,10.1145/3283289.3283325,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85060149697&doi=10.1145%2f3283289.3283325&partnerID=40&md5=fe169d4f69a8051c355d8e5d03748a5a,"Recently, the immersive virtual reality (VR) environment using the head mounted display (HMD) has attracted attention as a new growth market due to the reasonable consumer price and high accessibility compared to other VR devices. However, users feel the cognitive heterogeneity caused by low resolution images, and hence, it is difficult to use it for a long time. To solve it, transmission techniques based on image resolution conversion have studied. In this paper, we propose a novel foveated super-resolution convolutional neural network (SRCNN) for HMD using an object tracking algorithm to reduce computation load for rendering high resolution images. We implement the object tracking on the region to compensate for a frame processing speed of eye-tracking devices, relatively slow to apply the resolution conversion. SRCNN applies to cognitive regions, and typical interpolation applies to other regions to reduce the rendering cost. As a result, the computation is decreased by 90.4059%, and PSNR is higher than the conventional foveated rendering algorithm. Copyright is held by the owner/author(s).",Deep neural network; Head mounted display; Object tracking; Super resolution,Convolution; Deep neural networks; Eye tracking; Image resolution; Interactive computer graphics; Neural networks; Optical resolving power; Rendering (computer graphics); Virtual reality; Convolutional neural network; Head mounted displays; Image resolution conversion; Immersive virtual reality; Object Tracking; Object tracking algorithm; Super resolution; Transmission techniques; Helmet mounted displays,Conference paper,Final,,Scopus,2-s2.0-85060149697,Gaming / VR
Xu Y.; Dong Y.; Wu J.; Sun Z.; Shi Z.; Yu J.; Gao S.,"Xu, Yanyu (57192081433); Dong, Yanbing (57193158532); Wu, Junru (57206989929); Sun, Zhengzhong (57207759513); Shi, Zhiru (56510716400); Yu, Jingyi (8569656400); Gao, Shenghua (35224747100)",57192081433; 57193158532; 57206989929; 57207759513; 56510716400; 8569656400; 35224747100,Gaze Prediction in Dynamic 360° Immersive Videos,2018,Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition,,,8578657,5333,5342,9.0,262,10.1109/CVPR.2018.00559,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85061647977&doi=10.1109%2fCVPR.2018.00559&partnerID=40&md5=b38ffd0e5c42da06f5bc212c487fcf7d,"This paper explores gaze prediction in dynamic 360° immersive videos, i.e., based on the history scan path and VR contents, we predict where a viewer will look at an upcoming time. To tackle this problem, we first present the large-scale eye-tracking in dynamic VR scene dataset. Our dataset contains 208 360° videos captured in dynamic scenes, and each video is viewed by at least 31 subjects. Our analysis shows that gaze prediction depends on its history scan path and image contents. In terms of the image contents, those salient objects easily attract viewers' attention. On the one hand, the saliency is related to both appearance and motion of the objects. Considering that the saliency measured at different scales is different, we propose to compute saliency maps at different spatial scales: The sub-image patch centered at current gaze point, the sub-image corresponding to the Field of View (FoV), and the panorama image. Then we feed both the saliency maps and the corresponding images into a Convolutional Neural Network (CNN) for feature extraction. Meanwhile, we also use a Long-Short-Term-Memory (LSTM) to encode the history scan path. Then we combine the CNN features and LSTM features for gaze displacement prediction between gaze point at a current time and gaze point at an upcoming time. Extensive experiments validate the effectiveness of our method for gaze prediction in dynamic VR scenes. © 2018 IEEE.",,Computer vision; Eye tracking; Forecasting; Large dataset; Visual communication; Convolutional neural network; Displacement prediction; Dynamic scenes; Field of views; Image content; Panorama images; Salient objects; Spatial scale; Long short-term memory,Conference paper,Final,,Scopus,2-s2.0-85061647977,Gaming / VR
Probst E.; Dietrich M.; Suttner V.; Buehler T.,"Probst, Enzio (57212770899); Dietrich, Monja (57205729646); Suttner, Vincent (57205734031); Buehler, Theres (57205726205)",57212770899; 57205729646; 57205734031; 57205726205,Rapture of the Deep,2018,"SIGGRAPH Asia 2018 Virtual and Augmented Reality, SA 2018",,,a14,,,,7,10.1145/3275495.3275499,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85061315493&doi=10.1145%2f3275495.3275499&partnerID=40&md5=0e833f36d2274813ebe34d84e392d949,                             Rapture of the Deep is an interactive Virtual Reality experience with eye tracking. The Experience is set in an underwater scenario using eye tracking as the main mechanism which allows the environment to react to the player's gaze and attention. In this project we worked with a retrofitted version of the HTC Vive headset with a complete Eye Tracking integration by Tobii                             1                              Pro and the Tobii Pro SDK for the Unity3D                             2                              Engine. Rapture of the Deep seeks to test how eye tracking technology can be employed as an attentive and invisible user interface allowing people to use reflexive and emotional behavior as a game controller.                          © 2018 Copyright held by the owner/author(s).,CCS CONCEPTS; Contact-less interaction; Eye Tracking; Immersion; User Experience; Virtual Reality,Augmented reality; Interactive computer graphics; User interfaces; Virtual reality; CCS CONCEPTS; Contact less; Emotional behavior; Eye tracking technologies; Game controller; Immersion; Interactive virtual reality; User experience; Eye tracking,Conference paper,Final,,Scopus,2-s2.0-85061315493,Gaming / VR
Wang S.; Wang Q.; Chen H.,"Wang, Shu (57205223038); Wang, Qing (59837474500); Chen, Hong (57050586300)",57205223038; 59837474500; 57050586300,Research and Application of Eye Movement Interaction based on Eye Movement Recognition,2018,MATEC Web of Conferences,246,,3038,,,,5,10.1051/matecconf/201824603038,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85059116476&doi=10.1051%2fmatecconf%2f201824603038&partnerID=40&md5=f9edb665cff91c0beb697f6bfda18fd5,"Generally, human-computer interaction is an interaction and operation between users and machine hardware. The user submits instructions to the machine, and the machine outputs the processed data and results to the user after receiving the instructions from the user. Mouse, keyboard, etc. are common input channels. With the maturity of eye tracking technology and the development of equipment miniaturization, turning eye movements into human-computer interaction input channels has become a hot spot in the field of human-computer interaction. Therefore, this paper analysed the physiological characteristics of eye movement, proposed the design principles and framework of eye movement interaction, and designed three kinds of eye movement recognition algorithms of fixation, saccade and blink. On this basis, using Unity 3D cross-platform development engine as a development tool, a children's attention training game application based on eye movement interaction is designed. The game is designed to combine eye movement interaction technology with attention training mode, simplify the control mode of the game, get attention feedback at the first time, achieve better training effect and improve the efficiency of human-computer interaction. © The Authors, published by EDP Sciences, 2018.",,Computer games; Computer hardware; Eye tracking; Human computer interaction; Mammals; Motion estimation; Waterworks; Cross platform development; Design Principles; Equipment miniaturization; Eye tracking technologies; Interaction technology; Movement recognition; Physiological characteristics; Research and application; Eye movements,Conference paper,Final,All Open Access; Gold Open Access; Green Open Access,Scopus,2-s2.0-85059116476,Gaming / VR
Zaraki A.; Wood L.; Robins B.; Dautenhahn K.,"Zaraki, Abolfazl (35729920200); Wood, Luke (55629090000); Robins, Ben (8585983100); Dautenhahn, Kerstin (7003305185)",35729920200; 55629090000; 8585983100; 7003305185,Development of a Semi-Autonomous Robotic System to Assist Children with Autism in Developing Visual Perspective Taking Skills,2018,RO-MAN 2018 - 27th IEEE International Symposium on Robot and Human Interactive Communication,,,8525681,969,976,7.0,17,10.1109/ROMAN.2018.8525681,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85058126824&doi=10.1109%2fROMAN.2018.8525681&partnerID=40&md5=d0d092f526c4eede51d458227fe68ff1,"Robot-assisted therapy has been successfully used to help children with Autism Spectrum Condition (ASC) develop their social skills, but very often with the robot being fully controlled remotely by an adult operator. Although this method is reliable and allows the operator to conduct a therapy session in a customised child-centred manner, it increases the cognitive workload on the human operator since it requires them to divide their attention between the robot and the child to ensure that the robot is responding appropriately to the child's behaviour. In addition, a remote-controlled robot is not aware of the information regarding the interaction with children (e.g., body gesture and head pose, proximity etc) and consequently it does not have the ability to shape live HRIs. Further to this, a remote-controlled robot typically does not have the capacity to record this information and additional effort is required to analyse the interaction data. For these reasons, using a remote-controlled robot in robot-assisted therapy may be unsustainable for long-term interactions. To lighten the cognitive burden on the human operator and to provide a consistent therapeutic experience, it is essential to create some degrees of autonomy and enable the robot to perform some autonomous behaviours during interactions with children. Our previous research with the Kaspar robot either implemented a fully autonomous scenario involving pairs of children, which then lacked the often important input of the supervising adult, or, in most of our research, has used a remote control in the hand of the adult or the children to operate the robot. Alternatively, this paper provides an overview of the design and implementation of a robotic system called Sense- Think-Act which converts the remote-controlled scenarios of our humanoid robot into a semi-autonomous social agent with the capacity to play games autonomously (under human supervision) with children in the real-world school settings. The developed system has been implemented on the humanoid robot Kaspar and evaluated in a trial with four children with ASC at a local specialist secondary school in the UK where the data of 11 Child-Robot Interactions (CRIs) was collected. The results from this trial demonstrated that the system was successful in providing the robot with appropriate control signals to operate in a semi-autonomous manner without any latency, which supports autonomous CRIs, suggesting that the proposed architecture appears to have promising potential in supporting CRIs for real-world applications. © 2018 IEEE.",,Anthropomorphic robots; Control theory; Diseases; Machine design; Personnel; Remote control; Robotics; Sounding apparatus; Autonomous robotic systems; Child-robot interactions; Children with autisms; Design and implementations; Long-term interaction; Proposed architectures; Remote-controlled robots; Robot-assisted therapies; Human robot interaction,Conference paper,Final,All Open Access; Green Open Access,Scopus,2-s2.0-85058126824,Gaming / VR
Hömke P.; Holler J.; Levinson S.C.,"Hömke, Paul (56028249400); Holler, Judith (22979583800); Levinson, Stephen C. (8847256300)",56028249400; 22979583800; 8847256300,Eye blinks are perceived as communicative signals in human face-to-face interaction,2018,PLoS ONE,13,12,e0208030,,,,64,10.1371/journal.pone.0208030,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85058468626&doi=10.1371%2fjournal.pone.0208030&partnerID=40&md5=e9d0475d41ed7a3a4ade86a85dedcb6e,"In face-to-face communication, recurring intervals of mutual gaze allow listeners to provide speakers with visual feedback (e.g. nodding). Here, we investigate the potential feedback function of one of the subtlest of human movements—eye blinking. While blinking tends to be subliminal, the significance of mutual gaze in human interaction raises the question whether the interruption of mutual gaze through blinking may also be communicative. To answer this question, we developed a novel, virtual reality-based experimental paradigm, which enabled us to selectively manipulate blinking in a virtual listener, creating small differences in blink duration resulting in ‘short’ (208 ms) and ‘long’ (607 ms) blinks. We found that speakers unconsciously took into account the subtle differences in listeners’ blink duration, producing substantially shorter answers in response to long listener blinks. Our findings suggest that, in addition to physiological, perceptual and cognitive functions, listener blinks are also perceived as communicative signals, directly influencing speakers’ communicative behavior in face-to-face communication. More generally, these findings may be interpreted as shedding new light on the evolutionary origins of mental-state signaling, which is a crucial ingredient for achieving mutual understanding in everyday social interaction. © 2018 Hömke et al. This is an open access article distributed under the terms of the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.",,"Adult; Blinking; Communication; Empathy; Facial Recognition; Feedback, Sensory; Female; Healthy Volunteers; Humans; Male; Photic Stimulation; Psycholinguistics; Reaction Time; Verbal Behavior; Virtual Reality; Young Adult; article; blinking; cognition; eye movement; face; gaze; human; human experiment; mental health; signal transduction; social interaction; virtual reality; adult; empathy; facial recognition; female; interpersonal communication; male; normal human; photostimulation; physiology; procedures; psycholinguistics; reaction time; sensory feedback; verbal behavior; young adult",Article,Final,All Open Access; Gold Open Access; Green Open Access,Scopus,2-s2.0-85058468626,Gaming / VR
Yim M.Y.-C.; Abdourazakou Y.; Sauer P.L.; Park S.-Y.,"Yim, Mark Yi-Cheon (55914652400); Abdourazakou, Yann (57196049132); Sauer, Paul L. (7102773950); Park, Sun-Young (57191612660)",55914652400; 57196049132; 7102773950; 57191612660,Modelling the dimensionality effects on brand placement effectiveness in stereoscopic 3-D versus 2-D sports games,2018,International Journal of Advertising,37,6,,958,983,25.0,15,10.1080/02650487.2017.1347366,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031420801&doi=10.1080%2f02650487.2017.1347366&partnerID=40&md5=8083767187fe6c116fd13f91757b23d4,"This current study explores how stereoscopic three-dimensional (3-D) dimensionality affects the process by which viewers’ memory of brand names embedded in a soccer game is formed compared to the memory process in traditional 2-D display. To this end, we conduct two studies: a qualitative observation using an eye tracker; and an experiment to identify the difference and similarity of the viewing process across these two display technologies. Statistical test results reveal that sports involvement enhances viewers’ attention to a sports game, which is moderated by game enjoyment and negative viewing experience generated from the media features. Most importantly, it is found that as viewers pay more attention to a sports game in stereoscopic 3-D display, they are less likely to remember the brands embedded in the stadium, while the opposite is found in 2-D display. More findings and implications are discussed in the discussion section. © 2017, © 2017 Advertising Association.",brand placement; Dimensionality; sports involvement; stereoscopic 3-D,,Article,Final,,Scopus,2-s2.0-85031420801,Gaming / VR
Bourgeois A.; Badier E.; Baron N.; Carruzzo F.; Vuilleumier P.,"Bourgeois, Alexia (54942986700); Badier, Emmanuel (55496534500); Baron, Naem (56258963900); Carruzzo, Fabien (57204933522); Vuilleumier, Patrik (57200075380)",54942986700; 55496534500; 56258963900; 57204933522; 57200075380,Influence of reward learning on visual attention and eye movements in a naturalistic environment: A virtual reality study,2018,PLoS ONE,13,12,e0207990,,,,15,10.1371/journal.pone.0207990,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85058002614&doi=10.1371%2fjournal.pone.0207990&partnerID=40&md5=a09fc135a143b758a457603057d5533b,"Rewards constitute crucial signals that motivate approach behavior and facilitate the perceptual processing of objects associated with favorable outcomes in past encounters. Reward-related influences on perception and attention have been reliably observed in studies where a reward is paired with a unidimensional low-level visual feature, such as the color or orientation of a line in visual search tasks. However, our environment is drastically different and composed of multidimensional and changing visual features, encountered in complex and dynamic scenes. Here, we designed an immersive virtual reality (VR) experiment using a 4-frame CAVE system to investigate the impact of rewards on attentional orienting and gaze patterns in a naturalistic and ecological environment. Forty-one healthy participants explored a virtual forest and responded to targets appearing on either the left or right side of their path. To test for reward-induced biases in spatial orienting, targets on one side were associated with high reward, whereas those on the opposite side were paired with a low reward. Eye-movements recording showed that left-side high rewards led to subsequent increase of eye gaze fixations towards this side of the path, but no such asymmetry was found after exposure to right-sided high rewards. A milder spatial bias was also observed after left-side high rewards during subsequent exploration of a virtual castle yard, but not during route turn choices along the forest path. Our results indicate that reward-related influences on attention and behavior may be better learned in left than right space, in line with a right hemisphere dominance, and could generalize to another environment to some extent, but not to spatial choices in another decision task, suggesting some domain- or context-specificity. This proof-of-concept study also outlines the advantages and the possible drawbacks of the use of the 3D CAVE immersive platform for VR in neuroscience. © 2018 Bourgeois et al. This is an open access article distributed under the terms of the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.",,"Adolescent; Adult; Attention; Cerebrum; Female; Fixation, Ocular; Functional Laterality; Humans; Learning; Male; Orientation, Spatial; Reward; Space Perception; Virtual Reality; adult; article; clinical article; controlled study; eye movement; female; forest; gaze; hemispheric dominance; human; human experiment; learning; male; neuroscience; proof of concept; reward; right hemisphere; virtual reality; visual attention; adolescent; anatomy and histology; attention; depth perception; eye fixation; learning; physiology; spatial orientation",Article,Final,All Open Access; Gold Open Access; Green Open Access,Scopus,2-s2.0-85058002614,Gaming / VR
Fang Y.; Zhang X.; Imamoglu N.,"Fang, Yuming (8435698900); Zhang, Xiaoqiang (57203362331); Imamoglu, Nevrez (38061650600)",8435698900; 57203362331; 38061650600,A novel superpixel-based saliency detection model for 360-degree images,2018,Signal Processing: Image Communication,69,,,1,7,6.0,38,10.1016/j.image.2018.07.009,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85051383029&doi=10.1016%2fj.image.2018.07.009&partnerID=40&md5=879de84fee9e978a8ac0d0431820dbed,"Effective visual attention modeling is a key factor that helps enhance the overall Quality of Experience (QoE) of VR/AR data. Although a huge number of algorithms have been developed in recent years to detect salient regions in flat-2D images, the research on 360-degree image saliency is limited. In this study, we propose a superpixel-level saliency detection model for 360-degree images by figure-ground law of Gestalt theory. First, the input image is segmented into superpixels. CIE Lab color space is then used to extract the perceptual features. We extract luminance and texture features for 360-degree images from L channel, while color features are extracted from a and b channels. We compute two components for saliency prediction by figure-ground law of Gestalt theory: feature contrast and boundary connectivity. The feature contrast is computed on superpixel level by luminance and color features. The boundary connectivity is predicted for background measure and it describes the spatial layout of image region with two image boundaries (upper and lower boundary). The final saliency map of 360-degree image is calculated by fusing feature contrast map and boundary connectivity map. Experimental results on a public eye tracking database of 360-degree images show promising performance of saliency prediction from the proposed method. © 2018 Elsevier B.V.",360-degree image; Boundary connectivity; Figure-ground law; Gestalt theory; Saliency detection; Visual attention,Behavioral research; Color; Eye tracking; Image segmentation; Luminance; Pixels; Quality of service; 360-degree image; Boundary connectivity; Figure ground; Gestalt theory; Saliency detection; Visual Attention; Superpixels,Article,Final,,Scopus,2-s2.0-85051383029,Gaming / VR
Hashimura S.; Shimakawa H.; Kajiwara Y.,"Hashimura, Shota (57204808489); Shimakawa, Hiromitsu (8529351700); Kajiwara, Yusuke (54681995100)",57204808489; 8529351700; 54681995100,Automatic assessment of student understanding level using virtual reality,2018,"Proceedings of the 2018 Federated Conference on Computer Science and Information Systems, FedCSIS 2018",,,8511187,39,45,6.0,3,10.15439/2018F268,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85057246476&doi=10.15439%2f2018F268&partnerID=40&md5=a59e0c25c33e6d1afd52c0b77ddee4d7,"The improvement of the efficiency in teaching requires knowing the understanding level of each student. However, it is difficult due to limited time in a class. We propose a Virtual Reality (VR) space imposing assignments on students, to know their understanding level from their behavior which comes from cognitive loads during their answering. The VR space presents a student an assignment and a working space to answer it. In general, students solve assignments, using elements on their short term memory. When students solve same kind of assignments many times, they build generalized solution methods in their long term memory. When they engage in such assignments, their cognitive load is low enough to make them watch only the working spaces, keeping their hands working. On the other hand, when students have no solution pattern, their short term memory works hard. Their high cognitive load often stop their hands, because of confusion. They also look assignments and the working space many times, to reconsider solutions. Since answering behavior of students exposes their cognitive load, a VR space is ideal to estimate cognitive load. We conducted an experiment to evaluate the ability of the method to estimate the cognitive load. We examined the movement of the hand and the edit distance of student's answer from the correct sentence during their answering. We confirmed a fair correlation of the hands' stagnation with the confidence in students of good scores. We also found a relationship of eye movement with the change of the edit distance. The experiment result implies the possibility to estimate the cognitive load. The estimation would enable teachers to know students' understanding faults, which leads to education according to the understanding level. © 2018 Polish Information Processing Society.",,Brain; Eye movements; Information systems; Information use; Virtual reality; Automatic assessment; Cognitive loads; Edit distance; Generalized solution; Long term memory; Short term memory; Solution patterns; Understanding level; Students,Conference paper,Final,All Open Access; Gold Open Access,Scopus,2-s2.0-85057246476,Gaming / VR
Roose K.M.; Veinott E.S.; Mueller S.T.,"Roose, Kaitlyn M. (57197784809); Veinott, Elizabeth S. (6506037042); Mueller, Shane T. (24765109700)",57197784809; 6506037042; 24765109700,The tracer method: The dynamic duo combining cognitive task analysis and eye tracking,2018,CHI PLAY 2018 - Proceedings of the 2018 Annual Symposium on Computer-Human Interaction in Play Companion Extended Abstracts,,,,585,593,8.0,9,10.1145/3270316.3271522,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85058469553&doi=10.1145%2f3270316.3271522&partnerID=40&md5=39845f087ddfae52806144885ed0604e,"This paper introduces The Tracer Method that integrates two common approaches for understanding skilled performance: Cognitive Task Analysis (CTA) and Eye Tracking (ET). This combination has the potential to provide information for game designers and human computer interaction researchers that will guide feedback to areas with the greatest payoff. Historically, ET has been used to gain behavioral insight into visual search patterns and attention, whereas CTA has been used to understand higher-level goals, strategies, and decisions. We integrate the two by using CTA to identify key events during the game, and examine ET statistics conditioned on these. In this demonstration, ET behavior was recorded while 17 experienced Overwatch players engaged in competitive play. Through post-game CTA interviews, critical decisions were identified and analyzed post play. Results provide some examples of new insights that can be captured from a combination of these methods, and used for game design, play testing, or evaluation. © 2018 Copyright is held by the owner/author(s).",Cognitive Task Analysis; Critical Decision Method; Eye Tracking; Methodology; Video Games,Computer games; Human computer interaction; Interactive computer systems; Job analysis; Cognitive task analysis; Decision method; Game designers; Methodology; Play testing; Tracer methods; Video game; Visual search; Eye tracking,Conference paper,Final,,Scopus,2-s2.0-85058469553,Gaming / VR
Wagner G.; Sevincer A.T.; Keim R.; Fähnrich M.; Oettingen G.,"Wagner, Greta (57196448615); Sevincer, A. Timur (29867575700); Keim, Rebecca (58391772900); Fähnrich, Marén (57203999697); Oettingen, Gabriele (6701352863)",57196448615; 29867575700; 58391772900; 57203999697; 6701352863,Alcohol intake can reduce gambling behavior,2018,Psychology of Addictive Behaviors,32,7,,832,885,53.0,1,10.1037/adb0000396,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85054036954&doi=10.1037%2fadb0000396&partnerID=40&md5=77d6405b06791d823974403885e14dd9,"Prolonged and risky gambling can have negative consequences financially and in health (e.g., developing an addiction). As gambling frequently occurs together with alcohol intake, we investigated whether we could reduce persistent and risky gambling under the influence of alcohol. Specifically, following alcohol myopia theory (Steele & Josephs, 1990), stating that intoxicated people's behavior is disproportionally guided by salient cues, we propose that making low chances of winning salient in a gambling situation should reduce persistent and risky gambling in alcohol intoxicated participants. In 3 laboratory studies, participants either consumed alcohol or a placebo. We made low chances of winning salient (vs. not) by explicitly displaying the low chances in large letters. Making low chances salient led intoxicated participants to gamble less persistently on a computerized slot machine (Study 1 and 2) and with less risk in a lottery game (Study 3) compared with sober participants and compared with sober and intoxicated participants in a control condition in which low chances were not salient. Moreover, using eye-tracking in Study 3, we found that the effect of alcohol on less risky gambling was mediated by intoxicated participants' greater attention to the salient low chances. Finally, we replicated the findings from our laboratory studies in the field: When low chances were made salient, the more alcohol bar patrons had consumed, the less persistently they gambled on a slot machine (Study 4). The findings have applied implications for reducing excessive gambling under the influence of alcohol by making low chances salient on games of chance. © 2018 APA, all rights reserved.",alcohol myopia; field experiment; gambling; nudge; risk-taking,"Adult; Alcohol Drinking; Alcoholic Intoxication; Behavior, Addictive; Female; Gambling; Humans; Male; Young Adult; adult; alcohol blood level; alcohol consumption; alcohol intoxication; Article; attitude; eye tracking; female; gambling; game; goal attainment; human; male; money; pilot study; questionnaire; risk factor; young adult; addiction; drinking behavior; gambling; psychology",Article,Final,,Scopus,2-s2.0-85054036954,Gaming / VR
Da Silva M.R.D.; Rusz D.; Postma-Nilsenová M.,"Da Silva, Mariana Rachel Dias (57204687746); Rusz, Dorottya (57204092057); Postma-Nilsenová, Marie (55939937300)",57204687746; 57204092057; 55939937300,"Ruminative minds, wandering minds: Effects of rumination and mind wandering on lexical associations, pitch imitation and eye behaviour",2018,PLoS ONE,13,11,e0207578,,,,7,10.1371/journal.pone.0207578,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85056719900&doi=10.1371%2fjournal.pone.0207578&partnerID=40&md5=86c4051485963128df4e1fad52740077,"This study demonstrates that rumination is reflected in two behavioural signals that both play an important role in face-to-face interactions and provides evidence for the negative impact of rumination on social cognition. Sixty-one students were randomly assigned either to a condition in which rumination was induced or to a control condition. Their task was to play a speech-based word association game with an Embodied Conversational Agent during which their word associations, pitch imitation and eye movements were measured. Two questionnaires assessed their ruminative tendencies and mind wandering thoughts, respectively. Rumination predicted differences in task-related mind wandering, polarity of lexical associations, pitch imitation, and blinks while mind wandering predicted differences in saccades. This outcome may show that rumination has a negative impact on certain aspects of social interactions. © 2018 Dias da Silva et al. This is an open access article distributed under the terms of the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.",,"Adult; Eye Movements; Female; Humans; Male; Random Allocation; Rumination, Cognitive; Speech; Students; Surveys and Questionnaires; Wandering Behavior; Young Adult; article; controlled study; human; human experiment; imitation; major clinical study; pitch; questionnaire; randomized controlled trial; rumination; saccadic eye movement; social cognition; social interaction; speech; student; adult; cognitive rumination; eye movement; female; male; physiology; psychology; randomization; wandering behavior; young adult",Article,Final,All Open Access; Gold Open Access; Green Open Access,Scopus,2-s2.0-85056719900,Gaming / VR
Wade J.; Nichols H.S.; Ichinose M.; Bian D.; Bekele E.; Snodgress M.; Amat A.Z.; Granholm E.; Park S.; Sarkar N.,"Wade, Joshua (55803508600); Nichols, Heathman S. (54403563700); Ichinose, Megan (56765331800); Bian, Dayi (55804018400); Bekele, Esube (57204162101); Snodgress, Matthew (57200527130); Amat, Ashwaq Zaini (57203133051); Granholm, Eric (7004144492); Park, Sohee (7501834137); Sarkar, Nilanjan (7201361624)",55803508600; 54403563700; 56765331800; 55804018400; 57204162101; 57200527130; 57203133051; 7004144492; 7501834137; 7201361624,Extraction of emotional information via visual scanning patterns: A feasibility study of participants with schizophrenia and neurotypical individuals,2018,ACM Transactions on Accessible Computing,11,4,23,,,,7,10.1145/3282434,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85057177614&doi=10.1145%2f3282434&partnerID=40&md5=5db30b03366750c150ccbc1864e1fa90,"Emotion recognition impairment is a core feature of schizophrenia (SZ), present throughout all stages of this condition, and leads to poor social outcome. However, the underlying mechanisms that give rise to such deficits have not been elucidated and hence, it has been difficult to develop precisely targeted interventions. Evidence supports the use of methods designed to modify patterns of visual attention in individuals with SZ in order to effect meaningful improvements in social cognition. To date, however, attention-shaping systems have not fully utilized available technology (e.g., eye tracking) to achieve this goal. The current work consisted of the design and feasibility testing of a novel gaze-sensitive social skills intervention system called MASI-VR. Adults from an outpatient clinic with confirmed SZ diagnosis (n = 10) and a comparison sample of neurotypical participants (n = 10) were evaluated on measures of emotion recognition and visual attention at baseline assessment, and a pilot test of the intervention system was evaluated on the SZ sample following five training sessions over three weeks. Consistent with the literature, participants in the SZ group demonstrated lower recognition of faces showing medium intensity fear, spent more time deliberating about presented emotions, and had fewer fixations in comparison to neurotypical peers. Furthermore, participants in the SZ group showed significant improvement in the recognition of fearful faces post-training. Preliminary evidence supports the feasibility of a gaze-sensitive paradigm for use in assessment and training of emotion recognition and social attention in individuals with SZ, thus warranting further evaluation of the novel intervention. © 2018 Association for Computing Machinery.",,Behavioral research; Diseases; Speech recognition; Baseline assessment; Emotion recognition; Emotional information; Feasibility studies; Feasibility testing; Outpatient clinic; Training sessions; Visual Attention; Eye tracking,Article,Final,All Open Access; Bronze Open Access; Green Open Access,Scopus,2-s2.0-85057177614,Gaming / VR
Pan X.; Wang H.; Chen L.; Zhu Y.; Yang A.,"Pan, Xinxing (57195679579); Wang, Hui (59450687400); Chen, Ling (57191047600); Zhu, Yongxin (36903371100); Yang, Aolei (56359008700)",57195679579; 59450687400; 57191047600; 36903371100; 56359008700,Convolution object detection based depth estimation of 3D eye-tracking system; [基于卷积目标检测的3D眼球追踪系统深度估计],2018,Yi Qi Yi Biao Xue Bao/Chinese Journal of Scientific Instrument,39,10,,241,248,7.0,1,10.19650/j.cnki.cjsi.J1803784,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85060079273&doi=10.19650%2fj.cnki.cjsi.J1803784&partnerID=40&md5=ba9956a9677994024e2ea53bad852dfa,"With the development of virtual reality technology, eye-tracking, as one of its core technology, is paid more and more people's attention. On the basis of conventional 3D gaze estimation technique, in this paper, a 3D eye-tracking implementation method is proposed, which recovers the depth information of the object region through convolution object detection. Based on the image information collected by the world camera of the Pupil head wearable eye-tracking device, the TensorFlow convolution object detection framework is used to realize object recognition and its width measurement. Through establishing the function relationship between the detected width value and the distance value of actual measurement, the purpose of estimating the depth information in real time is achieved. The experiment data show that the average relative error is only 1.17% in the six sets of fixed-point tests with the sampled image resolution of 1 080p and the real-time processing speed reaches 15 frames per second, which can make an accurate prediction of the real-time depth information. Under the background that the eye tracking technology is getting more and more mature and the cost of eye trackers and etc. is decreased gradually, this study lays a solid foundation for the further development and application of eye-tracking technology. © 2018, Science Press. All right reserved.",Convolutional neural network; Eye-tracking; Monocular vision; Pinhole camera,Convolution; Image resolution; Neural networks; Object detection; Object recognition; Pinhole cameras; Virtual reality; Average relative error; Convolutional neural network; Development and applications; Eye tracking technologies; Function relationships; Monocular vision; Real-time processing speed; Virtual reality technology; Eye tracking,Article,Final,,Scopus,2-s2.0-85060079273,Gaming / VR
Hong S.; Cheng H.; Mao B.,"Hong, Shuai (57202045745); Cheng, Hui (57204071120); Mao, Bo (35753612000)",57202045745; 57204071120; 35753612000,Visual Saliency Detection Framework for 3D Environment using Virtual Reality Devices,2018,"Proceedings of the 2018 IEEE 22nd International Conference on Computer Supported Cooperative Work in Design, CSCWD 2018",,,8465363,666,671,5.0,0,10.1109/CSCWD.2018.8465363,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85054391290&doi=10.1109%2fCSCWD.2018.8465363&partnerID=40&md5=731c9c6df49ae819b076891234c4b52b,"Visual saliency is essential for attention analysis and it is usually measured by eye tracking system, which is widely used in the fields of design evaluation, user's gaze and behavior analysis. This study combines virtual reality with eye tracking technology and applies HTC Vive VR helmet to simulate eye tracker for both two-dimensional and three-dimensional cases study. We collect user's gaze points data and draw the scatter diagram and the thermodynamic map to analyze the user's visual saliency in completely 3D environment. The quantitative error of user's visual saliency data in virtual reality environment is analyzed by setting the average offset distance and offset degree indexes. The experiment results indicate that the proposed framework can be used to detect the visual saliency in 3D environment accurately that is has practical significance to the application of user's visual saliency analysis in the virtual reality environment. © 2018 IEEE.",Average Offset Distance; Offset; Virtual Reality; Visual Saliency,,Conference paper,Final,,Scopus,2-s2.0-85054391290,Gaming / VR
Chen C.-T.; Huang C.-Y.; Wang J.T.-Y.,"Chen, Chun-Ting (57202786344); Huang, Chen-Ying (26661666700); Wang, Joseph Tao-yi (35216562000)",57202786344; 26661666700; 35216562000,A window of cognition: Eyetracking the reasoning process in spatial beauty contest games,2018,Games and Economic Behavior,111,,,143,158,15.0,9,10.1016/j.geb.2018.05.007,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85049339198&doi=10.1016%2fj.geb.2018.05.007&partnerID=40&md5=2c54507bc6a3a8520880d8ec27db423d,"We study the reasoning process in an environment where final choices are well understood and the associated theory is procedural by introducing two-person beauty contest games played spatially on two-dimensional grid maps. Players choose locations and are rewarded by hitting targets dependent on opponents' choice locations. By tracking subjects' eye movements (lookups), we infer their reasoning process and classify subjects into various levels. More than a half of the subjects' classifications coincides with their classifications using final choices, supporting a literal interpretation of the level-k model for subject's reasoning process. Lookup analyses reveal that the center area is where most subjects initially look at. This sheds light on the level-0 belief. Moreover, learning lookups of a trial on average could increase payoffs of that trial and eliminates roughly 60% of the gap to empirical best response, indicating how valuable lookups can help predict choices. © 2018 Elsevier Inc.",Beauty contest game; Best response hierarchy; Cognitive hierarchy; Guessing game; Level-k model,,Article,Final,,Scopus,2-s2.0-85049339198,Gaming / VR
Melicio C.; Figueiredo R.; Almeida A.F.; Bernardino A.; Santos-Victor J.,"Melicio, Cristina (57210339734); Figueiredo, Rui (57224811686); Almeida, Ana Filipa (57212485538); Bernardino, Alexandre (7003407125); Santos-Victor, Jose (7003525618)",57210339734; 57224811686; 57212485538; 7003407125; 7003525618,Object detection and localization with artificial foveal visual attention,2018,"2018 Joint IEEE 8th International Conference on Development and Learning and Epigenetic Robotics, ICDL-EpiRob 2018",,,8761032,101,106,5.0,6,10.1109/DEVLRN.2018.8761032,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85070381129&doi=10.1109%2fDEVLRN.2018.8761032&partnerID=40&md5=fded8a26ced36fa67f81f5bbbf426498,"In the last decades, in order to make the processing of a scene more efficient, biologically inspired approaches have been proposed. Visual attention models are being studied and actively developed in order to reduce the complexity and computational time of the existing methods. We propose a biologically inspired model that combines a single pre-trained CNN architecture with an artificial foveal visual system that performs simultaneously the classification and localization of objects in images. This model is based on the fact that only a small part of the image is processed with high resolution at each time so we load a foveated image in the network and successively employ feed-forward passes to determine the class labels and then via backward propagation determine the object possible locations according to each semantic label. By directing the attention to the center of the proposed location we mimic the human saccadic eye movements. In the results obtained we used the ILSVRC 2012 validation data set in a GoogLeNet CNN. We demonstrate that for non-centered objects the gain of the classification performance between iterations is significant showing that when mimicking the human visual behaviour of foveation, saccades are needed to integrate the information at each time. © 2018 IEEE.",,Classification (of information); Eye movements; Object detection; Robotics; Semantics; Backward propagation; Biologically inspired; Biologically inspired models; Classification performance; Computational time; Object detection and localizations; Saccadic eye movements; Visual attention model; Behavioral research,Conference paper,Final,,Scopus,2-s2.0-85070381129,Gaming / VR
Frosina P.; Logue M.; Book A.; Huizinga T.; Amos S.; Stark S.,"Frosina, P. (56582048100); Logue, M. (56581638000); Book, A. (6701496733); Huizinga, T. (56581367700); Amos, S. (56581634200); Stark, S. (57201355838)",56582048100; 56581638000; 6701496733; 56581367700; 56581634200; 57201355838,The effect of cognitive load on nonverbal behavior in the cognitive interview for suspects,2018,Personality and Individual Differences,130,,,51,58,7.0,13,10.1016/j.paid.2018.03.012,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85044457657&doi=10.1016%2fj.paid.2018.03.012&partnerID=40&md5=ceb90bdbc267408395e75d18345b8369,"We investigated whether cognitive load results in changes to nonverbal behavior in the context of interrogation, and whether psychopathic traits affected this relationship. Cognitive load was implemented by using the cognitive interview for suspects (CIS). Onehundred- and-fifty undergraduate students were assigned to one of two conditions: 1) a true event, where they played a game with a confederate, and money went missing from a wallet in the room, or 2) a false-alibi condition, where they read a scenario similar to the true event (in order to create a feasible alibi), and were instructed to steal $10 from the wallet. Blinking, hand gestures, trunk movements, and direct eye gaze were coded at each point in the CIS. Regardless of condition, the increase in cognitive load had the effect of increasing blinking and decreasing hand gestures and direct eye gaze. There were significant interactions between CIS stage and experimental condition for blinks and hand gestures, where people in the false alibi condition had a sharper increase in blinking, and decrease in hand gestures when cognitive load was introduced. Psychopathic traits did not affect the utility of above cues, but change in trunk movements was positively correlated with psychopathy in the false alibi condition. © 2018 Elsevier Ltd",Cognitive interview; Cognitive load; Interrogation; Nonverbal cues; Psychopathy,,Article,Final,,Scopus,2-s2.0-85044457657,Gaming / VR
Mei C.; Zahed B.T.; Mason L.; Ouarles J.,"Mei, Chao (56421487500); Zahed, Bushra T. (54928797700); Mason, Lee (39061658300); Ouarles, John (57203969899)",56421487500; 54928797700; 39061658300; 57203969899,Towards Joint Attention Training for Children with ASD-a VR Game Approach and Eye Gaze Exploration,2018,"25th IEEE Conference on Virtual Reality and 3D User Interfaces, VR 2018 - Proceedings",,,8446242,289,296,7.0,27,10.1109/VR.2018.8446242,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85053823448&doi=10.1109%2fVR.2018.8446242&partnerID=40&md5=6bcaca4c7c2a2696d1ba10180e2f1571,"Joint attention is critical to the education and development of a child. Deficits in joint attention are considered by many researchers to be an early predictor of children with Autism Spectrum Disorder (ASD). Training of joint attention have been a significant topic in ASD intervention education research. We propose a novel joint attention training approach using a Customizable Virtual Human (CVH) and a Virtual Reality (VR) game to assist with joint attention training. Previous work has shown that CVHs potentially help the users with ASD to increase their performance in hand-eye coordination, motivate the users to play longer, as well as improve user experience in a training game. Based upon these discovered CVH benefits, we hypothesize that CVHs may also be beneficial in training joint attention for users with ASD. To test our hypothesis, we developed a CVH with customizable facial features in an educational game-Imagination Drums-and conducted a user study on adolescents with high functioning ASD to investigate the effects of CVHs. We collected users' eye-gaze data and task performance during the game to evaluate the users' joint attention with CVHs and the effectiveness of CVHs compared with Non-Customizable Virtual Humans (NCVHs). The study results showed that the CVH make the participants gaze less at the irrelevant area of the game's storyline (i.e. background), but surprisingly, also provided evidence that participants react slower to the CVH's joint attention bids, compared with NCVH. Overall, the study reveals insights of how users with ASD interact with CVHs and how these interactions affect joint attention. © 2018 IEEE.",Customizable virtual human. Autism Spectrum Disorder. 3D interaction; H.5.2 [Information Interfaces and Presentation]: User Interfaces-Evaluation/methodology,Diseases; Virtual reality; Children with autisms; Education research; Educational game; H.5.2 [Information Interfaces and Presentation]: User Interfaces - Evaluation/methodology; Hand eye coordination; Joint attention; Task performance; Virtual humans; User interfaces,Conference paper,Final,,Scopus,2-s2.0-85053823448,Gaming / VR
Amaral C.; Mouga S.; Simões M.; Pereira H.C.; Bernardino I.; Quental H.; Playle R.; McNamara R.; Oliveira G.; Castelo-Branco M.,"Amaral, Carlos (57202405321); Mouga, Susana (55256195500); Simões, Marco (37063659600); Pereira, Helena C. (44561538000); Bernardino, Inês (55162912000); Quental, Hugo (36446478700); Playle, Rebecca (7003294514); McNamara, Rachel (14719753800); Oliveira, Guiomar (57201814516); Castelo-Branco, Miguel (57527878600)",57202405321; 55256195500; 37063659600; 44561538000; 55162912000; 36446478700; 7003294514; 14719753800; 57201814516; 57527878600,A feasibility clinical trial to improve social attention in Autistic Spectrum Disorder (ASD) using a brain computer interface,2018,Frontiers in Neuroscience,12,JUL,477,,,,76,10.3389/fnins.2018.00477,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85049995456&doi=10.3389%2ffnins.2018.00477&partnerID=40&md5=eeb0827156721077607ea90fb14d88a4,"Deficits in the interpretation of others' intentions from gaze-direction or other social attention cues are well-recognized in ASD. Here we investigated whether an EEG brain computer interface (BCI) can be used to train social cognition skills in ASD patients. We performed a single-arm feasibility clinical trial and enrolled 15 participants (mean age 22y 2m) with high-functioning ASD (mean full-scale IQ 103). Participants were submitted to a BCI training paradigm using a virtual reality interface over seven sessions spread over 4 months. The first four sessions occurred weekly, and the remainder monthly. In each session, the subject was asked to identify objects of interest based on the gaze direction of an avatar. Attentional responses were extracted from the EEG P300 component. A final follow-up assessment was performed 6-months after the last session. To analyze responses to joint attention cues participants were assessed pre and post intervention and in the follow-up, using an ecologic ""Joint-attention task."" We used eye-tracking to identify the number of social attention items that a patient could accurately identify from an avatar's action cues (e.g., looking, pointing at). As secondary outcome measures we used the Autism Treatment Evaluation Checklist (ATEC) and the Vineland Adaptive Behavior Scale (VABS). Neuropsychological measures related to mood and depression were also assessed. In sum, we observed a decrease in total ATEC and rated autism symptoms (Sociability; Sensory/Cognitive Awareness; Health/Physical/Behavior); an evident improvement in Adapted Behavior Composite and in the DLS subarea from VABS; a decrease in Depression (from POMS) and in mood disturbance/depression (BDI). BCI online performance and tolerance were stable along the intervention. Average P300 amplitude and alpha power were also preserved across sessions. We have demonstrated the feasibility of BCI in this kind of intervention in ASD. Participants engage successfully and consistently in the task. Although the primary outcome (rate of automatic responses to joint attention cues) did not show changes, most secondary neuropsychological outcome measures showed improvement, yielding promise for a future efficacy trial. © 2018 Amaral, Mouga, Simões, Pereira, Bernardino, Quental, Playle, McNamara, Oliveira and Castelo-Branco.",Autism; Brain-computer interface; Clinical trial; EEG; Social attention; Virtual reality,adolescent; adult; Article; autism; brain computer interface; clinical article; depression; electroencephalogram; eye tracking; feasibility study; human; male; mood disorder; social cognition; social interaction; task performance; virtual reality,Article,Final,All Open Access; Gold Open Access; Green Open Access,Scopus,2-s2.0-85049995456,Gaming / VR
Zhou H.; Wei L.; Cao R.; Hanoun S.; Bhatti A.; Tai Y.; Nahavandi S.,"Zhou, Hailing (57198957382); Wei, Lei (35773955300); Cao, Ran (57207107724); Hanoun, Samer (24780376000); Bhatti, Asim (12345621500); Tai, Yonghang (36984437500); Nahavandi, Saeid (55992860000)",57198957382; 35773955300; 57207107724; 24780376000; 12345621500; 36984437500; 55992860000,The Study of Using Eye Movements to Control the Laparoscope under a Haptically-Enabled Laparoscopic Surgery Simulation Environment,2018,"Proceedings - 2018 IEEE International Conference on Systems, Man, and Cybernetics, SMC 2018",,,8616509,3022,3026,4.0,3,10.1109/SMC.2018.00513,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85062210153&doi=10.1109%2fSMC.2018.00513&partnerID=40&md5=aae4d43e327bd6e8ac29373bce8d40ec,"The purpose of this study is to investigate the possibility to use eye movements to control the laparoscope during a laparoscopic surgery. Laparoscopic surgery usually needs at least two doctors, a surgeon and a laparoscope assistant. The view of the operating surgeon is provided by the laparoscope assistant. As misunderstandings or conflicts of cooperation may happen, an ideal way is that the surgeon has a full control of all the instruments including the surgical tools and laparoscope. To achieve it, an eye based interaction method is introduced in this paper that allows surgeons to control the view by themselves. With recent developments in the eye tracker platforms and associated eye tracking technologies, many non-contact eye tracking systems are available. It can record where a person is looking at any time and a sequence of eye movements. This information can be used to know where is the attention and interest of the person on a display. As such, surgeon's attention can be captured and then be followed by moving the laparoscope to the region of interest. To have a safe and efficient evaluation on the usability, a virtual reality based laparoscopic surgery simulation is built. It is based on Unity with two haptic devices simulating the surgical tools, a 3D mouse providing 6 degrees-of-freedom control of the camera and an eye tracker capturing eyes' positions on a display. Experiments on moving a camera left, right, up, down, in, out and to specified locations using eyes are conducted, and moreover the performances of the proposed eye based self-control and the 3D mouse based other-control are compared. The results are promising where the proposed pointing method leads to 43.6% faster completion of the tasks against the traditional other-control method using the 3D mouse. © 2018 IEEE.",eye gaze; haptic; laparoscopic surgery simulation,Cameras; Degrees of freedom (mechanics); Display devices; Eye movements; Eye tracking; Image segmentation; Mammals; Stereo image processing; Surgical equipment; Virtual reality; 3D mouse; Eye trackers; Eye-gaze; Full control; Haptics; Laparoscopic surgery; Laparoscopic surgery simulation; Simulation environment; Surgery simulations; Surgical tools; Laparoscopy,Conference paper,Final,,Scopus,2-s2.0-85062210153,Gaming / VR
John B.; Raiturkar P.; Le Meur O.; Jain E.,"John, Brendan (57205639875); Raiturkar, Pallavi (57193240475); Le Meur, Olivier (8611330900); Jain, Eakta (36715118000)",57205639875; 57193240475; 8611330900; 36715118000,A benchmark of four methods for generating 360° saliency maps from eye tracking data,2018,"Proceedings - 2018 IEEE International Conference on Artificial Intelligence and Virtual Reality, AIVR 2018",,,8613647,136,139,3.0,3,10.1109/AIVR.2018.00028,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85062190222&doi=10.1109%2fAIVR.2018.00028&partnerID=40&md5=acfb303c55758dcb3a44249322e8eb6a,"Modeling and visualization of user attention in Virtual Reality is important for many applications, such as gaze prediction, robotics, retargeting, video compression, and rendering. Several methods have been proposed to model eye tracking data as saliency maps. We benchmark the performance of four such methods for 360° images. We provide a comprehensive analysis and implementations of these methods to assist researchers and practitioners. Finally, we make recommendations based on our benchmark analyses and the ease of implementation. © 2018 IEEE.",360 images; Eye movements in VR; Saliency; Visualization,Benchmarking; Data visualization; Eye movements; Eye tracking; Image compression; Virtual reality; 360 image; Comprehensive analysis; Eye movement in VR; Eye-tracking; Performance; Saliency; Saliency map; Tracking data; User attention; Video rendering; Visualization,Conference paper,Final,All Open Access; Green Open Access,Scopus,2-s2.0-85062190222,Gaming / VR
Rubo M.; Gamer M.,"Rubo, Marius (57196299847); Gamer, Matthias (8979946100)",57196299847; 8979946100,Tracing gaze-following behavior in virtual reality using Wiener-Granger Causality,2018,Eye Tracking Research and Applications Symposium (ETRA),,,a86,,,,0,10.1145/3204493.3208332,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85049667934&doi=10.1145%2f3204493.3208332&partnerID=40&md5=bf242feca88ef6cef6760b14c4e121ca,"We modelled gaze following behavior in a naturalistic virtual reality environment using Wiener-Granger causality. Using this method, gaze following was statistically tangible throughout the experiment, but could not easily be pinpointed to precise moments in time. © 2018 Copyright held by the owner/author(s).",Gaze following; Social attention; Virtual reality; Wiener-Granger Causality,Statistical tests; Virtual reality; Gaze following; Granger Causality; Social attention; Virtual-reality environment; Eye tracking,Conference paper,Final,,Scopus,2-s2.0-85049667934,Gaming / VR
Rubo M.; Gamer M.,"Rubo, Marius (57196299847); Gamer, Matthias (8979946100)",57196299847; 8979946100,Virtual reality as a proxy for real-life social attention?,2018,Eye Tracking Research and Applications Symposium (ETRA),,,a81,,,,9,10.1145/3204493.3207411,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85049696814&doi=10.1145%2f3204493.3207411&partnerID=40&md5=b936a44402a1adf57e3d01a8abd937c8,"Previous studies found large amounts of overt attention allocated towards human faces when they were presented as images or videos, but a relative avoidance of gaze at conspecifics’ faces in real-world situations. We measured gaze behavior in a complex virtual scenario in which a human face and an object were similarily exposed to the participants’ view. Gaze at the face was avoided compared to gaze at the object, providing support for the hypothesis that virtual reality scenarios are capable of eliciting modes of information processing comparable to real-world situations. © 2018 Copyright held by the owner/author(s).",Cognitive ethology; Gaze avoidance; Social attention; Virtual reality,Biology; Virtual reality; Cognitive ethology; Gaze avoidance; Gaze behavior; Large amounts; Overt attention; Real world situations; Social attention; Virtual scenario; Eye tracking,Conference paper,Final,,Scopus,2-s2.0-85049696814,Gaming / VR
,,,Eye Tracking Research and Applications Symposium (ETRA),2018,Eye Tracking Research and Applications Symposium (ETRA),Part F137344,,,,,574.0,0,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85049686500&partnerID=40&md5=99781783dcf10a2707ad1f46ea08b27f,"The proceedings contain 106 papers. The topics discussed include: an investigation of the effects of n-gram length in scanpath analysis for eye-tracking research; evaluating gender difference on algorithmic problems using eye-tracker; how many words is a picture worth? attention allocation on thumbnails versus title text regions; cross-subject workload classification using pupil-related measures; correlation between gaze and hovers during decision-making interaction; supervised descent method (SDM) applied to accurate pupil detection in off-the-shelf eye tracking systems; CBF: circular binary features for robust and real-time pupil center detection; a novel approach to single camera, glint-free 3D eye model fitting including corneal refraction; SMOOTH-I: smart re-calibration using smooth pursuit eye movements; comparison of mapping algorithms for implicit calibration using probable fixation targets; revisiting data normalization for appearance-based gaze estimation; leveraging eye-gaze and time-series features to predict user interests and build a recommendation model for visual analysis; gaze typing in virtual reality: impact of keyboard design, selection method, and motion; the eye of the Typer: a benchmark and analysis of gaze behavior during typing; and predicting the gaze depth in head-mounted displays using multiple feature regression.",,,Conference review,Final,,Scopus,2-s2.0-85049686500,Gaming / VR
Blattgerste J.; Renner P.; Pfeiffer T.,"Blattgerste, Jonas (57195037495); Renner, Patrick (56145227600); Pfeiffer, Thies (14027435500)",57195037495; 56145227600; 14027435500,Advantages of eye-gaze over head-gaze-based selection in virtual and augmented reality under varying field of views,2018,Proceedings - COGAIN 2018: Communication by Gaze Interaction,,,a1,,,,150,10.1145/3206343.3206349,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85050071352&doi=10.1145%2f3206343.3206349&partnerID=40&md5=3fb4a670b09a8e33e83148567f10dbab,"The current best practice for hands-free selection using Virtual and Augmented Reality (VR/AR) head-mounted displays is to use head-gaze for aiming and dwell-time or clicking for triggering the selection. There is an observable trend for new VR and AR devices to come with integrated eye-tracking units to improve rendering, to provide means for attention analysis or for social interactions. Eyegaze has been successfully used for human-computer interaction in other domains, primarily on desktop computers. In VR/AR systems, aiming via eye-gaze could be significantly faster and less exhausting than via head-gaze. To evaluate benefits of eye-gaze-based interaction methods in VR and AR, we compared aiming via head-gaze and aiming via eyegaze. We show that eye-gaze outperforms head-gaze in terms of speed, task load, required head movement and user preference. We furthermore show that the advantages of eye-gaze further increase with larger FOV sizes.",Assistance systems; Augmented reality; Eye-tracking; Field of view; Head-mounted displays; Human computer interaction.; Virtual reality,Augmented reality; Eye movements; Helmet mounted displays; Human computer interaction; Personal computers; Stereo vision; Virtual reality; Assistance system; Best practices; Dwell time; Field of views; Head mounted displays; Head movements; Social interactions; Virtual and augmented reality; Eye tracking,Conference paper,Final,All Open Access; Green Open Access,Scopus,2-s2.0-85050071352,Gaming / VR
Köster M.; Itakura S.; Yovsi R.; Kärtner J.,"Köster, Moritz (56549610800); Itakura, Shoji (7006345646); Yovsi, Relindis (7801621488); Kärtner, Joscha (23392384500)",56549610800; 7006345646; 7801621488; 23392384500,Visual attention in 5-year-olds from three different cultures,2018,PLoS ONE,13,7,e0200239,,,,22,10.1371/journal.pone.0200239,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85050017708&doi=10.1371%2fjournal.pone.0200239&partnerID=40&md5=6696a8339cca6d6543711dade45a2214,"Cognitive processes differ markedly between children from different cultures, with best evidence for attention to visual scenes and the activities of others. Children from urban Western cultures tend to focus on focal objects, whereas children from urban East-Asian cultures rather attend to contextual elements of a visual scene. Regarding the attention to others’ activities, children from subsistence-based farming communities often observe several activities simultaneously, while children from urban Western contexts focus on activities sequentially. Here we assessed 144 5-year-old children from three prototypical cultural contexts (urban Germany, rural Cameroon, urban Japan) to investigate variations in attention across a variety of tasks. Attention to the elements of a visual scene was assessed in an optical illusion task, in picture descriptions and an eye-tracking paradigm. Attention to and learning from others’ activities was assessed in a parallel action task and a rule-based game. Some tasks indicated higher context-sensitive attention in urban Japan, while other findings indicated higher context-sensitive attention in urban Germany. Levels of parallel attention and learning from others’ activities were lower in rural Cameroonian children compared to the urban samples. Across tasks, the visual attention measures were unrelated. These findings substantiate that culture has a profound influence on early cognitive development, already in the preschool years. Furthermore, they raise critical questions about the early origins of cultural specificities in attention and the generalizability of attention phenomena beyond specific tasks and populations. © 2018 Köster et al. This is an open access article distributed under the terms of the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.",,"Attention; Cameroon; Child Development; Child, Preschool; Cognition; Cross-Cultural Comparison; Culture; Female; Germany; Humans; Japan; Male; Rural Population; Urban Population; Visual Perception; Article; attention test; Cameroon; child; child development; cognitive development; controlled study; cultural factor; ethnic difference; eye tracking; female; Germany; higher context sensitive attention; human; human experiment; Japan; learning test; male; optical illusion task; parallel action task; picture description task; preschool child; psychomotor activity; rule based game; rural population; social learning; task performance; visual attention; attention; cognition; cultural anthropology; cultural factor; physiology; urban population; vision",Article,Final,All Open Access; Gold Open Access; Green Open Access,Scopus,2-s2.0-85050017708,Gaming / VR
Hu B.; Johnson-Bey I.; Sharma M.; Niebur E.,"Hu, Brian (56640331800); Johnson-Bey, Ishmael (57194454787); Sharma, Mansi (57203972764); Niebur, Ernst (7003990201)",56640331800; 57194454787; 57203972764; 7003990201,Head movements are correlated with other measures of visual attention at smaller spatial scales,2018,"2018 52nd Annual Conference on Information Sciences and Systems, CISS 2018",,,,1,6,5.0,0,10.1109/CISS.2018.8362264,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85048579441&doi=10.1109%2fCISS.2018.8362264&partnerID=40&md5=382b2e7e66f2a6d6cceb1f15573527a1,"Overt visual attention is traditionally studied by recording eye movements under head-fixed viewing conditions. However, during natural visual exploration, both head and eye movements can be used to redirect gaze to new points of interest. In order to better understand the role of head movements in this process, we recorded head movements while subjects explored a set of complex images from five different categories in a virtual reality environment. We found image-category specific differences in head movements, as quantified by the number and duration of head 'fixations' (periods of maintained head orientation) as well as the amplitude of head movements. We compared head fixations with several other behavioral measures of attentional selection and with a computational model of bottom-up saliency, using the same set of complex scenes in all experiments. Results show significant positive correlation between head fixations and all other measures of attentional deployment, suggesting that head movements are a readily measurable indicator of overt selective attention at a spatial scale exceeding that of eye movements. © 2018 IEEE.",,Behavioral research; Virtual reality; Behavioral measures; Bottom-up saliencies; Category specifics; Computational model; Overt visual attentions; Positive correlations; Selective attention; Virtual-reality environment; Eye movements,Conference paper,Final,,Scopus,2-s2.0-85048579441,Gaming / VR
Antunes J.; Santana P.,"Antunes, João (57215096436); Santana, Pedro (16022824300)",57215096436; 16022824300,A study on the use of eye tracking to adapt gameplay and procedural content generation in first-person shooter games,2018,Multimodal Technologies and Interaction,2,2,23,,,,27,10.3390/mti2020023,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85074341560&doi=10.3390%2fmti2020023&partnerID=40&md5=6ed7d6fa1e065c3e8a0d17d0ae6f0582,"This paper studies the use of eye tracking in a First-Person Shooter (FPS) game as a mechanism to: (1) control the attention of the player’s avatar according to the attention deployed by the player; and (2) guide the gameplay and game’s procedural content generation, accordingly. This results in a more natural use of eye tracking in comparison to a use in which the eye tracker directly substitutes control input devices, such as gamepads. The study was conducted on a custom endless runner FPS, Zombie Runner, using an affordable eye tracker. Evaluation sessions showed that the proposed use of eye tracking provides a more challenging and immersive experience to the player, when compared to its absence. However, a strong correlation between eye tracker calibration problems and player’s overall experience was found. This means that eye tracking technology still needs to evolve but also means that once technology gets mature enough players are expected to benefit greatly from the inclusion of eye tracking in their gaming experience. © 2018 by the authors. Licensee MDPI, Basel, Switzerland.",Computer games; Eye tracking; Gaze-oriented gameplay,,Article,Final,All Open Access; Gold Open Access; Green Open Access,Scopus,2-s2.0-85074341560,Gaming / VR
Raptis G.E.; Fidas C.; Avouris N.,"Raptis, George E. (57189313952); Fidas, Christos (6506412362); Avouris, Nikolaos (6603741790)",57189313952; 6506412362; 6603741790,Effects of mixed-reality on players’ behaviour and immersion in a cultural tourism game: A cognitive processing perspective,2018,International Journal of Human Computer Studies,114,,,69,79,10.0,109,10.1016/j.ijhcs.2018.02.003,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85043315152&doi=10.1016%2fj.ijhcs.2018.02.003&partnerID=40&md5=cc12140208c18e67a11fafa24f1ce3e8,"Mixed-reality environments introduce innovative human-computer interaction paradigms assisted by enhanced visual content presentation which require from end-users to perform excessive cognitive tasks related to visual attention, search, processing, and comprehension. In such visually enriched interaction realms, individual differences in perception and visual information processing might affect users’ behaviour and immersion, given that such effects are known to exist in conventional computer environments, like desktop or mobile. In an attempt to shed light on whether, how, and why such effects persist within mixed-reality contexts, we conducted a between-subjects eye-tracking study (N=73) in which users interacted within either a conventional or a mixed-reality technological context, and adopted an accredited cognitive style theory to interpret the derived results. Analysis of results yielded that mixed-reality interaction realms amplified the effects of human cognitive style towards game-specific interaction behaviour and visual behaviour. Findings further support the added value of incorporating human cognitive factors in both design and run-time, aiming to provide adaptive and personalised features to end-users within mixed-reality interaction contexts. Such practical implications are also discussed in this paper. © 2018 Elsevier Ltd",Cultural heritage; Cultural playful activities; Cultural tourism; Field dependence-independence; Game-specific behaviour; Human cognitive differences; Immersion; Mixed reality; Visual behaviour,Behavioral research; Eye tracking; Human computer interaction; Tourism; Cultural heritages; Cultural playful activities; Field dependence; Game-specific behaviour; Human cognitive differences; Immersion; Visual behaviour; Mixed reality,Article,Final,,Scopus,2-s2.0-85043315152,Gaming / VR
Rooney B.; Burke C.; Balint K.; O'leary T.; Parsons T.; Lee C.T.; Mantei C.,"Rooney, Brendan (55007994600); Burke, Colun (57191924985); Balint, Katalin (57189083580); O'leary, Tess (57220458072); Parsons, Thomas (57207899507); Lee, Chi Tak (57209475089); Mantei, Caroline (57201200460)",55007994600; 57191924985; 57189083580; 57220458072; 57207899507; 57209475089; 57201200460,"Virtual reality, presence and social cognition: The effect of eye-gaze and narrativity on character engagement",2018,"Proceedings of the 2017 23rd International Conference on Virtual Systems and Multimedia, VSMM 2017",2018-January,,,1,6,5.0,3,10.1109/VSMM.2017.8346272,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85049739112&doi=10.1109%2fVSMM.2017.8346272&partnerID=40&md5=06142f65878ffe8d54b458d0798af03c,"We manipulated the way in which viewers engaged with an interactive virtual reality, Coffee without Words. Participants sit over coffee in a virtual café, opposite the protagonist and wait for their bus to be repaired. Participants are instructed to engage with the experience as though it was real or as an artefact (to appraise its design). We also manipulated eye-gaze behaviour where half of the participants experience interactive and natural eye-contact with the character, while the other half experience no eye contact. We explore effects of manipulation on self-reported presence. Our findings demonstrate no significant effects of character's eye-gaze behaviour on participant's feelings of presence or their perception of time. However, we report here that eye-gaze behaviours associated with higher levels of social cognition towards the character. © 2017 IEEE.",Character Engagement; Eye-Gaze; Narrativity; Presence; Social Cognition; Virtual Reality,Character Engagement; Eye-gaze; Narrativity; Presence; Social cognition; Virtual reality,Conference paper,Final,,Scopus,2-s2.0-85049739112,Gaming / VR
Yan Y.; Yu C.; Ma X.; Huang S.; Iqbal H.; Shi Y.,"Yan, Yukang (57201449370); Yu, Chun (36555230900); Ma, Xiaojuan (56242668800); Huang, Shuai (57221153907); Iqbal, Hasan (58416447800); Shi, Yuanchun (7404964393)",57201449370; 36555230900; 56242668800; 57221153907; 58416447800; 7404964393,Eyes-free target acquisition in interaction space around the body for virtual reality,2018,Conference on Human Factors in Computing Systems - Proceedings,2018-April,,,,,,50,10.1145/3173574.3173616,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85046952997&doi=10.1145%2f3173574.3173616&partnerID=40&md5=29de1fae172bf7530fe3b7951574da51,"Eyes-free target acquisition is a basic and important human ability to interact with the surrounding physical world, relying on the sense of space and proprioception. In this research, we leverage this ability to improve interaction in virtual reality (VR), by allowing users to acquire a virtual object without looking at it. We expect this eyes-free approach can effectively reduce head movements and focus changes, so as to speed up the interaction and alleviate fatigue and VR sickness. We conduct three lab studies to progressively investigate the feasibility and usability of eyes-free target acquisition in VR. Results show that, compared with the eyes-engaged manner, the eyes-free approach is significantly faster, provides satisfying accuracy, and introduces less fatigue and sickness; Most participants (13/16) prefer this approach. We also measure the accuracy of motion control and evaluate subjective experience of users when acquiring targets at different locations around the body. Based on the results, we make suggestions on designing appropriate target layout and discuss several design issues for eyes-free target acquisition in VR. © 2018 ACM.",Eyes-free; Proprioception; Target acquisition; Virtual reality,Diseases; Fatigue of materials; Human engineering; Mergers and acquisitions; Sensory perception; User experience; Virtual reality; Design issues; Eyes-free; Head movements; Human abilities; Physical world; Subjective experiences; Target acquisition; Virtual objects; Eye movements,Conference paper,Final,,Scopus,2-s2.0-85046952997,Gaming / VR
Duan H.; Zhai G.; Min X.; Zhu Y.; Fang Y.; Yang X.,"Duan, Huiyu (57195265438); Zhai, Guangtao (15847120000); Min, Xiongkuo (56030205300); Zhu, Yucheng (57189598758); Fang, Yi (57204768019); Yang, Xiaokang (7406503333)",57195265438; 15847120000; 56030205300; 57189598758; 57204768019; 7406503333,Perceptual Quality Assessment of Omnidirectional Images,2018,Proceedings - IEEE International Symposium on Circuits and Systems,2018-May,,8351786,,,,146,10.1109/ISCAS.2018.8351786,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85057135505&doi=10.1109%2fISCAS.2018.8351786&partnerID=40&md5=4492329ad98cabc11da8963926e160dd,"Omnidirectional images and videos can provide immersive experience of real-world scenes in Virtual Reality (VR) environment. We present a perceptual omnidirectional image quality assessment (IQA) study in this paper since it is extremely important to provide a good quality of experience under the VR environment. We first establish an omnidirectional IQA (OIQA) database, which includes 16 source images and 320 distorted images degraded by 4 commonly encountered distortion types, namely JPEG compression, JPEG2000 compression, Gaussian blur and Gaussian noise. Then a subjective quality evaluation study is conducted on the OIQA database in the VR environment. Considering that humans can only see a part of the scene at one movement in the VR environment, visual attention becomes extremely important. Thus we also track head and eye movement data during the quality rating experiments. The original and distorted omnidirectional images, subjective quality ratings, and the head and eye movement data together constitute the OIQA database. State-of-the-art full-reference (FR) IQA measures are tested on the OIQA database, and some new observations different from traditional IQA are made. The OIQA database will be released to facilitate further research. © 2018 IEEE.",,Behavioral research; Database systems; Eye movements; Gaussian noise (electronic); Image compression; Quality of service; Virtual reality; Eye movement datum; JPEG2000 compression; Omnidirectional image; Perceptual quality; Quality of experience (QoE); State of the art; Subjective quality; Subjective quality ratings; Image quality,Conference paper,Final,All Open Access; Green Open Access,Scopus,2-s2.0-85057135505,Gaming / VR
Gredebäck G.; Lindskog M.; Juvrud J.C.; Green D.; Marciszko C.,"Gredebäck, Gustaf (6507731449); Lindskog, Marcus (54389567100); Juvrud, Joshua C. (56584633800); Green, Dorota (55212552100); Marciszko, Carin (56580424700)",6507731449; 54389567100; 56584633800; 55212552100; 56580424700,Action prediction allows hypothesis testing via internal forward models at 6 months of age,2018,Frontiers in Psychology,9,MAR,290,,,,33,10.3389/fpsyg.2018.00290,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85043448281&doi=10.3389%2ffpsyg.2018.00290&partnerID=40&md5=438c04da640bb79992e0deff6c3f6882,"We propose that action prediction provides a cornerstone in a learning process known as internal forward models. According to this suggestion infants' predictions (looking to the mouth of someone moving a spoon upward) will moments later be validated or proven false (spoon was in fact directed toward a bowl), information that is directly perceived as the distance between the predicted and actual goal. Using an individual difference approach we demonstrate that action prediction correlates with the tendency to react with surprise when social interactions are not acted out as expected (action evaluation). This association is demonstrated across tasks and in a large sample (n = 118) at 6 months of age. These results provide the first indication that infants might rely on internal forward models to structure their social world. Additional analysis, consistent with prior work and assumptions from embodied cognition, demonstrates that the latency of infants' action predictions correlate with the infant's own manual proficiency. © 2018 Gredebäck, Lindskog, Juvrud, Green and Marciszko.",Action; Eye tracking; Interaction; Internal model; Prediction; Pupil dilation,,Article,Final,All Open Access; Gold Open Access; Green Open Access,Scopus,2-s2.0-85043448281,Gaming / VR
Bossi F.; Gallucci M.; Ricciardelli P.,"Bossi, Francesco (56963595100); Gallucci, Marcello (24449026000); Ricciardelli, Paola (6602880233)",56963595100; 24449026000; 6602880233,How social exclusion modulates social information processing: A behavioural dissociation between facial expressions and gaze direction,2018,PLoS ONE,13,4,e0195100,,,,13,10.1371/journal.pone.0195100,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85044988163&doi=10.1371%2fjournal.pone.0195100&partnerID=40&md5=799fa4f374802571ab1c1ea7482dbdbe,"Social exclusion is a painful experience that is felt as a threat to the human need to belong and can lead to increased aggressive and anti-social behaviours, and results in emotional and cognitive numbness. Excluded individuals also seem to show an automatic tuning to positivity: they tend to increase their selective attention towards social acceptance signals. Despite these effects known in the literature, the consequences of social exclusion on social information processing still need to be explored in depth. The aim of this study was to investigate the effects of social exclusion on processing two features that are strictly bound in the appraisal of the meaning of facial expressions: gaze direction and emotional expression. In two experiments (N = 60, N = 45), participants were asked to identify gaze direction or emotional expressions from facial stimuli, in which both these features were manipulated. They performed these tasks in a four-block crossed design after being socially included or excluded using the Cyberball game. Participants' empathy and self-reported emotions were recorded using the Empathy Quotient (EQ) and PANAS questionnaires. The Need Threat Scale and three additional questions were also used as manipulation checks in the second experiment. In both experiments, excluded participants showed to be less accurate than included participants in gaze direction discrimination. Modulatory effects of direct gaze (Experiment 1) and sad expression (Experiment 2) on the effects of social exclusion were found on response times (RTs) in the emotion recognition task. Specific differences in the reaction to social exclusion between males and females were also found in Experiment 2: excluded male participants tended to be less accurate and faster than included male participants, while excluded females showed a more accurate and slower performance than included female participants. No influence of social exclusion on PANAS or EQ scores was found. Results are discussed in the context of the importance of identifying gaze direction in appraisal theories. © 2018 Bossi et al. This is an open access article distributed under the terms of the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.",,Adult; Emotions; Eye Movements; Facial Expression; Female; Humans; Male; Reaction Time; Social Distance; Social Perception; Young Adult; adult; article; empathy; facial expression; female; functional dissociation; gaze; human; human experiment; information processing; major clinical study; male; Positive and Negative Affect Schedule; questionnaire; response time; social exclusion; stimulus; emotion; eye movement; perception; physiology; reaction time; social distance; young adult,Article,Final,All Open Access; Gold Open Access; Green Open Access,Scopus,2-s2.0-85044988163,Gaming / VR
Iskander J.; Hossny M.; Nahavandi S.,"Iskander, Julie (57193686489); Hossny, Mohammed (23667683300); Nahavandi, Saeid (55992860000)",57193686489; 23667683300; 55992860000,A Review on Ocular Biomechanic Models for Assessing Visual Fatigue in Virtual Reality,2018,IEEE Access,6,,,19345,19361,16.0,70,10.1109/ACCESS.2018.2815663,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85044033636&doi=10.1109%2fACCESS.2018.2815663&partnerID=40&md5=aca8ecda5245be5acadeb794315783c0,"With the wide spread of affordable virtual reality headsets, virtual environments are rapidly changing the way humans interact with reality. Understanding the effects of virtual environments on the mental and cognitive state is essential. In addition, defining methods for measuring and assessing visual fatigue in virtual environments is still needed. While eye movements are tightly coupled to the mental state, analysis of eye movement can add insights for safer virtual environments. Biomechanical analysis has been used extensively in the analysis of human movement. Simulation of different scenarios such as injuries and surgeries provided insights and solutions to problems that were otherwise impossible. This includes understanding the effects of changing insertion points of muscle on range of motion or how muscle activation can affect the motion produced. Extending the use of biomechanical simulation analysis into eye movement can be used to deepen our understanding of how virtual environments affect our visual and mental capabilities. This paper presents a thorough review on ocular biomechanics and ocular models in literature. We start with a brief introduction on the anatomy of the eye and eye kinematics. In addition, properties of the extraocular muscles (EOM) are described and the difference between EOMs and skeletal muscle is highlighted. The challenges facing biomechanical simulation and analysis of eye movement are presented along with the role of ocular models in assessing visual fatigue. Furthermore, the compatibility of available biomechanical tools to analyze ocular movements is discussed. © 2013 IEEE.",Biomechanics; extraocular muscles; ocular; ocular motility; OpenSim; virtual environments; visual fatigue,Biological systems; Biomechanics; Biophysics; Fatigue of materials; Flow visualization; Muscle; Virtual reality; Biological system modeling; Extraocular muscles; Head; ocular; ocular motility; OpenSim; Retina; Visual fatigue; Eye movements,Article,Final,All Open Access; Gold Open Access; Green Open Access,Scopus,2-s2.0-85044033636,Gaming / VR
Kim H.; Shin J.E.; Hong Y.-J.; Shin Y.-B.; Shin Y.S.; Han K.; Kim J.-J.; Choi S.-H.,"Kim, Haena (57202061067); Shin, Jung Eun (7402724139); Hong, Yeon-Ju (57193901277); Shin, Yu-Bin (56404968500); Shin, Young Seok (57200983437); Han, Kiwan (16030803900); Kim, Jae-Jin (37106947600); Choi, Soo-Hee (56124204300)",57202061067; 7402724139; 57193901277; 56404968500; 57200983437; 16030803900; 37106947600; 56124204300,Aversive eye gaze during a speech in virtual environment in patients with social anxiety disorder,2018,Australian and New Zealand Journal of Psychiatry,52,3,,279,285,6.0,25,10.1177/0004867417714335,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85042779940&doi=10.1177%2f0004867417714335&partnerID=40&md5=80dedf3aa844f39bf13ed644be5fba30,"Objective: One of the main characteristics of social anxiety disorder is excessive fear of social evaluation. In such situations, anxiety can influence gaze behaviour. Thus, the current study adopted virtual reality to examine eye gaze pattern of social anxiety disorder patients while presenting different types of speeches. Methods: A total of 79 social anxiety disorder patients and 51 healthy controls presented prepared speeches on general topics and impromptu speeches on self-related topics to a virtual audience while their eye gaze was recorded. Their presentation performance was also evaluated. Results: Overall, social anxiety disorder patients showed less eye gaze towards the audience than healthy controls. Types of speech did not influence social anxiety disorder patients’ gaze allocation towards the audience. However, patients with social anxiety disorder showed significant correlations between the amount of eye gaze towards the audience while presenting self-related speeches and social anxiety cognitions. Conclusion: The current study confirms that eye gaze behaviour of social anxiety disorder patients is aversive and that their anxiety symptoms are more dependent on the nature of topic. © 2017, © The Royal Australian and New Zealand College of Psychiatrists 2017.",eye gaze; public speech; Social anxiety disorder; virtual environment,"Adult; Affect; Case-Control Studies; Eye Movement Measurements; Female; Fixation, Ocular; Humans; Male; Phobia, Social; Psychiatric Status Rating Scales; Speech; Virtual Reality; Young Adult; adult; Article; cognition; controlled study; female; gaze; human; major clinical study; male; mental patient; social phobia; speech; State Trait Anxiety Inventory; virtual reality; affect; case control study; eye fixation; oculography; psychological rating scale; psychology; social phobia; speech; virtual reality; young adult",Article,Final,,Scopus,2-s2.0-85042779940,Gaming / VR
Uttley J.; Simpson J.; Qasem H.,"Uttley, Jim (56031473200); Simpson, James (57201693518); Qasem, Hussain (57195260465)",56031473200; 57201693518; 57195260465,Eye-tracking in the real world: Insights about the urban environment,2018,Handbook of Research on Perception-Driven Approaches to Urban Assessment and Design,,,,368,395,27.0,23,10.4018/978-1-5225-3637-6.ch016,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85045792537&doi=10.4018%2f978-1-5225-3637-6.ch016&partnerID=40&md5=37cfe48f86483a3ef98080a57895d20b,"Visual behaviour provides an objective and measurable indication of cognitive processes and perceptions that may otherwise be difficult to assess. The development of eye-tracking technology has allowed the accurate and relatively convenient measurement of visual behaviour. Most research using this technology has been based in a laboratory setting. This is not without good reason, as eye-tracking 'in the wild'-in real, naturalistic, and outdoor settings-poses logistical and methodological difficulties. One particular limitation that afflicts eye-tracking research, including real-world eye-tracking, is the difficulty in directly attributing attention to what is being looked at. This chapter presents three case studies that illustrate the use of eye-tracking in real-world settings with attempts to overcome this limitation. The chapter concludes by discussing the future direction of eye-tracking research, including how to integrate it with multisensory experiences, its use in conjunction with virtual reality technology, and its implications for urban planning and environmental design. © 2018 by IGI Global. All rights reserved.",,,Book chapter,Final,,Scopus,2-s2.0-85045792537,Gaming / VR
McGrath D.S.; Meitner A.; Sears C.R.,"McGrath, Daniel S. (35503690100); Meitner, Amadeus (57200420367); Sears, Christopher R. (7005428510)",35503690100; 57200420367; 7005428510,The specificity of attentional biases by type of gambling: An eye-tracking study,2018,PLoS ONE,13,1,e0190614,,,,32,10.1371/journal.pone.0190614,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85041179703&doi=10.1371%2fjournal.pone.0190614&partnerID=40&md5=05f389aafeba4493f05fcc60e3d4efc8,"A growing body of research indicates that gamblers develop an attentional bias for gambling-related stimuli. Compared to research on substance use, however, few studies have examined attentional biases in gamblers using eye-gaze tracking, which has many advantages over other measures of attention. In addition, previous studies of attentional biases in gamblers have not directly matched type of gambler with personally-relevant gambling cues. The present study investigated the specificity of attentional biases for individual types of gambling using an eye-gaze tracking paradigm. Three groups of participants (poker players, video lottery terminal/slot machine players, and non-gambling controls) took part in one test session in which they viewed 25 sets of four images (poker, VLTs/slot machines, bingo, and board games). Participants’ eye fixations were recorded throughout each 8-second presentation of the four images. The results indicated that, as predicted, the two gambling groups preferentially attended to their primary form of gambling, whereas control participants attended to board games more than gambling images. The findings have clinical implications for the treatment of individuals with gambling disorder. Understanding the importance of personally-salient gambling cues will inform the development of effective attentional bias modification treatments for problem gamblers. © 2018 McGrath et al. This is an open access article distributed under the terms of the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.",,Adult; Attention; Case-Control Studies; Eye Movements; Gambling; Humans; Male; Young Adult; adult; Article; attentional bias; behavior disorder assessment; controlled study; eye fixation; eye tracking; gambling; gaze; human; human experiment; imagery; male; normal human; pathological gambling; Problem Gambling Severity Index; attention; case control study; eye movement; young adult,Article,Final,All Open Access; Gold Open Access; Green Open Access,Scopus,2-s2.0-85041179703,Gaming / VR
Streicher A.; Leidig S.; Roller W.,"Streicher, Alexander (37162259500); Leidig, Sebastian (57203852649); Roller, Wolfgang (6701455871)",37162259500; 57203852649; 6701455871,Eye-Tracking for User Attention Evaluation in Adaptive Serious Games,2018,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),11082 LNCS,,,583,586,3.0,7,10.1007/978-3-319-98572-5_50,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85053194833&doi=10.1007%2f978-3-319-98572-5_50&partnerID=40&md5=4a4a24f8c25ed257fb5fd09170316ab6,"The Ideal Path Score (IPS) developed in this work is able to improve adaptivity of serious games by more accurately estimating performance and need for help based on players’ interactions and eye movements. The automatic personalization of adaptive e-learning systems supports effective learning for users with varying levels of knowledge and skills. Particularly in games, indicators informing adaptivity, like attention and performance of the player, should be assessed non-invasively to avoid interrupting the player’s flow experience and to keep up the immersion. Passive sensors like eye tracking can solve this challenge. This paper presents the concept of the IPS and its integration in an adaptive serious game for image interpretation training. The realized IPS-adaptive game assesses performance and attention of players based on eye movements and interactions with the game. © 2018, Springer Nature Switzerland AG.",Adaptive games; Eye tracking; Ideal path; Serious games,Engineering education; Eye movements; Eye tracking; Adaptive e-learning systems; Adaptive games; Adaptive serious games; Effective learning; Flow experience; Ideal path; Image interpretation; Personalizations; Serious games,Conference paper,Final,,Scopus,2-s2.0-85053194833,Gaming / VR
Hou W.-J.; Chen K.-X.; Li H.; Zhou H.,"Hou, Wen-Jun (14627737100); Chen, Kai-Xiang (57199745070); Li, Hao (55203359800); Zhou, Hu (57203581963)",14627737100; 57199745070; 55203359800; 57203581963,User defined eye movement-based interaction for virtual reality,2018,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),10911 LNCS,,,18,30,12.0,7,10.1007/978-3-319-92141-9_2,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85050594451&doi=10.1007%2f978-3-319-92141-9_2&partnerID=40&md5=313310f7d9a91fb4ed6c23198240adda,"Most of the applications of eye movement-based interaction in VR are limited to blinking and gaze at present, however, gaze gestures were neglected. Therefore, the potential of eye movement-based interaction in VR is far from being realized. In addition, many scholars tried to define some special eye movements as input instructions, but these definitions are almost always empirical and neglect users’ habits and cultural background. In this paper, we focus on how Chinese users interact in VR using eye movements without relying on a graphical user interface. We present a guessability study focusing on intuitive eye movement-based interaction of common commands in 30 tasks of 3 categories in VR. A total of 360 eye movements were collected from 12 users and a consensus set of eye movements in VR that best met user’s cognition was obtained. This set can be applied to the design of eye movement-based interaction in VR to help designers to develop user-centered and intuitive eye movement-based interaction in VR. Meanwhile this set can be migrated to other interactive media and user interfaces, such as a Post-WIMP interface base on eye movement-based interaction, as a reference to design. © Springer International Publishing AG, part of Springer Nature 2018.",Eye movement-based interaction; Gaze gesture; Guessability; Intuitive interaction; Virtual reality,Graphical user interfaces; User centered design; Virtual reality; Cultural backgrounds; Gaze gesture; Guessability; Interactive media; Intuitive interaction; Movement-based interactions; Post-wimp interfaces; User-centered; Eye movements,Conference paper,Final,,Scopus,2-s2.0-85050594451,Gaming / VR
Ahonniska-Assa J.; Polack O.; Saraf E.; Wine J.; Silberg T.; Nissenkorn A.; Ben-Zeev B.,"Ahonniska-Assa, Jaana (6603279968); Polack, Orli (54279289600); Saraf, Einat (15058199400); Wine, Judy (24395646200); Silberg, Tamar (55848704700); Nissenkorn, Andreea (6602161119); Ben-Zeev, Bruria (6603893677)",6603279968; 54279289600; 15058199400; 24395646200; 55848704700; 6602161119; 6603893677,Assessing cognitive functioning in females with Rett syndrome by eye-tracking methodology,2018,European Journal of Paediatric Neurology,22,1,,39,45,6.0,47,10.1016/j.ejpn.2017.09.010,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85033498554&doi=10.1016%2fj.ejpn.2017.09.010&partnerID=40&md5=f172050fd2d50ce7673a4c9003009dc6,"Background While many individuals with severe developmental impairments learn to communicate with augmentative and alternative communication (AAC) devices, a significant number of individuals show major difficulties in the effective use of AAC. Recent technological innovations, i.e., eye-tracking technology (ETT), aim to improve the transparency of communication and may also enable a more valid cognitive assessment. Objectives To investigate whether ETT in forced-choice tasks can enable children with very severe motor and speech impairments to respond consistently, allowing a more reliable evaluation of their language comprehension. Methods Participants were 17 girls with Rett syndrome (M = 6:06 years). Their ability to respond by eye gaze was first practiced with computer games using ETT. Afterwards, their receptive vocabulary was assessed using the Peabody Picture Vocabulary Test-4 (PPVT-4). Target words were orally presented and participants responded by focusing their eyes on the preferred picture. Results Remarkable differences between the participants in receptive vocabulary were demonstrated using ETT. The verbal comprehension abilities of 32% of the participants ranged from low-average to mild cognitive impairment, and the other 68% of the participants showed moderate to severe impairment. Young age at the time of assessment was positively correlated with higher receptive vocabulary. Conclusions The use of ETT seems to make the communicational signals of children with severe motor and communication impairments more easily understood. Early practice of ETT may improve the quality of communication and enable more reliable conclusions in learning and assessment sessions. © 2017 European Paediatric Neurology Society",Augmentative and alternative communication (AAC); Cognitive assessment; Eye-gaze technology; Eye-tracking; Rett syndrome,"Child; Child, Preschool; Cognition; Comprehension; Eye Movements; Female; Humans; Neurologic Examination; Rett Syndrome; Vocabulary; anticonvulsive agent; Article; calibration; child; clinical article; cognition assessment; controlled study; eye tracking; female; forced choice method; gaze; human; language; language ability; language test; mild cognitive impairment; motor dysfunction; Peabody picture vocabulary test; preschool child; priority journal; Rett syndrome; school child; speech disorder; video game; visual feedback; cognition; comprehension; eye movement; neurologic examination; physiology; procedures; vocabulary",Article,Final,,Scopus,2-s2.0-85033498554,Gaming / VR
He X.; Liu Z.,"He, Xuanchao (57203137505); Liu, Zhejun (56242452300)",57203137505; 56242452300,A novel way of estimating a user’s focus of attention in a virtual environment,2018,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),10909 LNCS,,,71,81,10.0,2,10.1007/978-3-319-91581-4_6,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85050606843&doi=10.1007%2f978-3-319-91581-4_6&partnerID=40&md5=f28f9426196c0b77ba60e398bb86d857,"Results from prior experiments suggested that measuring immersion objectively (using eye trackers) can be a very important supplement to subjective tests (with questionnaires). But, traditional eye trackers are not usable together with VR HMDs (Head Mounted Displays) because they cannot “see” an audience’s eyes occluded by helmets. The eye trackers compatible with HMDs are not easily accessible to students, researchers and developers in small studios because of the high prices. This paper explores a novel way of estimating a user’s focus of attention in a virtual environment. An experiment measuring the relationship between subject’s head movement and eyesight was conducted to investigate whether eye movement can be closely approximated by head rotation. The findings suggested that people’s eyesight tended to remain in the central area of the HMD when playing a VR game and the HMD orientation data was very close to the eyesight direction. And therefore, this novel way that employs no other equipment than HMDs themselves can hopefully be used to estimate a user’s focus of attention in a much more economic and convenient manner. © Springer International Publishing AG, part of Springer Nature 2018.",Evaluation; Focus of attention; Head movement; HMD; VR games,Eye tracking; Helmet mounted displays; Mixed reality; Surveys; Evaluation; Eye trackers; Focus of Attention; Head mounted displays; Head movements; Head rotation; Orientation data; VR games; Eye movements,Conference paper,Final,,Scopus,2-s2.0-85050606843,Gaming / VR
Wu Z.,"Wu, Zhendong (55500650300)",55500650300,Empirical study on the optimization strategy of subject metro design based on virtual reality,2018,Informatica (Slovenia),42,3,,467,475,8.0,3,10.31449/inf.v42i3.2424,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85056282096&doi=10.31449%2finf.v42i3.2424&partnerID=40&md5=38034e833da120d0183ff5542b0db1f3,"A three-dimensional simulation interactive virtual scene was established taking the theme subway in Chengdu and Guangzhou as the typical case, and the standard metro in Xiamen as the reference. An experiment was designed using virtual reality built-in eye movement equipment and following the principle of visual attentiveness. The conscious and unconscious visual behaviors of users were analyzed and the impacts of different design methods on user experience and behaviors were analyzed. This study extracted the key elements of the theme subway design and recombine them to compared the design of facilities at the same position but in different themes and the design of space interface at different positions but in the same theme. Moreover, optimization strategies were put forward for the design of theme subway space to enhance the availability of the design. © 2018 Slovene Society Informatika. All rights reserved.",Design strategy; Theme subway; Virtual reality eye movement; Visual attention,Behavioral research; Design; Eye movements; Railroads; Subways; Virtual reality; Design strategies; Empirical studies; Optimization strategy; Three dimensional simulations; User experience; Virtual scenes; Visual Attention; Visual behavior; Availability,Article,Final,All Open Access; Gold Open Access; Green Open Access,Scopus,2-s2.0-85056282096,Gaming / VR
Abbaszadegan M.; Yaghoubi S.; MacKenzie I.S.,"Abbaszadegan, Mahdieh (57205876178); Yaghoubi, Sohrab (57205876383); MacKenzie, I. Scott (7202956135)",57205876178; 57205876383; 7202956135,"TrackMaze: A comparison of head-tracking, eye-tracking, and tilt as input methods for mobile games",2018,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),10903 LNCS,,,393,405,12.0,17,10.1007/978-3-319-91250-9_31,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85061555568&doi=10.1007%2f978-3-319-91250-9_31&partnerID=40&md5=5f959857834503280e72b9857133c724,"A user study was performed to compare three input methods (tilt, eye-tracking, head-tracking) with two gain levels (low, high) on a custom-made TrackMaze mobile game. The task involved maneuvering a virtual ball through a maze while trying to avoid walls. The game was developed in Swift using the ARKit framework. The TrueDepth front-facing camera of an Apple iPhone X was used for the eye-tracking and head-tracking conditions. We evaluated user performance (maze completion time, number of wall hits) and qualitative measures (ease of use, enjoyment, fatigue). Tilt input showed the best performance and eye-tracking showed the worst performance. The mean maze completion time was 12.3 s for tilt, 22.5 s for head-tracking, and 31.8 s for eye-tracking. High gain was 26% faster than low gain. Tilt was the most precise input method with only 1.06 wall hits per trial, compared to head-tracking (2.30) and eye-tracking (4.00). Participants preferred tilt and head-tracking over eye-tracking and noted that the eye-tracking interface was fatiguing and hard to use. © Springer International Publishing AG, part of Springer Nature 2018.",ARKit; Augmented reality on mobile devices; Eye-tracking; HCI; Head-tracking; iOS; Mobile games; Tilt-input,Augmented reality; Human computer interaction; ARKit; Completion time; Ease-of-use; Head tracking; Input methods; Mobile games; Tilt input; User performance; Eye tracking,Conference paper,Final,,Scopus,2-s2.0-85061555568,Gaming / VR
Gorbet D.J.; Sergio L.E.,"Gorbet, Diana J. (6603962513); Sergio, Lauren E. (6602173207)",6603962513; 6602173207,"Move faster, think later: Women who play action video games have quicker visually-guided responses with later onset visuomotor-related brain activity",2018,PLoS ONE,13,1,e0189110,,,,25,10.1371/journal.pone.0189110,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85041108836&doi=10.1371%2fjournal.pone.0189110&partnerID=40&md5=468a63b28f2e53b3414b5a2e610d8ccb,"A history of action video game (AVG) playing is associated with improvements in several visuospatial and attention-related skills and these improvements may be transferable to unrelated tasks. These facts make video games a potential medium for skill-training and rehabilitation. However, examinations of the neural correlates underlying these observations are almost non-existent in the visuomotor system. Further, the vast majority of studies on the effects of a history of AVG play have been done using almost exclusively male participants. Therefore, to begin to fill these gaps in the literature, we present findings from two experiments. In the first, we use functional MRI to examine brain activity in experienced, female AVG players during visually-guided reaching. In the second, we examine the kinematics of visually-guided reaching in this population. Imaging data demonstrate that relative to women who do not play, AVG players have less motor-related preparatory activity in the cuneus, middle occipital gyrus, and cerebellum. This decrease is correlated with estimates of time spent playing. Further, these correlations are strongest during the performance of a visuomotor mapping that spatially dissociates eye and arm movements. However, further examinations of the full time-course of visuomotor-related activity in the AVG players revealed that the decreased activity during motor preparation likely results from a later onset of activity in AVG players, which occurs closer to beginning motor execution relative to the non-playing group. Further, the data presented here suggest that this later onset of preparatory activity represents greater neural efficiency that is associated with faster visually-guided responses. © 2018 Gorbet, Sergio. This is an open access article distributed under the terms of the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.",,"Adult; Brain; Female; Humans; Magnetic Resonance Imaging; Male; Psychomotor Performance; Reaction Time; Video Games; Vision, Ocular; adult; arm movement; article; brain function; cerebellum; cuneus; dissociation; eye movement; female; functional magnetic resonance imaging; human; human experiment; kinematics; male; middle occipital gyrus; video game; brain; diagnostic imaging; nuclear magnetic resonance imaging; physiology; psychomotor performance; reaction time; vision",Article,Final,All Open Access; Gold Open Access; Green Open Access,Scopus,2-s2.0-85041108836,Gaming / VR
Belousov L.S.; Napalkov D.A.; Zhigulskaja D.D.; Pushing N.L.; Velichkovsky B.M.,"Belousov, L.S. (57198303575); Napalkov, D.A. (58631627200); Zhigulskaja, D.D. (57206904893); Pushing, N.L. (57206898910); Velichkovsky, B.M. (57216017177)",57198303575; 58631627200; 57206904893; 57206898910; 57216017177,Cognitive research and new technologies in sport,2018,Voprosy Psikhologii,2018-January,5,,117,135,18.0,1,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85062080751&partnerID=40&md5=1ffe56cdc073249b25f59404c4975b68,"In this overview article the authors consider practical perspectives and still unresolved problem on the way towards application in sport of three new technologies stemmed from contemporary psychological and neurophysiological research. The first of them is the technology of eye-tracking (i.e. registration of eye movements), which is increasingly used for controlling and correcting attention and visual perception in sportsmen and their trainers. The second technology consists in creating artificial -virtual- environments for training as well as for competitions in such emerging fields as cyberpsychology and e-Sport. Finally, the third technology is about to realize a neurobiocontrol of sportsmen's functional state on the base of methods for analysis of brain electrophysiological activity. As these three technologies are completely non-invasive they are all-around accepted alternatives to the use of chemical substances that may well improve human resilience and complex skill learning in sport but are prohibited by national and international antidoping laws. Still unresolved problems of their application are analyzed. Two such problems are, first, the necessity of interrupting the continuous activity in eye-tracking to recalibrate the measurement instruments and, second, an underestimation of spatial depth as well as insufficient supply of sensors and actuator; for producing somatosensory and vestibular sensations in the virtual reality systems. In the field of neurobiocontrol, the paradox behavior of the EEG alpha rhythm in shouting is discussed: the rhythm is suppressed during aiming in beginners but is disinhibited in the sportsmen of highest qualification. The authors tentatively explain this phenomenon and argue for a strictly individual approach in using these high-tech instruments. © 2018 Russian Academy of Sciences. All rights reserved.",Attention; Biofeedback; Electroencephalography (EEG); Emotions; Eye-tracking; Individual approach; K1NECT; Neurobiofeedback; Resilience; Serious games; Shouting; Sport; Virtual reality,,Article,Final,,Scopus,2-s2.0-85062080751,Gaming / VR
Aljojo N.; Munshi A.; Almukadi W.; Hossain A.; Omar N.; Aqel B.; Almhuemli S.; Asirri F.; Alshamasi A.,"Aljojo, Nahla (56150970100); Munshi, Asmaa (57208148897); Almukadi, Wfaa (58457295100); Hossain, Anhar (57207945402); Omar, Noran (57207944702); Aqel, Bashair (57207953465); Almhuemli, Shahad (57207950894); Asirri, Fatma (57207944160); Alshamasi, Areej (57191410440)",56150970100; 57208148897; 58457295100; 57207945402; 57207944702; 57207953465; 57207950894; 57207944160; 57191410440,Arabic Alphabetic puzzle game using eye tracking and chatbot for dyslexia,2018,International Journal of Interactive Mobile Technologies,12,5,,58,80,22.0,16,10.3991/ijim.v12i5.8957,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85063325976&doi=10.3991%2fijim.v12i5.8957&partnerID=40&md5=88c0a723458866da0551fe0f6948a348,"Dyslexia is a problem that an individual has since birth; it poses difficulties for the rest of their life, similar to other learning disabilities. Spelling, writing, reading and in certain instances speech can all be undermined by the language processing disorder. Idleness or a lack of intellect are not associated with dyslexia. Furthermore, sight problems do not cause dyslexia. Rather, data is evaluated and understood by the brain in alternative ways, making dyslexia a neurological ailment afflicting both children and grown-ups. This paper seeks to devise a puzzle game application based on eye tracking, which will assist with focusing attention, as well as a chatbot that can motivate users. This should prove beneficial to individuals with dyslexia, parents of dyslexics, or experts such as reading professionals, instructors and teachers who are assisting dyslexics. Based on the best current understanding of how to assist dyslexics, we meticulously assessed every application prior to its inclusion. © 2018, International Association of Online Engineering.",Arabic Alphabetic; Chatbot; Dyslexia; Eye tracking; Learning disabilities; Puzzle game,,Article,Final,All Open Access; Gold Open Access,Scopus,2-s2.0-85063325976,Gaming / VR
Kiran R.; Salehi S.; Jeon J.; Kang Z.,"Kiran, Raj (58694362200); Salehi, Saeed (59266592100); Jeon, Jiwon (57191577718); Kang, Ziho (56486906100)",58694362200; 59266592100; 57191577718; 56486906100,Real-time eye-tracking system to evaluate and enhance situation awareness and process safety in drilling operations,2018,"Society of Petroleum Engineers - IADC/SPE Drilling Conference and Exhibition, DC 2018",2018-March,,,,,,8,10.2118/189678-ms,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85048576573&doi=10.2118%2f189678-ms&partnerID=40&md5=e148d6aabfda3ee3164cdd447b4f1361,"Lack of situational awareness in drilling operations has become an important factor in causing safety accidents or cause of non-productive time. Process safety is another critical aspect for such high risk operations and cannot be ignored. A review of testimony from offshore rigs worker suggests long working shifts and fatigue as one of the crucial issues impacting performance. The study here presents results from experiments conducted in a Virtual Reality Drilling Simulator (VRDS) equipped with eye tracking technology. The eye tracking technology can be used to distinguish between less and more aware/alert participants which is most of the times related to fatigue or onsite distraction. The ocular activity can be used to obtain visual cues that can quantify the state of drilling operator while operating. These cues can help in generating some warning alarms to alert the driller. Therefore, the system can reduce not only accidents but can also save a tremendous amount of non-productive time by improving their efficiency. Since human eyes express the most direct reaction during less alert or distracted mental state, ocular activity data has been used as the basis for quantification of situation awareness by researchers. In recent years, eye tracking system has been developed in the form of static and dynamic devices. The camera installed in these devices capture different characteristics of the participants' ocular movement in real-time. These oculomotor data such as eye fixation count and duration and pupil size, has been implemented in several industries such as aviation and medical to assess the performance of participants in recent years. In this paper, we use eye tracking techniques for investigation of alertness and awareness of participants while conducting different drilling operations on the simulator. Results obtained from this study indicate that the system can detect the distraction and alertness exhibited by humans. The results show the very promising application of this technology on the drilling rigs. The novelty in this work is the development of framework for implementation of real-time eye tracking technology in various drilling operations such as drilling rigs and Real Time Operation Centers (RTOCs). © 2018, IADC/SPE Drilling Conference and Exhibition.",,Accidents; Drilling platforms; Drilling rigs; Infill drilling; Offshore oil well production; Safety factor; Virtual reality; Drilling simulators; Eye tracking systems; Eye tracking technologies; High-risk operations; Real-time eye tracking; Real-time operation; Situation awareness; Situational awareness; Eye tracking,Conference paper,Final,,Scopus,2-s2.0-85048576573,Gaming / VR
Korhonen V.; Räty H.; Kärnä E.,"Korhonen, Vesa (56214945200); Räty, Hannu (6603820879); Kärnä, Eija (36185272600)",56214945200; 6603820879; 36185272600,High support need and minimally verbal children with autism playing a preference based computer game: A pilot eye-tracking study of four individual’s attendance to eyes,2018,International Journal of Special Education,33,1,,212,228,16.0,3,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85066271813&partnerID=40&md5=fb6c85ea87b2ccf67647b5224fd86731,"Individuals with autism often exhibit atypical levels of attention to eyes. High support need and minimally verbal individuals with autism have typically received less attention in research. This study explored a preference based computer game to include the less-studied individuals with autism in their own school environment. Four high support need and minimally verbal children with autism played a familiar computer game where correct decisions were contingent on attending to the eyes of a virtual character. Case control analyses were used to compare individual’s results to a control group. The analyses revealed that one child spent less time looking at the eyes than did the controls, and two children did not differ from the controls. There was no usable data for the fourth child. Our results suggest that high support need and minimally verbal children can be included in eye tracking research by using familiar positive environments. © 2018, International Journal of Special Education. All rights reserved.",Autism; Case- control method; Eye-tracking; High support need; Visual perspective taking,,Article,Final,,Scopus,2-s2.0-85066271813,Gaming / VR
Othlinghaus-Wulhorst J.; Jedich A.; Hoppe H.U.; Harrer A.,"Othlinghaus-Wulhorst, Julia (55377341300); Jedich, Anna (57203839019); Hoppe, H. Ulrich (34769829400); Harrer, Andreas (6602832111)",55377341300; 57203839019; 34769829400; 6602832111,Using eye-tracking to analyze collaboration in a virtual role play environment,2018,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),11001 LNCS,,,185,197,12.0,2,10.1007/978-3-319-99504-5_15,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85053161226&doi=10.1007%2f978-3-319-99504-5_15&partnerID=40&md5=f636736d3f49649cb12eaa20400ba5c1,"The ColCoMa environment supports the training of workplace-oriented conflict management strategies through virtual role play. The role play relies on a web-based environment in which the participants interact through chat dialogues. Two of the participants (the parties in conflict) are human actors whereas the third role (“mediator”) is occupied by a chatbot. Our study aims at exploring the potential of eye-tracking analyses to assess the quality of cooperation in this situation. The standard assumption is that a certain “convergence” of the visual foci of attention between cooperation partners indicates better coordination and consideration of the other party. In our scenario, this assumption has to be refined by taking into account the different roles (including the role of the chatbot) and the distribution of utterances on the chat history. The eye-tracking parameters are compared to quality criteria such as successful completion of the game or richness/mutuality of the chat interactions. There are quite strong correlations on the aggregate level (taking overall eye-tracking convergence as a global parameter), yet not in terms of synchronicity between convergent eye-tracking and chat interaction. This is possibly due to the specific distribution of roles in our virtual training environment. © Springer Nature Switzerland AG 2018.",Collaboration; Eye-tracking; Role play,Artificial intelligence; Computer science; Computers; Collaboration; Conflict management; Eye-tracking analysis; Role play; Specific distribution; Standard assumptions; Virtual training environments; Web-based environment; Eye tracking,Conference paper,Final,,Scopus,2-s2.0-85053161226,Gaming / VR
Harteis C.; Fischer C.; Töniges T.; Wrede B.,"Harteis, Christian (6507325151); Fischer, Christoph (57201970406); Töniges, Torben (56741455000); Wrede, Britta (8297851900)",6507325151; 57201970406; 56741455000; 8297851900,"Do we betray errors beforehand? The use of eye tracking, automated face recognition and computer algorithms to analyse learning from errors",2018,Frontline Learning Research,6,3,,37,56,19.0,5,10.14786/flr.v6i3.370,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85061989342&doi=10.14786%2fflr.v6i3.370&partnerID=40&md5=9ba58466bf3875d62477c203a36e211b,"Preventing humans from committing errors is a crucial aspect of man-machine interaction and systems of computer assistance. It is a basic implication that those systems need to recognise errors before they occur. This paper reports an exploratory study that utilises eye-tracking technology and automated face recognition in order to analyse test persons’ emotional reactions and cognitive load during a computer game and learning through trial and error. Computer algorithms based on machine learning and big data were tested that identify particular patterns of test persons’ gaze behaviour and facial expressions that antecede errors in a computer game. The results show that emotions and learning from errors are positively correlated and that gaze behaviour and facial expressions inform about the errors that follow. However, the algorithms still need to be improved through further studies to be suitable for daily use. This research is innovative in its use of mathematical formulae to operationalise learning through errors and the use of computer algorithms to predict errors in human behaviour in trial-and-error situations. © 2018, European Association for Research on Learning and Instruction. All rights reserved.",Emotions; Eye tracking; Face recognition; Learning from errors,,Article,Final,All Open Access; Gold Open Access; Green Open Access,Scopus,2-s2.0-85061989342,Gaming / VR
Moleta T.; Wang B.; Schnabel M.A.,"Moleta, Tane (56039524400); Wang, Brandon (57209660217); Schnabel, Marc Aurel (44861619400)",56039524400; 57209660217; 44861619400,The Virtual Mirror Cognitive Loads in VR and VR Visualisations,2018,Proceedings of the International Conference on Education and Research in Computer Aided Architectural Design in Europe,2,,,815,822,7.0,5,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85088364760&partnerID=40&md5=6e8e2916f5206eddd499c70d38eb9302,"This paper begins to look at how human data can be collected via Virtual, Augmented and Mixed Reality alongside Eye Tracking data for design Verification. This paper presents preliminary testing and results from participants to demonstrate a data pipeline methodology and data processing to begin to understand and verify the impact of certain design elements have on ones cognitive experience.All testing and aims have been focused on basic design elements and how they may effect the experience of pathfinding and navigating through a conceptual design within an architectural practice situation. © 2018.",Cognitive Loads; Design Verification; Eye Tracking; Virtual Reality,,Conference paper,Final,,Scopus,2-s2.0-85088364760,Gaming / VR
Lee G.I.; Lee M.R.,"Lee, Gyusung I. (16022478000); Lee, Mija R. (56126544100)",16022478000; 56126544100,Can a virtual reality surgical simulation training provide a self-driven and mentor-free skills learning? Investigation of the practical influence of the performance metrics from the virtual reality robotic surgery simulator on the skill learning and associated cognitive workloads,2018,Surgical Endoscopy,32,1,,62,72,10.0,43,10.1007/s00464-017-5634-6,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85021127892&doi=10.1007%2fs00464-017-5634-6&partnerID=40&md5=c98aaa8ff23cc0ff7e7daeaf55a61487,"Background: While it is often claimed that virtual reality (VR) training system can offer self-directed and mentor-free skill learning using the system’s performance metrics (PM), no studies have yet provided evidence-based confirmation. This experimental study investigated what extent to which trainees achieved their self-learning with a current VR simulator and whether additional mentoring improved skill learning, skill transfer and cognitive workloads in robotic surgery simulation training. Methods: Thirty-two surgical trainees were randomly assigned to either the Control-Group (CG) or Experiment-Group (EG). While the CG participants reviewed the PM at their discretion, the EG participants had explanations about PM and instructions on how to improve scores. Each subject completed a 5-week training using four simulation tasks. Pre- and post-training data were collected using both a simulator and robot. Peri-training data were collected after each session. Skill learning, time spent on PM (TPM), and cognitive workloads were compared between groups. Results: After the simulation training, CG showed substantially lower simulation task scores (82.9 ± 6.0) compared with EG (93.2 ± 4.8). Both groups demonstrated improved physical model tasks performance with the actual robot, but the EG had a greater improvement in two tasks. The EG exhibited lower global mental workload/distress, higher engagement, and a better understanding regarding using PM to improve performance. The EG’s TPM was initially long but substantially shortened as the group became familiar with PM. Conclusion: Our study demonstrated that the current VR simulator offered limited self-skill learning and additional mentoring still played an important role in improving the robotic surgery simulation training. © 2017, Springer Science+Business Media, LLC.",Mentoring; Performance metrics; Robotic surgery; Simulation; Training; Virtual reality,Adult; Clinical Competence; Cognition; Humans; Internship and Residency; Mentoring; Mentors; Robotic Surgical Procedures; Simulation Training; Surveys and Questionnaires; Virtual Reality; Workload; adult; Article; cognition; controlled study; human; learning; physical model; priority journal; resident; robot assisted surgery; simulation training; surgical training; task performance; virtual reality; workload; clinical competence; cognition; education; medical education; mentor; mentoring; procedures; questionnaire; robotic surgical procedure; simulation training; statistics and numerical data,Article,Final,,Scopus,2-s2.0-85021127892,Gaming / VR
Parsons T.D.; Courtney C.G.,"Parsons, Thomas D. (57207899507); Courtney, Christopher G. (24381620300)",57207899507; 24381620300,Interactions between Threat and Executive Control in a Virtual Reality Stroop Task,2018,IEEE Transactions on Affective Computing,9,1,,66,75,9.0,30,10.1109/TAFFC.2016.2569086,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85043287836&doi=10.1109%2fTAFFC.2016.2569086&partnerID=40&md5=0990f69bff2c8bbf5483a131b13ae23c,"Understanding the ways in which persons rapidly transfer attention between tasks while still retaining ability to perform these tasks is an important area of study. Everyday activities commonly come in the form of emotional distractors. A recently developed Virtual Reality Stroop Task (VRST) allows for assessing neurocognitive and psychophysiological responding while traveling through simulated safe and ambush desert environments as Stroop stimuli appear on the windshield. We evaluated differences in psychophysiological response patterns associated with completion of an affective task alone versus completion of an affective task that also included a Stroop task. The VRST elicited increased heart rate, respiration rate, skin conductance level, and number of spontaneous fluctuations in electrodermal activity. Increased cognitive workload was found to be associated with the more cognitively challenging Stroop conditions which led to an increase in response level. This expands on previous findings and indicates that allocating attention away from the environment and toward Stroop stimuli likely requires greater inhibitory control. This is corroborated by behavioral findings from previous investigations with the VRST. The VRST revealed that the increased difficulty found in tasks like the Stroop interference task directly evoke autonomic changes in psychophysiological arousal beyond the threatening stimuli themselves. © 2010-2012 IEEE.",affect recognition; Affective computing; arousal classification; psychology; Stroop task; virtual reality,Computer programming; Computer science; Affect recognition; Affective Computing; Arousal classifications; psychology; Stroop task; Virtual reality,Article,Final,,Scopus,2-s2.0-85043287836,Gaming / VR
Choi G.; Kim M.,"Choi, GyuHyeok (56367257600); Kim, Mijin (55686287400)",56367257600; 55686287400,Eye gaze information of player using objects in FPS game space,2017,"2017 IEEE 6th Global Conference on Consumer Electronics, GCCE 2017",2017-January,,,1,3,2.0,3,10.1109/GCCE.2017.8229420,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85045739848&doi=10.1109%2fGCCE.2017.8229420&partnerID=40&md5=8a928e122a5e592909ba94fff046f770,"A game space is designed and placed by considering the beats (Events or Quests) inducing the behaviors of a player. Beats of FPS game can be divided into three types. It is possible to evaluate the placement of the pieces of space in case of analyzing the visual cognition information obtained by the player depending on each kind of beat. In this paper, players were divided into two groups (Novices and Experts) by their levels of experiences. Eye gaze information were extracted from the player's movement on stairs by repeated experiments continuously. This kind of analysis is useful to confirm the relations between beats and visual cognition information easily and might be helpful for suggesting some specific modification methods for setting level of game. © 2017 IEEE.",beats; eye gaze information; game space,Electronics engineering; Electronics industry; beats; Eye-gaze; Game space; Modification methods; Novices and experts; Visual Cognition; Eye movements,Conference paper,Final,,Scopus,2-s2.0-85045739848,Gaming / VR
Lim C.H.; Hur Y.; Song S.M.,"Lim, Choong Hoon (27867862100); Hur, Youngjin (22985434000); Song, Song Mi (57202864618)",27867862100; 22985434000; 57202864618,Sponsorship information reception and processing: Explicit and implicit memory of in-game advertising,2018,Social Behavior and Personality,46,6,,935,952,17.0,8,10.2224/sbp.6803,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85049620121&doi=10.2224%2fsbp.6803&partnerID=40&md5=575e3a0e0fac79eaa09c47f57b22aa4e,"Researchers have attempted to identify the impact of sponsors’ advertising on spectators at sports events, but it is necessary to understand the dynamic mechanisms of the effect of advertising on signboards at these events. Thus, our primary purpose was to investigate the effects of emotion on spectators’ implicit/explicit memory of sponsors’ advertising. Further, we examined how visual attention can mediate the effects of emotion on explicit/implicit memory. We asked 81 undergraduates to watch an edited soccer match while wearing an eye tracker in order to examine their visual attention. The results showed that their visual attention to sponsor signage significantly varied as a function of both their pleasure and arousal, and visual attention significantly influenced their explicit memory, but not their implicit memory. We also found a full mediating effect of visual attention between pleasure and explicit memory, and a partial mediating effect of visual attention between arousal and explicit memory. Results are discussed, along with the limitations and scope for future research. © 2018 Scientific Journal Publishers Limited. All rights reserved.",Arousal; Explicit memory; Implicit memory; In-game advertising; Pleasure; Sponsorship advertising; Visual attention,,Article,Final,,Scopus,2-s2.0-85049620121,Gaming / VR
Scherf K.S.; Griffin J.W.; Judy B.; Whyte E.M.; Geier C.F.; Elbich D.; Smyth J.M.,"Scherf, K. Suzanne (55664600900); Griffin, Jason W. (57213343803); Judy, Brian (57204097888); Whyte, Elisabeth M. (56053310000); Geier, Charles F. (16039536100); Elbich, Daniel (56431127800); Smyth, Joshua M. (7201478047)",55664600900; 57213343803; 57204097888; 56053310000; 16039536100; 56431127800; 7201478047,Improving sensitivity to eye gaze cues in autism using serious game technology: Study protocol for a phase i randomised controlled trial,2018,BMJ Open,8,9,e023682,,,,16,10.1136/bmjopen-2018-023682,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85054433746&doi=10.1136%2fbmjopen-2018-023682&partnerID=40&md5=330fa3880218e5b579916cf7febd7981,"Introduction Autism spectrum disorder (ASD) is characterised by impairments in social communication. Core symptoms are deficits in social looking behaviours, including limited visual attention to faces and sensitivity to eye gaze cues. We designed an intervention game using serious game mechanics for adolescents with ASD. It is designed to train individuals with ASD to discover that the eyes, and shifts in gaze specifically, provide information about the external world. We predict that the game will increase understanding of gaze cues and attention to faces. Methods and analysis The Social Games for Adolescents with Autism (SAGA) trial is a preliminary, randomised controlled trial comparing the intervention game with a waitlist control condition. 34 adolescents (10-18 years) with ASD with a Full-Scale IQ between 70 and 130 and a minimum second grade reading level, and their parents, will be randomly assigned (equally to intervention or the control condition) following baseline assessments. Intervention participants will be instructed to play the computer game at home on a computer for ∼30 min, three times a week. All families are tested in the lab at baseline and approximately 2 months following randomisation in all measures. Primary outcomes are assessed with eye tracking to measure sensitivity to eye gaze cues and social visual attention to faces; secondary outcomes are assessed with questionnaires to measure social skills and autism-like behaviours. The analyses will focus on evaluating the feasibility, safety and preliminary effectiveness of the intervention. Ethics and dissemination SAGA is approved by the Institutional Review Board at Pennsylvania State University (00005097). Findings will be disseminated via scientific conferences and peer-reviewed journals and to participants via newsletter. The intervention game will be available to families in the control condition after the full data are collected and if analyses indicate that it is effective. © Author(s) (or their employer(s)) 2018.",,"Adolescent; Attention; Autistic Disorder; Cues; Female; Fixation, Ocular; Humans; Interpersonal Relations; Male; Social Behavior; Social Skills; Teaching; Treatment Outcome; Video Games; adolescent; adult; Article; autism; child; clinical article; clinical effectiveness; controlled study; eye tracking; face; feasibility study; gaze; human; outcome assessment; patient safety; questionnaire; randomized controlled trial; social competence; video game; visual attention; association; attention; autism; clinical trial; eye fixation; female; human relation; male; phase 1 clinical trial; physiology; psychology; social behavior; teaching; treatment outcome",Article,Final,All Open Access; Gold Open Access; Green Open Access,Scopus,2-s2.0-85054433746,Gaming / VR
Vreugdenhil S.; Weidenaar A.C.; de Jong I.J.; van Driel M.F.,"Vreugdenhil, Sanne (57197521464); Weidenaar, Alida Cornelia (23020598300); de Jong, Igle Jan (7005938218); van Driel, Mels Frank (7004947034)",57197521464; 23020598300; 7005938218; 7004947034,Sleep-Related Painful Erections—A Case Series of 24 Patients Regarding Diagnostics and Treatment Options,2017,Sexual Medicine,5,4,,e237,e243,6.0,15,10.1016/j.esxm.2017.09.001,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85033998930&doi=10.1016%2fj.esxm.2017.09.001&partnerID=40&md5=69cfeee3cce743ed0ef4b580e111a059,"Background Patients with sleep-related painful erections (SRPEs) have deep penile pain during nocturnal erection that wakes them up and disturbs their nights of sleep. This rare parasomnia is poorly recognized by general practitioners and by urologists and sexologists. Aim To gain more insight into diagnostics and therapeutic options. Methods Data from a series of 24 consecutive patients who presented with SRPEs at the outpatient clinic from 1996 to 2015 were retrospectively analyzed. Additional questionnaires were completed to complement data and to obtain information about follow-up. Long-term treatment efficacy of baclofen was assessed using the Wilcoxon signed rank test. Outcomes SRPEs were not associated with urologic, surgical, or psychiatric history or with serum testosterone levels. The mean doctors’ delay was 3.5 years. 14 of the 24 patients were treated with baclofen (10–75 mg). In 11 of them, complete remission was observed within a few weeks. 2 of the 3 remaining patients noticed a slight improvement of SPRE symptoms and only 1 patient experienced no effect at all. After an average follow-up of 4.5 years, only 41.6% of patients who had used baclofen were satisfied with their SRPEs. The others (58.4%) were dissatisfied, mostly owing to relapse of symptoms after the discontinuation of baclofen. Other treatment forms were applied sporadically, with strongly varying results. Clinical Implications This overview of SRPE contributes to a better clinical understanding and recognition of the phenomenon and provides new, more constructed advice about therapeutic implications, especially concerning the use of baclofen. Strengths and Limitations This study provides a systematic overview of a relatively large series of patients with SRPE, which provides substantiated treatment advice. However, treatment efficacy was based mainly on the patients’ subjective perception and it was not possible to compare the results of baclofen with other forms of pharmacologic treatment, because these alternative drugs were applied only sporadically. Nevertheless, this study is directional for future research. Conclusions This study confirmed a long doctors’ delay in patients with SRPE. There was no association between SRPEs and comorbidity and total serum testosterone levels. Treatment with baclofen proved successful and safe in the short term. Long-term feasibility needs further investigation. Vreugdenhil S, Weidenaar AC, de Jong IJ, van Driel MF. Sleep-Related Painful Erections—A Case Series of 24 Patients Regarding Diagnostics and Treatment Options. Sex Med 2017;5:e237–e243. © 2017 The Authors",Baclofen; Humans; Parasomnia; Rapid Eye Movement Sleep; Sleep-Related Painful Erection,amitriptyline; baclofen; carbamazepine; cyproterone acetate; tadalafil; testosterone; adult; aged; Article; clinical article; comorbidity; Doppler flowmetry; drowsiness; drug efficacy; electromyography; erectile dysfunction; fatigue; fever; follow up; headache; human; lethargy; libido disorder; male; men's health; middle aged; mood disorder; myalgia; painful erection; polysomnography; priority journal; pruritus; relapse; remission; retrospective study; sleep realted painful erection; sleep related painful erection; symptom; testosterone blood level; treatment duration; treatment outcome; Wilcoxon signed ranks test,Article,Final,All Open Access; Gold Open Access; Green Open Access,Scopus,2-s2.0-85033998930,Gaming / VR
Arshad Q.; Nigmatullina Y.; Siddiqui S.; Franka M.; Mediratta S.; Ramachandaran S.; Lobo R.; Malhotra P.A.; Roberts R.E.; Bronstein A.M.,"Arshad, Qadeer (54419464900); Nigmatullina, Yuliya (55588392500); Siddiqui, Shuaib (57189090104); Franka, Mustafa (57160514200); Mediratta, Saniya (57198761558); Ramachandaran, Sanjeev (57198778827); Lobo, Rhannon (57191193059); Malhotra, Paresh A. (8086798900); Roberts, R.E. (56662403700); Bronstein, Adolfo M. (7103163120)",54419464900; 55588392500; 57189090104; 57160514200; 57198761558; 57198778827; 57191193059; 8086798900; 56662403700; 7103163120,Influence of biases in numerical magnitude allocation on human prosocial decision making,2017,Journal of Neurophysiology,118,6,,3007,3013,6.0,1,10.1152/jn.00372.2017,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85037040618&doi=10.1152%2fjn.00372.2017&partnerID=40&md5=c611f926d474bc96344557903f2d5983,"Over the past decade neuroscientific research has attempted to probe the neurobiological underpinnings of human prosocial decision making. Such research has almost ubiquitously employed tasks such as the dictator game or similar variations (i.e., ultimatum game). Considering the explicit numerical nature of such tasks, it is surprising that the influence of numerical cognition on decision making during task performance remains unknown. While performing these tasks, participants typically tend to anchor on a 50:50 split that necessitates an explicit numerical judgement (i.e., number-pair bisection). Accordingly, we hypothesize that the decision-making process during the dictator game recruits overlapping cognitive processes to those known to be engaged during number-pair bisection. We observed that biases in numerical magnitude allocation correlated with the formulation of decisions during the dictator game. That is, intrinsic biases toward smaller numerical magnitudes were associated with the formulation of less favorable decisions, whereas biases toward larger magnitudes were associated with more favorable choices. We proceeded to corroborate this relationship by subliminally and systematically inducing biases in numerical magnitude toward either higher or lower numbers using a visuo-vestibular stimulation paradigm. Such subliminal alterations in numerical magnitude allocation led to proportional and corresponding changes to an individual’s decision making during the dictator game. Critically, no relationship was observed between neither intrinsic nor induced biases in numerical magnitude on decision making when assessed using a nonnumerical-based prosocial questionnaire. Our findings demonstrate numerical influences on decisions formulated during the dictator game and highlight the necessity to control for confounds associated with numerical cognition in human decision-making paradigms. NEW & NOTEWORTHY We demonstrate that intrinsic biases in numerical magnitude can directly predict the amount of money donated by an individual to an anonymous stranger during the dictator game. Furthermore, subliminally inducing perceptual biases in numerical-magnitude allocation can actively drive prosocial choices in the corresponding direction. Our findings provide evidence for numerical influences on decision making during performance of the dictator game. Accordingly, without the implementation of an adequate control for numerical influences, the dictator game and other tasks with an inherent numerical component (i.e., ultimatum game) should be employed with caution in the assessment of human behavior. © 2017 the American Physiological Society.",Decision making; Dictator game; Numerical magnitude allocation; Vestibular cognition,Adolescent; Adult; Altruism; Bias; Decision Making; Female; Humans; Male; adult; altruism; arousal; Article; attention; attentional bias; behavior; cognition; cognitive bias; deception; decision making; dictator game; double dissociation; eye movement; female; game; human; male; medical history; neurobiology; neuromodulation; numerical magnitude allocation; priority journal; prosocial decision making; right handedness; task performance; vestibular stimulation; working memory; adolescent; altruism; decision making; physiology; statistical bias,Article,Final,All Open Access; Green Open Access,Scopus,2-s2.0-85037040618,Gaming / VR
Fang Y.; Lei J.; Li J.; Xu L.; Lin W.; Callet P.L.,"Fang, Yuming (8435698900); Lei, Jianjun (14037882800); Li, Jia (37067453800); Xu, Long (55660493400); Lin, Weisi (8574872000); Callet, Patrick Le (57200770358)",8435698900; 14037882800; 37067453800; 55660493400; 8574872000; 57200770358,Learning visual saliency from human fixations for stereoscopic images,2017,Neurocomputing,266,,,284,292,8.0,13,10.1016/j.neucom.2017.05.050,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85019705975&doi=10.1016%2fj.neucom.2017.05.050&partnerID=40&md5=5d3c50807133bf7597319efc2bc7f811,"In the previous years, a lot of saliency detection algorithms have been designed for saliency computation of visual content. Recently, stereoscopic display techniques have developed rapidly, which results in much requirement of stereoscopic saliency detection for emerging stereoscopic applications. Different from 2D saliency prediction, stereoscopic saliency detection methods have to consider depth factor. We design a novel stereoscopic saliency detection algorithm by machine learning technique. First, the features of luminance, color and texture are extracted to calculate the feature contract for predicting feature maps of stereoscopic images. Furthermore, the depth features are extracted for depth feature map computation. Sematic features including the center-bias factor and other top-down cues are also applied as the features in the proposed stereoscopic saliency detection method. Support Vector Regression (SVR) is applied to learn the saliency detection model of stereoscopic images. Experimental results obtained on a public large-scale eye tracking database demonstrate that the proposed method can predict better saliency results for stereoscopic images than other existing ones. © 2017 Elsevier B.V.",3D image; Machine learning; Stereoscopic image; Stereoscopic saliency detection; Support Vector Regression; Visual attention,Artificial intelligence; Behavioral research; Eye movements; Forecasting; Learning systems; Signal detection; 3-D image; Saliency detection; Stereoscopic image; Support vector regression (SVR); Visual Attention; algorithm; Article; color; controlled study; data base; experimental study; eye fixation; eye tracking; imaging and display; luminance; machine learning; physical parameters; prediction; priority journal; regression analysis; stereoscopic image; texture; visual saliency; Stereo image processing,Article,Final,,Scopus,2-s2.0-85019705975,Gaming / VR
Rogerson M.J.; Gibbs M.R.; Smith W.,"Rogerson, Melissa J. (57192204355); Gibbs, Martin R. (10040667200); Smith, Wally (34880686900)",57192204355; 10040667200; 34880686900,What can we learn from eye tracking boardgame play?,2017,CHI PLAY 2017 Extended Abstracts - Extended Abstracts Publication of the Annual Symposium on Computer-Human Interaction in Play,,,,519,525,6.0,10,10.1145/3130859.3131314,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85034735967&doi=10.1145%2f3130859.3131314&partnerID=40&md5=62fac75574c2b9c7d9799c402a1b158b,"In this paper, we discuss exploratory gaze data findings from a series of co-located tabletop boardgame play sessions. We identified clear patterns of repeated gaze jumps between people and components, as players looked back and forth between various game materials in common play areas as well as their own and those of other players. Of interest was the repetition that occurred - players frequently referred back to specific elements several times - as well as the relatively short dwell times, which were typically well under a second. Contrary to our expectations, this gaze pattern occurred during other players' turns as well as the player's own. This work contributes to understanding of players' attention during play, and may be extensible to other forms of play and interaction outside the immediate traditional form boardgame setting. © 2017 Copyright is held by the owner/author(s).",Board games; Boardgames; Eye gaze; Leisure activities; Resting gaze; Social gaze,Abstracting; Interactive computer systems; Board games; Eye-gaze; Leisure activities; Resting gaze; Social gaze; Human computer interaction,Conference paper,Final,,Scopus,2-s2.0-85034735967,Gaming / VR
Newn J.; Velloso E.; Allison F.; Abdelrahman Y.; Vetere F.,"Newn, Joshua (57188820341); Velloso, Eduardo (53364337000); Allison, Fraser (57144060800); Abdelrahman, Yomna (56156577200); Vetere, Frank (10039716000)",57188820341; 53364337000; 57144060800; 56156577200; 10039716000,Evaluating real-time gaze representations to infer intentions in competitive turn-based strategy games,2017,CHI PLAY 2017 - Proceedings of the Annual Symposium on Computer-Human Interaction in Play,,,,541,552,11.0,54,10.1145/3116595.3116624,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85034654908&doi=10.1145%2f3116595.3116624&partnerID=40&md5=8b610ba68240a4956e9eb06134a4525d,"In this paper, we investigate nine different visual representations of gaze in a competitive digital game setting. We evaluate the ability of spectators to infer a player's intentions in the game for each visual representation. Our results show that spectators have a remarkable ability to infer intent accurately using all nine visualizations, but that visualizations with certain characteristics were more comprehensible and more readily revealed the player's intent. The real-time Heatmap visualization was the most highly preferred by participants and the most effective in revealing intent, due to its ability to balance real-time gaze information with a persistent summary of recent gaze behaviour. Our findings show that eye-tracking visualization can enable playful interactions in competitive games based on players' ability to interpret opponents' attention and intention through gaze information. © 2017 ACM.",Competitive gaming; Eye tracking; Gaze; Gaze awareness; Intent prediction; Nonverbal leakage; Shared gaze,Interactive computer systems; Visualization; Competitive gamings; Eye-tracking; Gaze; Gaze awareness; Shared gazes; Human computer interaction,Conference paper,Final,,Scopus,2-s2.0-85034654908,Gaming / VR
Renner P.; Pfeiffer T.,"Renner, Patrick (56145227600); Pfeiffer, Thies (14027435500)",56145227600; 14027435500,Augmented Reality Assistance in the Central Field-of-View Outperforms Peripheral Displays for Order Picking: Results from a Virtual Reality Simulation Study,2017,"Adjunct Proceedings of the 2017 IEEE International Symposium on Mixed and Augmented Reality, ISMAR-Adjunct 2017",,,8088476,176,181,5.0,34,10.1109/ISMAR-Adjunct.2017.59,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85040248667&doi=10.1109%2fISMAR-Adjunct.2017.59&partnerID=40&md5=48051ab3e444d81ee6f3ca20c6c180e9,"One area in which glasses-based augmented reality (AR) is successfully applied in industry is order picking in logistics (pick-byvision). Here, the almost hands-free operation and the direct integration into the digital workflow provided by augmented reality glasses are direct advantages. A common non-AR guidance technique for order picking is pick-by-light. This is an efficient approach for single users and low numbers of alternative targets. AR glasses have the potential to overcome these limitations. However, making a grounded decision on the specific AR device and the particular guidance techniques to choose for a specific scenario is difficult, given the diversity of device characteristics and the lack of experience with smart glasses in industry at larger scale. The contributions of the paper are twofold. First, we present a virtual reality (VR) simulation approach to ground design decisions for AR-based solutions and apply it to the scenario of order picking. Second, we present results from a simulator study with implemented simulations for monocular and binocular head-mounted displays and compared existing techniques for attention guiding with our own SWave approach and the integration of eye tracking. Our results show clear benefits for the use of pick-by-vision compared to pick-by-light. In addition to that, we can show that binocular AR solutions outperform monocular ones in the attention guiding task. © 2017 IEEE.",,Binoculars; Bins; Glass; Helmet mounted displays; Virtual reality; Design decisions; Device characteristics; Digital workflows; Direct integration; Hands free operation; Peripheral displays; Simulation approach; Virtual reality simulations; Augmented reality,Conference paper,Final,All Open Access; Green Open Access,Scopus,2-s2.0-85040248667,Gaming / VR
Larradet F.; Barresi G.; Mattos L.S.,"Larradet, Fanny (57196196474); Barresi, Giacinto (55877110100); Mattos, Leonardo S. (7005199692)",57196196474; 55877110100; 7005199692,Effects of galvanic skin response feedback on user experience in gaze-controlled gaming: A pilot study,2017,"Proceedings of the Annual International Conference of the IEEE Engineering in Medicine and Biology Society, EMBS",,,8037354,2458,2461,3.0,12,10.1109/EMBC.2017.8037354,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032212821&doi=10.1109%2fEMBC.2017.8037354&partnerID=40&md5=397501b68d020271063b5c92ea2ee171,"Eye-tracking (ET) is one of the most intuitive solutions for enabling people with severe motor impairments to control devices. Nevertheless, even such an effective assistive solution can detrimentally affect user experience during demanding tasks because of, for instance, the user's mental workload - using gaze-based controls for an extensive period of time can generate fatigue and cause frustration. Thus, it is necessary to design novel solutions for ET contexts able to improve the user experience, with particular attention to its aspects related to workload. In this paper, a pilot study evaluates the effects of a relaxation biofeedback system on the user experience in the context of a gaze-controlled task that is mentally and temporally demanding: ET-based gaming. Different aspects of the subjects' experience were investigated under two conditions of a gaze-controlled game. In the Biofeedback group (BF), the user triggered a command by means of voluntary relaxation, monitored through Galvanic Skin Response (GSR) and represented by visual feedback. In the No Biofeedback group (NBF), the same feedback was timed according to the average frequency of commands in BF. After the experiment, each subject filled out a user experience questionnaire. The results showed a general appreciation for BF, with a significant between-group difference in the perceived session time duration, with the latter being shorter for subjects in BF than for the ones in NBF. This result implies a lower mental workload for BF than for NBF subjects. Other results point toward a potential role of user's engagement in the improvement of user experience in BF. Such an effect highlights the value of relaxation biofeedback for improving the user experience in a demanding gaze-controlled task. © 2017 IEEE.",,"Biofeedback, Psychology; Feedback, Sensory; Fixation, Ocular; Galvanic Skin Response; Humans; Pilot Projects; biofeedback; electrodermal response; eye fixation; human; pilot study; sensory feedback",Conference paper,Final,,Scopus,2-s2.0-85032212821,Gaming / VR
Upenik E.; Ebrahimi T.,"Upenik, Evgeniy (57192586356); Ebrahimi, Touradj (35560920500)",57192586356; 35560920500,A simple method to obtain visual attention data in head mounted virtual reality,2017,"2017 IEEE International Conference on Multimedia and Expo Workshops, ICMEW 2017",,,8026231,73,78,5.0,65,10.1109/ICMEW.2017.8026231,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85025606382&doi=10.1109%2fICMEW.2017.8026231&partnerID=40&md5=bd7d3f64437acda3144d44e1ba960772,"Automatic prediction of salient regions in images is a well developed topic in the field of computer vision. Yet, virtual reality omnidirectional visual content brings new challenges to this topic, due to a different representation of visual information and additional degrees of freedom available to viewers. Having a model for visual attention is important to continue research in this direction. In this paper we develop such a model for head direction trajectories. The method consists of three basic steps: First, a computed head angular speed is used to exclude the parts of a trajectory where motion is too fast to fixate viewer's attention. Second, fixation locations of different subjects are fused together, optionally preceded by a re-sampling step to conform to the equal distribution of points on a sphere. Finally, a Gaussian based filtering is performed to produce continuous fixation maps. The developed model can be used to obtain ground truth experimental data when eye tracking is not available. © 2017 IEEE.",360-degree images and video; fixation maps; omnidirectional visual content; virtual reality; visual attention,Degrees of freedom (mechanics); Virtual reality; Visual communication; 360-degree images and video; Automatic prediction; Fixation map; Head mounted virtual reality; Salient regions; Visual Attention; Visual content; Visual information; Behavioral research,Conference paper,Final,All Open Access; Green Open Access,Scopus,2-s2.0-85025606382,Gaming / VR
Birmingham E.; Johnston K.H.S.; Iarocci G.,"Birmingham, Elina (12781566500); Johnston, Krista Haley Smith (56848673300); Iarocci, Grace (6508392031)",12781566500; 56848673300; 6508392031,Spontaneous Gaze Selection and Following during Naturalistic Social Interactions in School-Aged Children and Adolescents with Autism Spectrum Disorder,2017,Canadian Journal of Experimental Psychology,71,3,,243,257,14.0,15,10.1037/cep0000131,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85020667703&doi=10.1037%2fcep0000131&partnerID=40&md5=32e3131ce51a7b071ff109b7e167f47f,"Using a novel naturalistic paradigm allowing participants the freedom to spontaneously select and follow gaze cues in their environment, this study extends previous research conducted with younger children to determine whether school-age children with autism spectrum disorder (ASD, n = 17) demonstrate abnormal gaze following relative to typically developing (TD, n = 15) children. The participant and experimenter played a series of games, during which the experimenter pseudorandomly averted her gaze toward a social target (person) or a nonsocial target (object). A significant finding was that, relative to TD children, children with ASD were slower to follow the experimenter's gaze relative to the start of the trial (social targets d = -.93 [-1.70, -.16], nonsocial targets d = -1.05 [-1.88, -.20]). When we analyzed the duration of glances to the experimenter, we found that the ASD group made longer glances relative to TD children, but only in the nonsocial target condition (social targets d = .01 [-.68, .71], nonsocial targets d = -.81 [-1.53, -.08]). Other analyses revealed patterns of gaze selection and following that may help interpret the main findings. Despite the differences in the timing of gaze selection and following, the most common type of responder in both groups was one who followed the experimenter's gaze on over half of the trials. This pattern of results argues against a clear deficit in social attention in school-age children with ASD and underscores the importance of measuring both the timing of distinct mechanisms of social attention and the context in which these behaviors occur. © 2017 Canadian Psychological Association.",Autism spectrum disorder; gaze following; gaze selection; naturalistic; real-world,"Adolescent; Attention; Autism Spectrum Disorder; Child; Eye Movement Measurements; Female; Fixation, Ocular; Humans; Interpersonal Relations; Male; Social Perception; adolescent; attention; autism; child; eye fixation; female; human; human relation; male; oculography; pathophysiology; perception; physiology",Article,Final,,Scopus,2-s2.0-85020667703,Gaming / VR
Flechsenhar A.F.; Gamer M.,"Flechsenhar, Aleya Felicia (57195483556); Gamer, Matthias (8979946100)",57195483556; 8979946100,Top-down influence on gaze patterns in the presence of social features,2017,PLoS ONE,12,8,e0183799,,,,20,10.1371/journal.pone.0183799,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028387076&doi=10.1371%2fjournal.pone.0183799&partnerID=40&md5=c587806b94b13c2678c0c04a5f9511f2,"Visual saliency maps reflecting locations that stand out from the background in terms of their low-level physical features have proven to be very useful for empirical research on attentional exploration and reliably predict gaze behavior. In the present study we tested these predictions for socially relevant stimuli occurring in naturalistic scenes using eye tracking. We hypothesized that social features (i.e. human faces or bodies) would be processed preferentially over non-social features (i.e. objects, animals) regardless of their low-level saliency. To challenge this notion, we included three tasks that deliberately addressed nonsocial attributes. In agreement with our hypothesis, social information, especially heads, was preferentially attended compared to highly salient image regions across all tasks. Social information was never required to solve a task but was regarded nevertheless. More so, after completing the task requirements, viewing behavior reverted back to that of free-viewing with heavy prioritization of social features. Additionally, initial eye movements reflecting potentially automatic shifts of attention, were predominantly directed towards heads irrespective of top-down task demands. On these grounds, we suggest that social stimuli may provide exclusive access to the priority map, enabling social attention to override reflexive and controlled attentional processes. Furthermore, our results challenge the generalizability of saliency-based attention models. © 2017 Flechsenhar, Gamer. This is an open access article distributed under the terms of the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.",,"Adolescent; Adult; Animals; Attention; Eye Movements; Face; Female; Fixation, Ocular; Humans; Photic Stimulation; Young Adult; attention; behavior; eye tracking; face; gaze; human; model; prediction; stimulus; adolescent; adult; animal; eye fixation; eye movement; face; female; photostimulation; young adult",Article,Final,All Open Access; Gold Open Access; Green Open Access,Scopus,2-s2.0-85028387076,Gaming / VR
Lutz O.H.-M.; Kruger J.,"Lutz, Otto Hans-Martin (56286094800); Kruger, Jorg (36190849500)",56286094800; 36190849500,Assessing visual attention in virtual reality: Automatic one-point calibration for eye-tracking,2017,"International Conference on Virtual Rehabilitation, ICVR",2017-June,,8007455,,,,3,10.1109/ICVR.2017.8007455,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85034215365&doi=10.1109%2fICVR.2017.8007455&partnerID=40&md5=4a9dcf22092b231d7941c832729bcfd9,"To assess visual attention in head-mounted displays (HMDs), a re-calibration of the eye-tracker is necessary each time the HMD is moved or taken off and put on again. This occurs several times per therapy session in some use cases of virtual reality based rehabilitation. The process is time-consuming and likely a demanding task for patients. Hence, a solution is needed which reduces the workload and does not require explicit user input for calibration while wearing a HMD. We propose a method for a one-point calibration, which is very fast, appropriately accurate, and can be performed automatically. A user study with healthy subjects showed significant improvements in eye-tracking accuracy using our method after reattaching the HMD. © 2017 IEEE.",,Behavioral research; Calibration; Virtual reality; Eye trackers; Eye-tracking; Head mounted displays; Healthy subjects; Recalibrations; User input; User study; Visual Attention; Helmet mounted displays,Conference paper,Final,,Scopus,2-s2.0-85034215365,Gaming / VR
Caro K.; Tentori M.; Martinez-Garcia A.I.; Alvelais M.,"Caro, Karina (56422179500); Tentori, Mónica (56002718500); Martinez-Garcia, Ana I. (36882629300); Alvelais, Marina (55914598700)",56422179500; 56002718500; 36882629300; 55914598700,Using the FroggyBobby exergame to support eye-body coordination development of children with severe autism,2017,International Journal of Human Computer Studies,105,,,12,27,15.0,46,10.1016/j.ijhcs.2017.03.005,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85015909796&doi=10.1016%2fj.ijhcs.2017.03.005&partnerID=40&md5=67b7046007b49e2634b5c50e298aa212,"Children with severe autism lack of the eye-body coordination skills which are needed to conduct aimed limb movements. Physical therapy relies on the repetition of limb movements that demands children with severe autism to aim for a visual target. But their movements during physical therapies are most of the time aimless, and they found the visual stimuli confusing and not engaging. Exergames could support motor therapies as they combine game technology with exercise activity. This technology can offer a natural interaction and use multisensory stimuli appropriate to keep children with autism focused during motor therapeutic interventions. In this paper, we hypothesize that exergames supporting motor therapeutic interventions and alleviating such attention and motor challenges could help children with severe autism to develop the necessary coordination skills needed to follow up visual targets. We present a 7-weeks evaluation study of the deployment of an exergame supporting the practice of eye-body coordination exercises. Seven children with severe autism and three psychotherapists participated in the study. Our results indicate children with severe autism maintained their attention for the total duration of the therapy, reduced their aimless limb movements and developed aimed limb movements, as a result of weeks of usage of the exergame. We close discussing challenges for existing clinical practice from a design and clinical point of view. © 2017 Elsevier Ltd",Aimed limb movements; Autism; Exergame; Eye-body coordination skills; Motor therapeutic interventions,Diseases; Education; Physical therapy; Autism; Co-ordination skills; Exergame; Limb movements; Therapeutic intervention; Eye movements,Article,Final,,Scopus,2-s2.0-85015909796,Gaming / VR
Sengupta A.; Dasgupta A.; Chaudhuri A.; George A.; Routray A.; Guha R.,"Sengupta, Anwesha (57212934197); Dasgupta, Anirban (56402020900); Chaudhuri, Aritra (55638576400); George, Anjith (55638380800); Routray, Aurobinda (55927861800); Guha, Rajlakshmi (57189351098)",57212934197; 56402020900; 55638576400; 55638380800; 55927861800; 57189351098,A multimodal system for assessing alertness levels due to cognitive loading,2017,IEEE Transactions on Neural Systems and Rehabilitation Engineering,25,7,7859391,1037,1046,9.0,20,10.1109/TNSRE.2017.2672080,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029698928&doi=10.1109%2fTNSRE.2017.2672080&partnerID=40&md5=fffbd8f1e093b6e5617348bac1cec96a,"This paper proposes a scheme for assessing the alertness levels of an individual using simultaneous acquisition of multimodal physiological signals and fusing the information into a single metric for quantification of alertness. The system takes electroencephalogram, high-speed image sequence, and speech data as inputs. Certain parameters are computed from each of these measures as indicators of alertness and a metric is proposed using a fusion of the parameters for indicating alertness level of an individual at an instant. The scheme has been validated experimentally using standard neuropsychological tests, such as the Visual Response Test (VRT), Auditory Response Test (ART), a Letter Counting (LC) task, and the Stroop Test. The tests are used both as cognitive tasks to induce mental fatigue as well as tools to gauge the present degree of alertness of the subject. Correlation between the measures has been studied and the experimental variables have been statistically analyzed using measures such as multivariate linear regression and analysis of variance. Correspondence of trends obtained from biomarkers and neuropsychological measures validate the usability of the proposed metric. © 2001-2011 IEEE.",Alertness; cognitive loading; EEG; eye saccade; speech,"Adult; Attention; Cognition; Diagnosis, Computer-Assisted; Electroencephalography; Eye Movements; Female; Fixation, Ocular; Humans; Male; Mental Fatigue; Neuropsychological Tests; Photography; Psychomotor Performance; Reproducibility of Results; Sensitivity and Specificity; Speech Production Measurement; Computer applications; Human rehabilitation engineering; Speech; Alertness; Eye saccades; Multimodal system; Multivariate linear regressions; Neuropsychological; Neuropsychological tests; Physiological signals; Simultaneous acquisition; alertness; algorithm; Article; artifact; attention; auditory response; auditory vigilance test; cognition; electroencephalography; eye tracking; eyelid closure; human; illumination; iris; mathematical phenomena; multimodal imaging; neuropsychological test; questionnaire; saccadic velocity; Stroop test; validation process; visual evoked potential; visual stimulation; voice parameter; adult; attention; computer assisted diagnosis; eye fixation; eye movement; female; male; Mental Fatigue; neuropsychological test; pathophysiology; photography; procedures; psychomotor performance; reproducibility; sensitivity and specificity; speech analysis; Electroencephalography",Article,Final,,Scopus,2-s2.0-85029698928,Gaming / VR
Tomlin D.; Nedic A.; Prentice D.A.; Holmes P.; Cohen J.D.,"Tomlin, Damon (8321497100); Nedic, Andrea (54684660900); Prentice, Deborah A. (7005410922); Holmes, Philip (7202078308); Cohen, Jonathan D. (54992680400)",8321497100; 54684660900; 7005410922; 7202078308; 54992680400,The integration of social influence and reward: Computational approaches and neural evidence,2017,"Cognitive, Affective and Behavioral Neuroscience",17,4,,784,808,24.0,3,10.3758/s13415-017-0512-1,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85019539674&doi=10.3758%2fs13415-017-0512-1&partnerID=40&md5=05f2becf45916962dd15ca5e189e13fa,"Decades of research have established that decision-making is dramatically impacted by both the rewards an individual receives and the behavior of others. How do these distinct influences exert their influence on an individual’s actions, and can the resulting behavior be effectively captured in a computational model? To address this question, we employed a novel spatial foraging game in which groups of three participants sought to find the most rewarding location in an unfamiliar two-dimensional space. As the game transitioned from one block to the next, the availability of information regarding other group members was varied systematically, revealing the relative impacts of feedback from the environment and information from other group members on individual decision-making. Both reward-based and socially-based sources of information exerted a significant influence on behavior, and a computational model incorporating these effects was able to recapitulate several key trends in the behavioral data. In addition, our findings suggest how these sources were processed and combined during decision-making. Analysis of reaction time, location of gaze, and functional magnetic resonance imaging (fMRI) data indicated that these distinct sources of information were integrated simultaneously for each decision, rather than exerting their influence in a separate, all-or-none fashion across separate subsets of trials. These findings add to our understanding of how the separate influences of reward from the environment and information derived from other social agents are combined to produce decisions. © 2017, Psychonomic Society, Inc.",Computational model; Decision-making; Neuroimaging; Social cognition,"Adolescent; Adult; Brain; Cohort Studies; Computer Simulation; Decision Making; Eye Movement Measurements; Eye Movements; Feedback, Psychological; Female; Humans; Magnetic Resonance Imaging; Male; Middle Aged; Models, Psychological; Neuroimaging; Reaction Time; Reward; Social Behavior; Spatial Behavior; Young Adult; adolescent; adult; brain; clinical trial; cohort analysis; computer simulation; decision making; diagnostic imaging; eye movement; female; human; male; middle aged; multicenter study; neuroimaging; nuclear magnetic resonance imaging; oculography; physiology; psychological feedback; psychological model; reaction time; reward; social behavior; spatial behavior; young adult",Article,Final,All Open Access; Bronze Open Access,Scopus,2-s2.0-85019539674,Gaming / VR
Rai Y.; Le Callet P.; Guillotel P.,"Rai, Yashas (57188570944); Le Callet, Patrick (57200770358); Guillotel, Philippe (6506461641)",57188570944; 57200770358; 6506461641,Which saliency weighting for omni directional image quality assessment?,2017,"2017 9th International Conference on Quality of Multimedia Experience, QoMEX 2017",,,7965659,,,,83,10.1109/QoMEX.2017.7965659,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85026772496&doi=10.1109%2fQoMEX.2017.7965659&partnerID=40&md5=9d2856612732f8a4d66a336286a724be,"With the explosion of Virtual Reality technologies, the production and usage of omni directional images (a.k.a 360 images) is presenting new challenges in the domains of compression, transmission and rendering. The evaluation of the quality of images generated by these technologies is therefore paramount. As the exploration of 360 images within a Head-Mounted Display (HMD) is non-uniform, current state of the art proposes a saliency weighting of distortions (between a reference and an impaired version) thus allowing us to highlight impairments in frequently attended regions. So far, saliency maps have been generated by tracking head motion alone, and consider that the view-port orientation alone is sufficient to determine saliency. The added value of eye gaze tracking within the viewport has not been studied in this domain yet. In this work, an eye-tracking experiment is performed using a HMD and is followed by subsequent gaze analysis to appreciate the visual attention behavior within a view-port. Results suggest that most eye-gaze fixations are rather far away from the center of the viewport. Across contents and observers, gaze fixations are quasi-isotropically distributed in orientation. The average location of gaze fixation (across contents and observers) from the center of the view-port varies between 14 and 20 visual degrees and these values correspond to a shift in retina beyond the para and peri fovea, into the extra-perifoveal region. A saliency weighting model based on foveation, centered at the middle of the view-port seems to be a correct assumption in only 2.5% of the overall scenarios and is consequently questionable. Therefore there is a need to refine saliency modeling and weighting for quality assessment in case of panoramic viewing. © 2017 IEEE.",eye-tracking; omni directional image quality assessment; Omni directional saliency; saliency in 360; visual attention in head mounted displays,Behavioral research; Eye movements; Helmet mounted displays; Multimedia systems; Quality control; Sensory perception; Street traffic control; Virtual reality; Eye gaze tracking; Eye-tracking; Head mounted displays; Omni-directional; Quality assessment; saliency in 360; Saliency modeling; Virtual reality technology; Image quality,Conference paper,Final,,Scopus,2-s2.0-85026772496,Gaming / VR
De Abreu A.; Ozcinar C.; Smolic A.,"De Abreu, Ana (56102661500); Ozcinar, Cagri (35847657400); Smolic, Aljosa (6602582385)",56102661500; 35847657400; 6602582385,Look around you: Saliency maps for omnidirectional images in VR applications,2017,"2017 9th International Conference on Quality of Multimedia Experience, QoMEX 2017",,,7965634,,,,131,10.1109/QoMEX.2017.7965634,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85025650485&doi=10.1109%2fQoMEX.2017.7965634&partnerID=40&md5=da5d50951a9c6036a2d50a84844bd706,"Understanding visual attention has always been a topic of great interest in the graphics, image/video processing, robotics and human-computer interaction communities. By understanding salient image regions, the compression, transmission and rendering algorithms can be optimized. This is particularly important in omnidirectional images (ODIs) viewed with a head-mounted display (HMD), where only a fraction of the captured scene is displayed at a time, namely viewport. In order to predict salient image regions, saliency maps are estimated either by using an eye tracker to collect eye fixations during subjective tests or by using computational models of visual attention. However, eye tracking developments for ODIs are still in the early stages and although a large list of saliency models are available, no particular attention has been dedicated to ODIs. Therefore, in this paper, we consider the problem of estimating saliency maps for ODIs viewed with HMDs, when the use of an eye tracker device is not possible. We collected viewport center trajectories (VCTs) of 32 participants for 21 ODIs and propose a method to transform the gathered data into saliency maps. The obtained saliency maps are compared in terms of image exposition time used to display each ODI in the subjective tests. Then, motivated by the equator bias tendency in ODIs, we propose a post-processing method, namely fused saliency maps (FSM), to adapt current saliency models to ODIs requirements. We show that the use of FSM on current models improves their performance by up to 20%. The developed database and testbed are publicly available with this paper. © 2017 IEEE.",Fixations; head-mounted display (HMD); omnidirectional images (ODIs); saliency maps; viewport; virtual reality (VR),Behavioral research; Computer graphics; Eye movements; Human computer interaction; Human robot interaction; Multimedia systems; Processing; Virtual reality; Fixations; Head mounted displays; Omnidirectional image; Saliency map; viewport; Helmet mounted displays,Conference paper,Final,,Scopus,2-s2.0-85025650485,Gaming / VR
Palaus M.; Marron E.M.; Viejo-Sobera R.; Redolar-Ripoll D.,"Palaus, Marc (56576434300); Marron, Elena M. (55355728900); Viejo-Sobera, Raquel (56576404400); Redolar-Ripoll, Diego (8573873800)",56576434300; 55355728900; 56576404400; 8573873800,Neural basis of video gaming: A systematic review,2017,Frontiers in Human Neuroscience,11,,248,,,,177,10.3389/fnhum.2017.00248,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85022207914&doi=10.3389%2ffnhum.2017.00248&partnerID=40&md5=6cca22d1ec288bfaa122ec217048475d,"Background: Video gaming is an increasingly popular activity in contemporary society, especially among young people, and video games are increasing in popularity not only as a research tool but also as a field of study. Many studies have focused on the neural and behavioral effects of video games, providing a great deal of video game derived brain correlates in recent decades. There is a great amount of information, obtained through a myriad of methods, providing neural correlates of video games. Objectives: We aim to understand the relationship between the use of video games and their neural correlates, taking into account the whole variety of cognitive factors that they encompass. Methods: A systematic review was conducted using standardized search operators that included the presence of video games and neuro-imaging techniques or references to structural or functional brain changes. Separate categories were made for studies featuring Internet Gaming Disorder and studies focused on the violent content of video games. Results: A total of 116 articles were considered for the final selection. One hundred provided functional data and 22 measured structural brain changes. One-third of the studies covered video game addiction, and 14%focused on video game related violence. Conclusions: Despite the innate heterogeneity of the field of study, it has been possible to establish a series of links between the neural and cognitive aspects, particularly regarding attention, cognitive control, visuospatial skills, cognitive workload, and reward processing. However, many aspects could be improved. The lack of standardization in the different aspects of video game related research, such as the participants’ characteristics, the features of each video game genre and the diverse study goals could contribute to discrepancies in many related studies. ©2017 Palaus, Marron, Viejo-Sobera and Redolar-Ripoll.",Addiction; Cognitive improvement; Functional changes; Internet gaming disorder; Neural correlates; Neuroimaging; Structural changes; Video games,attention; brain function; cognition; depth perception; executive function; game addiction; human; mental performance; neuroimaging; Review; reward; sex difference; systematic review; video game; violence; working memory,Review,Final,All Open Access; Gold Open Access; Green Open Access,Scopus,2-s2.0-85022207914,Gaming / VR
Hochhauser M.; Grynszpan O.,"Hochhauser, Michal (35362153200); Grynszpan, Ouriel (22034481500)",35362153200; 22034481500,Methods Investigating How Individuals with Autism Spectrum Disorder Spontaneously Attend to Social Events,2017,Review Journal of Autism and Developmental Disorders,4,1,,82,93,11.0,5,10.1007/s40489-016-0099-4,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85011918311&doi=10.1007%2fs40489-016-0099-4&partnerID=40&md5=158722055446a607b6074ef012dbce22,"It has been recognized that individuals with autism spectrum disorder (ASD) show discrepancies between their abstract capacities to solve social cognition dilemmas and their ability to spontaneously decipher live social interactions. In the last 15 years, different paradigms have been designed to investigate how individuals with ASD grasp information when emerged in naturalistic or live social interactions. The present paper reviews three categories of such paradigms that focus on (1) verbal questionnaires and interviews while participants view a naturalistic social scenario, (2) eye tracking methods while participants view naturalistic settings, and (3) simulation of social interactions using virtual reality or robotics. This paper discusses the advantages and limitations of each paradigm and suggests a new concept for combining these paradigms. © 2016, Springer Science+Business Media New York.",Attention; Autism spectrum disorders; Eye-tracking; Social cognition; Virtual reality,,Article,Final,,Scopus,2-s2.0-85011918311,Gaming / VR
Lutz O.H.-M.; Burmeister C.; Dos Santos L.F.; Morkisch N.; Dohle C.; Krüger J.,"Lutz, Otto Hans-Martin (56286094800); Burmeister, Charlotte (57205360095); Dos Santos, Luara Ferreira (56009418700); Morkisch, Nadine (56800857800); Dohle, Christian (6601952185); Krüger, Jörg (36190849500)",56286094800; 57205360095; 56009418700; 56800857800; 6601952185; 36190849500,Application of head-mounted devices with eye-tracking in virtual reality therapy,2017,Current Directions in Biomedical Engineering,3,1,,53,56,3.0,35,10.1515/cdbme-2017-0012,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85041043709&doi=10.1515%2fcdbme-2017-0012&partnerID=40&md5=8c8dcf69f2082ab375afdf52627ae09d,"Using eye-tracking to assess visual attention in head-mounted devices (HMD) opens up many possibilities for virtual reality (VR)-based therapy. Existing therapy concepts where attention plays a major role can be transferred to VR. Furthermore, they can be expanded to a precise real-time attention assessment, which can serve as a foundation for new therapy approaches. Utilizing HMDs and eye-tracking in a clinical environment is challenging because of hygiene issues and requirements of patients with heterogeneous cognitive and motor impairments. In this paper, we provide an overview of those challenges, discuss possible solutions and present preliminary results of a study with patients. © 2017 Otto Hans-Martin Lutz et al., licensee De Gruyter.",Eye-tracking; Head-mounted devices; Hygiene; Virtual reality therapy,Behavioral research; Helmet mounted displays; Virtual reality; Clinical environments; Cognitive impairment; Eye-tracking; Head-mounted device; Hygiene; Motor impairments; Real- time; Virtual reality therapies; Visual Attention; Eye tracking,Conference paper,Final,All Open Access; Gold Open Access,Scopus,2-s2.0-85041043709,Gaming / VR
Rantala J.; Kangas J.; Raisamo R.,"Rantala, Jussi (25925262500); Kangas, Jari (7005151992); Raisamo, Roope (35610443700)",25925262500; 7005151992; 35610443700,Directional cueing of Gaze with a vibrotactile headband,2017,ACM International Conference Proceeding Series,,,7,,,,5,10.1145/3041164.3041176,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85018316017&doi=10.1145%2f3041164.3041176&partnerID=40&md5=20e332568cc2370ead5e82be19e7e52c,"Augmented attention is one of the ways human action can be enhanced with technologies. Still, more research is needed to find out effective ways to direct human attention to objects that have been determined as important to pay attention to. We investigated how vibrotactile stimulation given to the forehead could be used to cue gaze direction. We built a headband with an array of six vibrotactile actuators that presented short, tap like cues on the forehead. In an experiment, a horizontal line was shown on a computer display, and the participant's task was to look at the point of the line that they thought the vibrotactile cue was pointing to. Information of participant's gaze points was recorded using an eye tracker attached to the display. Analysis of the gaze points showed that for the majority of participants there were statistically significant differences between cues from different actuators. This indicated that the six actuators could successfully direct the participant's gaze to different areas of the visual field. On average, the precision of gaze points related to each actuator was comparable to the width of two to three fingers at arm's length. The findings of this study showed that there is potential in using vibrotactile cueing of gaze direction, for example, for directing visual attention and providing navigation cues with wearable devices such as headbands, head-mounted displays, and virtual reality headsets. © 2017 Copyright held by the owner/author(s).",Attention pointing; Gaze; Gaze movements; Haptics; Tactile augmentation; Tactile cueing; Vibrotactile actuators; Vibrotactile cueing,Behavioral research; Display devices; Helmet mounted displays; Virtual reality; Gaze; Gaze movements; Haptics; Tactile augmentation; Tactile cueing; Vibrotactile; Actuators,Conference paper,Final,,Scopus,2-s2.0-85018316017,Gaming / VR
Finke E.H.; Wilkinson K.M.; Hickerson B.D.,"Finke, Erinn H. (24469956100); Wilkinson, Krista M. (7103117868); Hickerson, Benjamin D. (23018699500)",24469956100; 7103117868; 23018699500,Social Referencing Gaze Behavior During a Videogame Task: Eye Tracking Evidence from Children With and Without ASD,2017,Journal of Autism and Developmental Disorders,47,2,,415,423,8.0,13,10.1007/s10803-016-2968-1,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84995793009&doi=10.1007%2fs10803-016-2968-1&partnerID=40&md5=9cd01d1517483e140c3789e1609ef3fb,"The purpose of this study was to understand the social referencing behaviors of children with and without autism spectrum disorder (ASD) while visually attending to a videogame stimulus depicting both the face of the videogame player and the videogame play action. Videogames appear to offer a uniquely well-suited environment for the emergence of friendships, but it is not known if children with and without ASD attend to and play videogames similarly. Eyetracking technology was used to investigate visual attention of participants matched based on chronological age. Parametric and nonparametric statistical analyses were used and results indicated the groups did not differ on percentage of time spent visually attending to any of the areas of interest, with one possible exception. © 2016, Springer Science+Business Media New York.",Autism; Eyetracking; Social referencing; Videogames,Adolescent; Attention; Autism Spectrum Disorder; Child; Comprehension; Eye Movements; Female; Humans; Male; Photic Stimulation; Psychomotor Performance; Social Behavior; Video Games; adolescent; adult; age; Article; autism; child; clinical article; eye tracking; female; friendship; gaze; human; male; nonparametric test; parametric test; priority journal; social behavior; social environment; social interaction; social referencing; task performance; video game; visual attention; attention; Autism Spectrum Disorder; comparative study; comprehension; eye movement; photostimulation; physiology; procedures; psychology; psychomotor performance; video game,Article,Final,,Scopus,2-s2.0-84995793009,Gaming / VR
de Vries I.E.J.; van Driel J.; Olivers C.N.L.,"de Vries, Ingmar E. J. (57192235802); van Driel, Joram (55363297600); Olivers, Christian N. L. (6603803862)",57192235802; 55363297600; 6603803862,Posterior α EEG dynamics dissociate current from future goals in working memory-guided visual search,2017,Journal of Neuroscience,37,6,,1591,1603,12.0,66,10.1523/JNEUROSCI.2945-16.2016,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85012000609&doi=10.1523%2fJNEUROSCI.2945-16.2016&partnerID=40&md5=332e739e0fad9587eb270ac586d056e9,"Current models of visual search assume that search is guided by an active visual working memory representation of what we are currently looking for. This attentional template for currently relevant stimuli can be dissociated from accessory memory representations that are only needed prospectively, for a future task, and that should be prevented from guiding current attention. However, it remains unclear what electrophysiological mechanisms dissociate currently relevant (serving upcoming selection) from prospectively relevant memories (serving future selection). We measured EEG of 20 human subjects while they performed two consecutive visual search tasks. Before the search tasks, a cue instructed observers which item to look for first (current template) and which second (prospective template). During the delay leading up to the first search display, we found clear suppression of α band (8–14 Hz) activity in regions contralateral to remembered items, comprising both local power and interregional phase synchronization within a posterior parietal network. Importantly, these lateralization effects were stronger when the memory item was currently relevant (i.e., for the first search) compared with when it was prospectively relevant (i.e., for the second search), consistent with current templates being prioritized over future templates. In contrast, event-related potential analysis revealed that the contralateral delay activity was similar for all conditions, suggesting no difference in storage. Together, these findings support the idea that posterior α oscillations represent a state of increased processing or excitability in task-relevant cortical regions, and reflect enhanced cortical prioritization of memory representations that serve as a current selection filter. © 2017 de Vries et al.",EEG; Priority; Selective attention; States; Visual search template; Visual working memory,"Adult; Alpha Rhythm; Electroencephalography; Female; Forecasting; Goals; Humans; Male; Memory, Short-Term; Photic Stimulation; Prospective Studies; Random Allocation; Reaction Time; Visual Perception; Young Adult; adult; Article; brain electrophysiology; cortical synchronization; electrocorticography; eye movement; female; human; human experiment; male; noise; normal human; priority journal; prospective study; vision; visual stimulation; working memory; alpha rhythm; electroencephalography; forecasting; motivation; photostimulation; physiology; procedures; randomization; reaction time; short term memory; young adult",Article,Final,All Open Access; Green Open Access; Hybrid Gold Open Access,Scopus,2-s2.0-85012000609,Gaming / VR
Mallick R.; Slayback D.; Touryan J.; Ries A.J.; Lance B.J.,"Mallick, Rohit (59881151600); Slayback, David (57188967073); Touryan, Jon (8954586100); Ries, Anthony J. (36848887500); Lance, Brent J. (23091636200)",59881151600; 57188967073; 8954586100; 36848887500; 23091636200,The use of eye metrics to index cognitive workload in video games,2017,"Proceedings of the 2nd Workshop on Eye Tracking and Visualization, ETVIS 2016",,,7851168,60,64,4.0,26,10.1109/ETVIS.2016.7851168,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85016016045&doi=10.1109%2fETVIS.2016.7851168&partnerID=40&md5=38c4cad029fc090e4cc754a7f26e2156,"Eye tracking metrics may provide unobtrusive measures of cognitive states such as workload and fatigue and can serve as useful inputs into future human computer interface technologies. To further explore the usefulness of eye tracking for the estimation of cognitive state, the current experiment evaluated saccade, fixation, and pupil-based measures to identify which metrics reliably indexed cognitive workload in a dynamic, unconstrained task (Tetris®). In line with previous studies, our results show that some eye movement features are correlated with changes in workload, manipulated here via task difficulty. Among these were blink duration, saccade velocity, and tonic pupil dilation. © 2016 IEEE.",Cognitive workload; Eye tracking; Fixation; Pupil diameter; Saccade; Tetris,Human computer interaction; Interface states; Nitrogen fixation; Visualization; Blink duration; Cognitive state; Cognitive workloads; Eye-tracking; Human computer interfaces; Pupil diameter; Task difficulty; Tetris; Eye movements,Conference paper,Final,All Open Access; Green Open Access,Scopus,2-s2.0-85016016045,Gaming / VR
Rosa P.J.; Morais D.; Oliveira J.; Gamito P.; Smyth O.; Pavlovic M.,"Rosa, Pedro J. (7102096807); Morais, Diogo (35242615600); Oliveira, Jorge (35243297300); Gamito, Pedro (35217715700); Smyth, Olivia (57144698800); Pavlovic, Matthew (57143743000)",7102096807; 35242615600; 35243297300; 35217715700; 57144698800; 57143743000,Assessment of attentional and mnesic processes through gaze tracking analysis: Inferences from comparative search tasks embedded in vr serious games,2017,Communications in Computer and Information Science,665,,,26,34,8.0,1,10.1007/978-3-319-69694-2_3,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85080114766&doi=10.1007%2f978-3-319-69694-2_3&partnerID=40&md5=6d3ad8ace0026b9f01eedab84da20b96,"The impairment of basic cognitive functions such as attention and visual working memory can have a significant negative impact in our ability to adapt to the permanent changes in the environment. VR Serious Games are being used as a new tool for both assessment, stimulation and rehabilitation of such impaired functions. Even though the results of these novel applications seem to be promising, some of the assessments based in these solutions use indirect measures to evaluate attentional and mnesic performance (e.g. number of errors, task completion time). Gaze tracking (GT) can provide more accurate and direct indicators of these cognitive processes. On a sample of 46 non-clinical participants (33 Female; 71.7%), with an age average of 27.96 years old (SD = 11.92), ocular movements were recorded in two different comparative visual search tasks (CVSTs) that are an integrant part of the cognitive assessment protocol of the Systemic Lisbon Battery (SLB). Number of visits and total fixations differed based on the assessment with the Mini-mental state examination test (MMSE). These results highlight the possibility of combining both the data from the GT and the results of the “spot the differences” tasks in SLB, adding an unobtrusive and reliable solution for cognitive assessment in clinical and non-clinical settings. © Springer International Publishing AG 2017.",Attention; Comparative search-task; Eye movements; Gaze tracking; Memory; Serious games,Data storage equipment; Eye movements; Patient rehabilitation; Serious games; Attention; Cognitive assessments; Cognitive functions; Gaze tracking; Mini-Mental State Examination; Non-clinical settings; Search tasks; Task completion time; Eye tracking,Conference paper,Final,,Scopus,2-s2.0-85080114766,Gaming / VR
Ibrahim F.N.; Ibrahim N.; Zin Z.M.,"Ibrahim, Farah Nadia (57204889647); Ibrahim, Norazlin (55432531800); Zin, Zalhan Mohd (36730188700)",57204889647; 55432531800; 36730188700,A study towards improving eye tracking calibration technique using support vector regression,2017,International Journal of Applied Engineering Research,12,24,,14652,14657,5.0,0,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85057643968&partnerID=40&md5=33bf53c02b592f78dfe57aae8a1e5502,"Visual attention and movement have evolved by many ways in eye tracking system. With the mechanism and functionality of eye movement, tracking human eye with high accuracy is possible. In eye tracking system, calibration technique can be considered as the first crucial step to be taken before eye can be tracked. The technique highly depends on a number of calibration points used and the se- lection of these points influences the overall performance of eye tracking system. Knowing the possibility, there are various methods of eye tracking with a good calibration process. However, determining the optimum number of calibration points is a huge challenge when developing eye tracking system. The research focus is then oriented towards determining an optimum number of calibration points by integrating the method of Support Vector Regression (SVR) with the conventional calibration technique. The proposed method should be able to reduce processing time while having high eye tracking accuracy. This paper describes the calibration process and analyses the possible optimum number of calibration points which will be able to reduce processing time and improve accuracy of eye tracking system. © Research India Publications.",Accuracy; Calibration system; Eye tracking; Processing time; Support vector regression,,Article,Final,,Scopus,2-s2.0-85057643968,Gaming / VR
Rosa P.J.; Gamito P.; Oliveira J.; Morais D.; Pavlovic M.; Smyth O.; Maia I.; Gomes T.,"Rosa, Pedro J. (7102096807); Gamito, Pedro (35217715700); Oliveira, Jorge (35243297300); Morais, Diogo (35242615600); Pavlovic, Matthew (57143743000); Smyth, Olivia (57144698800); Maia, Inês (57194205730); Gomes, Tiago (57194202674)",7102096807; 35217715700; 35243297300; 35242615600; 57143743000; 57144698800; 57194205730; 57194202674,Eye movement analysis and cognitive assessment: The use of comparative visual search tasks in a non-immersive vr application,2017,Methods of Information in Medicine,56,2,,112,116,4.0,7,10.3414/ME16-02-0006,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85019239461&doi=10.3414%2fME16-02-0006&partnerID=40&md5=6ed843dfd0812837da72e496bd05bb8f,"Background: An adequate behavioral response depends on attentional and mnesic processes. When these basic cognitive functions are impaired, the use of non-immersive Virtual Reality Applications (VRAs) can be a reliable technique for assessing the level of impairment. However, most non-immersive VRAs use indirect measures to make inferences about visual attention and mnesic processes (e.g., time to task completion, error rate). Objectives: To examine whether the eye movement analysis through eye tracking (ET) can be a reliable method to probe more effectively where and how attention is deployed and how it is linked with visual working memory during comparative visual search tasks (CVSTs) in non-immersive VRAs. Methods: The eye movements of 50 healthy participants were continuously recorded while CVSTs, selected from a set of cognitive tasks in the Systemic Lisbon Battery (SLB). Then a VRA designed to assess of cognitive impairments were randomly presented. Results: The total fixation duration, the number of visits in the areas of interest and in the interstimulus space, along with the total execution time was significantly different as a function of the Mini Mental State Examination (MMSE) scores. Conclusions: The present study demonstrates that CVSTs in SLB, when combined with ET, can be a reliable and unobtrusive method for assessing cognitive abilities in healthy individuals, opening it to potential use in clinical samples. © Schattauer 2017.",Attention; Comparative visual search task; Eye movements; Memory; Virtual reality,Adolescent; Adult; Cognition; Eye Movements; Female; Humans; Male; Middle Aged; Task Performance and Analysis; Time Factors; User-Computer Interface; Young Adult; adolescent; adult; cognition; comparative study; computer interface; eye movement; female; human; male; middle aged; physiology; task performance; time factor; young adult,Article,Final,All Open Access; Green Open Access,Scopus,2-s2.0-85019239461,Gaming / VR
Song H.; Lee J.; Kim T.J.; Lee K.H.; Kim B.; Seo J.,"Song, Hyunjoo (36141903000); Lee, Jeongjin (55894798200); Kim, Tae Jung (57754270000); Lee, Kyoung Ho (57203464195); Kim, Bohyoung (23389723000); Seo, Jinwook (35303195300)",36141903000; 55894798200; 57754270000; 57203464195; 23389723000; 35303195300,GazeDx: Interactive Visual Analytics Framework for Comparative Gaze Analysis with Volumetric Medical Images,2017,IEEE Transactions on Visualization and Computer Graphics,23,1,7539334,311,320,9.0,23,10.1109/TVCG.2016.2598796,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84998814343&doi=10.1109%2fTVCG.2016.2598796&partnerID=40&md5=46d476cb662a1e89d6f13aa6340c6ca4,"We present an interactive visual analytics framework, GazeDx (abbr. of GazeDiagnosis), for the comparative analysis of gaze data from multiple readers examining volumetric images while integrating important contextual information with the gaze data. Gaze pattern comparison is essential to understanding how radiologists examine medical images, and to identifying factors influencing the examination. Most prior work depended upon comparisons with manually juxtaposed static images of gaze tracking results. Comparative gaze analysis with volumetric images is more challenging due to the additional cognitive load on 3D perception. A recent study proposed a visualization design based on direct volume rendering (DVR) for visualizing gaze patterns in volumetric images; however, effective and comprehensive gaze pattern comparison is still challenging due to a lack of interactive visualization tools for comparative gaze analysis. We take the challenge with GazeDx while integrating crucial contextual information such as pupil size and windowing into the analysis process for more in-depth and ecologically valid findings. Among the interactive visualization components in GazeDx, a context-embedded interactive scatterplot is especially designed to help users examine abstract gaze data in diverse contexts by embedding medical imaging representations well known to radiologists in it. We present the results from two case studies with two experienced radiologists, where they compared the gaze patterns of 14 radiologists reading two patients' volumetric CT images. © 2016 IEEE.",context-embedded interactive scatterplot; Eye tracking; gaze pattern comparison; gaze visualization; interactive temporal chart; volumetric medical images,"Algorithms; Eye Movements; Humans; Imaging, Three-Dimensional; Medical Informatics; Radiography, Abdominal; Radiography, Thoracic; Radiologists; Tomography, X-Ray Computed; User-Computer Interface; Computerized tomography; Data visualization; Image analysis; Medical imaging; Visualization; Volume rendering; Contextual information; Direct volume rendering; gaze pattern comparison; interactive temporal chart; Interactive visualization tool; Interactive visualizations; Scatter plots; Visualization designs; abdominal radiography; algorithm; computer interface; eye movement; human; medical informatics; physiology; procedures; radiologist; thorax radiography; three dimensional imaging; x-ray computed tomography; Eye tracking",Article,Final,,Scopus,2-s2.0-84998814343,Gaming / VR
,,,"19th International Conference on Human-Computer Interaction, HCI International 2017",2017,Communications in Computer and Information Science,713,,,1,595,594.0,0,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85025135086&partnerID=40&md5=d5bd743f3a82206f34e9a040a39a9910,The proceedings contain 176 papers. The special focus in this conference is on Human-Computer Interaction. The topics include: Developing and evaluating a Thai website accessibility checker; usability tool to support the development process of e-commerce website; exploring the building blocks of personas for children with autism spectrum disorders; usability methods and evaluation criteria for published clinical guidelines on the web; the assessment tool for user perceived interactivity from ACG website interactivity on imagination; understanding game design for the development of a game environment; communication model of web accessibility; a study of the team management in design organizations; immersive 3D environment for data centre monitoring based on gesture based interaction; effects of electrode configuration on pattern recognition based finger movement classification; use of vibration for touch pen to provide the feel of writing on paper; enhancement of ANN-based offline hand written character recognition using gradient and geometric feature extraction techniques; shortening selection time using plural cursor in multi-display environment and its preliminary evaluation; creating a playful digital catalogue system using technology enhanced physical objects; automatic classification of eye blinks and eye movements for an input interface using eye motion; a pen gesture-based editing system for online handwritten objects on a pen computer; creating a gesture-speech dataset for speech based automatic gesture generation; stress measurement and inducement in experiments with low cost flight simulator for testing of general aviation pilots and attention value of motion graphics on digital signages.,,,Conference review,Final,,Scopus,2-s2.0-85025135086,Gaming / VR
Pfeiffer J.; Pfeiffer T.; Greif-Winzrieth A.; Meißner M.; Renner P.; Weinhardt C.,"Pfeiffer, Jella (19638908300); Pfeiffer, Thies (14027435500); Greif-Winzrieth, Anke (57195073006); Meißner, Martin (36621114600); Renner, Patrick (56145227600); Weinhardt, Christof (6604021298)",19638908300; 14027435500; 57195073006; 36621114600; 56145227600; 6604021298,Adapting human-computer-interaction of attentive smart glasses to the trade-off conflict in purchase decisions: An experiment in a virtual supermarket,2017,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),"10284 11th International Conference, AC 2017, Held as Part of HCI International 2017, Vancouver, BC, Canada, July 9-14, 2017, Proceedings, Part I",,,219,235,16.0,6,10.1007/978-3-319-58628-1_18,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85025157312&doi=10.1007%2f978-3-319-58628-1_18&partnerID=40&md5=dbbe8533a5fcfb399049000537db0030,"In many everyday purchase decisions, consumers have to trade-off their decisions between alternatives. For example, consumers often have to decide whether to buy the more expensive high quality product or the less expensive product of lower quality. Marketing researchers are especially interested in finding out how consumers make decisions when facing such trade-off conflicts and eye-tracking has been used as a tool to investigate the allocation of attention in such situations. Conflicting decision situations are also particularly interesting for human-computer interaction research because designers may use knowledge about the information acquisition behavior to build assistance systems which can help the user to solve the trade-off conflict. In this paper, we build and test such an assistance system that monitors the user’s information acquisition processes using mobile eye-tracking in the virtual reality. In particular, we test whether and how strongly the tradeoff conflict influences how consumers direct their attention to products and features. We find that trade-off conflict, task experience and task involvement significantly influence how much attention products receive. We discuss how this knowledge might be used in the future to build assistance systems in the form of attentive smart glasses. © Springer International Publishing AG 2017.",,Economic and social effects; Glass; Human computer interaction; Information use; Virtual reality; Allocation of attentions; Assistance system; Expensive products; Eye-tracking; High-quality products; Information acquisitions; Low qualities; Purchase decision; Smart glass; Trade off; Eye tracking,Conference paper,Final,,Scopus,2-s2.0-85025157312,Gaming / VR
Dhawan P.S.; Leong D.; Tapsell L.; Starling A.J.; Galetta S.L.; Balcer L.J.; Overall T.L.; Adler J.S.; Halker-Singh R.B.; Vargas B.B.; Dodick D.,"Dhawan, Priya S. (55498042100); Leong, Danielle (56061782200); Tapsell, Lisa (8365443500); Starling, Amaal J. (55889768300); Galetta, Steven L. (7005526795); Balcer, Laura J. (7004524080); Overall, Trenton L. (57201309820); Adler, Jennifer S. (56522403100); Halker-Singh, Rashmi B. (21734218000); Vargas, Bert B. (23570108000); Dodick, David (7005602717)",55498042100; 56061782200; 8365443500; 55889768300; 7005526795; 7004524080; 57201309820; 56522403100; 21734218000; 23570108000; 7005602717,King-Devick Test identifies real-Time concussion and asymptomatic concussion in youth athletes,2017,Neurology: Clinical Practice,7,6,,464,473,9.0,24,10.1212/CPJ.0000000000000381,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85044335574&doi=10.1212%2fCPJ.0000000000000381&partnerID=40&md5=71d06408313d8ee50fa28d525716a72f,"Background: Sports concussion has an annual incidence of approximately 3.8 million. Over half go unreported and a substantial number may be asymptomatic. A rapid, cost-effective, and reliable tool that facilitates diagnosis of concussion is needed. The King-Devick (K-D) test is a visionbased tool of rapid number naming for assessment of concussion. In this study, we evaluated the utility of the K-D test in real time for identification of symptomatic concussion in youth athletes and to determine if similar impairment (subclinical concussion) exists in youth athletes without an obvious head injury or symptoms. Methods: Youth hockey players underwent K-D testing preseason, postseason, and immediately after suspected concussion. Additional testing was performed in a subgroup of nonconcussed athletes immediately before and after a game to determine effects of fatigue on K-D scores. Results: Among 141 players tested, 20 had clinically diagnosed concussion. All 20 had immediate postconcussion K-D times .5 seconds from baseline (average 7.3 seconds) and all but 2 had worse postseason scores (46.4 seconds vs 52.4 seconds, p < 0.05, Wilcoxon signed rank test). Nonconcussed athletes saw minimal improvement postseason (43.9 seconds vs 42.1 seconds, p < 0.05) and 51 nonconcussed players assessed before and after a game revealed no significant time change as a result of fatigue. Conclusions: Rapid number naming using the K-D test accurately identifies realtime, symptomatic concussion in youth athletes. Scores in concussed players may remain abnormal over time. Athletes should undergo preseason and postseason K-D testing, with additional evaluation real time to inform the assessment of suspected concussion. Classification of Evidence: This study provides Class III evidence that the K-D test accurately identifies real-Time concussions in youth athletes. © 2017 American Academy of Neurology.",,adolescent; adult; Article; asymptomatic disease; attention test; brain concussion; brain cortex; brain stem; cerebellum; diagnostic test accuracy study; eye movement; fatigue; high school student; hockey player; human; King Devick test; language test; major clinical study; neurologic examination; predictive value; priority journal; receiver operating characteristic; sensitivity and specificity; sport injury; task performance; test retest reliability; vision test,Article,Final,All Open Access; Green Open Access,Scopus,2-s2.0-85044335574,Gaming / VR
Soler-Dominguez J.L.; Camba J.D.; Contero M.; Alcañiz M.,"Soler-Dominguez, Jose L. (55992360200); Camba, Jorge D. (7801470334); Contero, Manuel (6603334082); Alcañiz, Mariano (36921902100)",55992360200; 7801470334; 6603334082; 36921902100,A proposal for the selection of eye-tracking metrics for the implementation of adaptive gameplay in virtual reality based games,2017,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),10280,,,369,380,11.0,25,10.1007/978-3-319-57987-0_30,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85021665033&doi=10.1007%2f978-3-319-57987-0_30&partnerID=40&md5=acf091f86d74c8ad27398bf004b9d15b,"Eye tracking has been shown to be an effective method for evaluating usability and User eXperience (UX) in various environments. With the advent of affordable high performance Head Mounted Displays (HMDs), Virtual Reality (VR) has rapidly expanded and opened new channels for fun and “serious” game experiences. However, there is a lack of effective quality indicators that measure satisfaction, frustration, boredom, or attention of users immersed in VR environments. Traditional approaches such as questionnaires, surveys, and interviews can easily interfere with the user’s sense of presence if the assessment is performed while the experience is in progress. If performed after completing the experience, users are forced to recall memories that may be incomplete or inaccurate. In this context, the integration of eye tracking technology into HMDs provides a reliable and non-intrusive mechanism to assess UX and feed real-time data which can be used to design and develop adaptive VR experiences. In this paper, we discuss the assessment capabilities of eye tracking technologies in immersive VR environments and describe strategies to apply this information to the creation of VR games that can adapt to the player’s performance, mood and behavior. © 2017, Springer International Publishing AG.",Adaptive game design; Eye tracking; Games; Serious games; Virtual reality,Serious games; Virtual reality; Helmet mounted displays; Human computer interaction; Interactive computer graphics; Surveys; Adaptive games; Eye tracking technologies; Eye-tracking; Games; Head mounted displays; Quality indicators; Traditional approaches; User experiences (ux); Surveys; Virtual reality,Book chapter,Final,,Scopus,2-s2.0-85021665033,Gaming / VR
Rooney B.; Balint K.; Parsons T.D.; Burke C.; O’Leary T.; Lee S.C.T.; Mantei C.,"Rooney, Brendan (55007994600); Balint, Katalin (57189083580); Parsons, Thomas D. (57207899507); Burke, Colin (57191924985); O’Leary, Tess (59273982100); Lee, Sharon Chi Tak (57201198697); Mantei, Caroline (57201200460)",55007994600; 57189083580; 57207899507; 57191924985; 59273982100; 57201198697; 57201200460,Attention and social cognition in virtual reality: The effect of engagement mode and character eye-gaze,2017,Annual Review of CyberTherapy and Telemedicine,15,,,82,87,5.0,2,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85043783890&partnerID=40&md5=ff985484dfffca0cfac287688d88dc59,"Technical developments in virtual humans are manifest in modern character design. Specifically, eye gaze offers a significant aspect of such design. There is need to consider the contribution of participant control of engagement. In the current study, we manipulated participants’ engagement with an interactive virtual reality narrative called Coffee without Words. Participants sat over coffee opposite a character in a virtual café, where they waited for their bus to be repaired. We manipulated character eye-contact with the participant. For half the participants in each condition, the character made no eye-contact for the duration of the story. For the other half, the character responded to participant eye-gaze by making and holding eye contact in return. To explore how participant engagement interacted with this manipulation, half the participants in each condition were instructed to appraise their experience as an artefact (i.e., drawing attention to technical features), while the other half were introduced to the fictional character, the narrative, and the setting as though they were real. This study allowed us to explore the contributions of character features (interactivity through eye-gaze) and cognition (attention/engagement) to the participants’ perception of realism, feelings of presence, time duration, and the extent to which they engaged with the character and represented their mental states (Theory of Mind). Importantly it does so using a highly controlled yet ecologically valid virtual experience. © 2017, Interactive Media Institute. All rights reserved.",Attention; Social cognition; Virtual human; Virtual reality,adult; article; artifact; attention; coffee; drawing; female; gaze; human; human experiment; male; mental health; narrative; perception; social cognition; theory of mind; virtual reality,Article,Final,,Scopus,2-s2.0-85043783890,Gaming / VR
Pfeuffer K.; Alexander J.; Gellersen H.,"Pfeuffer, Ken (36141954200); Alexander, Jason (14035159600); Gellersen, Hans (6701531333)",36141954200; 14035159600; 6701531333,GazeArchers: Playing with individual and shared attention in a two-player look&shoot tabletop game,2016,ACM International Conference Proceeding Series,,,,213,216,3.0,22,10.1145/3012709.3012717,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85014875247&doi=10.1145%2f3012709.3012717&partnerID=40&md5=f61c1db8a15e5476037b937f62e11d30,"Gaze can complement touch on surfaces for fast target selection and occlusion-free input. In this work, we look beyond single-user application of gaze and touch and explore how gaze can be leveraged for collaborative use. We present the design of a two-player shooter game in which targets are gaze-aware and able to react differently to attention by one of the players versus shared attention of both players. The gameplay, evaluated in a study with 14 users, encourages users to adopt different strategies switching between individual and shared attention to achieve their collaborative goal. © 2016 ACM.",Eye gaze; Game; Joint attention; Multi user; Tabletop,Computer programming; Eye-gaze; Game; Joint attention; Multi-user; Tabletop; Computer applications,Conference paper,Final,All Open Access; Green Open Access,Scopus,2-s2.0-85014875247,Gaming / VR
Daly T.; Murphy J.; Anglin K.; Szalma J.; Acree M.; Landsberg C.; Bowens L.,"Daly, Tarah (57195070588); Murphy, Jennifer (36019271800); Anglin, Katlin (57189711437); Szalma, James (6603405857); Acree, Max (57195065097); Landsberg, Carla (37085268700); Bowens, Laticia (24068790700)",57195070588; 36019271800; 57189711437; 6603405857; 57195065097; 37085268700; 24068790700,Moving vigilance out of the laboratory: Dynamic scenarios for UAS operator vigilance training,2017,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),"10285 11th International Conference, AC 2017, Held as Part of HCI International 2017, Vancouver, BC, Canada, July 9-14, 2017, Proceedings, Part II",,,20,35,15.0,7,10.1007/978-3-319-58625-0_2,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85025125818&doi=10.1007%2f978-3-319-58625-0_2&partnerID=40&md5=867aa3cd0651f5f86621490b8d3e881e,"Our technology laden world continues to push the limits of human cognitive performance. Human performers are increasingly expected to assume roles of passive monitors rather than active engagers of technology systems [1]. Active and physical tasks have shifted to more sedentary tasks requiring significant cognitive workload at a rapid pace. Consequently, researchers and academics alike struggle to find a balance between effective user interface, usability, and ergonomic designs that will allow the performer to successfully complete their tasks while sustaining attention in these complex environments. It is no surprise that human error is at the root of tragic mishaps relating to vigilance across a wide range of applications and operational environments [2–4]. Researching vigilance is not new [5–7]. In fact, vigilance has been studied in laboratory settings for nearly seventy years across many conditions and tasks [8]. Traditional laboratory tasks involve static displays with simple image targets presented to individuals over prolonged periods of time. Participants are required to detect rare and temporally spaced targets among abundant “noise” images while sustaining their attention. The results using these vigilance tasks have found evidence of vigilance decrements, increased stress [7], and high cognitive demand [9]. The issue of training the skill to sustain attention has also been addressed [10, 11]. Findings from traditional research show that the most effective way of improving vigilance performance is through providing feedback in the form of knowledge of results [12]. Although the contrived, laboratory-based vigilance tasks can produce and mitigate the vigilance decrement, tasks that directly relate to complex operational environments are severely underrepresented in research. There have only been few researchers that utilize dynamic environments in vigilance research. For example, Szalma et al. [13] developed a video game-based training platform with the goal to extend the traditional vigilance training paradigm to complex, dynamic, and virtual environments that are more representative of visual detection tasks in the real world. Our current research is focused on extending the vigilance training paradigm to operationally relevant areas with the development of a game-based system for training operator attention within unmanned aerial systems (UAS). UAS are an integral part of mission operations within many branches of our military. New developments and improved technology allow extended mission operations of UAS up to, and exceeding 12 h. However, many UAS mishaps are the result of mechanical failures, and an alarming rate – 60.2% – of mishaps have been attributed to operator error [2]. This finding is not surprising, as UAS operations are highly cognitively demanding. Prolonged shiftwork and surveillance missions require sustained attention toward tracking or identifying rare targets, often in visually degraded conditions. This paper discusses current efforts to take the vigilance training paradigm out of the laboratory setting and into operational environments, including our current work in creating game-based training of vigilance for UAS operators. We describe the challenges associated with defining and standardizing targets, developing scenarios, and assessing performance. © Springer International Publishing AG 2017.",Game attributes; Game-based training; Operational environment; Sustaining attention; UAS training; Vigilance; Vigilance decrement,Antennas; Ergonomics; Human computer interaction; Personnel training; Unmanned aerial vehicles (UAV); User interfaces; Virtual reality; Game attribute; Game-based trainings; Games-based training; Operational environments; Sustaining attention; Systems trainings; Unmanned aerial system training; Unmanned aerial systems; Vigilance; Vigilance decrement; Laboratories,Conference paper,Final,,Scopus,2-s2.0-85025125818,Gaming / VR
Ganis G.; Bridges D.; Chun-Wei; Schendan H.E.,"Ganis, Giorgio (8151679600); Bridges, David (57191229219); Chun-Wei (57191221403); Schendan, Haline E. (6602996520)",8151679600; 57191229219; 57191221403; 6602996520,Is anterior N2 enhancement a reliable electrophysiological index of concealed information?,2016,NeuroImage,143,,,152,165,13.0,12,10.1016/j.neuroimage.2016.08.042,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84988005883&doi=10.1016%2fj.neuroimage.2016.08.042&partnerID=40&md5=ea85548e92fc789760310fee9c527f86,"Concealed information tests (CITs) are used to determine whether an individual possesses information about an item of interest. Event-related potential (ERP) measures in CITs have focused almost exclusively on the P3b component, showing that this component is larger when lying about the item of interest (probe) than telling the truth about control items (irrelevants). Recent studies have begun to examine other ERP components, such as the anterior N2, with mixed results. A seminal CIT study found that visual probes elicit a larger anterior N2 than irrelevants (Gamer and Berti, 2010) and suggested that this component indexes cognitive control processes engaged when lying about probes. However, this study did not control for potential intrinsic differences among the stimuli: the same probe and irrelevants were used for all participants, and there was no control condition composed of uninformed participants. Here, first we show that the N2 effect found in the study by Gamer and Berti (2010) was in large part due to stimulus differences, as the effect observed in a concealed information condition was comparable to that found in two matched control conditions without any concealed information (Experiments 1 and 2). Next, we addressed the issue of the generality of the N2 findings by counterbalancing a new set of stimuli across participants and by using a control condition with uninformed participants (Experiment 3). Results show that the probe did not elicit a larger anterior N2 than the irrelevants under these controlled conditions. These findings suggest that caution should be taken in using the N2 as an index of concealed information in CITs. Furthermore, they are a reminder that results of CIT studies (not only with ERPs) performed without stimulus counterbalancing and suitable control conditions may be confounded by differential intrinsic properties of the stimuli employed. © 2016 Elsevier Inc.",Cognitive control; Concealed information; Deception; Event-related potentials; N2,Adult; Cerebral Cortex; Deception; Electroencephalography; Evoked Potentials; Executive Function; Female; Humans; Male; Young Adult; Article; brain electrophysiology; cognition; concealed information test; concept formation; controlled study; deception; discrimination learning; electroencephalography; event related potential; eye movement; female; functional magnetic resonance imaging; human; human experiment; male; priority journal; receiver operating characteristic; response time; stimulus response; task performance; adult; brain cortex; deception; evoked response; executive function; physiology; procedures; young adult,Article,Final,All Open Access; Green Open Access,Scopus,2-s2.0-84988005883,Gaming / VR
Myrden A.; Chau T.,"Myrden, A. (49961942000); Chau, T. (26643317800)",49961942000; 26643317800,Towards psychologically adaptive brain-computer interfaces,2016,Journal of Neural Engineering,13,6,66022,,,,20,10.1088/1741-2560/13/6/066022,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85002335957&doi=10.1088%2f1741-2560%2f13%2f6%2f066022&partnerID=40&md5=d48bfe671f1851f2f5c5a3f1c8075756,"Objective. Brain-computer interface (BCI) performance is sensitive to short-term changes in psychological states such as fatigue, frustration, and attention. This paper explores the design of a BCI that can adapt to these short-term changes. Approach. Eleven able-bodied individuals participated in a study during which they used a mental task-based EEG-BCI to play a simple maze navigation game while self-reporting their perceived levels of fatigue, frustration, and attention. In an offline analysis, a regression algorithm was trained to predict changes in these states, yielding Pearson correlation coefficients in excess of 0.45 between the self-reported and predicted states. Two means of fusing the resultant mental state predictions with mental task classification were investigated. First, single-trial mental state predictions were used to predict correct classification by the BCI during each trial. Second, an adaptive BCI was designed that retrained a new classifier for each testing sample using only those training samples for which predicted mental state was similar to that predicted for the current testing sample. Main results. Mental state-based prediction of BCI reliability exceeded chance levels. The adaptive BCI exhibited significant, but practically modest, increases in classification accuracy for five of 11 participants and no significant difference for the remaining six despite a smaller average training set size. Significance. Collectively, these findings indicate that adaptation to psychological state may allow the design of more accurate BCIs. © 2016 IOP Publishing Ltd.",brain-computer interfaces; electroencephalography; mental state,"Adaptation, Psychological; Adult; Algorithms; Attention; Brain-Computer Interfaces; Electroencephalography; Equipment Design; Female; Frustration; Humans; Male; Maze Learning; Mental Fatigue; Psychomotor Performance; Reproducibility of Results; Classification (of information); Correlation methods; Electroencephalography; Electrophysiology; Forecasting; Interface states; Interfaces (computer); Classification accuracy; Mental state; Mental task classification; Off-line analysis; Pearson correlation coefficients; Psychological state; Regression algorithms; Testing samples; adult; Article; attention; blinking; brain computer interface; clinical article; diagnostic accuracy; electroencephalogram; eye movement; fatigue; frustration; human; male; maze test; mental health; mental task; prediction; priority journal; psychological aspect; self report; signal processing; algorithm; coping behavior; dysthymia; electroencephalography; equipment design; female; physiology; psychology; psychomotor performance; reproducibility; Brain computer interface",Article,Final,,Scopus,2-s2.0-85002335957,Gaming / VR
Lee J.; Cheon M.; Moon S.-E.; Lee J.-S.,"Lee, Jooyeon (57188765223); Cheon, Manri (55938054900); Moon, Seong-Eun (56452144400); Lee, Jong-Seok (36062379400)",57188765223; 55938054900; 56452144400; 36062379400,Peripersonal space in virtual reality: Navigating 3D space with different perspectives,2016,UIST 2016 Adjunct - Proceedings of the 29th Annual Symposium on User Interface Software and Technology,,,,207,208,1.0,10,10.1145/2984751.2984772,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84995663200&doi=10.1145%2f2984751.2984772&partnerID=40&md5=73ff1da6777a34c1dd35b1cb7ad28559,"We introduce the concept of ""peripersonal space"" of an avatar in 3D virtual reality and discuss how it plays an important role on 3D navigation with different perspectives. By analyzing the eye-gaze data of avatar-based navigation with first-person perspective and third-person perspective, we examine the effects of an avatar's peripersonal space on the users' perceptual scopes within 3D virtual environments. We propose that manipulating peripersonal space of an avatar with various perspectives has the immediate effects on the users' scopes of perception as well as the patterns of attentional capture. This study provides a helpful guideline for designing more effective navigation system with an avatar in 3D virtual environment. © 2016 Copyright held by the author/owner(s).",Eye-tracking; Gaze Analysis; Human Perception and Cognition; Navigation; Peripersonal Space; Perspective; Virtual Space,Navigation; Navigation systems; Three dimensional computer graphics; User interfaces; Eye-tracking; Gaze analysis; Human perception; Peripersonal Space; Perspective; Virtual spaces; Virtual reality,Conference paper,Final,,Scopus,2-s2.0-84995663200,Gaming / VR
Farran E.K.; Formby S.; Daniyal F.; Holmes T.; Van Herwegen J.,"Farran, E.K. (6602319057); Formby, S. (56493136400); Daniyal, F. (57191191665); Holmes, T. (57215098375); Van Herwegen, J. (14631196700)",6602319057; 56493136400; 57191191665; 57215098375; 14631196700,Route-learning strategies in typical and atypical development; eye tracking reveals atypical landmark selection in Williams syndrome,2016,Journal of Intellectual Disability Research,60,10,,933,944,11.0,21,10.1111/jir.12331,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84987811288&doi=10.1111%2fjir.12331&partnerID=40&md5=95fd402976d459f8c9c51feedbce8cc1,"Background: Successful navigation is crucial to everyday life. Individuals with Williams syndrome (WS) have impaired spatial abilities. This includes a deficit in spatial navigation abilities such as learning the route from A to B. To-date, to determine whether participants attend to landmarks when learning a route, landmark recall tasks have been employed after the route learning experience. Here, we combined virtual reality and eye tracking technologies, for the first time, to measure landmark use in typically developing (TD) children and participants with WS during route-learning. Method: Nineteen individuals with WS were asked to learn a route in a sparse environment (few landmarks) and in a rich environment (many landmarks) whilst their eye movements were recorded. Looking times towards landmarks were compared to TD children aged 6, 8 and 10 years. Changes in attention to landmarks during the learning process were also recorded. Results: The WS group made fewer looks to landmarks overall, but all participants looked for longer at landmarks that were at junctions and along the paths of the maze than landmarks that were in the distance. Few differences were observed in route learning between the sparse and rich environments. In contrast to the TD groups, those in the WS group were as likely to look at non-unique landmarks as landmarks at junctions and on paths. Discussion: The current results demonstrate that attention to landmarks during route learning reflects the types of landmarks remembered in memory tasks, that individuals with WS can learn a route if given sufficient exposure, but that this is accomplished within the context of an impaired ability to select appropriate landmarks. © 2016 MENCAP and International Association of the Scientific Study of Intellectual and Developmental Disabilities and John Wiley & Sons Ltd",eye tracking; navigation; spatial cognition; Williams syndrome,adolescent; adult; attention; child; child development; clinical article; cognition; Conference Paper; development; eye tracking; human; memory; spatial learning; virtual reality; Williams Beuren syndrome,Conference paper,Final,All Open Access; Green Open Access,Scopus,2-s2.0-84987811288,Gaming / VR
Tula A.D.; Kurauchi A.; Coutinho F.; Morimoto C.,"Tula, Antonio Diaz (55848005200); Kurauchi, Andrew (57095009900); Coutinho, Flávio (22233254400); Morimoto, Carlos (7102275798)",55848005200; 57095009900; 22233254400; 7102275798,Heatmap explorer: An interactive gaze data visualization tool for the evaluation of computer interfaces,2016,ACM International Conference Proceeding Series,Part F128046,,a24,,,,7,10.1145/3033701.3033725,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85021654553&doi=10.1145%2f3033701.3033725&partnerID=40&md5=13729033e0c129f180122a9a1ce2c0a7,"Eye gaze is an important source of information to evaluate computer interfaces. Typically, visualization of gaze data is performed using heatmaps and gaze scanpaths displayed on top of images of the interface, enhancing regions that have attracted the user's visual attention. Such tools work well for static interfaces but they are not appropriate to visualize dynamic interfaces where the object of interaction is always changing, such as games, web browsing, or even common applications that change the interface according to the status of the application. In this paper we introduce an interactive tool to explore the spatial-temporal distribution of visual attention called Heatmap Explorer (HME). HME allows the experimenter to control the visualization by selecting temporal intervals and adjusting filter parameters of the eye movement classification algorithm. We show results of three typical application scenarios and discuss how HME can be an effective usability evaluation tool. © 2016 ACM.",Eye gaze visualization; Heatmap Explorer; Information visualization; User interface evaluation,Eye movements; Human engineering; Information systems; User interfaces; Visualization; Data visualization tools; Eye movement classifications; Eye-gaze; Heatmap Explorer; Information visualization; Interface evaluation; Spatial-temporal distribution; Usability evaluation; Data visualization,Conference paper,Final,,Scopus,2-s2.0-85021654553,Gaming / VR
Castaneda L.; Sidhu M.K.; Azose J.J.; Swanson T.,"Castaneda, Lisa (57189055915); Sidhu, Manrita Kaur (58359129400); Azose, Jonathan J. (56835790400); Swanson, Tom (57193354929)",57189055915; 58359129400; 56835790400; 57193354929,"Game play differences by expertise level in Dota 2, a complex multiplayer video game",2016,International Journal of Gaming and Computer-Mediated Simulations,8,4,,1,24,23.0,16,10.4018/IJGCMS.2016100101,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85013339793&doi=10.4018%2fIJGCMS.2016100101&partnerID=40&md5=0a9ec81824779afb05744a76b7624d20,"Dota 2, a complex team based video game, was used to study expertise and attentional allocation in a multiplayer online battle arena (MOBA) setting. Pre- and post-play survey questions and eyetracker data were collected from 67 video game players during a session of Dota 2 play. Questions explored abstract versus concrete conceptualizations of game-play and individual versus team focus. Quantitative eye-tracker data was evaluated for differences in visual attention and scan patterns. The authors noted that novices reflected on more concrete game elements and were likely to look back at the same location twice in a row. There was no difference among player categories in amount of time looking at mini-map or in self vs. team focus; however, experts were more able to reflect on abstract game concepts. Expert-novice differences in this study are similar to expertise research findings from other domains. The qualitative and unique quantitative metrics that can be gathered from complex games may provide insight into the development of expertise. Copyright © 2016, IGI Global.",Complex games; Dota 2; Expertise; Eye-tracking; MOBA; Multiplayer; Scan patterns; Video game; Visual attention,Behavioral research; Concretes; Complex games; Dota 2; Expertise; Eye-tracking; MOBA; Multiplayers; Scan patterns; Video game; Visual Attention; Human computer interaction,Article,Final,,Scopus,2-s2.0-85013339793,Gaming / VR
Toivanen M.; Häkkinen J.; Puolamäki K.; Radun J.; Lukander K.,"Toivanen, Miika (11840148200); Häkkinen, Jukka (7007052231); Puolamäki, Kai (12141481400); Radun, Jenni (12760114200); Lukander, Kristian (34976891300)",11840148200; 7007052231; 12141481400; 12760114200; 34976891300,Inferring user action with mobile gaze tracking,2016,"Proceedings of the 18th International Conference on Human-Computer Interaction with Mobile Devices and Services Adjunct, MobileHCI 2016",,,,1026,1028,2.0,2,10.1145/2957265.2965016,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84991093290&doi=10.1145%2f2957265.2965016&partnerID=40&md5=2287aca7b8c4191127b85260c9f02af0,"Gaze tracking in psychological, cognitive, and user interaction studies has recently evolved toward mobile solutions, as they enable direct assessing of users' visual attention in natural environments, and augmented and virtual reality (AR/VR) applications. Productive approaches in analyzing and predicting user actions with gaze data require a multi-disciplinary approach with experts in cognitive and behavioral sciences, machine vision, and machine learning. This workshop brings together a cross-domain group of individuals to (i) discuss and contribute to the problem of using mobile gaze tracking for inferring user action, (ii) advance the sharing of data and analysis algorithms as well as device solutions, and (iii) increase understanding of behavioral aspects of gaze-action sequences in natural environments and AR/VR applications.",Action inference; Augmented reality; Behavioral analysis; Gaze tracking algorithms; Machine learning; Mobile gaze tracking; Natural environments; Virtual reality,Artificial intelligence; Augmented reality; Behavioral research; Computer vision; Inference engines; Learning systems; Mobile devices; Tracking (position); Virtual reality; Action inference; Analysis algorithms; Augmented and virtual realities; Behavioral analysis; Behavioral science; Gaze tracking; Multi-disciplinary approach; Natural environments; Human computer interaction,Conference paper,Final,,Scopus,2-s2.0-84991093290,Gaming / VR
Geiselhart F.; Rietzler M.; Rukzio E.,"Geiselhart, Florian (56940701200); Rietzler, Michael (55877208500); Rukzio, Enrico (18233783900)",56940701200; 55877208500; 18233783900,EyeVR-Low-Cost VR eye-based interaction,2016,UbiComp 2016 Adjunct - Proceedings of the 2016 ACM International Joint Conference on Pervasive and Ubiquitous Computing,,,,277,280,3.0,6,10.1145/2968219.2971384,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84991071274&doi=10.1145%2f2968219.2971384&partnerID=40&md5=703fba4fd29ad82393ae8886a8d32ace,"The EyeVR system enables eye and gaze interactions in VR glasses at a price below 100, while providing sufficient performance for many typical VR use cases, like foveated rendering, gaming, or attention-based storytelling. It is based on off-The-shelf hardware like a Raspberry Pi, can be used in wireless, mobile settings and allows for a widespread use of gaze tracking in different applications through open interfaces. Besides standard gaze tracking, another focus of the system lies on exploring new forms of interactions besides only gaze direction, by providing additional details about the eye like pupil size or eyelid movement. © 2016 ACM.",Eye tracking; Gaze tracking; Mobile VR.; VR Interaction,Eye movements; Tracking (position); Eye-tracking; Gaze interaction; Gaze tracking; Mobile settings; Mobile VR; Off-the-shelf hardwares; Open Interface; VR Interaction; Ubiquitous computing,Conference paper,Final,,Scopus,2-s2.0-84991071274,Gaming / VR
Luxenburger A.; Moniri M.M.; Prange A.; Sonntag D.,"Luxenburger, Andreas (57194645943); Moniri, Mohammad Mehdi (35096486100); Prange, Alexander (57127695000); Sonntag, Daniel (12241487800)",57194645943; 35096486100; 57127695000; 12241487800,MedicalVR: Towards medical remote collaboration using virtual reality,2016,UbiComp 2016 Adjunct - Proceedings of the 2016 ACM International Joint Conference on Pervasive and Ubiquitous Computing,,,,321,324,3.0,14,10.1145/2968219.2971392,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84991079039&doi=10.1145%2f2968219.2971392&partnerID=40&md5=a491eaf15d9a3ea5bf5331f09870a0aa,"We present a virtual reality framework and assistive tool for design practices for new medical environments, grounded on human visual perception, attention and action. This includes an interactive visualization of shared electronic patient records, previously acquired with a remote tablet device, in a virtual environment incorporating hand tracking, eye tracking and a vision-based peripheral view monitoring. The goal is to influence medical environments' affordances, especially for e-health and m-health applications as well as user experience and design conception for tele-medicine. © 2016 ACM.",Assistive Technologies; Collaboration; Eye Tracking; Hand Tracking; Medical Virtual Reality; Multi-Modal Interaction,Palmprint recognition; Remote patient monitoring; Ubiquitous computing; Visualization; Assistive technology; Collaboration; Eye-tracking; Hand tracking; Multi-Modal Interactions; Virtual reality,Conference paper,Final,,Scopus,2-s2.0-84991079039,Gaming / VR
van Wyk A.; Eksteen C.A.; Becker P.J.; Heinze B.M.,"van Wyk, Andoret (56405276900); Eksteen, Carina A. (56405561300); Becker, Piet J. (35599924500); Heinze, Barbara M. (7004093325)",56405276900; 56405561300; 35599924500; 7004093325,A cross-sectional survey and cross-sectional clinical trial to determine the prevalence and management of eye movement disorders and vestibular dysfunction in post-stroke patients in the sub-acute phase: Protocol,2016,Frontiers in Neurology,7,SEP,140,,,,9,10.3389/fneur.2016.00140,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84992126269&doi=10.3389%2ffneur.2016.00140&partnerID=40&md5=29da0988d557ff072dab55d6a2ce8c11,"Introduction: Visual impairment, specifically eye movement disorders and vestibular dysfunction may have a negative influence on the functional recovery in post-stroke patients. This type of sensory dysfunction may further be associated with poor functional outcome in patients' post-stroke. Methods: In phase 1, a cross-sectional survey (n = 100) will be conducted to determine the prevalence of eye movement disorders and vestibular dysfunction in patients who sustained a stroke. A cross-sectional clinical trial (n = 60) will be conducted during phase 2 of the study to determine the effect of the combination of vestibular rehabilitation therapy (VRT) and visual scanning exercises (VSE) (experimental group) integrated with task-specific activities compared with the effect of task-specific activities as an intervention (control group) on patients who present with eye movement impairment and central vestibular dysfunction post-stroke. An audiologist will assess (a) visual acuity (static and dynamic), (b) nystagmus, (c) saccadic eye movements, (d) smooth pursuit eye movements, (e) vestibulo-ocular reflex, and (f) saccular, utricular, and vestibular nerve function. An independent physiotherapist will assess (1) cognitive function, (2) residual oculomotor visual performance, (3) visual-perceptual system, (4) functional balance, (5) gait, (6) functional ability, (7) presence of anxiety and/or depression, and (8) level of participation in physical activity. Ethics and dissemination: Ethics approval has been obtained from the Ethics Committee of the Faculty of Health Sciences at the University of Pretoria (UP) (374/2015). The study will be submitted as fulfillment for the PhD degree at UP. Dissemination will include submission to peer-reviewed professional journals and presentation at congresses. Training of rehabilitation team members on the integration of VSE and VRT into task-specific activities in rehabilitation will be done if the outcome of the experimental group's functional performance is clinically and statistically significantly better than the control group on the Barthel Index. © 2016 van Wyk, Eksteen, Becker and Heinze.",Eye movement disorders; Physiotherapy; Prevalence; Rehabilitation; Stroke; Vestibular dysfunction; Visual impairment,anxiety; Article; body equilibrium; cerebrovascular accident; cognition; controlled study; cross-sectional study; depression; eye movement control; eye movement disorder; functional status; gait; human; neurorehabilitation; nystagmus; physical activity; prevalence; randomized controlled trial; randomized controlled trial (topic); saccadic eye movement; smooth pursuit eye movement; vestibular disorder; vestibular nerve; vestibular rehabilitation therapy; vestibuloocular reflex; visual acuity; visual scanning exercise,Article,Final,All Open Access; Gold Open Access; Green Open Access,Scopus,2-s2.0-84992126269,Gaming / VR
Belov D.R.; Milutina E.A.; Kolodiazhnyi S.F.,"Belov, D.R. (7004241404); Milutina, E.A. (57204063116); Kolodiazhnyi, S.F. (6505470411)",7004241404; 57204063116; 6505470411,SACCADES AND PRESACCADIC SPIKE POTENTIAL DURING THE GAME OF TETRIS,2016,Rossiiskii fiziologicheskii zhurnal imeni I.M. Sechenova,102,10,,1233,1245,12.0,1,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85054375001&partnerID=40&md5=cd56d3bf79de9496305c3f692cf2501e,"Saccades and microsaccades were studied during a game of Tetris using the method of EOG. In the experiments 19 players of different skill level were involved. EOG was recorded in monopolar mode with respect to the ear electrodes in 6-points around both eyes to account for the gaze displacement in 8-direction. The game of Tetris was on 3 difficulty levels. It is shown that each saccade on EOG preceded by a sharp negative potential of 10-15 ms or «pre-saccadic peak». Its parameters are very stable at the given person, while EOG parameters of themselves saccade vary from their direction. When bipolar registration peak is not detected. The peak amplitude does not depend on the saccade direction, but correlates with saccade amplitude. It is assumed that the peak generators are neuromuscular synapses. Everything else being equal men were characterized by shorter saccades and greater percentage of microsaccades than women. On the middle level of difficulty at the optimal level of attention (but without the stress) are observed smallest saccades. For all players have dominated vertical saccades from the block to the glassful bottom and backward. In the transition from light to hard level for experienced players is increasing the proportion of «extra» non-vertical saccades, while the proportion of beginners «extra» saccades always relatively high.",,Action Potentials; Adult; Electrooculography; Female; Humans; Male; Saccades; Video Games; action potential; adult; electrooculography; female; human; male; physiology; saccadic eye movement; video game,Article,Final,,Scopus,2-s2.0-85054375001,Gaming / VR
Moniri M.M.; Luxenburger A.; Sonntag D.,"Moniri, Mohammad Mehdi (35096486100); Luxenburger, Andreas (57194645943); Sonntag, Daniel (12241487800)",35096486100; 57194645943; 12241487800,Peripheral view calculation in virtual reality applications,2016,UbiComp 2016 Adjunct - Proceedings of the 2016 ACM International Joint Conference on Pervasive and Ubiquitous Computing,,,,333,336,3.0,2,10.1145/2968219.2971391,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84991100110&doi=10.1145%2f2968219.2971391&partnerID=40&md5=842ec3325a63bec0d0e87721412a644b,"We present an application based on a general peripheral view calculation model which extends previous work on attention-based user interfaces that use eye gaze. An intuitive, two dimensional visibility measure based on the concept of solid angle is developed. We determine to which extent an object of interest, observed by a user, intersects with each region of the underlying visual field model. The results are weighted (thereby considering the visual acuity in each visual field) to determine the total visibility of the object. As a proof of concept, we exemplify the proposed model in a virtual reality application which incorporates a head-mounted display with integrated eye tracking functionality. In this context, we implement several proactive system behaviors including contextual information presentation with an adaptive level of detail and attention guidance; the latter is implemented by detecting visual acuity limitations or attention drifts. © 2016 ACM.",Eye Tracking; Gazebased Interactive Technology; Human Peripheral Vision; Proactive Systems; Solid Angle,Helmet mounted displays; User interfaces; Virtual reality; Visibility; Vision; Eye-tracking; Interactive technology; Peripheral vision; Proactive systems; Solid angle; Ubiquitous computing,Conference paper,Final,,Scopus,2-s2.0-84991100110,Gaming / VR
Quinlivan B.; Butler J.S.; Beiser I.; Williams L.; McGovern E.; O'Riordan S.; Hutchinson M.; Reilly R.B.,"Quinlivan, Brendan (56728607900); Butler, John S. (35110895000); Beiser, Ines (56801048100); Williams, Laura (56799634400); McGovern, Eavan (36537458000); O'Riordan, Sean (55893470600); Hutchinson, Michael (7201506609); Reilly, Richard B. (7102935766)",56728607900; 35110895000; 56801048100; 56799634400; 36537458000; 55893470600; 7201506609; 7102935766,Application of virtual reality head mounted display for investigation of movement: A novel effect of orientation of attention,2016,Journal of Neural Engineering,13,5,56006,,,,13,10.1088/1741-2560/13/5/056006,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84989838321&doi=10.1088%2f1741-2560%2f13%2f5%2f056006&partnerID=40&md5=87fc4e6b2e677fa85d39284480590aee,"Objective. To date human kinematics research has relied on video processing, motion capture and magnetic search coil data acquisition techniques. However, the use of head mounted display virtual reality systems, as a novel research tool, could facilitate novel studies into human movement and movement disorders. These systems have the unique ability of presenting immersive 3D stimulus while also allowing participants to make ecologically valid movement-based responses. Approach. We employed one such system (Oculus Rift DK2) in this study to present visual stimulus and acquire head-turn data from a cohort of 40 healthy adults. Participants were asked to complete head movements towards eccentrically located visual targets following valid and invalid cues. Such tasks are commonly employed for investigating the effects orientation of attention and are known as Posner cueing paradigms. Electrooculography was also recorded for a subset of 18 participants. Main results. A delay was observed in onset of head movement and saccade onset during invalid trials, both at the group and single participant level. We found that participants initiated head turns 57.4 ms earlier during valid trials. A strong relationship between saccade onset and head movement onset was also observed during valid trials. Significance. This work represents the first time that the Posner cueing effect has been observed in onset of head movement in humans. The results presented here highlight the role of head-mounted display systems as a novel and practical research tool for investigations of normal and abnormal movement patterns. © 2016 IOP Publishing Ltd.",attention; head turn; HMD; movement; Posner; virtual reality,"Adult; Attention; Cohort Studies; Cues; Electrooculography; Female; Head; Head Movements; Humans; Male; Orientation; Phantoms, Imaging; Psychomotor Performance; Saccades; Virtual Reality; Young Adult; Data acquisition; Display devices; Video signal processing; Virtual reality; Abnormal movement; attention; Head mounted display systems; Head mounted displays; movement; Movement disorders; Posner; Virtual reality system; adult; Article; association; attention; cell differentiation; comparative study; controlled study; electrooculography; head movement; human; human experiment; male; normal human; priority journal; saccadic eye movement; velocity; virtual reality; visual stimulation; anatomy and histology; attention; cohort analysis; female; head; head movement; imaging phantom; orientation; physiology; psychomotor performance; young adult; Helmet mounted displays",Article,Final,,Scopus,2-s2.0-84989838321,Gaming / VR
Van Der Togt C.; StǍnişor L.; Pooresmaeili A.; Albantakis L.; Deco G.; Roelfsema P.R.,"Van Der Togt, Chris (6603558285); StǍnişor, Liviu (57190573154); Pooresmaeili, Arezoo (26633313600); Albantakis, Larissa (26667568400); Deco, Gustavo (7006674531); Roelfsema, Pieter R. (7007150730)",6603558285; 57190573154; 26633313600; 26667568400; 7006674531; 7007150730,Learning a New Selection Rule in Visual and Frontal Cortex,2016,Cerebral Cortex,26,8,,3611,3626,15.0,1,10.1093/cercor/bhw155,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84981187606&doi=10.1093%2fcercor%2fbhw155&partnerID=40&md5=1695d1cb85d4ee14dfe38cc1839dcee2,"How do you make a decision if you do not know the rules of the game? Models of sensory decision-making suggest that choices are slow if evidence is weak, but they may only apply if the subject knows the task rules. Here, we asked how the learning of a new rule influences neuronal activity in the visual (area V1) and frontal cortex (area FEF) of monkeys. We devised a new icon-selection task. On each day, the monkeys saw 2 new icons (small pictures) and learned which one was relevant. We rewarded eye movements to a saccade target connected to the relevant icon with a curve. Neurons in visual and frontal cortex coded the monkey's choice, because the representation of the selected curve was enhanced. Learning delayed the neuronal selection signals and we uncovered the cause of this delay in V1, where learning to select the relevant icon caused an early suppression of surrounding image elements. These results demonstrate that the learning of a new rule causes a transition from fast and random decisions to a more considerate strategy that takes additional time and they reveal the contribution of visual and frontal cortex to the learning process. © 2016 Published by Oxford University Press.",frontal eye fields; learning; primary visual cortex; visual attention; visual routines,Action Potentials; Animals; Choice Behavior; Eye Movement Measurements; Frontal Lobe; Haplorhini; Learning; Microelectrodes; Neurons; Reward; Saccades; Visual Cortex; Visual Perception; animal experiment; Article; attention; brain nerve cell; controlled study; decision making; frontal cortex; Haplorhini; learning; nonhuman; priority journal; response time; reward; saccadic eye movement; striate cortex; task performance; action potential; animal; frontal lobe; learning; microelectrode; nerve cell; oculography; physiology; vision; visual cortex,Article,Final,All Open Access; Green Open Access; Hybrid Gold Open Access,Scopus,2-s2.0-84981187606,Gaming / VR
Keefe B.D.; Wincenciak J.; Jellema T.; Ward J.W.; Barraclough N.E.,"Keefe, Bruce D. (36108435000); Wincenciak, Joanna (55768887500); Jellema, Tjeerd (6601990081); Ward, James W. (36119465400); Barraclough, Nick E. (8300619800)",36108435000; 55768887500; 6601990081; 36119465400; 8300619800,Action adaptation during natural unfolding social scenes influences action recognition and inferences made about actor beliefs,2016,Journal of Vision,16,9,,1,20,19.0,3,10.1167/16.9.9,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84992529846&doi=10.1167%2f16.9.9&partnerID=40&md5=378338982ce0cb05a310f881f0b00ce6,"When observing another individual's actions, we can both recognize their actions and infer their beliefs concerning the physical and social environment. The extent to which visual adaptation influences action recognition and conceptually later stages of processing involved in deriving the belief state of the actor remains unknown. To explore this we used virtual reality (life-size photorealistic actors presented in stereoscopic three dimensions) to see how visual adaptation influences the perception of individuals in naturally unfolding social scenes at increasingly higher levels of action understanding. We presented scenes in which one actor picked up boxes (of varying number and weight), after which a second actor picked up a single box. Adaptation to the first actor's behavior systematically changed perception of the second actor. Aftereffects increased with the duration of the first actor's behavior, declined exponentially over time, and were independent of view direction. Inferences about the second actor's expectation of box weight were also distorted by adaptation to the first actor. Distortions in action recognition and actor expectations did not, however, extend across different actions, indicating that adaptation is not acting at an action-independent abstract level but rather at an action-dependent level. We conclude that although adaptation influences more complex inferences about belief states of individuals, this is likely to be a result of adaptation at an earlier action recognition stage rather than adaptation operating at a higher, more abstract level in mentalizing or simulation systems.",,"Adaptation, Physiological; Attention; Cues; Eye Movements; Female; Fixation, Ocular; Humans; Male; Pattern Recognition, Visual; Pilot Projects; Young Adult; adaptation; association; attention; eye fixation; eye movement; female; human; male; pattern recognition; physiology; pilot study; young adult",Article,Final,All Open Access; Gold Open Access; Green Open Access,Scopus,2-s2.0-84992529846,Gaming / VR
Khoramshahi M.; Shukla A.; Raffard S.; Bardy B.G.; Billard A.,"Khoramshahi, Mahdi (55841144900); Shukla, Ashwini (50462350700); Raffard, Stéphane (23568550800); Bardy, Benoît G. (7004216646); Billard, Aude (7006389948)",55841144900; 50462350700; 23568550800; 7004216646; 7006389948,Role of gaze cues in interpersonal motor coordination: Towards higher affiliation in human-robot interaction,2016,PLoS ONE,11,6,e0156874,,,,24,10.1371/journal.pone.0156874,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84976293898&doi=10.1371%2fjournal.pone.0156874&partnerID=40&md5=fa8ee0fd9343304b24cbc17a6905b1db,"Background: The ability to follow one another's gaze plays an important role in our social cognition; especially when we synchronously perform tasks together. We investigate how gaze cues can improve performance in a simple coordination task (i.e., the mirror game), whereby two players mirror each other's hand motions. In this game, each player is either a leader or follower. To study the effect of gaze in a systematic manner, the leader's role is played by a robotic avatar. We contrast two conditions, in which the avatar provides or not explicit gaze cues that indicate the next location of its hand. Specifically, we investigated (a) whether participants are able to exploit these gaze cues to improve their coordination, (b) how gaze cues affect action prediction and temporal coordination, and (c) whether introducing active gaze behavior for avatars makes them more realistic and human-like (from the user point of view). Methodology/Principal Findings: 43 subjects participated in 8 trials of the mirror game. Each subject performed the game in the two conditions (with and without gaze cues). In this within-subject study, the order of the conditions was randomized across participants, and subjective assessment of the avatar's realism was assessed by administering a post-hoc questionnaire. When gaze cues were provided, a quantitative assessment of synchrony between participants and the avatar revealed a significant improvement in subject reaction-time (RT). This confirms our hypothesis that gaze cues improve the follower's ability to predict the avatar's action. An analysis of the pattern of frequency across the two players' hand movements reveals that the gaze cues improve the overall temporal coordination across the two players. Finally, analysis of the subjective evaluations from the questionnaires reveals that, in the presence of gaze cues, participants found it not only more human-like/realistic, but also easier to interact with the avatar. Conclusion/Significance: This work confirms that people can exploit gaze cues to predict another person's movements and to better coordinate their motions with their partners, even when the partner is a computer-animated avatar. Moreover, this study contributes further evidence that implementing biological features, here task-relevant gaze cues, enable the humanoid robotic avatar to appear more human-like, and thus increase the user's sense of affiliation. © 2016 Khoramshahi et al. This is an open access article distributed under the terms of the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.",,Adult; Biomimetics; Computer Simulation; Computers; Cues; Discrimination (Psychology); Eye Movements; Female; Hand; Humans; Interpersonal Relations; Male; Movement; Reaction Time; Reality Testing; Robotics; Self Concept; Social Behavior; Young Adult; clinical article; clinical trial; controlled clinical trial; gaze; hand movement; human; leadership; motion; motor coordination; prediction; quantitative study; questionnaire; randomized controlled trial; reaction time; robotics; social cognition; adult; association; biomimetics; computer; computer simulation; ego; eye movement; female; hand; human relation; male; movement (physiology); perceptive discrimination; physiology; self concept; social behavior; young adult,Article,Final,All Open Access; Gold Open Access; Green Open Access,Scopus,2-s2.0-84976293898,Gaming / VR
Ahmadi H.; Tootaghaj S.Z.; Mowlaei S.; Hashemi M.R.; Shirmohammadi S.,"Ahmadi, Hamed (56377313500); Tootaghaj, Saman Zad (56191312100); Mowlaei, Sajad (57189662934); Hashemi, Mahmoud Reza (7005774450); Shirmohammadi, Shervin (6602324454)",56377313500; 56191312100; 57189662934; 7005774450; 6602324454,GSET Somi: A game-specific eye tracking dataset for Somi,2016,"Proceedings of the 7th International Conference on Multimedia Systems, MMSys 2016",,,2910616,332,337,5.0,9,10.1145/2910017.2910616,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84973868045&doi=10.1145%2f2910017.2910616&partnerID=40&md5=b619717105857ef71e957e36bf91e098,"In this paper, we present an eye tracking dataset of computer game players who played the side-scrolling cloud game Somi. The game was streamed in the form of video from the cloud to the player. This dataset can be used for designing and testing game-specific visual attention models. The source code of the game is also available to facilitate further modifications and adjustments. For collecting this data, male and female candidates were asked to play the game in front of a remote eye-tracking device. For each player, we recorded gaze points, video frames of the gameplay, and mouse and keyboard commands. For each video frame, a list of its game objects with their locations and sizes was also recorded. This data, synchronized with eye-tracking data, allows one to calculate the amount of attention that each object or group of objects draw from each player. As a benchmark, we also show various attention patterns could be identified among players. © 2016 ACM.",Eye-tracking dataset; Game dataset; Perceptual video coding; Visual attention model,Behavioral research; Computer games; Multimedia systems; Statistical tests; Video signal processing; Eye tracking devices; Game dataset; Gaze point; Keyboard commands; Perceptual video coding; Source codes; Video frame; Visual attention model; Eye tracking,Conference paper,Final,,Scopus,2-s2.0-84973868045,Gaming / VR
Pfeiffer T.; Memili C.,"Pfeiffer, Thies (14027435500); Memili, Cem (57063541000)",14027435500; 57063541000,Model-based real-time visualization of realistic three-dimensional heat maps for mobile eye tracking and eye tracking in virtual reality,2016,Eye Tracking Research and Applications Symposium (ETRA),14,,,95,102,7.0,36,10.1145/2857491.2857541,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84975260964&doi=10.1145%2f2857491.2857541&partnerID=40&md5=1dab5a3d38768965bc25ceded44d6ec7,"Heat maps, or more generally, attention maps or saliency maps are an often used technique to visualize eye-tracking data. With heat maps qualitative information about visual processing can be easily visualized and communicated between experts and laymen. They are thus a versatile tool for many disciplines, in particular for usability engineering, and are often used to get a first overview about recorded eye-tracking data. Today, heat maps are typically generated for 2D stimuli that have been presented on a computer display. In such cases the mapping of overt visual attention on the stimulus is rather straight forward and the process is well understood. However, when turning towards mobile eye tracking and eye tracking in 3D virtual environments, the case is much more complicated. In the first part of the paper, we discuss several challenges that have to be considered in 3D environments, such as changing perspectives, multiple viewers, object occlusions, depth of fixations, or dynamically moving objects. In the second part, we present an approach for the generation of 3D heat maps addressing the above mentioned issues while working in real-time. Our visualizations provide high-quality output for multi-perspective eye-tracking recordings of visual attention in 3D environments. © 2016 Copyright held by the owner/author(s). Publication rights licensed to ACM.",3D; Eye tracking; Heat map; Visualization,Behavioral research; Flow visualization; Three dimensional computer graphics; Usability engineering; Virtual reality; Visual communication; Visualization; 3-D virtual environment; Eye-tracking; Heat maps; Mobile eye-tracking; Multi-perspective; Overt visual attentions; Qualitative information; Real time visualization; Eye movements,Conference paper,Final,All Open Access; Green Open Access,Scopus,2-s2.0-84975260964,Gaming / VR
Inoue H.; Hirayama T.; Doman K.; Kawanishi Y.; Ide I.; Deguchi D.; Murase H.,"Inoue, Hiroya (57189849987); Hirayama, Takatsugu (55531799600); Doman, Keisuke (35931846700); Kawanishi, Yasutomo (36188305700); Ide, Ichiro (13406373500); Deguchi, Daisuke (6602666462); Murase, Hiroshi (7101900108)",57189849987; 55531799600; 35931846700; 36188305700; 13406373500; 6602666462; 7101900108,A classification method of cooking operations based on eye movement patterns,2016,Eye Tracking Research and Applications Symposium (ETRA),14,,,205,208,3.0,4,10.1145/2857491.2857500,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84975291112&doi=10.1145%2f2857491.2857500&partnerID=40&md5=04eb8556150c58c29d30d4e3809e143d,"We are developing a cooking support system that coaches beginners. In this work, we focus on eye movement patterns while cooking meals because gaze dynamics include important information for understanding human behavior. The system first needs to classify typical cooking operations. In this paper, we propose a gaze-based classification method and evaluate whether or not the eye movement patterns have a potential to classify the cooking operations. We improve the conventional N-gram model of eye movement patterns, which was designed to be applied for recognition of office work. Conventionally, only relative movement from the previous frame was used as a feature. However, since in cooking, users pay attention to cooking ingredients and equipments, we consider fixation as a component of the N-gram. We also consider eye blinks, which is related to the cognitive state. Compared to the conventional method, instead of focusing on statistical features, we consider the ordinal relations of fixation, blink, and the relative movement. The proposed method estimates the likelihood of the cooking operations by Support Vector Regression (SVR) using frequency histograms of N-grams as explanatory variables. © 2016 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Blink; Cooking operations; Eye movement pattern; Fixation; Gaze analysis; N-gram; SVR,Behavioral research; Nitrogen fixation; Blink; Cooking operations; Eye movement patterns; Gaze analysis; N-grams; Eye movements,Conference paper,Final,,Scopus,2-s2.0-84975291112,Gaming / VR
Rêgo G.G.; Campanhã C.; Kassab A.P.; Romero R.L.; Minati L.; Boggio P.S.,"Rêgo, Gabriel Gaudencio (56480308400); Campanhã, Camila (35363584900); Kassab, Ana Paula (56684657400); Romero, Ruth Lyra (56683997700); Minati, Ludovico (15844021600); Boggio, Paulo Sérgio (10340985700)",56480308400; 35363584900; 56684657400; 56683997700; 15844021600; 10340985700,Adult-like neuroelectrical response to inequity in children: Evidence from the ultimatum game,2016,Social Neuroscience,11,2,,193,206,13.0,6,10.1080/17470919.2015.1057295,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84958945809&doi=10.1080%2f17470919.2015.1057295&partnerID=40&md5=f54c56ab18cd490e0e8618e5c7481c1a,"People react aversely when faced with unfair situations, a phenomenon that has been related to an electroencephalographic (EEG) potential known as medial frontal negativity (MFN). To our knowledge, the existence of the MFN in children has not yet been demonstrated. Here, we recorded EEG activity from 15 children playing the ultimatum game (UG) and who afterward performed a recognition task, in order to assess whether they could recognize the unfair and fair (familiar) proposers among unfamiliar faces. During the recognition task, we also acquired pupil dilation data to investigate subconscious recognition processes. A typical (adult-like) MFN component was detected in reaction to unfair proposals. We found a positive correlation between reaction time and empathy, as well as a negative correlation between reaction time and systematic reasoning scores. Finally, we detected a significant difference in pupil dilation in response to unfamiliar faces versus UG proposers. Our data provide the first evidence of MFN in children, which appears to index similar neurophysiological phenomena as in adults. Also, reaction time to fair proposals seems to be related to individual traits, as represented by empathy and systematizing. Our pupil dilation data provide evidence that automatic responses to faces did not index fairness, but familiarity. These findings have implications for our understanding of social development in typically developing children. © 2015 Taylor & Francis.",Cheater detection theory; Children; Face recognition; Inequity aversion; Medial frontal negativity; Ultimatum game,"Analysis of Variance; Attention; Brain; Brain Mapping; Child; Decision Making; Electroencephalography; Empathy; Evoked Potentials; Eye Movements; Female; Games, Experimental; Humans; Intelligence; Male; Neuropsychological Tests; Recognition (Psychology); Social Behavior; Wechsler Scales; analysis of variance; attention; brain; brain mapping; child; decision making; electroencephalography; empathy; evoked response; eye movement; female; game; human; intelligence; male; neuropsychological test; physiology; recognition; social behavior; Wechsler intelligence scale",Article,Final,,Scopus,2-s2.0-84958945809,Gaming / VR
Tsuchida S.; Matsuura S.,"Tsuchida, Shohei (56239044900); Matsuura, Shu (37073140200)",56239044900; 37073140200,A role of augmented reality in educational contents: Intermediating between reality and virtual reality,2016,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),9740,,,736,745,9.0,4,10.1007/978-3-319-39907-2_70,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84978829291&doi=10.1007%2f978-3-319-39907-2_70&partnerID=40&md5=13591f9ca8456511674d82d7d1e0ddc5,"Many science education materials exhibit simplified models of nature. This simplification is beneficial to represent the essential characteristics of nature, but it forces the learners to cognitively assign the model to reality. This paper describes a traditional content exhibiting the lunar phase, which is taught in the elementary school. The use of multiple frames of observation and the necessity of dual concept required for perceiving the lunar orbital motion and Earth’s rotational motion around its axis creates an extraneous cognitive load. This study introduces a simple desktop model of the planets and a multi-view display of the model planets using augmented reality (AR) and virtual reality (VR). Eye-tracking experiments are performed to examine the role of AR to intermediate between the spatial arrangement of real objects and the VR display observed from a fixed position on an object that represents the Earth. The results indicate that participants who experimented with the desktop model took more time to check and move their eye gaze between AR, VR, and the real model, in the beginning phase of the trials. Therefore, it is suggested that AR intermediates cognition of a view outside of the orbit and a surface of the Earth. © Springer International Publishing Switzerland 2016.",Cognitive load; Desktop model; Learning,Augmented reality; Eye movements; Human computer interaction; Orbits; Planets; Cognitive loads; Educational contents; Elementary schools; Essential characteristic; Learning; Multi-view displays; Rotational motion; Spatial arrangements; Virtual reality,Conference paper,Final,,Scopus,2-s2.0-84978829291,Gaming / VR
Ellis M.J.; Ritchie L.; Cordingley D.; Essig M.; Mansouri B.,"Ellis, Michael J. (56848011400); Ritchie, Lesley (16310488400); Cordingley, Dean (56943512500); Essig, Marco (55576467900); Mansouri, Behzad (6603378276)",56848011400; 16310488400; 56943512500; 55576467900; 6603378276,Traumatic optic neuropathy: A potentially unrecognized diagnosis after sports-related concussion,2016,Current Sports Medicine Reports,15,1,,27,32,5.0,15,10.1249/JSR.0000000000000223,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84954136813&doi=10.1249%2fJSR.0000000000000223&partnerID=40&md5=0a54ab8cba0bbc040dfe63d8ede89982,"Traumatic optic neuropathy is a rare cause of visual disturbance after head injury that can be difficult to distinguish from coexisting vestibulo-ocular dysfunction because of the overlap in presenting symptoms in patients with these conditions. We present a case report of a 13-year-old girl who sustained a head injury during a ringette game leading to blurred vision and diplopia persisting 5 months after injury. Clinical history and physical examination findings were consistent with a traumatic optic neuropathy, convergence insufficiency, and postconcussion syndrome. Neuroimaging was normal. The patient was managed using a multidisciplinary approach. At 6 months of follow-up, neuro-ophthalmological examination demonstrated evidence of permanent partial optic nerve injury, and formal neuropsychological testing fell primarily within normal limits. The patient was advised to retire from collision sports. The authors discuss the value of a comprehensive multidisciplinary approach to the evaluation and management of concussion patients presenting with persistent visual symptoms. Copyright © 2016 by the American College of Sports Medicine.",,Adolescent; Athletic Injuries; Brain Concussion; Female; Hockey; Humans; Optic Nerve Injuries; adolescent; aerobic exercise; afferent pupillary defect; anterograde amnesia; Article; athlete; binocular convergence; blurred vision; case report; clinical assessment; clinical evaluation; clinical examination; color vision defect; color vision test; concussion; diffusion tensor imaging; diplopia; divergent strabismus; dizziness; eye movement; fatigue; female; follow up; head injury; headache; human; medical history; neuroimaging; neuropsychological test; neurosurgeon; ophthalmologist; ophthalmoscopy; optic nerve; optic nerve disease; optic nerve injury; optical coherence tomography; pallor; pathophysiology; peripheral vision; personal experience; photophobia; physical examination; postconcussion syndrome; quantitative analysis; sports related concussion; strabismus; task performance; tractography; traumatic optic neuropathy; visual acuity; visual disorder; visual field; visual field defect; Athletic Injuries; brain concussion; complication; hockey; injuries; Optic Nerve Injuries,Article,Final,,Scopus,2-s2.0-84954136813,Gaming / VR
Grabska-Gradzińśka I.,"Grabska-Gradzińśka, Iwona (55326872600)",55326872600,How to predict behavior at risk - Educational game story analysis,2016,"23rd International Workshop of the European Group for Intelligent Computing in Engineering, EG-ICE 2016",,,,,,,0,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84987748130&partnerID=40&md5=b3e19773aa86685b85e421abcba61b48,"This paper discusses the prospects of using layered graphs for eye-tracking video game analyzing especially serious game, where player attention affects learning activity. It is shown how the user strategy can be described by graph structure. Eye tracking system is used for examination of users attention and interest and perception related to understanding and recall. Comparing user decisions with eye-tracking data give us possibilities to make correlations between users perception of object shown during the gameplay and his decisions. Visual elements of game scene impact on users decision and can support or disturb learning process. Two types of games were analysed for this paper: adventure game and serious game based on occupational safety and health.",,Intelligent computing; Interactive computer graphics; Occupational risks; Risk assessment; Educational game; Eye tracking systems; Graph structures; Learning Activity; Learning process; Occupational safety and health; Users perceptions; Visual elements; Human computer interaction,Conference paper,Final,,Scopus,2-s2.0-84987748130,Gaming / VR
Pinheiro R.B.O.; Pradhananga N.; Jianu R.; Orabi W.,"Pinheiro, R.B.O. (57191868738); Pradhananga, N. (35099310500); Jianu, R. (55921296000); Orabi, W. (34868684600)",57191868738; 35099310500; 55921296000; 34868684600,Eye-tracking technology for construction safety: A feasibility study,2016,ISARC 2016 - 33rd International Symposium on Automation and Robotics in Construction,,,,282,290,8.0,19,10.22260/isarc2016/0035,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84994355335&doi=10.22260%2fisarc2016%2f0035&partnerID=40&md5=84cc8f6ecf4cd4fef8f21bfa0a57d2cc,"A construction site is a harsh environment demanding entire human senses and attention, even for regular scheduled tasks. Many accidents occur on construction sites because of inability of workers to identify hazards and make timely decisions. To understand why some hazards go unseen, it is crucial to study how workers perceive the site. The objective of this research is to leverage eye-tracking technology to study workers' gazing pattern in a construction environment. A real picture from an active construction site is modified to introduce hazards and a desktop experiment is conducted, in which, subjects are asked to identify the hazards. A different group of subjects are made to make similar observations on a 2D sketch-representation of the same construction scenario. Eye-tracking data gathered from their observations is analyzed to understand when, how, and which hazards do they recognize and the pattern of recognition is studied. The results of this study will enhance our understanding on the visual factors that govern attention and help workers recognize potential hazards in a construction site. The comparison between the observation pattern in real and sketch-representation is done to assess how subjects respond to artificial images compared to real images. This comparison will test the feasibility of using virtual reality for safety training and simulations.",Construction Safety; Eye-tracking; Hazard Recognition; Site Perception; Virtual Reality,Hazards; Pattern recognition; Robotics; Virtual reality; Artificial image; Construction environment; Construction safety; Construction sites; Eye tracking technologies; Feasibility studies; Harsh environment; Potential hazards; Eye tracking,Conference paper,Final,All Open Access; Green Open Access,Scopus,2-s2.0-84994355335,Gaming / VR
Davis R.; Ohman J.,"Davis, Rebecca (24279725800); Ohman, Jennifer (57147176700)",24279725800; 57147176700,Wayfinding in ageing and Alzheimer’s disease within a virtual senior residence: study protocol,2016,Journal of Advanced Nursing,72,7,,1677,1688,11.0,27,10.1111/jan.12945,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84959420451&doi=10.1111%2fjan.12945&partnerID=40&md5=c6f7b5c2824674a6c60986b31def8c09,"Aim: To report a study protocol that examines the impact of adding salient cues in a virtual reality simulation of a senior residential building on wayfinding for older adults with and without Alzheimer’s disease. Background: An early symptom of Alzheimer’s disease is the inability to find one’s way (wayfinding). Senior residential environments are especially difficult for wayfinding. Salient cues may be able to help persons with Alzheimer’s disease find their way more effectively so they can maintain independence. Design: A repeated measures, within and between subjects design. Methods: This study was funded by the National Institutes of Health (August 2012). Older adults (N = 40) with normal cognition and older adults with early stage Alzheimer’s disease/mild cognitive impairment (N = 40) will try to find their way to a location repeatedly in a virtual reality simulation of senior residence. There are two environments: standard (no cues) and salient (multiple cues). Outcome measures include how often and how quickly participants find the target location in each cue condition. Conclusion: The results of this study have the potential to provide evidence for ways to make the environment more supportive for wayfinding for older adults with Alzheimer’s disease. This study is registered at Trialmatch.alz.org (Identifier 260425-5). © 2016 John Wiley & Sons Ltd",ageing; Alzheimer’s disease; environment; eye tracking; mild cognitive impairment; nursing; repeated measures; repeated measures; virtual reality; wayfinding,"Aged; Aged, 80 and over; Aging; Alzheimer Disease; Cognition; Cognitive Dysfunction; Female; Housing for the Elderly; Humans; Male; Neuropsychological Tests; Virtual Reality; aged; aging; Alzheimer disease; cognition; cognitive defect; female; home for the aged; human; male; neuropsychological test; nursing; very elderly; virtual reality",Article,Final,All Open Access; Green Open Access,Scopus,2-s2.0-84959420451,Gaming / VR
Shafi J.; Angelov P.; Umair M.,"Shafi, Jawad (51462034200); Angelov, Plamen (7003690831); Umair, Muhammad (57294509000)",51462034200; 7003690831; 57294509000,Prediction of the attention area in ambient intelligence tasks,2016,Studies in Computational Intelligence,623,,,33,56,23.0,2,10.1007/978-3-319-27267-2_2,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84957552315&doi=10.1007%2f978-3-319-27267-2_2&partnerID=40&md5=cffa3de5f530440cd636c5064b08f91a,"With recent advances in Ambient Intelligence (AmI), it is becoming possible to provide support to a human in an AmI environment. This paper presents an Adaptive Neuro-Fuzzy Inference System (ANFIS) model based scheme, named as prediction of the attention area using ANFIS (PAA_ANFIS), which predicts the human attention area on visual display with ordinary web camera. The PAA_ANFIS model was designed using trial and error based on various experiments in simulated gaming environment. This study was conducted to illustrate that ANFIS is effective with hybrid learning, for the prediction of eye-gaze area in the environment. PAA_ANFIS results show that ANFIS has been successfully implemented for predicting within different learning context scenarios in a simulated environment. The performance of the PAA_ANFIS model was evaluated using standard error measurements techniques. The Matlab® simulation results indicate that the performance of the ANFIS approach is valuable, accurate and easy to implement. The PAA_ANFIS results are based on analysis of different model settings in our environment. To further validate the PAA_ANFIS, forecasting results are then compared with linear regression. The comparative results show the superiority and higher accuracy achieved by applying the ANFIS, which is equipped with the capability of generating linear relationship and the fuzzy inference system in input-output data. However, it should be noted that an increase in the number of membership functions (MF) will increase the system response time. © Springer International Publishing Switzerland 2016.",,,Book chapter,Final,,Scopus,2-s2.0-84957552315,Gaming / VR
Martínez-Moreno J.M.; Sánchez-González P.; Luna M.; Roig T.; Tormos J.M.; Gómez E.J.,"Martínez-Moreno, José María (55978248900); Sánchez-González, P. (24512442400); Luna, M. (55749028500); Roig, T. (56814733300); Tormos, J.M. (55156701400); Gómez, E.J. (7201729604)",55978248900; 24512442400; 55749028500; 56814733300; 55156701400; 7201729604,Modelling ecological cognitive rehabilitation therapies for building virtual environments in brain injury,2016,Methods of Information in Medicine,55,1,,50,59,9.0,7,10.3414/ME15-01-0050,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84954505991&doi=10.3414%2fME15-01-0050&partnerID=40&md5=76fa4028f2d3929ae5d73086f8b86f8d,"Background: Brain Injury (BI) has become one of the most common causes of neurological disability in developed countries. Cognitive disorders result in a loss of independence and patients’ quality of life. Cognitive rehabilitation aims to promote patients’ skills to achieve their highest degree of personal autonomy. New technologies such as virtual reality or interactive video allow developing rehabilitation therapies based on reproducible Activities of Daily Living (ADLs), increasing the ecological validity of the therapy. However, the lack of frameworks to formalize and represent the definition of this kind of therapies can be a barrier for widespread use of interactive virtual environments in clinical routine. Objectives: To provide neuropsychologists with a methodology and an instrument to design and evaluate cognitive rehabilitation therapeutic interventions strategies based on ADLs performed in interactive virtual environments. Methods: The proposed methodology is used to model therapeutic interventions during virtual ADLs considering cognitive deficit, expected abnormal interactions and therapeutic hypotheses. It allows identifying abnormal behavioural patterns and designing interventions strategies in order to achieve errorless-based rehabilitation. Results: An ADL case study (’buying bread’) is defined according to the guidelines established by the ADL intervention model. This case study is developed, as a proof of principle, using interactive video technology and is used to assess the feasibility of the proposed methodology in the definition of therapeutic intervention procedures. Conclusions: The proposed methodology provides neuropsychologists with an instrument to design and evaluate ADL-based therapeutic intervention strategies, attending to solve actual limitation of virtual scenarios, to be use for ecological rehabilitation of cognitive deficit in daily clinical practice. The developed case study proves the potential of the methodology to design therapeutic interventions strategies; however our current work is devoted to designing more experiments in order to present more evidence about its values. © Schattauer 2016.",Activity of daily living; Brain injury; Cognitive rehabilitation; Ecological rehabilitation; Eye-tracking; Interactive video; Neurorehabilitation; Virtual rehabilitation,"Activities of Daily Living; Brain Injuries; Cognition; Computer Simulation; Equipment Design; Humans; Models, Theoretical; Neurological Rehabilitation; Neuropsychology; Psychometrics; Quality of Life; Software; Spain; Telemedicine; Therapy, Computer-Assisted; User-Computer Interface; Brain Injuries; cognition; computer assisted therapy; computer interface; computer simulation; daily life activity; equipment design; human; neuropsychology; neurorehabilitation; procedures; psychometry; quality of life; software; Spain; telemedicine; theoretical model",Article,Final,All Open Access; Green Open Access,Scopus,2-s2.0-84954505991,Gaming / VR
Raptis G.E.; Fidas C.A.; Avouris N.M.,"Raptis, George E. (57189313952); Fidas, Christos A. (6506412362); Avouris, Nikolaos M. (6603741790)",57189313952; 6506412362; 6603741790,Differences of field dependent/independent gamers on cultural heritage playing: Preliminary findings of an eye–tracking study,2016,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),10059 LNCS,,,199,206,7.0,15,10.1007/978-3-319-48974-2_22,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84994890230&doi=10.1007%2f978-3-319-48974-2_22&partnerID=40&md5=8fb7e60ca988d60eaa26cab27946c836,"Based on a large number of different cognitive theories on information processing procedure, suggesting that individuals have different approaches in the way they forage, retrieve, process, store and recall information, this paper investigates the effect of field dependence/ independence with regards to visual attention of gamers in the context of a cultural heritage game. Gaze data were collected and analysed from fourteen participants, who were classified as field dependent or independent according to Group Embedded Figures Test (GEFT), a cognitive style elicitation instrument. The collected data were analysed quantitatively to examine visual attention in terms of fixation count and fixation impact. The results revealed statistically significant differences in both fixation count and fixation impact towards interactive game elements. Statistically significant differences were also measured for specific types of game elements. Findings are expected to provide insights for designers and researchers aiming to design more user–centric cultural heritage games. © Springer International Publishing AG 2016.",Cognitive style; Cultural heritage; Eye–tracking; Field dependence/independence; Game design; Games; Visual attention,Behavioral research; Cognitive styles; Cultural heritages; Field dependence; Game design; Games; Visual Attention; Historic preservation,Conference paper,Final,,Scopus,2-s2.0-84994890230,Gaming / VR
,,,"10th International Conference on Universal Access in Human-Computer Interaction, UAHCI 2016",2016,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),9737,,,1,650,649.0,0,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84978877452&partnerID=40&md5=77cda9967b617406a9908e73829fa076,"The proceedings contain 46 papers. The special focus in this conference is on Novel Approaches to Accessibility. The topics include: A framework for the development of localised web accessibility guidelines for university websites in Saudi Arabia; accessibility in virtual communities of practice under the optics of inclusion of visually impaired; ontology-based adaptive interfaces for colorblind users; usability, accessibility and gameplay heuristics to evaluate audiogames for users who are blind; personalizing interaction focused on a user's interactive experience and potential; how blind and sighted individuals perceive the typographic text-signals of a document; methodology for heuristic evaluation of web accessibility oriented to types of disabilities; developing accessibility design guidelines for wearables; place brand-building; investigating motivational aspects of Brazilian elderly to interact with digital games; using self-determination theory to increase motivation to participate in sporting events; developing a research method of urban planning; architecture of absurd; impact of new construction technologies on sustainable hotel design; the role of architecture and ergonomics on shaping the domestic kitchen; the unconventional tribune profiles in architectural designing of stadiums; a study of product form design using the theory of archetypes; group level versus society level of computing; complexities in personal data collection; work motivating factors of the communications in a crowd-powered microvolunteering site; from quantified self to qualitative space; a provenance model for quantified self data and assessing levels of attention using low cost eye tracking.",,,Conference review,Final,,Scopus,2-s2.0-84978877452,Gaming / VR
Li X.; Shan Y.; Chen W.; Wu Y.; Hansen P.; Perrault S.,"Li, Xiangdong (57202025304); Shan, Yifei (57222640348); Chen, Wenqian (57210164314); Wu, Yue (57211682110); Hansen, Praben (8862059000); Perrault, Simon (39362211300)",57202025304; 57222640348; 57210164314; 57211682110; 8862059000; 39362211300,Predicting user visual attention in virtual reality with a deep learning model,2021,Virtual Reality,25,4,,1123,1136,13.0,13,10.1007/s10055-021-00512-7,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85103552320&doi=10.1007%2fs10055-021-00512-7&partnerID=40&md5=af96679c1af24a3c9ee1da006204e00d,"Recent studies show that user’s visual attention during virtual reality museum navigation can be effectively estimated with deep learning models. However, these models rely on large-scale datasets that usually are of high structure complexity and context specific, which is challenging for nonspecialist researchers and designers. Therefore, we present the deep learning model, ALRF, to generalise on real-time user visual attention prediction in virtual reality context. The model combines two parallel deep learning streams to process the compact dataset of temporal–spatial salient features of user’s eye movements and virtual object coordinates. The prediction accuracy outperformed the state-of-the-art deep learning models by reaching record high 91.03%. Importantly, with quick parametric tuning, the model showed flexible applicability across different environments of the virtual reality museum and outdoor scenes. Implications for how the proposed model may be implemented as a generalising tool for adaptive virtual reality application design and evaluation are discussed. © 2021, The Author(s), under exclusive licence to Springer-Verlag London Ltd., part of Springer Nature.",Deep learning model; Eye tracking; Virtual reality; Visual attention,Behavioral research; E-learning; Eye movements; Forecasting; Large dataset; Learning systems; Virtual reality; Application design; Large-scale datasets; Parametric tunings; Prediction accuracy; Salient features; State of the art; Structure complexity; Visual Attention; Deep learning,Article,Final,,Scopus,2-s2.0-85103552320,Gaming / VR
Polet K.; Hesse S.; Morisot A.; Kullmann B.; Louchart de la Chapelle S.; Iakimova G.; Pesce A.,"Polet, Kevin (57188639534); Hesse, Solange (57188638308); Morisot, Adeline (56747859700); Kullmann, Benoît (57188640178); Louchart de la Chapelle, Sandrine (8253965700); Iakimova, Galina (8925601200); Pesce, Alain (46761317200)",57188639534; 57188638308; 56747859700; 57188640178; 8253965700; 8925601200; 46761317200,"Theory of mind, empathy and eye gaze strategies during an artwork observation in neurodegenerative pathologies; [Théorie de l’esprit, empathie et stratégies de regard durant l’observation d’une œuvre d’art dans les pathologies neurodégénératives]",2021,Geriatrie et psychologie neuropsychiatrie du vieillissement,19,4,,427,439,12.0,0,10.1684/pnv.2021.0978,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123227780&doi=10.1684%2fpnv.2021.0978&partnerID=40&md5=8860cad694f298686793412ea2df11b2,"OBJECTIVE: Theory of mind (ToM) and empathy are severely impaired in the behavioral-variant of frontotemporal dementia (bvFTD) and more mildly in Alzheimer's (AD) and Parkinson's diseases (PD). Such impairments are associated with behavioral disorders (BD). Modification of visual scanning strategies of complex visual scenes is also found in these pathologies. We hypothesized that these patients applied atypical gaze strategies when observing social events, which would not allow to properly process social cues and would result in the production of erroneous inferences and lack of empathy towards others. METHODS: Fifty-five participants were divided into four groups: five bvFTD, 19 AD, 17 PD and 14 matched controls subjects. ToM and empathy were assessed by eye movements recording (eye-tracking) and by a questionnaire during a painting observation. Scores obtained were compared between each group and to social cognition reference tests, and correlated to the NeuroPsychiatric Inventory. RESULTS: Our paradigm was suitable for assessing cognitive ToM while it lacked sensitivity for empathy assessment. Severe ToM impairment was highlighted in bvFTD while milder difficulties were observed in AD and for PD. bvFTD and AD groups produced erroneous inferences from cognitive mental states. ToM performances were linked to visual exploration strategies of the painting. Atypical visual observation was highlighted in bvFTD and AD groups causing a time shift in perspective taking of the character. Finally, we have highlighted that social cognition performances, gaze strategies and BD were correlated. CONCLUSION: The observation of a painting in association with eye-tracking technology can be a good support for social cognition assessment. We highlighted a link between atypical visual scanning strategies, ToM impairment and BD in these pathologies. ToM skills could be improved by training in the search for visual social cues. Therefore, this kind of remediation could have positive effects on BD.",empathy; eye-tracking; neurodegenerative diseases; Rückenfigur; theory of mind,"Empathy; Fixation, Ocular; Frontotemporal Dementia; Humans; Neuropsychological Tests; Theory of Mind; empathy; eye fixation; frontotemporal dementia; human; neuropsychological test; theory of mind",Article,Final,,Scopus,2-s2.0-85123227780,Gaming / VR
Peters J.L.; Crewther S.G.; Murphy M.J.; Bavin E.L.,"Peters, Jessica L. (57206841247); Crewther, Sheila G. (7005744574); Murphy, Melanie J. (14007819500); Bavin, Edith L. (6506452153)",57206841247; 7005744574; 14007819500; 6506452153,"Action video game training improves text reading accuracy, rate and comprehension in children with dyslexia: a randomized controlled trial",2021,Scientific Reports,11,1,18584,,,,35,10.1038/s41598-021-98146-x,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85115372532&doi=10.1038%2fs41598-021-98146-x&partnerID=40&md5=b9801954aeaaea20c63fdcb2a72ecccf,"Dynamic visual attention training using Action Video Games (AVGs) is a promising intervention for dyslexia. This study investigated the efficacy of 5 h (10 × 30 min) of AVG training in dyslexic children (aged 8–13) using ‘Fruit Ninja’, while exploring whether increasing attentional and eye movement demands enhanced AVG effectiveness. Regular (AVG-R; n = 22) and enhanced AVG training (AVG+; n = 23) were compared to a treatment-as-usual comparison group (n = 19) on reading, rapid naming, eye movements and visuo-temporal processing. Playing ‘Fruit Ninja’ for only 5 h significantly improved reading accuracy, rate, comprehension and rapid naming of both AVG groups, compared to the comparison group, though increasing attentional demands did not enhance AVG efficacy. Participants whose low contrast magnocellular-temporal processing improved most following training also showed significantly greater improvement in reading accuracy. The findings demonstrate a clear role for visual attention in reading and highlight the clinical applicability of AVGs as a fun, motivational and engaging intervention for dyslexia. © 2021, The Author(s).",,Adolescent; Child; Comprehension; Dyslexia; Female; Humans; Male; Reading; Treatment Outcome; Video Games; adolescent; child; comprehension; controlled study; dyslexia; female; human; male; physiology; randomized controlled trial; reading; treatment outcome; video game,Article,Final,All Open Access; Gold Open Access; Green Open Access,Scopus,2-s2.0-85115372532,Gaming / VR
Wechsler T.F.; Pfaller M.; Eickels R.E.V.; Schulz L.H.; Mühlberger A.,"Wechsler, Theresa F. (57210105561); Pfaller, Michael (55258589800); Eickels, Rahel E. van (57387707600); Schulz, Luise H. (57387707700); Mühlberger, Andreas (6507375232)",57210105561; 55258589800; 57387707600; 57387707700; 6507375232,Look at the Audience? A Randomized Controlled Study of Shifting Attention From Self-Focus to Nonsocial vs. Social External Stimuli During Virtual Reality Exposure to Public Speaking in Social Anxiety,2021,Frontiers in Psychiatry,12,,751272,,,,14,10.3389/fpsyt.2021.751272,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85121866171&doi=10.3389%2ffpsyt.2021.751272&partnerID=40&md5=b4f6ccd3b373147173793eae1c4fd378,"Background: Enhanced self-focused attention plays a central role in the maintenance and treatment of Social Anxiety and is targeted in contemporary cognitive behavioral therapy. Actual developments use Virtual Reality (VR) for behavioral training. However, no VR attention training combining exposure to public speaking with shifting attention from self-focus to external focus has been investigated, and no experimental evidence exists on different kinds of external cues as targets of attention. Therefore, we investigated the effects of an attention training during public speaking in VR and examined differential effects of an external focus on nonsocial vs. social stimuli. Methods: In this randomized controlled study, highly socially anxious participants were instructed to focus on either objects or the audience within a virtual speech task. We assessed the pre-post effects on affective reactions, self-perception, and attentional processes during public speaking as well as general Social Anxiety using subjective, physiological, and eye-tracking measures. Repeated-measures analyses of variance (ANOVAs) were calculated to detect changes from pretest to posttest over both groups, and time × group interaction effects. Results: Within the analysis sample (n = 41), anxiety during public speaking and fear of negative evaluation significantly decreased, with no significant differences between groups. No significant time effect, but a significant time × group effect, was found for the looking time proportion on the audience members' heads. Follow-up tests confirmed a significant increase in the social-focus group and a significant decrease in the nonsocial-focus group. For all other variables, except external focus and fear of public speaking, significant improvements were found over both groups. Further significant time x group effects were found for positive affect during public speaking, with a significant increase in the social focus, and no significant change in the nonsocial-focus group. Conclusion: Our findings suggest that attention training to reduce self-focus can be successfully conducted in VR. Both training versions showed positive short-term effects in the highly socially anxious, with particular advantages of an external social focus concerning eye contact to the audience and positive affect. Further research should investigate whether social focus is even more advantageous long term and if reinterpretations of dysfunctional beliefs could be achieved by not avoiding social cues. Copyright © 2021 Wechsler, Pfaller, Eickels, Schulz and Mühlberger.",attention training; cognitive behavioral therapy; exposure therapy; eye tracking; public speaking; self-focused attention; social anxiety; virtual reality,adult; analysis of variance; article; attention; clinical article; cognitive behavioral therapy; controlled study; exposure therapy; eye tracking; female; follow up; group dynamics; human; male; perception; pretest posttest design; public speaking; randomized controlled trial; social anxiety; speech test; virtual reality,Article,Final,All Open Access; Gold Open Access; Green Open Access,Scopus,2-s2.0-85121866171,Gaming / VR
Iskander J.; Hossny M.,"Iskander, Julie (57193686489); Hossny, Mohammed (23667683300)",57193686489; 23667683300,Measuring the likelihood of VR visual fatigue through ocular biomechanics,2021,Displays,70,,102105,,,,13,10.1016/j.displa.2021.102105,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85119051639&doi=10.1016%2fj.displa.2021.102105&partnerID=40&md5=8d1f94d4ab6e70599b0bd7620b45311f,"Immersion in virtual reality is still linked to symptoms of visual fatigue such as eye strain, dizziness, and overall discomfort. Studies have investigated visual fatigue through pre- and post-immersion tests of the visual function. In this work, we extend on our previous study and derive a visual fatigue likelihood metric using biomechanical analysis. Previously, we have investigated the effect of VR on the vergence system during immersion. The proposed visual fatigue metric exhibited a significant correlation to vergence angle variability which was previously linked to vergence accommodation conflict in VR. We also discuss subjective feedback and its relationship with the proposed visual fatigue metric. © 2021 Elsevier B.V.",Extraocular muscles; Eye movement; Neural control; Ocular Biomechanics; Visual fatigue,Biomechanics; Muscle; Virtual reality; Biomechanical analysis; Extraocular muscles; Eye strain; Immersion tests; Neural control; Ocular biomechanics; Overall discomforts; Vergences; Visual fatigue; Visual functions; Eye movements,Article,Final,,Scopus,2-s2.0-85119051639,Gaming / VR
Lapborisuth P.; Koorathota S.; Wang Q.; Sajda P.,"Lapborisuth, Pawan (57203037889); Koorathota, Sharath (57203854483); Wang, Qi (57225186205); Sajda, Paul (57204342918)",57203037889; 57203854483; 57225186205; 57204342918,Integrating neural and ocular attention reorienting signals in virtual reality,2021,Journal of Neural Engineering,18,6,66052,,,,10,10.1088/1741-2552/ac4593,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123388266&doi=10.1088%2f1741-2552%2fac4593&partnerID=40&md5=6212099cf88635f344f0134f8822cabb,"Objective. Reorienting is central to how humans direct attention to different stimuli in their environment. Previous studies typically employ well-controlled paradigms with limited eye and head movements to study the neural and physiological processes underlying attention reorienting. Here, we aim to better understand the relationship between gaze and attention reorienting using a naturalistic virtual reality (VR)-based target detection paradigm. Approach. Subjects were navigated through a city and instructed to count the number of targets that appeared on the street. Subjects performed the task in a fixed condition with no head movement and in a free condition where head movements were allowed. Electroencephalography (EEG), gaze and pupil data were collected. To investigate how neural and physiological reorienting signals are distributed across different gaze events, we used hierarchical discriminant component analysis (HDCA) to identify EEG and pupil-based discriminating components. Mixed-effects general linear models (GLM) were used to determine the correlation between these discriminating components and the different gaze events time. HDCA was also used to combine EEG, pupil and dwell time signals to classify reorienting events. Main results. In both EEG and pupil, dwell time contributes most significantly to the reorienting signals. However, when dwell times were orthogonalized against other gaze events, the distributions of the reorienting signals were different across the two modalities, with EEG reorienting signals leading that of the pupil reorienting signals. We also found that the hybrid classifier that integrates EEG, pupil and dwell time features detects the reorienting signals in both the fixed (AUC = 0.79) and the free (AUC = 0.77) condition. Significance. We show that the neural and ocular reorienting signals are distributed differently across gaze events when a subject is immersed in VR, but nevertheless can be captured and integrated to classify target vs. distractor objects to which the human subject orients.  © 2022 The Author(s). Published by IOP Publishing Ltd.",attention; BCI; EEG; eyetracking,"Electroencephalography; Eye; Fixation, Ocular; Humans; Virtual Reality; Air navigation; Discriminant analysis; Electrophysiology; Eye movements; Virtual reality; Attention; BCI; Condition; Discriminant component analysis; Dwell time; Eye-tracking; Head movements; Neural process; Physiological process; Targets detection; adult; article; attention; classifier; controlled study; dwell time; electroencephalography; female; gaze; head movement; human; human experiment; male; pupil; virtual reality; electroencephalography; eye; eye fixation; procedures; Electroencephalography",Article,Final,All Open Access; Hybrid Gold Open Access,Scopus,2-s2.0-85123388266,Gaming / VR
Mazza M.; Kammler-Sücker K.; Leménager T.; Kiefer F.; Lenz B.,"Mazza, Massimiliano (57367795500); Kammler-Sücker, Kornelius (57311470000); Leménager, Tagrid (25947667800); Kiefer, Falk (55629837400); Lenz, Bernd (7004037267)",57367795500; 57311470000; 25947667800; 55629837400; 7004037267,Virtual reality: a powerful technology to provide novel insight into treatment mechanisms of addiction,2021,Translational Psychiatry,11,1,617,,,,33,10.1038/s41398-021-01739-3,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85120899758&doi=10.1038%2fs41398-021-01739-3&partnerID=40&md5=383abc4724eee4738b894cfb868a4162,"Due to its high ecological validity, virtual reality (VR) technology has emerged as a powerful tool for mental health research. Despite the wide use of VR simulations in research on mental illnesses, the study of addictive processes through the use of VR environments is still at its dawn. In a systematic literature search, we identified 38 reports of research projects using highly immersive head-mounted displays, goggles, or CAVE technologies to provide insight into treatment mechanisms of addictive behaviors. So far, VR research has mainly addressed the roles of craving, psychophysiology, affective states, cognition, and brain activity in addiction. The computer-generated VR environments offer very realistic, dynamic, interactive, and complex real-life simulations requesting active participation. They create a high sense of immersion in users by combining stereoscopic three-dimensional visual, auditory, olfactory, and tactile perceptions, tracking systems responding to user movements, and social interactions. VR is an emerging tool to study how proximal multi-sensorial cues, contextual environmental cues, as well as their interaction (complex cues) modulate addictive behaviors. VR allows for experimental designs under highly standardized, strictly controlled, predictable, and repeatable conditions. Moreover, VR simulations can be personalized. They are currently refined for psychotherapeutic interventions. Embodiment, eye-tracking, and neurobiological factors represent novel future directions. The progress of VR applications has bred auspicious ways to advance the understanding of treatment mechanisms underlying addictions, which researchers have only recently begun to exploit. VR methods promise to yield significant achievements to the addiction field. These are necessary to develop more efficacious and efficient preventive and therapeutic strategies. © 2021, The Author(s).",,"Behavior, Addictive; Cues; Movement; Technology; Virtual Reality; cannabis; central stimulant agent; cocaine; illicit drug; methamphetamine; nicotine; addiction; adolescent; adult; alcoholism; attention; brain function; cannabis addiction; cigarette smoking; cocaine dependence; craving; eye tracking; female; game addiction; hearing; human; male; methamphetamine dependence; movement (physiology); neurobiology; pathological gambling; protective glasses; psychophysiology; Review; smelling; social interaction; stereoradiography; systematic review; tobacco dependence; touch; virtual reality; vision; addiction; association; technology",Review,Final,All Open Access; Gold Open Access; Green Open Access,Scopus,2-s2.0-85120899758,Gaming / VR
Fan X.; Cai Y.; Yang Y.; Xu T.; Li Y.; Zhang S.; Zhang F.,"Fan, Xiaoxiong (57421235600); Cai, Yun (57216855611); Yang, Yufei (57421550900); Xu, Tianxing (57224901771); Li, Yike (57421443800); Zhang, Songhai (36968379500); Zhang, Fanglue (36500463800)",57421235600; 57216855611; 57421550900; 57224901771; 57421443800; 36968379500; 36500463800,Detection of scene-irrelevant head movements via eye-head coordination information,2021,Virtual Reality and Intelligent Hardware,3,6,,501,514,13.0,5,10.1016/j.vrih.2021.08.007,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123243429&doi=10.1016%2fj.vrih.2021.08.007&partnerID=40&md5=8bf08894a8e78a6e1e0e5d32adcb7fd5,"Background: Accurate motion tracking in head-mounted displays (HMDs) has been widely used in immersive VR interaction technologies. However, tracking the head motion of users at all times is not always desirable. During a session of HMD usage, users may make scene-irrelevant head rotations, such as adjusting the head position to avoid neck pain or responding to distractions from the physical world. To the best of our knowledge, this is the first study that addresses the problem of scene-irrelevant head movements. Methods: We trained a classifier to detect scene-irrelevant motions using temporal eyehead-coordinated information sequences. To investigate the usefulness of the detection results, we propose a technique to suspend motion tracking in HMDs where scene-irrelevant motions are detected. Results: /Conclusions Experimental results demonstrate that the scene-relevancy of movements can be detected using eye-head coordination information, and that ignoring scene-irrelevant head motions in HMDs improves user continuity without increasing sickness or breaking immersion. © 2021 Beijing Zhongke Journal Publishing Co. Ltd",HCI design and evaluation methods; Human-centered computing; Human-computer interaction (HCI); Interaction paradigms; User models; Virtual reality,Classification (of information); Eye movements; Human computer interaction; Information use; Motion analysis; Virtual reality; Design and evaluation methods; Head-mounted-displays; Human-centered computing; Human-computer interaction; Human-computer interaction design and evaluation method; Human-computer-interaction designs; Interaction design methods; Interaction evaluations; Interaction paradigm; User Modelling; Helmet mounted displays,Article,Final,All Open Access; Gold Open Access,Scopus,2-s2.0-85123243429,Gaming / VR
Putra P.U.; Shima K.; Alvarez S.A.; Shimatani K.,"Putra, Prasetia Utama (57203001552); Shima, Keisuke (24068050800); Alvarez, Sergio A. (7103265371); Shimatani, Koji (16314367300)",57203001552; 24068050800; 7103265371; 16314367300,Identifying autism spectrum disorder symptoms using response and gaze behavior during the Go/NoGo game CatChicken,2021,Scientific Reports,11,1,22012,,,,12,10.1038/s41598-021-01050-7,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85118861127&doi=10.1038%2fs41598-021-01050-7&partnerID=40&md5=3b1b8b17eda5031da90d55d115fe7171,"Previous studies have found that Autism Spectrum Disorder (ASD) children scored lower during a Go/No-Go task and faced difficulty focusing their gaze on the speaker’s face during a conversation. To date, however, there has not been an adequate study examining children’s response and gaze during the Go/No-Go task to distinguish ASD from typical children. We investigated typical and ASD children’s gaze modulation when they played a version of the Go/No-Go game. The proposed system represents the Go and the No-Go stimuli as chicken and cat characters, respectively. It tracks children’s gaze using an eye tracker mounted on the monitor. Statistically significant between-group differences in spatial and auto-regressive temporal gaze-related features for 21 ASD and 31 typical children suggest that ASD children had more unstable gaze modulation during the test. Using the features that differ significantly as inputs, the AdaBoost meta-learning algorithm attained an accuracy rate of 88.6% in differentiating the ASD subjects from the typical ones. © 2021, The Author(s).",,"Algorithms; Attention; Autism Spectrum Disorder; Child, Preschool; Eye-Tracking Technology; Female; Fixation, Ocular; Humans; Inhibition, Psychological; Japan; Male; Psychomotor Performance; algorithm; attention; autism; eye fixation; female; human; Japan; male; physiology; preschool child; psychology; psychomotor performance",Article,Final,All Open Access; Gold Open Access; Green Open Access,Scopus,2-s2.0-85118861127,Gaming / VR
Lewandowska A.; Olejnik-Krugly A.; Jankowski J.; Dziśko M.,"Lewandowska, Anna (57195332084); Olejnik-Krugly, Agnieszka (44861766100); Jankowski, Jarosław (38961526700); Dziśko, Malwina (57202819975)",57195332084; 44861766100; 38961526700; 57202819975,Subjective and objective user behavior disparity: Towards balanced visual design and color adjustment,2021,Sensors,21,24,8502,,,,7,10.3390/s21248502,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85121464961&doi=10.3390%2fs21248502&partnerID=40&md5=8f0bfa064abd68abebe1cb7910e29399,"Interactive environments create endless possibilities for the design of websites, games, online platforms, and mobile applications. Their visual aspects and functional characteristics influence the user experience. Depending on the project, the purpose of the environment can be oriented toward marketing targets, user experience, or accessibility. Often, these conflicting aspects should be integrated within a single project, and a search for trade-offs is needed. One of these conflicts involves a disparity in user behavior concerning declared preferences and real observed activity in terms of visual attention. Taking into account accessibility guidelines (WCAG) further complicates the problem. In our study, we focused on the analysis of color combinations and their contrast in terms of user-friendliness; visual intensity, which is important for attracting user attention; and recommendations from the Web Accessibility Guidelines (WCAG). We took up the challenge to reduce the disparity between user preferences and WCAG contrast, on one hand, and user natural behavior registered with an eye-tracker, on the other. However, we left the choice of what is more important—human conscious reaction or objective user behavior results—to the designer. The former corresponds to user-friendliness, while the latter, visual intensity, is consistent with marketing expectations. The results show that the ranking of visual objects characterized by different levels of contrast differs when considering the perspectives of user experience, commercial goals, and objective recording. We also propose an interactive tool with the possibility of assigning weights to each criterion to generate a ranking of objects. © 2021 by the authors. Licensee MDPI, Basel, Switzerland.",Accessibility; Color contrast; WCAG; Website design,Behavioral research; Color; Commerce; Economic and social effects; Eye tracking; Marketing; Accessibility guidelines; Color adjustments; Colour contrast; Interactive Environments; User behaviors; User friendliness; Users' experiences; Visual colors; Visual design; WCAG; Web Design,Article,Final,All Open Access; Gold Open Access; Green Open Access,Scopus,2-s2.0-85121464961,Gaming / VR
Smith M.E.; Loschky L.C.; Bailey H.R.,"Smith, Maverick E. (57205466383); Loschky, Lester C. (6602946442); Bailey, Heather R. (26434244300)",57205466383; 6602946442; 26434244300,Knowledge guides attention to goal-relevant information in older adults,2021,Cognitive Research: Principles and Implications,6,1,56,,,,15,10.1186/s41235-021-00321-1,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85112742139&doi=10.1186%2fs41235-021-00321-1&partnerID=40&md5=f205682b65511bb19d8445958a34af91,"How does viewers’ knowledge guide their attention while they watch everyday events, how does it affect their memory, and does it change with age? Older adults have diminished episodic memory for everyday events, but intact semantic knowledge. Indeed, research suggests that older adults may rely on their semantic memory to offset impairments in episodic memory, and when relevant knowledge is lacking, older adults’ memory can suffer. Yet, the mechanism by which prior knowledge guides attentional selection when watching dynamic activity is unclear. To address this, we studied the influence of knowledge on attention and memory for everyday events in young and older adults by tracking their eyes while they watched videos. The videos depicted activities that older adults perform more frequently than young adults (balancing a checkbook, planting flowers) or activities that young adults perform more frequently than older adults (installing a printer, setting up a video game). Participants completed free recall, recognition, and order memory tests after each video. We found age-related memory deficits when older adults had little knowledge of the activities, but memory did not differ between age groups when older adults had relevant knowledge and experience with the activities. Critically, results showed that knowledge influenced where viewers fixated when watching the videos. Older adults fixated less goal-relevant information compared to young adults when watching young adult activities, but they fixated goal-relevant information similarly to young adults, when watching more older adult activities. Finally, results showed that fixating goal-relevant information predicted free recall of the everyday activities for both age groups. Thus, older adults may use relevant knowledge to more effectively infer the goals of actors, which guides their attention to goal-relevant actions, thus improving their episodic memory for everyday activities. © 2021, The Author(s).",Aging; Attention; Event comprehension; Eye movements; SPECT,"Aged; Aging; Goals; Humans; Memory, Episodic; Mental Recall; Recognition, Psychology; Young Adult; aged; aging; episodic memory; human; motivation; recall; young adult",Article,Final,All Open Access; Gold Open Access; Green Open Access,Scopus,2-s2.0-85112742139,Gaming / VR
Banire B.; Al Thani D.; Qaraqe M.; Mansoor B.; Makki M.,"Banire, Bilikis (56653445400); Al Thani, Dena (56444337800); Qaraqe, Marwa (55491116600); Mansoor, Bilal (26531600300); Makki, Mustapha (56347966300)",56653445400; 56444337800; 55491116600; 26531600300; 56347966300,Impact of mainstream classroom setting on attention of children with autism spectrum disorder: an eye-tracking study,2021,Universal Access in the Information Society,20,4,,785,795,10.0,17,10.1007/s10209-020-00749-0,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85088580931&doi=10.1007%2fs10209-020-00749-0&partnerID=40&md5=251aa141e11d4125e1d06e94ecc82152,"It has long been reported that children with autism spectrum disorder (ASD) exhibit attention difficulties while learning. They tend to focus on irrelevant information and can easily be distracted. As a result, they are often confined to a one-to-one teaching environment, with fewer distractions and social interactions than would be present in a mainstream educational setting. In recent years, inclusive mainstream schools have been growing in popularity due to government policies on equality rights. Therefore, it is crucial to investigate attentional patterns of children with ASD in mainstream schools. This study aims to explore the attentional behaviors of children with ASD in a virtual reality simulated classroom. We analyzed four eye-gaze behaviors and performance scores of 45 children: children with ASD (ASD n = 20) and typically developing children (TD n = 25) when performing attention tasks. The gaze behaviors included time to first fixate (TTFF), first fixation duration (FFD), average fixation duration (AFD) and the sum of fixation count (SFC) on fourteen areas of interest (AOIs) in the classroom. Our results showed that children with ASD exhibit similar gaze behaviors to TD children, but with significantly lower performance scores and SFC on the target AOI. These findings showed that classroom settings can influence attentional patterns and the academic performance of children with ASD. Further studies are needed on different modalities for supporting the attention of children with ASD in a mainstream setting. © 2020, The Author(s).",Attention assessment; Autism spectrum disorder; Eye tracking; Mainstream school; Virtual classroom,Diseases; Academic performance; Average fixation durations; Children with autisms; Classroom settings; Educational settings; Eye-tracking studies; Fixation duration; Social interactions; Eye tracking,Article,Final,All Open Access; Green Open Access; Hybrid Gold Open Access,Scopus,2-s2.0-85088580931,Gaming / VR
Souza R.H.C.E.; Naves E.L.M.,"Souza, Rhaíra Helena Caetano e (57209361634); Naves, Eduardo Lázaro Martins (25121715300)",57209361634; 25121715300,Attention Detection in Virtual Environments Using EEG Signals: A Scoping Review,2021,Frontiers in Physiology,12,,727840,,,,54,10.3389/fphys.2021.727840,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85120871683&doi=10.3389%2ffphys.2021.727840&partnerID=40&md5=b3c6d28c0a1a6fc8aaabc2544d675883,"The competitive demand for attention is present in our daily lives, and the identification of neural processes in the EEG signals associated with the demand for specific attention can be useful to the individual’s interactions in virtual environments. Since EEG-based devices can be portable, non-invasive, and present high temporal resolution technology for recording neural signal, the interpretations of virtual systems user’s attention, fatigue and cognitive load based on parameters extracted from the EEG signal are relevant for several purposes, such as games, rehabilitation, and therapies. However, despite the large amount of studies on this subject, different methodological forms are highlighted and suggested in this work, relating virtual environments, demand of attention, workload and fatigue applications. In our summarization, we discuss controversies, current research gaps and future directions together with the background and final sections. Copyright © 2021 Souza and Naves.",attentional orientation; cognitive workload; EEG signal; fatigue; virtual reality,alpha rhythm; attention; brain function; cognition assessment; electroencephalogram; electroencephalography phase synchronization; event related potential; evoked response; human; mental fatigue; nervous system electrophysiology; Review; somatosensory evoked potential; virtual reality,Review,Final,All Open Access; Gold Open Access; Green Open Access,Scopus,2-s2.0-85120871683,Gaming / VR
Kullmann A.; Ashmore R.C.; Braverman A.; Mazur C.; Snapp H.; Williams E.; Szczupak M.; Murphy S.; Marshall K.; Crawford J.; Balaban C.D.; Hoffer M.; Kiderman A.,"Kullmann, Aura (57197866531); Ashmore, Robin C. (59847137300); Braverman, Alexandr (56921632300); Mazur, Christian (57226867188); Snapp, Hillary (36982460000); Williams, Erin (57222079019); Szczupak, Mikhaylo (8320975900); Murphy, Sara (57050032200); Marshall, Kathryn (56365747100); Crawford, James (7401850299); Balaban, Carey D. (7005544876); Hoffer, Michael (7006165812); Kiderman, Alexander (6506842178)",57197866531; 59847137300; 56921632300; 57226867188; 36982460000; 57222079019; 8320975900; 57050032200; 56365747100; 7401850299; 7005544876; 7006165812; 6506842178,"Portable eye-tracking as a reliable assessment of oculomotor, cognitive and reaction time function: Normative data for 18–45 year old",2021,PLoS ONE,16,11-Nov,e0260351,,,,21,10.1371/journal.pone.0260351,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85119955868&doi=10.1371%2fjournal.pone.0260351&partnerID=40&md5=346d3382b79a0683ddae4bd4a0a0fa60,"Eye movements measured by high precision eye-tracking technology represent a sensitive, objective, and non-invasive method to probe functional neural pathways. Oculomotor tests (e.g., saccades and smooth pursuit), tests that involve cognitive processing (e.g., antisaccade and predictive saccade), and reaction time tests have increasingly been showing utility in the diagnosis and monitoring of mild traumatic brain injury (mTBI) in research settings. Currently, the adoption of these tests into clinical practice is hampered by a lack of a normative data set. The goal of this study was to construct a normative database to be used as a reference for comparing patients’ results. Oculomotor, cognitive, and reaction time tests were administered to male and female volunteers, aged 18–45, who were free of any neurological, vestibular disorders, or other head injuries. Tests were delivered using either a rotatory chair equipped with video-oculography goggles (VOG) or a portable virtual reality-like VOG goggle device with incorporated infrared eye-tracking technology. Statistical analysis revealed no effects of age on test metrics when participant data were divided into pediatric (i.e.,18–21 years, following FDA criteria) and adult (i.e., 21–45 years) groups. Gender (self-reported) had an effect on auditory reaction time, with males being faster than females. Pooled data were used to construct a normative database using 95% reference intervals (RI) with 90% confidence intervals on the upper and lower limits of the RI. The availability of these RIs readily allows clinicians to identify specific metrics that are deficient, therefore aiding in rapid triage, informing and monitoring treatment and/or rehabilitation protocols, and aiding in the return to duty/activity decision. This database is FDA cleared for use in clinical practice (K192186). Copyright: This is an open access article, free of all copyright, and may be freely reproduced, distributed, transmitted, modified, built upon, or otherwise used by anyone for any lawful purpose. The work is made available under the Creative Commons CC0 public domain dedication.",,Adolescent; Adult; Cognition; Eye Movements; Eye-Tracking Technology; Female; Humans; Male; Middle Aged; Reaction Time; Young Adult; adult; Article; auditory reaction time; clinical assessment; cognition; controlled study; emergency health service; eye movement control; eye tracking; eye-tracking technology; factual database; female; gender; human; human experiment; infrared radiation; male; normal human; protective glasses; self report; videooculography; virtual reality; young adult; adolescent; clinical trial; cognition; devices; eye movement; middle aged; reaction time,Article,Final,,Scopus,2-s2.0-85119955868,Gaming / VR
Bouchard S.; Berthiaume M.; Robillard G.; Forget H.; Daudelin-Peltier C.; Renaud P.; Blais C.; Fiset D.,"Bouchard, Stéphane (7006810892); Berthiaume, Maxine (57209470992); Robillard, Geneviève (23006370100); Forget, Hélène (6603146662); Daudelin-Peltier, Camille (57112694300); Renaud, Patrice (7103298339); Blais, Caroline (24779482800); Fiset, Daniel (8532606600)",7006810892; 57209470992; 23006370100; 6603146662; 57112694300; 7103298339; 24779482800; 8532606600,Arguing in Favor of Revising the Simulator Sickness Questionnaire Factor Structure When Assessing Side Effects Induced by Immersions in Virtual Reality,2021,Frontiers in Psychiatry,12,,739742,,,,52,10.3389/fpsyt.2021.739742,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85119514106&doi=10.3389%2ffpsyt.2021.739742&partnerID=40&md5=83c5198937636133854ac31ad7f749a5,"Two issues are increasingly of interest in the scientific literature regarding unwanted virtual reality (VR) induced side effects: (1) whether the latent structure of the Simulator Sickness Questionnaire (SSQ) is comprised of two or three factors, and (2) if the SSQ measures symptoms of anxiety that can be misattributed to unwanted negative side effects induced by immersions in VR. Study 1 was conducted with a sample of 876 participants. A confirmatory factor analysis clearly supported a two-factor model composed of nausea and oculomotor symptoms instead of the 3-factor structure observed in simulators. To tease-out symptoms of anxiety from unwanted negative side effects induced by immersions in VR, Study 2 was conducted with 88 participants who were administered the Trier Stress Social Test in groups without being immersed in VR. A Spearman correlation showed that 11 out of 16 side effects correlated significantly with anxiety. A factor analysis revealed that items measuring general discomfort, difficulty concentrating, sweating, nausea, and vertigo loaded significantly on the anxiety factor comprised of items from the State-Trait Anxiety Inventory. Finally, a multiple regression indicated that the items measuring general discomfort and difficulty concentrating significantly predicted increases in anxiety. The overall results support the notion that side effects associated with immersions in VR consist mostly of a nausea and an oculomotor latent structure and that a few items are confounding anxiety and cybersickness. The data support the suggestion to revise the scoring procedures of the Simulator Sickness Questionnaire when using this instrument with immersions in VR. Copyright © 2021 Bouchard, Berthiaume, Robillard, Forget, Daudelin-Peltier, Renaud, Blais and Fiset.",anxiety; cybersickness; simulator sickness; simulator sickness questionnaire; trier stress social test; virtual reality,adult; anxiety disorder; Article; attention disturbance; blurred vision; confounding variable; controlled study; correlational study; cybersickness; disease association; dizziness; eye movement disorder; fatigue; female; headache; human; hypersalivation; immersion; major clinical study; male; nausea; scoring system; Simulator Sickness Questionnaire; State Trait Anxiety Inventory; stomach disease; sweat gland disease; sweating; Trier Stress Social Test; vertigo; virtual reality,Article,Final,All Open Access; Gold Open Access; Green Open Access,Scopus,2-s2.0-85119514106,Gaming / VR
Kandana Arachchige K.G.; Blekic W.; Simoes Loureiro I.; Lefebvre L.,"Kandana Arachchige, Kendra Gimhani (57195997249); Blekic, Wivine (57202892290); Simoes Loureiro, Isabelle (56055321200); Lefebvre, Laurent (36905284900)",57195997249; 57202892290; 56055321200; 36905284900,Covert Attention to Gestures Is Sufficient for Information Uptake,2021,Frontiers in Psychology,12,,776867,,,,3,10.3389/fpsyg.2021.776867,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85121223996&doi=10.3389%2ffpsyg.2021.776867&partnerID=40&md5=e7e35d7dc2bad6676e59e67a25b74d59,"Numerous studies have explored the benefit of iconic gestures in speech comprehension. However, only few studies have investigated how visual attention was allocated to these gestures in the context of clear versus degraded speech and the way information is extracted for enhancing comprehension. This study aimed to explore the effect of iconic gestures on comprehension and whether fixating the gesture is required for information extraction. Four types of gestures (i.e., semantically and syntactically incongruent iconic gestures, meaningless configurations, and congruent iconic gestures) were presented in a sentence context in three different listening conditions (i.e., clear, partly degraded or fully degraded speech). Using eye tracking technology, participants’ gaze was recorded, while they watched video clips after which they were invited to answer simple comprehension questions. Results first showed that different types of gestures differently attract attention and that the more speech was degraded, the less participants would pay attention to gestures. Furthermore, semantically incongruent gestures appeared to particularly impair comprehension although not being fixated while congruent gestures appeared to improve comprehension despite also not being fixated. These results suggest that covert attention is sufficient to convey information that will be processed by the listener. Copyright © 2021 Kandana Arachchige, Blekic, Simoes Loureiro and Lefebvre.",covert attention; eye tracking; iconic gestures; incongruency effect; information uptake,,Article,Final,All Open Access; Gold Open Access; Green Open Access,Scopus,2-s2.0-85121223996,Gaming / VR
Hougaard B.I.; Knoche H.; Jensen J.; Evald L.,"Hougaard, Bastian I. (57209474113); Knoche, Hendrik (13005272900); Jensen, Jim (57374215900); Evald, Lars (56394542900)",57209474113; 13005272900; 57374215900; 56394542900,Spatial Neglect Midline Diagnostics From Virtual Reality and Eye Tracking in a Free-Viewing Environment,2021,Frontiers in Psychology,12,,742445,,,,17,10.3389/fpsyg.2021.742445,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85121268903&doi=10.3389%2ffpsyg.2021.742445&partnerID=40&md5=9af3cdbad171439732f1443bf51e01cb,"Purpose: Virtual reality (VR) and eye tracking may provide detailed insights into spatial cognition. We hypothesized that virtual reality and eye tracking may be used to assess sub-types of spatial neglect in stroke patients not readily available from conventional assessments. Method: Eighteen stroke patients with spatial neglect and 16 age and gender matched healthy subjects wearing VR headsets were asked to look around freely in a symmetric 3D museum scene with three pictures. Asymmetry of performance was analyzed to reveal group-level differences and possible neglect sub-types on an individual level. Results: Four out of six VR and eye tracking measures revealed significant differences between patients and controls in this free-viewing task. Gaze-asymmetry between-pictures (including fixation time and count) and head orientation were most sensitive to spatial neglect behavior on a group level analysis. Gaze-asymmetry and head orientation each identified 10 out of 18 (56%), compared to 12 out of 18 (67%) for the best conventional test. Two neglect patients without deviant performance on conventional measures were captured by the VR and eyetracking measures. On the individual level, five stroke patients revealed deviant gaze-asymmetry within-pictures and six patients revealed deviant eye orientation in either direction that were not captured by the group-level analysis. Conclusion: This study is a first step in using VR in combination with eye tracking measures as individual differential neglect subtype diagnostics. This may pave the way for more sensitive and elaborate sub-type diagnostics of spatial neglect that may respond differently to various treatment approaches. Copyright © 2021 Hougaard, Knoche, Jensen and Evald.",acquired brain injury; diagnostic techniques and procedures; eye tracking; head rotation; hemispatial neglect; stroke; unilateral spatial neglect; virtual reality immersion therapy,,Article,Final,All Open Access; Gold Open Access; Green Open Access,Scopus,2-s2.0-85121268903,Gaming / VR
Rivu R.; Pfeuffer K.; Müller P.; Abdelrahman Y.; Bulling A.; Alt F.,"Rivu, Radiah (57217113594); Pfeuffer, Ken (36141954200); Müller, Philipp (57188879130); Abdelrahman, Yomna (56156577200); Bulling, Andreas (6505807414); Alt, Florian (27267528900)",57217113594; 36141954200; 57188879130; 56156577200; 6505807414; 27267528900,Altering Non-verbal Cues to Implicitly Direct Attention in Social VR,2021,Proceedings - SUI 2021: ACM Spatial User Interaction 2021,,,3485309,,,,6,10.1145/3485279.3485309,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85119372761&doi=10.1145%2f3485279.3485309&partnerID=40&md5=a986a6d19a58fb1803efc9262227646f,"In this work we explore a concept system that alters the virtual eye movements without the user's awareness, and whether this can affect social attention among others. Our concept augments the real movements with subtle redirected gazes to people, that occur in intervals to remain unnoticed. We present a user study with groups of people conversing on a topic, and measure the level of visual attention among users. Compared to a baseline of natural eye movements, we find that the method has indeed affected the overall attention in the group, but in unexpected ways. Our work points to a new way to exploit the inherent role of eyes in social virtual reality. © 2021 Owner/Author.",Collaboration; Eye-tracking; User Attention; Virtual Reality,Behavioral research; Eye tracking; Virtual reality; Collaboration; Concept Systems; Eye-tracking; User attention; User study; Visual Attention; Eye movements,Conference paper,Final,,Scopus,2-s2.0-85119372761,Gaming / VR
Asish S.M.; Kulshreshth A.K.; Borst C.W.,"Asish, Sarker Monojit (57215357842); Kulshreshth, Arun K (55315352400); Borst, Christoph W (9736479200)",57215357842; 55315352400; 9736479200,Supervised vs Unsupervised Learning on Gaze Data to Classify Student Distraction Level in an Educational VR Environment,2021,Proceedings - SUI 2021: ACM Spatial User Interaction 2021,,,3488283,,,,3,10.1145/3485279.3488283,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85119328865&doi=10.1145%2f3485279.3488283&partnerID=40&md5=2f22f3b67f11364d48d1d2ddea7f2bf0,"Educational VR may help students by being more engaging or improving retention compared to traditional learning methods. However, a student can get distracted in a VR environment due to stress, mind-wandering, unwanted noise, external alerts, etc. Student eye gaze can be useful for detecting these distraction. We explore deep-learning-based approaches to detect distractions from gaze data. We designed an educational VR environment and trained three deep learning models (CNN, LSTM, and CNN-LSTM) to gauge a student's distraction level from gaze data, using both supervised and unsupervised learning methods. Our results show that supervised learning provided better test accuracy compared to unsupervised learning methods. © 2021 Owner/Author.",Deep Learning; Distraction; Education; Virtual Reality,E-learning; Education computing; Long short-term memory; Unsupervised learning; Virtual reality; Deep learning; Distraction; Eye-gaze; Learning methods; Learning models; Learning-based approach; Supervised and unsupervised learning; Supervised learning methods; Traditional learning; Unsupervised learning method; Students,Conference paper,Final,,Scopus,2-s2.0-85119328865,Gaming / VR
Bennett C.R.; Bauer C.M.; Bex P.J.; Bottari D.; Merabet L.B.,"Bennett, Christopher R. (55385456500); Bauer, Corinna M. (36603985800); Bex, Peter J. (7007010524); Bottari, Davide (24365931900); Merabet, Lotfi B. (6603146795)",55385456500; 36603985800; 7007010524; 24365931900; 6603146795,Visual search performance in cerebral visual impairment is associated with altered alpha band oscillations,2021,Neuropsychologia,161,,108011,,,,10,10.1016/j.neuropsychologia.2021.108011,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85114398512&doi=10.1016%2fj.neuropsychologia.2021.108011&partnerID=40&md5=3fc6407d22b5522d1795a73180b8b2cb,"Individuals with cerebral visual impairment (CVI) often present with deficits related to visuospatial processing. However, the neurophysiological basis underlying these higher order perceptual dysfunctions have not been clearly identified. We assessed visual search performance using a novel virtual reality based task paired with eye tracking to simulate the exploration of a naturalistic scene (a virtual toy box). This was combined with electroencephalography (EEG) recordings and an analysis pipeline focusing on time frequency decomposition of alpha oscillatory activity. We found that individuals with CVI showed an overall impairment in visual search performance (as indexed by decreased success rate, as well as increased reaction time, visual search area, and gaze error) compared to controls with neurotypical development. Analysis of captured EEG activity following stimulus onset revealed that in the CVI group, there was a distinct lack of strong and well defined posterior alpha desynchronization; an important signal involved in the coordination of neural activity related to visual processing. Finally, an exploratory analysis revealed that in CVI, the magnitude of alpha desynchronization was associated with impaired visual search performance as well as decreased volume of specific thalamic nuclei implicated in visual processing. These results suggest that impairments in visuospatial processing related to visual search in CVI are associated with alterations in alpha band oscillations as well as early neurological injury at the level of visual thalamic nuclei. © 2021 Elsevier Ltd",,Cognition; Electroencephalography; Humans; Reaction Time; Vision Disorders; Visual Perception; adolescent; adult; Article; brain development; brain size; cerebral visual impairment; clinical article; clinical assessment; cognition; controlled study; electroencephalogram; electroencephalography; exploratory research; eye tracking; female; human; male; morphometry; nerve stimulation; oscillation; outcome assessment; reaction time; thalamus nucleus; virtual reality; vision; visual acuity; visual impairment; visuomotor coordination; young adult; visual disorder,Article,Final,All Open Access; Green Open Access,Scopus,2-s2.0-85114398512,Gaming / VR
Loiseau-Taupin M.; Ruffault A.; Slawinski J.; Delabarre L.; Bayle D.,"Loiseau-Taupin, Mildred (57451039900); Ruffault, Alexis (56925052800); Slawinski, Jean (25422871500); Delabarre, Lucile (57451040000); Bayle, Dimitri (16444180600)",57451039900; 56925052800; 25422871500; 57451040000; 16444180600,Effects of Acute Physical Fatigue on Gaze Behavior and Performance During a Badminton Game,2021,Frontiers in Sports and Active Living,3,,725625,,,,9,10.3389/fspor.2021.725625,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124547739&doi=10.3389%2ffspor.2021.725625&partnerID=40&md5=37d414232bb9276628cf663fc400bb15,"In badminton, the ability to quickly gather relevant visual information is one of the most important determinants of performance. However, gaze behavior has never been investigated in a real-game setting (with fatigue), nor related to performance. The aim of this study was to evaluate the effect of fatigue on gaze behavior during a badminton game setting, and to determine the relationship between fatigue, performance and gaze behavior. Nineteen novice badminton players equipped with eye-tracking glasses played two badminton sets: one before and one after a fatiguing task. The duration and number of fixations for each exchange were evaluated for nine areas of interest. Performance in terms of points won or lost and successful strokes was not impacted by fatigue, however fatigue induced more fixations per exchange on two areas of interest (shuttlecock and empty area after the opponent's stroke). Furthermore, two distinct gaze behaviors were found for successful and unsuccessful performance: points won were associated with fixations on the boundary lines and few fixation durations on empty area before the participant's stroke; successful strokes were related to long fixation durations, few fixation durations on empty area and a large number of fixations on the shuttlecock, racket, opponent's upper body and anticipation area. This is the first study to use a mobile eye-tracking system to capture gaze behavior during a real badminton game setting: fatigue induced changes in gaze behavior, and successful and unsuccessful performance were associated with two distinct gaze behaviors. Copyright © 2021 Loiseau-Taupin, Ruffault, Slawinski, Delabarre and Bayle.",eye movements; physiological load; racket sports; visual perception; visual search strategy,,Article,Final,All Open Access; Gold Open Access; Green Open Access,Scopus,2-s2.0-85124547739,Gaming / VR
Kullmann A.; Ashmore R.C.; Braverman A.; Mazur C.; Snapp H.; Williams E.; Szczupak M.; Murphy S.; Marshall K.; Crawford J.; Balaban C.D.; Hoffer M.; Kiderman A.,"Kullmann, Aura (57197866531); Ashmore, Robin C. (59847137300); Braverman, Alexandr (56921632300); Mazur, Christian (57226867188); Snapp, Hillary (36982460000); Williams, Erin (57222079019); Szczupak, Mikhaylo (8320975900); Murphy, Sara (57050032200); Marshall, Kathryn (56365747100); Crawford, James (7401850299); Balaban, Carey D. (7005544876); Hoffer, Michael (7006165812); Kiderman, Alexander (6506842178)",57197866531; 59847137300; 56921632300; 57226867188; 36982460000; 57222079019; 8320975900; 57050032200; 56365747100; 7401850299; 7005544876; 7006165812; 6506842178,Normative data for ages 18-45 for ocular motor and vestibular testing using eye tracking,2021,Laryngoscope Investigative Otolaryngology,6,5,,1116,1127,11.0,15,10.1002/lio2.632,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85113161587&doi=10.1002%2flio2.632&partnerID=40&md5=47ebe0a0081735a6c031c42bedf94a04,"Objective: Eye tracking technology has been employed in assessing ocular motor and vestibular function following vestibular and neurologic conditions, including traumatic brain injury (TBI). Assessments include tests that provide visual and motion (rotation) stimuli while recording horizontal, vertical, and torsional eye movements. While some of these tests have shown diagnostic promise in previous studies, their use in clinical practice is limited by the lack of normative data. The goal of this study was to construct normative reference ranges to be used when comparing patients' results. Methods: Optokinetic response, subjective visual horizontal and vertical, and rotation tests were administered to male and female volunteers, ages 18-45, who were free from neurological, vestibular disorders, or other head injuries. Tests were administered using either a rotatory chair or a portable virtual reality-like goggle equipped with video-oculography. Results: Reference values for eye movements in response to different patterns of stimuli were analyzed from 290 to 449 participants. Analysis of gender (self-reported) or age when grouped as pediatric (late adolescent; 18-21 years of age) and adult (21-45 years of age) revealed no effects on the test metrics. Data were pooled and presented for each test metric as the 95% reference interval (RI) with 90% confidence intervals (CI) on upper and lower limits of the RI. Conclusions: This normative database can serve as a tool to aid in diagnosis, treatment, and/or rehabilitation protocols for vestibular and neurological conditions, including mild TBI (mTBI). This database has been cleared by the FDA for use in clinical practice (K192186). Level of evidence: 2b. © 2021 The Authors. Laryngoscope Investigative Otolaryngology published by Wiley Periodicals LLC on behalf of The Triological Society.",concussion; eye tracking; mild traumatic brain injury; mTBI; Neurolign Dx 100; neurological conditions; NOTC; rotation tests; vestibular conditions,adult; aphasia; Article; auditory stimulation; binocular vision; biomechanics; clinical practice; cognition; controlled study; degenerative disease; dizziness; eye movement; eye position; eye tracking; female; gender; glaucoma; head impulse test; hearing; hearing impairment; human; kinematics; knee function; liver cirrhosis; major clinical study; male; mental disease; motor dysfunction assessment; myringotomy; neurologic disease; otosclerosis; Parkinson disease; prospective study; protective glasses; reference value; refraction error; retina ganglion cell; retina maculopathy; retinal nerve fiber layer thickness; scanning laser ophthalmoscopy; semicircular canal; sudden deafness; task performance; traumatic brain injury; vestibular disorder; vestibular evoked myogenic potential; vestibular function; vestibular system; videooculography; virtual reality; visual stimulation; young adult,Article,Final,,Scopus,2-s2.0-85113161587,Gaming / VR
Wang C.-C.; Hung J.C.; Chen H.-C.,"Wang, Chun-Chia (7501632228); Hung, Jason C. (7201963626); Chen, Hsuan-Chu (56078732800)",7501632228; 7201963626; 56078732800,How prior knowledge affects visual attention of japanese mimicry and onomatopoeia and learning outcomes: Evidence from virtual reality eye tracking,2021,Sustainability (Switzerland),13,19,11058,,,,14,10.3390/su131911058,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85116768677&doi=10.3390%2fsu131911058&partnerID=40&md5=82ac41a67d3e7ff319584a2de8400949,"According to the United Nations Sustainable Development Goal (SDG) 4, “achieving inclusive and quality education for all”, foreign language learning has come to be seen as a process of integrating sustainable development into the socio-cultural aspects of education and learning. The aim of this study was to employ virtual reality (VR) eye tracker to examine how students with different levels of prior knowledge process visual behaviors for Japanese Mimicry and Onomatopoeia (MIO) while learning Japanese as a second foreign language. A total of 20 students studying at the Department of Applied Japanese at the university of Southern Taiwan were recruited. Based on the Japanese language proficiency test (JLPT) level, 20 participants were divided into high prior knowledge group (levels N1–N3) with 7 participants, and low prior knowledge group (level N4 or below) with 13 participants. The learning stimuli materials were created by Unreal Engine 4 (UE4) development tool to design a 3D virtual MIO paradise, including 5 theme amusement parks. Through a VR eye tracker, participants’ visual behaviors were tracked and recorded based on 24 different regions of interest (ROIs) (i.e., ROI1–ROI24). This was done to discuss the distribution of visual attention in terms of different ROIs of each theme amusement park based on four eye movement indicators, including latency of first fixation (LFF), duration of first fixation (DFF), total fixation durations (TFD), and fixation counts (FC). Each ROI of the two groups were then compared. In addition, a heat zone map was also generated to show the overall visual distribution of each group. After the experiment, based on the eye movement indicators and test scores in the pre-test and posttest phases, statistical analysis was used to examine and evaluate the differences in visual attention and learning outcomes. The results revealed that the gaze sequences of the two prior knowledge groups gazing at the ROIs in theme parks were different, except for the gaze sequence in the circus theme park. Different prior knowledge groups exhibited differences in visual attention in the ROIs fixated on in each amusement park. Additionally, in terms of TFD and FC of different groups in each amusement park, there was no significant difference except in ROI10, ROI16, and ROI18. Moreover, after receiving cognitive comprehension processes introduced in the VR-simulated MIO scenes, students from both groups achieved higher post-test scores compared with pre-test scores, and such differences had statistical significance. In conclusion, the implications of VR eye movement analysis on developing students’ competence related to learning Japanese and cross-cultural aspects, compatible with sustainable development, were presented. © 2021 by the authors. Licensee MDPI, Basel, Switzerland.",Education for sustainable development; Eye tracking; Mimicry and onomatopoeia; Prior knowledge; Virtual reality; Visual attention,Taiwan; knowledge; learning; mimicry; student; Sustainable Development Goal; United Nations; virtual reality,Article,Final,All Open Access; Gold Open Access,Scopus,2-s2.0-85116768677,Gaming / VR
Hsiao S.-W.; Peng P.-H.; Tsao Y.-C.,"Hsiao, Shih-Wen (9841389000); Peng, Po-Hsiang (58328095600); Tsao, Yi-Cheng (57195065642)",9841389000; 58328095600; 57195065642,A method for the analysis of the interaction between users and objects in 3D navigational space,2021,Advanced Engineering Informatics,50,,101364,,,,7,10.1016/j.aei.2021.101364,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85112602928&doi=10.1016%2fj.aei.2021.101364&partnerID=40&md5=48f19e940dab022f9954a9d5d64cdc90,"Along with the improvement of eye-tracking technology, more and more distinct field of researches have introduced movements of the eye in relation to the head to understand user behavior. Most of current researches focus on the perception process of single 2-dimensional images by fixed eye-tracking devices or the head-mount devices. A method of applying eye-tracking on the analysis of the interaction between users and objects in 3D navigational space is proposed in this article. It aims to understand the visual stimulation of 3D objects and the user's spatial navigational reactions while receiving the stimulation, and proposes the concept of 3D object attention heat map. It also proposes to construct a computational visual attention model for different geometric featured 3D objects by applying the method of feature curves. The VR results of this study also provide future assistance in the incoming immersive world. This study sets to promote eye-tracking from the mainstream of 2D field to 3D spaces and points to a deeper understanding between human and artificial product or natural objects. It would also serve an important role in the field of human-computer interaction, product usability, aids devices for cognition degenerative individuals, and even the field of visual recognition of daily human behavior. © 2021 Elsevier Ltd",3D Interactive Cognition; Eye-Tracking; Human-Computer Interaction (HCI); Visual attention,Behavioral research; Eye movements; Eye tracking; Human computer interaction; Navigation; Eye tracking devices; Eye tracking technologies; Human behaviors; Natural objects; Product usability; Visual attention model; Visual recognition; Visual stimulation; Object tracking,Article,Final,,Scopus,2-s2.0-85112602928,Gaming / VR
Wechsler T.F.; Brockelmann M.; Kulik K.; Kopf F.M.; Kocur M.; Lankes M.; Mühlberger A.; Wolff C.,"Wechsler, Theresa F. (57210105561); Brockelmann, Martin (7801531982); Kulik, Konstantin (57322263400); Kopf, Felicitas M. (57322383800); Kocur, Martin (57197792311); Lankes, Michael (18935743600); Mühlberger, Andreas (6507375232); Wolff, Christian (57203714801)",57210105561; 7801531982; 57322263400; 57322383800; 57197792311; 18935743600; 6507375232; 57203714801,SpEYEders: Adults' and children's affective responses during immersive playful gaze interactions transforming virtual spiders,2021,CHI PLAY 2021 - Extended Abstracts of the 2021 Annual Symposium on Computer-Human Interaction in Play,,,,74,79,5.0,6,10.1145/3450337.3483463,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85118471945&doi=10.1145%2f3450337.3483463&partnerID=40&md5=26eab5fdd6838b32be070b1e5553db10,"Specific phobias like spider phobia represent a frequent mental health problem in children and adolescents, demanding innovative prevention and treatment approaches. We therefore develop an eye tracking supported Virtual Reality serious game for school-aged children, realizing gaze interactions to promote attention towards, and positive experiences during exposure to spiders. Within pilot studies in adults (n=30) and children (n=14) without fear of spiders, we assessed positive and negative affect during prototype gaze feedback through five different variants: If gazed for few seconds, the virtual spider changed into a shrunk, a rainbow coloured, or dying spider, or morphed into a smileyball, or speaks friendly. We found the highest positive affect for the rainbow and smileyball variant, followed by the shrunk and friendly speaking variant. In contrast, the dying variant was excluded due to the possible induction of negative affect. Findings indicate eligible variants for the further development of the VR serious game. © 2021 Owner/Author.",Eye tracking; Gaze interaction; Serious game; Spider phobia; Virtual reality,Serious games; Virtual reality; Affective response; Children and adolescents; Eye-tracking; Gaze interaction; Immersive; Mental health; Negative affects; Positive affects; Positive experiences; Spiders phobia; Eye tracking,Conference paper,Final,,Scopus,2-s2.0-85118471945,Gaming / VR
Pamir Z.; Bauer C.M.; Bennett C.R.; Kran B.S.; Merabet L.B.,"Pamir, Zahide (57188638237); Bauer, Corinna M. (36603985800); Bennett, Christopher R. (55385456500); Kran, Barry S. (24485230200); Merabet, Lotfi B. (6603146795)",57188638237; 36603985800; 55385456500; 24485230200; 6603146795,Visual perception supported by verbal mediation in an individual with cerebral visual impairment (CVI),2021,Neuropsychologia,160,,107982,,,,6,10.1016/j.neuropsychologia.2021.107982,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85112009242&doi=10.1016%2fj.neuropsychologia.2021.107982&partnerID=40&md5=2a2ae654acfc61119e1787eaaab32cfe,"Cerebral visual impairment (CVI) often presents with deficits associated with higher order visual processing. We report a case of an individual with CVI who uses a verbal mediation strategy to perceive and interact with his visual surroundings. Visual perceptual performance was assessed using a virtual reality based visual search task combined with eye tracking. Functional magnetic resonance imaging (fMRI) was employed to identify the neural correlates associated with this strategy. We found that when using verbal mediation, the individual could readily detect and track the target within the visual scene which was associated with robust activation within a network of occipito-parieto-temporal visual cortical areas. In contrast, when not using verbal mediation, the individual was completely unable to perform the task, and this was associated with dramatically reduced visual cortical activation. This unique compensatory strategy may be related to the individual's use of verbal working memory for the purposes of understanding complex visual information. © 2021 Elsevier Ltd",Cerebral visual impairment; fMRI; Higher order processing; Verbal mediation; Visual impairment,"Brain Mapping; Cognition; Humans; Magnetic Resonance Imaging; Memory, Short-Term; Vision Disorders; Visual Perception; brain mapping; case report; cognition; human; nuclear magnetic resonance imaging; short term memory; vision; visual disorder",Article,Final,All Open Access; Green Open Access,Scopus,2-s2.0-85112009242,Gaming / VR
Wen D.; Liang B.; Zhou Y.; Chen H.; Jung T.-P.,"Wen, Dong (26322164100); Liang, Bingbing (57221412600); Zhou, Yanhong (55718596200); Chen, Hongqian (57221413761); Jung, Tzyy-Ping (7201389395)",26322164100; 57221412600; 55718596200; 57221413761; 7201389395,The Current Research of Combining Multi-Modal Brain-Computer Interfaces with Virtual Reality,2021,IEEE Journal of Biomedical and Health Informatics,25,9,9310231,3278,3287,9.0,33,10.1109/JBHI.2020.3047836,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85099111820&doi=10.1109%2fJBHI.2020.3047836&partnerID=40&md5=0915c49422d9dcb7278e872ec6d28ebe,"Combing brain-computer interfaces (BCI) and virtual reality (VR) is a novel technique in the field of medical rehabilitation and game entertainment. However, the limitations of BCI such as a limited number of action commands and low accuracy hinder the widespread use of BCI-VR. Recent studies have used hybrid BCIs that combine multiple BCI paradigms and/or the multi-modal biosensors to alleviate these issues, which may become the mainstream of BCIs in the future. The main purpose of this review is to discuss the current status of multi-modal BCI-VR. This study first reviewed the development of the BCI-VR, and explored the advantages and disadvantages of incorporating eye tracking, motor capture, and myoelectric sensing into the BCI-VR system. Then, this study discussed the development trend of the multi-modal BCI-VR, hoping to provide a pathway for further research in this field. © 2013 IEEE.",Brain-computer interface; multi-modal biosensor; virtual reality,Brain-Computer Interfaces; Electroencephalography; Humans; User-Computer Interface; Virtual Reality; Computer games; Eye tracking; Motion tracking; Virtual reality; Current status; Development trends; Multi-modal; Novel techniques; VR systems; adult; algorithm; Article; artificial intelligence; automation; bioinformatics; bioremediation; brain development; breathing rate; cognition; dyskinesia; dysthymia; electroencephalography; electromyography; eye movement; eye tracking; facial expression; facial nerve paralysis; fatigue; female; functional magnetic resonance imaging; heart rate; human; information technology; learning; locomotion; machine learning; male; mathematical model; motion; motor performance; muscle contraction; neurofeedback; questionnaire; robotics; signal noise ratio; signal processing; skin conductance; speech intelligibility; sport injury; stroke rehabilitation; telerehabilitation; virtual reality; visual feedback; visual field; visual stimulation; working memory; brain computer interface; computer interface; Brain computer interface,Article,Final,,Scopus,2-s2.0-85099111820,Gaming / VR
Ghani U.; Signal N.; Niazi I.K.; Taylor D.,"Ghani, Usman (59597564900); Signal, Nada (56079751100); Niazi, Imran Khan (56658453700); Taylor, Denise (37860944700)",59597564900; 56079751100; 56658453700; 37860944700,Efficacy of a Single-Task ERP Measure to Evaluate Cognitive Workload During a Novel Exergame,2021,Frontiers in Human Neuroscience,15,,742384,,,,12,10.3389/fnhum.2021.742384,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85115364420&doi=10.3389%2ffnhum.2021.742384&partnerID=40&md5=a8674862ece51370b8493e9af4428752,"This study aimed to validate the efficacy of single-task event-related potential (ERP) measures of cognitive workload to be implemented in exergame-based rehabilitation. Twenty-four healthy participants took part in a novel gamified balance task where task-irrelevant auditory tones were presented in the background to generate ERPs in the participants’ electroencephalogram (EEG) as a measure of cognitive workload. For the balance task, a computer-based tilt-ball game was combined with a balance board. Participants played the game by shifting their weight to tilt the balance board, which moved a virtual ball to score goals. The game was manipulated by adjusting the size of the goalposts to set three predefined levels of game difficulty (easy, medium, and hard). The participant’s experience of game difficulty was evaluated based on the number of goals scored and their subjective reporting of perceived difficulty. Participants experienced a significant difference in the three levels of task difficulty based on the number of goals scored and perceived difficulty (p < 0.001). Post hoc analysis revealed the lowest performance for the hardest level. The mean amplitude of the N1 ERP component was used to measure the cognitive workload associated with the three difficulty levels. The N1 component’s amplitude decreased significantly (p < 0.001), with an increase in the task difficulty. Moreover, the amplitude of the N1 component for the hard level was significantly smaller compared to medium (p = 0.0003) and easy (p < 0.001) levels. These results support the efficacy of the N1 ERP component to measure cognitive workload in dynamic and real-life scenarios such as exergames and other rehabilitation exercises. © Copyright © 2021 Ghani, Signal, Niazi and Taylor.",cognitive workload; electroencephalogram; event-related potentials; exergame; rehabilitation,action potential amplitude; adult; Article; birth weight; body equilibrium; clinical effectiveness; cognition; cognitive load; cognitive workload; computer simulation; controlled study; dynamics; electroencephalogram; event related potential; exergame based rehabilitation; game; human; neurorehabilitation; post hoc analysis; tone recognition; validation study,Article,Final,All Open Access; Gold Open Access; Green Open Access,Scopus,2-s2.0-85115364420,Gaming / VR
Zhu G.; Wang J.; Xiao L.; Yang K.; Huang K.; Li B.; Huang S.; Hu B.; Xiao B.; Liu D.; Feng L.; Wang Q.,"Zhu, Guangpu (57211872645); Wang, Jing (57204397284); Xiao, Ling (58373892200); Yang, Ke (57203157381); Huang, Kailing (57222063616); Li, Beibin (57189846063); Huang, Sha (57741237900); Hu, Bingliang (26660863200); Xiao, Bo (57200030735); Liu, Ding (8543105200); Feng, Li (56965973100); Wang, Quan (56145277600)",57211872645; 57204397284; 58373892200; 57203157381; 57222063616; 57189846063; 57741237900; 26660863200; 57200030735; 8543105200; 56965973100; 56145277600,Memory Deficit in Patients With Temporal Lobe Epilepsy: Evidence From Eye Tracking Technology,2021,Frontiers in Neuroscience,15,,716476,,,,14,10.3389/fnins.2021.716476,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85115365531&doi=10.3389%2ffnins.2021.716476&partnerID=40&md5=4487ae13cb1b21d66d262ad9f87ca560,"Objective: To explore quantitative measurements of the visual attention and neuroelectrophysiological relevance of memory deficits in temporal lobe epilepsy (TLE) by eye tracking and electroencephalography (EEG). Methods: Thirty-four TLE patients and twenty-eight healthy controls were invited to complete neurobehavioral assessments, cognitive oculomotor tasks, and 24-h video EEG (VEEG) recordings using an automated computer-based memory assessment platform with an eye tracker. Visit counts, visit time, and time of first fixation on areas of interest (AOIs) were recorded and analyzed in combination with interictal epileptic discharge (IED) characteristics from the bilateral temporal lobes. Results: The TLE patients had significantly worse Wechsler Digit Span scores [F(1, 58) = 7.49, p = 0.008]. In the Short-Term Memory Game with eye tracking, TLE patients took a longer time to find the memorized items [F(1, 57) = 17.30, p < 0.001]. They had longer first fixation [F(1, 57) = 4.06, p = 0.049] and more visit counts [F(1, 57) = 7.58, p = 0.008] on the target during the recall. Furthermore, the performance of the patients in the Digit Span task was negatively correlated with the total number of IEDs [r(28) = −0.463, p = 0.013] and the number of spikes per sleep cycle [r(28) = −0.420, p = 0.026]. Conclusion: Eye tracking appears to be a quantitative, objective measure of memory evaluation, demonstrating memory retrieval deficits but preserved visual attention in TLE patients. Nocturnal temporal lobe IEDs are closely associated with memory performance, which might be the electrophysiological mechanism for memory impairment in TLE. © Copyright © 2021 Zhu, Wang, Xiao, Yang, Huang, Li, Huang, Hu, Xiao, Liu, Feng and Wang.",electroencephalography; eye tracking; memory impairment; temporal lobe epilepsy; visual attention,adult; amnesia; Article; behavior assessment; brain electrophysiology; clinical article; clinical feature; computer analysis; controlled study; disease association; electroencephalography; eye movement control; female; human; male; memory assessment; neurologic examination; pilot study; quantitative analysis; recall; Short Term Memory Game; sleep pattern; spike; temporal lobe epilepsy; videorecording; visual attention; Wechsler Digit Span score,Article,Final,All Open Access; Gold Open Access; Green Open Access,Scopus,2-s2.0-85115365531,Gaming / VR
Mavromoustakos-Blom P.; Kosa M.; Bakkes S.; Spronck P.,"Mavromoustakos-Blom, Paris (56443691300); Kosa, Mehmet (57027872500); Bakkes, Sander (22333308100); Spronck, Pieter (57195385803)",56443691300; 57027872500; 22333308100; 57195385803,Correlating Facial Expressions and Subjective Player Experiences in Competitive Hearthstone,2021,ACM International Conference Proceeding Series,,,3472577,,,,2,10.1145/3472538.3472577,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85118231365&doi=10.1145%2f3472538.3472577&partnerID=40&md5=ea760f9031bc5624a7bd8220b6ce4a18,"In this study, we used recordings of players' facial expressions that are captured during competitive Hearthstone games to analyse the correlation between in-game player affective responses and subjective post-game self-reports. With this, we aimed to examine whether eye gaze, head pose and emotions gathered as objective data from face recordings would be associated with subjective experiences of players which were collected in the form of a post-game survey. Data was collected during a live offline Hearthstone competition, which involved a total of 17 players and 31 matches played. Correlation analyses between in-game and post-game variables show that players' facial expressions and eye gaze measurements are associated with both players' attention to the opponent and their mood influenced by the opponent. In future research, these results may be used to implement predictive player models.  © 2021 ACM.",competitive games; Facial expression analysis; game experience questionnaire; hearthstone; player affect; player experience,Competitive games; Eye-gaze; Facial Expressions; Facial expressions analysis; Game experience; Game experience questionnaire; Game players; Hearthstone; Player affect; Player experience; Surveys,Conference paper,Final,All Open Access; Bronze Open Access; Green Open Access,Scopus,2-s2.0-85118231365,Gaming / VR
Gündoğdu S.; Çolak Ö.H.; Doğan E.A.; Gülbetekin E.; Polat Ö.,"Gündoğdu, Serdar (16052854500); Çolak, Ömer Halil (57224872197); Doğan, Ebru Apaydın (8658309100); Gülbetekin, Evrim (16549540300); Polat, Övünç (14056893000)",16052854500; 57224872197; 8658309100; 16549540300; 14056893000,Assessment of mental fatigue and stress on electronic sport players with data fusion,2021,Medical and Biological Engineering and Computing,59,9,,1691,1707,16.0,25,10.1007/s11517-021-02389-9,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85109263610&doi=10.1007%2fs11517-021-02389-9&partnerID=40&md5=1dfe171a4a26f3719a446a3dd8fbcc36,"Stress and mental fatigue are in existence constantly in daily life, and decrease our productivity while performing our daily routines. The purpose of this study was to analyze the states of stress and mental fatigue using data fusion while e-sport activity. In the study, ten volunteers performed e-sport duty which required both physical and mental effort and skills for 2 min. Volunteers’ electroencephalogram (EEG), galvanic skin response (GSR), heart rate variability (HRV), and eye tracking data were obtained before and during game and then were analyzed. In addition, the effects of e-sports were evaluated with visual analogue scale and d2 attention tests. The d2 tests are performed after the game, and the game has a positive effect on attention and concentration. EEG from the frontal region indicates that the game is partly caused by stress and mental fatigue. HRV analysis showed that the sympathetic and vagal activities created by e-sports on people are different. By evaluating HRV and GSR together, it was seen that the emotional processes of the participants were stressed in some and excited in others. Data fusion can serve a variety of purposes such as determining the effect of e-sports activity on the person and the appropriate game type. Graphical abstract: [Figure not available: see fulltext.] © 2021, International Federation for Medical and Biological Engineering.",Attention test; Electroencephalogram; Electronic sports; Eye tracking system; Galvanic skin response; Heart rate variability; Mental fatigue; N-back test; Stress,Electroencephalography; Electronics; Galvanic Skin Response; Heart Rate; Humans; Mental Fatigue; Sports; Data fusion; Diseases; Electroencephalography; Electronic assessment; Electrophysiology; Eye tracking; Daily routines; Electro-encephalogram (EEG); Electronic sports; Frontal regions; Galvanic skin response; Heart rate variability; Mental fatigue; Visual analogue scale; adult; Article; attention; attention test; controlled study; electrodermal response; electroencephalogram; emotion regulation; eye tracking; female; game; heart rate variability; human; human experiment; male; mental fatigue; mental stress; n-back test; normal human; signal processing; skill; skin conductance; vagus nerve stimulation; visual analog scale; working memory; dysthymia; electroencephalography; electronics; heart rate; sport; Sports,Article,Final,,Scopus,2-s2.0-85109263610,Gaming / VR
Marchiori D.; Di Guida S.; Polonio L.,"Marchiori, Davide (23667863200); Di Guida, Sibilla (55357030300); Polonio, Luca (56529761100)",23667863200; 55357030300; 56529761100,Plasticity of strategic sophistication in interactive decision-making,2021,Journal of Economic Theory,196,,105291,,,,8,10.1016/j.jet.2021.105291,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85109019706&doi=10.1016%2fj.jet.2021.105291&partnerID=40&md5=190ee3c10019e7545da33e8621cc4f5c,"We propose an experimental eye-tracking study to test how strategic sophistication is shaped by experience in 3×3 two-person normal-form games. Although strategic sophistication has been shown to be linked to a variety of endogenous and exogenous factors, little is known about how it is affected by previous interactive decisions. We show that complete feedback in previous games can significantly enhance strategic sophistication, and that games that in principle provide equivalent learning opportunities lead instead to substantially different learning outcomes. Specifically, only repeated play with feedback of games that emphasize strategic interdependence significantly enhances strategic learning, producing an increase in the frequency of equilibrium play and a shift of attention to the incentives of the counterpart. Moreover, we find that the type of learning underlying newly gained strategic skills can vary substantially across players. Whereas some players eventually learn to visually analyze the payoff matrix consistently with equilibrium reasoning, others appear to use experience with previous interactions to devise simple heuristics of play. Our results have implications for theoretical and computational modeling of learning. © 2021 Elsevier Inc.",Experimental economics; Eye-tracking; Learning; Repeated games; Strategic sophistication,,Article,Final,All Open Access; Green Open Access,Scopus,2-s2.0-85109019706,Gaming / VR
Geraets C.N.W.; Klein Tuente S.; Lestestuiver B.P.; van Beilen M.; Nijman S.A.; Marsman J.B.C.; Veling W.,"Geraets, C.N.W. (57053538000); Klein Tuente, S. (56464290200); Lestestuiver, B.P. (57221734323); van Beilen, M. (6603265975); Nijman, S.A. (57205171580); Marsman, J.B.C. (25957645800); Veling, W. (19934411900)",57053538000; 56464290200; 57221734323; 6603265975; 57205171580; 25957645800; 19934411900,Virtual reality facial emotion recognition in social environments: An eye-tracking study,2021,Internet Interventions,25,,100432,,,,52,10.1016/j.invent.2021.100432,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85110623695&doi=10.1016%2fj.invent.2021.100432&partnerID=40&md5=50a7216d7b406d9a20b090130fc8e249,"Background: Virtual reality (VR) enables the administration of realistic and dynamic stimuli within a social context for the assessment and training of emotion recognition. We tested a novel VR emotion recognition task by comparing emotion recognition across a VR, video and photo task, investigating covariates of recognition and exploring visual attention in VR. Methods: Healthy individuals (n = 100) completed three emotion recognition tasks; a photo, video and VR task. During the VR task, emotions of virtual characters (avatars) in a VR street environment were rated, and eye-tracking was recorded in VR. Results: Recognition accuracy in VR (overall 75%) was comparable to the photo and video task. However, there were some differences; disgust and happiness had lower accuracy rates in VR, and better accuracy was achieved for surprise and anger in VR compared to the video task. Participants spent more time identifying disgust, fear and sadness than surprise and happiness. In general, attention was directed longer to the eye and nose areas than the mouth. Discussion: Immersive VR tasks can be used for training and assessment of emotion recognition. VR enables easily controllable avatars within environments relevant for daily life. Validated emotional expressions and tasks will be of relevance for clinical applications. © 2021 The Authors",Affect; Avatars; Emotion; Emotion recognition; Eye-tracking; Virtual reality,adult; anger; article; controlled study; disgust; emotion assessment; eye tracking; face; fear; female; happiness; human; major clinical study; male; mouth; nose; sadness; social environment; videorecording; virtual reality; visual attention,Article,Final,All Open Access; Gold Open Access; Green Open Access,Scopus,2-s2.0-85110623695,Gaming / VR
Barragan J.A.; Chanci D.; Yu D.; Wachs J.P.,"Barragan, Juan Antonio (57218000158); Chanci, Daniela (57210210753); Yu, Denny (55190007200); Wachs, Juan P. (9241519000)",57218000158; 57210210753; 55190007200; 9241519000,SACHETS: Semi-autonomous cognitive hybrid emergency teleoperated suction,2021,"2021 30th IEEE International Conference on Robot and Human Interactive Communication, RO-MAN 2021",,,,1243,1248,5.0,8,10.1109/RO-MAN50785.2021.9515517,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85115048162&doi=10.1109%2fRO-MAN50785.2021.9515517&partnerID=40&md5=cba8125f0a725173e70d4a423402890f,"Blood suction and irrigation are among the most critical support tasks in robotic-assisted minimally invasive surgery (RMIS). Usually, suction/irrigation tools are controlled by a surgical assistant to maintain a clear view of the surgical field. Thus, the assistant's contribution to other emergency support tasks is limited. Similarly, when the surgical assistant is not available to perform the blood suction, the leading surgeon must take over this task, which in a complex surgical procedure can result in an unnecessary increment in the cognitive load. To alleviate this problem, we have developed a semi-autonomous robotic suction assistant, which was integrated with a Da Vinci Research Kit (DVRK). At the heart of the algorithm, there is an autonomous control based on a deep learning model to segment and identify the location of blood accumulations. This system provides automatic suction allowing the leading surgeon to focus exclusively on the main task through the control of key instruments of the robot. We conducted a user study to evaluate the user's workload demands and performance while doing a surgical task under two modalities: (1) autonomous suction action and (2) a surgeon-controlled-suction. Our results indicate that users working with the autonomous system completed the task 161 seconds faster than in the surgeon-controlled-suction modality. Furthermore, the autonomous modality led to a lower percentage of bleeding in the surgical field and workload demands on the users (p-value<0.05). These results show how leveraging state-of-the-art AI algorithms can reduce cognitive demands and enhance performance. © 2021 IEEE.",Cognitive workload; Human robotic interaction; Robotic surgery; Semi-autonomous assistant,Agricultural robots; Blood; Deep learning; Robotics; Social robots; Surgery; Autonomous control; Autonomous robotics; Autonomous systems; Cognitive demands; Cognitive loads; Minimally invasive surgery; State of the art; Surgical procedures; Robotic surgery,Conference paper,Final,,Scopus,2-s2.0-85115048162,Gaming / VR
Moen F.; Olsen M.; Halmøy G.; Hrozanova M.,"Moen, Frode (35269291200); Olsen, Maja (57215696359); Halmøy, Gunvor (57450140100); Hrozanova, Maria (57189262474)",35269291200; 57215696359; 57450140100; 57189262474,"Variations in Elite Female Soccer Players' Sleep, and Associations With Perceived Fatigue and Soccer Games",2021,Frontiers in Sports and Active Living,3,,694537,,,,6,10.3389/fspor.2021.694537,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124569580&doi=10.3389%2ffspor.2021.694537&partnerID=40&md5=4e1cef208fcd75076fb46106c395661d,"The current study investigated the associations between female perceived fatigue of elite soccer players and their sleep, and the associations between the sleep of players and soccer games. The sample included 29 female elite soccer players from the Norwegian national soccer team with a mean age of ~26 years. Perceived fatigue and sleep were monitored over a period of 124 consecutive days. In this period, 12.8 ± 3.9 soccer games per player took place. Sleep was monitored with an unobtrusive impulse radio ultra-wideband Doppler radar (Somnofy). Perceived fatigue was based on a self-report mobile phone application that detected daily experienced fatigue. Multilevel analyses of day-to-day associations showed that, first, increased perceived fatigue was associated with increased time in bed (3.6 ± 1.8 min, p = 0.037) and deep sleep (1.2 ± 0.6 min, p = 0.007). Increased rapid eye movement (REM) sleep was associated with subsequently decreased perceived fatigue (−0.21 ± 0.08 arbitrary units [AU], p = 0.008), and increased respiration rate in non-REM sleep was associated with subsequently increased fatigue (0.27 ± 0.09 AU, p = 0.002). Second, game night was associated with reduced time in bed (−1.0 h ± 8.4 min, p = <0.001), total sleep time (−55.2 ± 6.6 min, p = <0.001), time in sleep stages (light: −27.0 ± 5.4 min, p = <0.001; deep: −3.6 ± 1.2 min, p = 0.001; REM: −21.0 ± 3.0 min, p = <0.001), longer sleep-onset latency (3.0 ± 1.2 min, p = 0.013), and increased respiration rate in non-REM sleep (0.32 ± 0.08 respirations per min, p = <0.001), compared to the night before the game. The present findings show that deep and REM sleep and respiration rate in non-REM sleep are the key indicators of perceived fatigue in female elite soccer players. Moreover, sleep is disrupted during game night, likely due to the high physical and mental loads experienced during soccer games. Sleep normalizes during the first and second night after soccer games, likely preventing further negative performance-related consequences. Copyright © 2021 Moen, Olsen, Halmøy and Hrozanova.",fatigue; freshness; games; sleep; soccer,,Article,Final,All Open Access; Gold Open Access; Green Open Access,Scopus,2-s2.0-85124569580,Gaming / VR
Kim N.; Lee H.,"Kim, Nayeon (57216898685); Lee, Hyunsoo (54393374900)",57216898685; 54393374900,Assessing Consumer Attention and Arousal Using Eye-Tracking Technology in Virtual Retail Environment,2021,Frontiers in Psychology,12,,665658,,,,40,10.3389/fpsyg.2021.665658,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85113719062&doi=10.3389%2ffpsyg.2021.665658&partnerID=40&md5=f80392a08f754bae1bfa9fcd4f40a3be,"This study aims at investigating how consumers experience the retail environment visually, thus establishing a foundation for deeper insights into visual merchandising strategies. Specifically, we experimentally recorded and analyzed the visual attention and emotional arousal of the consumers in a test setting and examined the influence of various elements as well as gender differences in the recorded consumer responses. We conducted an experiment utilizing eye-tracking and virtual reality to analyze visual attention and emotional arousal in response to spatial and design elements in an immersive retail environment. We examined real-time measures of consumer interest and emotional responses during the retail experience. Valid gaze data from 24 male and 22 female participants were used for the analysis of total dwell time (TDT), total fixation count (TFC), and average pupil diameter (APD). The visual attention and emotional arousal of consumers showed different responses to specific areas of interest according to different spatial arrangements in the sales and service areas. This study statistically analyzed gender differences in consumer responses and performed a correlation analysis between visual attention and emotional arousal. Our findings provide insight into improving the design of retail environments for target consumers and contribute to building visual merchandising strategies. © Copyright © 2021 Kim and Lee.",emotional arousal; eye-tracking; retail environment; virtual reality; visual attention; visual merchandising,,Article,Final,All Open Access; Gold Open Access; Green Open Access,Scopus,2-s2.0-85113719062,Gaming / VR
Dimitrova M.; Wagatsuma H.; Krastev A.; Vrochidou E.; Nunez-Gonzalez J.D.,"Dimitrova, Maya (7006541001); Wagatsuma, Hiroaki (6603005439); Krastev, Aleksandar (48561297600); Vrochidou, Eleni (53867522300); Nunez-Gonzalez, J. David (55890456200)",7006541001; 6603005439; 48561297600; 53867522300; 55890456200,"A Review of Possible EEG Markers of Abstraction, Attentiveness, and Memorisation in Cyber-Physical Systems for Special Education",2021,Frontiers in Robotics and AI,8,,715962,,,,4,10.3389/frobt.2021.715962,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85115000536&doi=10.3389%2ffrobt.2021.715962&partnerID=40&md5=44a0fea8ab14c670c7e1fa25ecb96812,"Cyber-physical systems (CPSs) for special education rely on effective mental and brain processing during the lesson, performed with the assistance of humanoid robots. The improved diagnostic ability of the CPS is a prerogative of the system for efficient technological support of the pedagogical process. The article focuses on the available knowledge of possible EEG markers of abstraction, attentiveness, and memorisation (in some cases combined with eye tracking) related to predicting effective mental and brain processing during the lesson. The role of processing abstraction is emphasised as the learning mechanism, which is given priority over the other mechanisms by the cognitive system. The main markers in focus are P1, N170, Novelty P3, RewP, N400, and P600. The description of the effects is accompanied by the analysis of some implications for the design of novel educational scenarios in inclusive classes. © Copyright © 2021 Dimitrova, Wagatsuma, Krastev, Vrochidou and Nunez-Gonzalez.",abstraction; curiosity; cyber-physical systems for special education; EEG marker; involuntary attention; memory; novelty; surprise,,Review,Final,All Open Access; Gold Open Access; Green Open Access,Scopus,2-s2.0-85115000536,Gaming / VR
Lin P.-H.; Chen H.-J.; Wang Z.-Q.,"Lin, Po-Hung (37665372500); Chen, Hung-Jen (16038680500); Wang, Zhi-Qian (57232932500)",37665372500; 16038680500; 57232932500,Visual performance assessment of videos—a case study of the game “spot the difference”,2021,Applied Sciences (Switzerland),11,16,7628,,,,2,10.3390/app11167628,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85113512591&doi=10.3390%2fapp11167628&partnerID=40&md5=ed873116f0ed9c8decd7d5a07737a65d,"“Spot the Difference” is a well-known game where players must find subtle differences between two almost identical pictures. If “Spot the Difference” is designed for videos, what is the difference between videos and pictures? If the performance of videos is measured by an eye tracker, what scan paths will be conducted? In this study, we explored this game using a video to conduct a visual performance evaluation. Twenty-five subjects were recruited in a full-factorial experiment to investigate the effect of background (with background, without background), video type (animation, text), and arrangement (left-to-right, top-to-bottom) on searching, eye tracking performance, and visual fatigue. The results showed that the video type had a significant effect on the accuracy and subjective visual fatigue, with the accuracy and subjective visual fatigue for animation being better than for text. The results also indicated that the arrangement had a significant effect on the number of fixations, where top-to-bottom arrangement brought a higher number of fixations. The background had a significant effect on accuracy and subjective visual fatigue, where the accuracy and subjective visual fatigue without a background was better than with a background. For the analysis of the scan path, a denser scan path was found in text than in animation, in top-to-bottom arrangement than in left-to-right arrangement, and without a background than with a background. In the future, game manufacturers should use the results of this research to design different “Spot the Difference” videos. When designing a simple game, an animation without a background and involving a left-to-right arrangement was recommended. When designing a difficult game, the opposite settings should be used. © 2021 by the authors. Licensee MDPI, Basel, Switzerland.",Arrangement; Background; Eye tracking; Spot the difference; Video type,,Article,Final,All Open Access; Gold Open Access,Scopus,2-s2.0-85113512591,Gaming / VR
Pallavicini F.; Pepe A.; Mantovani F.,"Pallavicini, Federica (57226210876); Pepe, Alessandro (55744410200); Mantovani, Fabrizia (7006190897)",57226210876; 55744410200; 7006190897,Commercial off-the-shelf video games for reducing stress and anxiety: Systematic review,2021,JMIR Mental Health,8,8,e28150,,,,69,10.2196/28150,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85113303149&doi=10.2196%2f28150&partnerID=40&md5=1b8228a9d7be552ca13f05821f3a44da,"Background: Using commercial off-the-shelf video games rather than custom-made computer games could have several advantages for reducing stress and anxiety, including their low cost, advanced graphics, and the possibility to reach millions of individuals worldwide. However, it is important to emphasize that not all commercial video games are equal, and their effects strongly depend on specific characteristics of the games. Objective: The aim of this systematic review was to describe the literature on the use of commercial off-the-shelf video games for diminishing stress and anxiety, examining the research outcomes along with critical variables related to computer game characteristics (ie, genre, platform, time of play). Methods: A systematic search of the literature was performed following the PRISMA (Preferred Reporting Items for Systematic Reviews and Meta-Analysis) guidelines. The search databases were PsycINFO, Web of Science, Medline, IEEExplore, and the Cochrane Library. The search string was: [(“video game*”) OR (“computer game*”)] AND [(“stress”) OR (“anxiety”) OR (“relaxation”)] AND [(“study”) OR (“trial”) OR (“training”)]. Results: A total of 28 studies met the inclusion criteria for the publication period 2006-2021. The findings demonstrate the benefit of commercial off-the-shelf video games for reducing stress in children, adults, and older adults. The majority of the retrieved studies recruited young adults, and fewer studies have involved children, middle-aged adults, and older adults. In addition to exergames and casual video games, other genres of commercial off-the-shelf games helped to reduce stress and anxiety. Conclusions: Efficacy in reducing stress and anxiety has been demonstrated not only for exergames and casual video games but also for other genres such as action games, action-adventure games, and augmented reality games. Various gaming platforms, including consoles, PCs, smartphones, mobile consoles, and virtual reality systems, have been used with positive results. Finally, even single and short sessions of play had benefits in reducing stress and anxiety. © Federica Pallavicini, Alessandro Pepe, Fabrizia Mantovani. Originally published in JMIR Mental Health (https://mental.jmir.org),16.08.2021. This is an open-access article distributed under the terms of the Creative Commons Attribution License (https://creativecommons.org/licenses/by/4.0/), which permits unrestricted use, distribution, and reproduction in any medium, provided the original work, first published in JMIR Mental Health, is properly cited. The complete bibliographic information, a link to the original publication on https://mental.jmir.org/, as well as this copyright and license information must be included.",Anxiety; Commercial off-the-shelf video games; Relaxation; Stress; Video games,anxiolytic agent; acute stress; adult; anxiety; anxiety disorder; arousal; augmented reality; burn; car racing; cesarean section; child; chronic wound; clinical effectiveness; Cochrane Library; cognition; comparative effectiveness; competition game; coordination game; data quality; depression; emergency surgery; emotional stress; eye movement desensitization and reprocessing; fighting; happiness; hematologic malignancy; Hospital Anxiety and Depression Scale; human; internet use; kinesiotherapy; limb injury; male; Medline; mental stress; middle aged; military personnel; MMORPG (game); nursing home patient; obesity; Parkinson disease; Perceived Stress Scale; physical disability; physiotherapy; posttraumatic stress disorder; psychopharmacotherapy; psychotrauma; PsycINFO; quality control; randomization; randomized controlled trial (topic); relaxation training; Review; role playing; sitting; sport; State Trait Anxiety Inventory; study design; systematic review; systemic lupus erythematosus; task performance; tooth disease; treatment duration; university student; video game; virtual reality; Web of Science; worker; young adult,Review,Final,All Open Access; Gold Open Access; Green Open Access,Scopus,2-s2.0-85113303149,Gaming / VR
Ai X.; Wu Z.; Guo T.; Zhong J.; Hu N.; Fu C.,"Ai, Xiaoqun (57226658411); Wu, Zhendong (55500650300); Guo, Ting (57226671674); Zhong, Jiayan (57226680733); Hu, Nan (57226663114); Fu, Chunliu (57226680080)",57226658411; 55500650300; 57226671674; 57226680733; 57226663114; 57226680080,The effect of visual attention on stereoscopic lighting of museum ceramic exhibits: A virtual environment mixed with eye-tracking,2021,Informatica (Slovenia),45,5,,713,729,16.0,7,10.31449/inf.v45i5.3454,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85112283145&doi=10.31449%2finf.v45i5.3454&partnerID=40&md5=ed0892b48593e1067101668b6782d06c,"Stereoscopic lighting is one of the influencing visual quality factors improving the visual presentation of ceramic exhibits. However, the current research only has a specific definition, and there is no clear measurement and evaluation method. This research adopted the method of bidirectional verification: Psychophysical experiment and immersive virtual reality (VR) with eye-movement tracking. We proposed a standard questionnaire for the stereoscopic visual experience. The eye-tracking data show that the visible fixation duration with stereoscopic lighting is (M = 5.18 s), and the number of fixation points accounted for 63.48% of the whole number. Both indexes were higher than the situation without stereoscopic lighting. The result shows that the accuracy of finding the detail changes with or without stereoscopic lighting was 12.5% through the comparison of visual identification tasks, and finding modeling changes was 11.5%. In the situation with stereoscopic lighting, the first identification frequency increased by 15.9% for small pattern changes of exhibits, as local modeling changes increased by 19%. These results indicated that the stereoscopic lighting set according to the exhibits' volume could effectively improve the viewer's visual guide, strengthen the visual attention of the details, and model elements of the exhibits to achieve the optimal appreciation. © 2021 Slovene Society Informatika. All rights reserved.",Ceramic exhibit; Eye-movement tracking; Immersive virtual environment; Stereoscopic lighting,Behavioral research; Eye movements; Eye tracking; Lighting; Stereo image processing; Bidirectional verifications; Eye-movement tracking; Immersive virtual reality; Number of fixations; Psychophysical experiments; Visual experiences; Visual identification; Visual presentation; Virtual reality,Article,Final,All Open Access; Gold Open Access,Scopus,2-s2.0-85112283145,Gaming / VR
Hu B.; Liu Y.; Gu X.,"Hu, Bihao (57258527300); Liu, Yan (57203675598); Gu, Xiaoqing (26323563700)",57258527300; 57203675598; 26323563700,"Is it true that ""If you are not immersed, you are not learning""where we will be?",2021,"Proceedings - IEEE 21st International Conference on Advanced Learning Technologies, ICALT 2021",,,,414,415,1.0,0,10.1109/ICALT52272.2021.00131,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85114857137&doi=10.1109%2fICALT52272.2021.00131&partnerID=40&md5=51ce9727f63e81f251efe0597d935015,"The purpose of this research was to explore how learners' non-cognitive abilities (including attention, engagement, presence, curiosity) differ in the learning process and the final learning results in VR and non-VR learning environments. This can be used to explain how the learning process of the learner in the VR environment occurs and changes, and the impact of the VR learning environment on the learner's non-cognitive ability. At the same time, it provided suggestions for the wider application of VR in education and the further development of personalized learning in education. Therefore, this research identified two research questions. The first one was whether the learning results of learners were different in VR and non-VR learning environments. The second was through the description of multimodal data such as eye movements and GSR. What was the relationship between the learner's non-cognitive abilities (attention, presence, engagement) and learning outcomes during the learning process? This study measured the curiosity of learners using a questionnaire before the experiment and measured the presence and engagement, compared scores between experimental group's and control groups' to understand how is immersed environment learning.  © 2021 IEEE.",Attention span; Engagement span; Eye-tracking; VR environment,Eye movements; Learning systems; Cognitive ability; Experimental groups; Learning environments; Learning outcome; Learning process; Multi-modal data; Personalized learning; Research questions; Computer aided instruction,Conference paper,Final,,Scopus,2-s2.0-85114857137,Gaming / VR
Ferreira C.P.; González-González C.S.; Adamatti D.F.,"Ferreira, Cleiton Pons (57222126032); González-González, Carina Soledad (22234459200); Adamatti, Diana Francisca (25929020300)",57222126032; 22234459200; 25929020300,Business simulation games analysis supported by human-computer interfaces: A systematic review,2021,Sensors,21,14,4810,,,,19,10.3390/s21144810,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85109943382&doi=10.3390%2fs21144810&partnerID=40&md5=2a111df9eb45e2135846d8369c993985,"This article performs a Systematic Review of studies to answer the question: What are the researches related to the learning process with (Serious) Business Games using data collection techniques with Electroencephalogram or Eye tracking signals? The PRISMA declaration method was used to guide the search and inclusion of works related to the elaboration of this study. The 19 references resulting from the critical evaluation initially point to a gap in investigations into using these devices to monitor serious games for learning in organizational environments. An approximation with equivalent sensing studies in serious games for the contribution of skills and competencies indicates that continuous monitoring measures, such as mental state and eye fixation, proved to identify the playersʹ attention levels effectively. Also, these studies showed effectiveness in the flow at different moments of the task, motivating and justifying the replication of these studies as a source of insights for the optimized design of business learning tools. This study is the first systematic review and consolidates the existing literature on user experience analysis of business simulation games supported by human-computer interfaces. © 2021 by the authors. Licensee MDPI, Basel, Switzerland.",Business simulation game; EEG; Eye tracking; Learning; Neuroscience; Serious game,Computers; Humans; Problem Solving; Psychotherapy; User-Computer Interface; Video Games; Computer aided software engineering; Eye tracking; Learning systems; User experience; Business simulation games; Continuous monitoring; Critical evaluation; Games for learning; Human computer interfaces; Learning process; Optimized designs; Systematic Review; computer; computer interface; human; problem solving; psychotherapy; video game; Serious games,Review,Final,All Open Access; Gold Open Access; Green Open Access,Scopus,2-s2.0-85109943382,Gaming / VR
Davis R.,"Davis, Rebecca (24279725800)",24279725800,The Feasibility of Using Virtual Reality and Eye Tracking in Research With Older Adults With and Without Alzheimer's Disease,2021,Frontiers in Aging Neuroscience,13,,607219,,,,16,10.3389/fnagi.2021.607219,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85109712577&doi=10.3389%2ffnagi.2021.607219&partnerID=40&md5=0fdfa76e85ba346be789f90d6adec163,"Aim: To examine the feasibility of using large scale spatial, self-mobile, virtual reality, and eye tracking in older adults with and without Alzheimer's disease (AD). Methods: Older adults with early stage AD (n = 38) and a control group without AD (n = 50) were asked to find their way in a large, projected VR simulation of a retirement community repeatedly over 10 trials for each of 2 days, while wearing eye tracking glasses. Feasibility measures, including tolerance, side effects, and ability to complete the VR and eye tracking were collected. This study reports the analysis of the feasibility data for the VR and eye tracking and comparison of findings between the groups. Results: Over 80% of the subjects were able to complete the VR portion of the study. Only four subjects, all in the AD group, could not use the joystick and were excluded. Withdrawal rate (18%) was similar between the groups [X2(2) = 2.82, N = 88, p = 0.245] with most withdrawals occurring after the fourth trial. Simulation sickness was not significantly different between the groups. Only 60% of the subjects had completed eye tracking videos; more subjects in the AD group had complete eye tracking videos than the control group; X2(1) = 7.411, N = 88, p = 0.006. Eye tracking incompletion was primarily due to inability to calibration issues. Conclusion: Virtual reality testing and eye tracking can be used in older adults with and without AD in a large-scale way-finding task, but that there are some limitations. © Copyright © 2021 Davis.",aging; Alzheimer's disease; eye tracking; feasibility; spatial cognition; virtual navigation,,Article,Final,All Open Access; Gold Open Access; Green Open Access,Scopus,2-s2.0-85109712577,Gaming / VR
Eckert M.; Habets E.A.P.; Rummukainen O.S.,"Eckert, Marie (57210227449); Habets, Emanuel A. P. (13608885900); Rummukainen, Olli S. (55354761500)",57210227449; 13608885900; 55354761500,Cognitive load estimation based on pupillometry in virtual reality with uncontrolled scene lighting,2021,"2021 13th International Conference on Quality of Multimedia Experience, QoMEX 2021",,,9465417,73,76,3.0,12,10.1109/QoMEX51781.2021.9465417,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85113849264&doi=10.1109%2fQoMEX51781.2021.9465417&partnerID=40&md5=26e733a84346b456ce266e823ace591b,"Virtual reality (VR) technology enables and requires new ways of user experience testing in immersive environments. For various aspects of user experience, objective assessment of the cognitive load can be a useful parameter. With eye-tracking becoming a more widespread feature of current VR headsets, pupillometry is an appealing option to unobtrusively measure cognitive load during a VR experience. This paper shows that pupil size measured by an off-the-shelf VR headset with an integrated eye tracker positively correlates with the self-reported cognitive load during a standard n-back task adapted to a VR environment. To overcome the need for steady scene-lighting conditions, we present a method to correct for the light-induced pupil size changes, otherwise masking the cognitive load effects. Our results show that a commercially available VR headset with eye tracking can be used to measure the cognitive load in unpredictable lighting conditions without additional hardware. © 2021 IEEE.",cognitive load; compensation of pupillary light reflex; eye tracking; virtual reality,Eye tracking; Lighting; Multimedia systems; User experience; Cognitive loads; Eye trackers; Immersive environment; Light-induced; Lighting conditions; Objective assessment; Pupillometry; Scene lighting; Virtual reality,Conference paper,Final,,Scopus,2-s2.0-85113849264,Gaming / VR
Parra E.; Chicchi Giglioli I.A.; Philip J.; Carrasco-Ribelles L.A.; Marín-Morales J.; Alcañiz Raya M.,"Parra, Elena (55547606400); Chicchi Giglioli, Irene Alice (56994284500); Philip, Jestine (57201796092); Carrasco-Ribelles, Lucia Amalia (57221713820); Marín-Morales, Javier (57191977544); Alcañiz Raya, Mariano (36921902100)",55547606400; 56994284500; 57201796092; 57221713820; 57191977544; 36921902100,Combining virtual reality and organizational neuroscience for leadership assessment,2021,Applied Sciences (Switzerland),11,13,5956,,,,9,10.3390/app11135956,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85109277670&doi=10.3390%2fapp11135956&partnerID=40&md5=d76c6e3fb9ea97df1a43be7345f78000,"In this article, we introduce three-dimensional Serious Games (3DSGs) under an evidencecentered design (ECD) framework and use an organizational neuroscience-based eye-tracking measure to capture implicit behavioral signals associated with leadership skills. While ECD is a wellestablished framework used in the design and development of assessments, it has rarely been utilized in organizational research. The study proposes a novel 3DSG combined with organizational neuroscience methods as a promising tool to assess and recognize leadership-related behavioral patterns that manifest during complex and realistic social situations. We offer a research protocol for assessing task-and relationship-oriented leadership skills that uses ECD, eye-tracking measures, and machine learning. Seamlessly embedding biological measures into 3DSGs enables objective assessment methods that are based on machine learning techniques to achieve high ecological validity. We conclude by describing a future research agenda for the combined use of 3DSGs and organizational neuroscience methods for leadership and human resources. © 2021 by the authors. Licensee MDPI, Basel, Switzerland.",Assessment method; Decision-making behaviors; Evidence-centered design; Leadership style; Machine learning; Serious game; Visual attention,,Article,Final,All Open Access; Gold Open Access; Green Open Access,Scopus,2-s2.0-85109277670,Gaming / VR
Ward C.; Chiat S.; Townend G.S.,"Ward, Callie (57224619830); Chiat, Shula (6603670635); Townend, Gillian S. (56600700500)",57224619830; 6603670635; 56600700500,A comparison of formal and informal methods for assessing language and cognition in children with Rett syndrome,2021,Research in Developmental Disabilities,114,,103961,,,,15,10.1016/j.ridd.2021.103961,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85108024483&doi=10.1016%2fj.ridd.2021.103961&partnerID=40&md5=559a5fcd61f1537e1ef61e2188f42978,"Background: Opinions about the cognitive and receptive language skills of people with Rett syndrome (RTT) range from severe intellectual impairment to near-normal development. Assessment is challenging because most are non-verbal, with no purposeful hand use. Clarkson et al. (2017) adapted the Mullen Scales of Early Learning for use with eye gaze technology (MSEL-A/ET) for people with RTT. Aims: To investigate and compare the performance of children with RTT on formal and newly-designed informal assessments of language and cognition using eye gaze/tracking technology. Methods and procedures: Ten children with RTT aged 4:0–6:8 were assessed on the MSEL-A/ET for Visual Reception (VR) and Receptive Language (RL), and standard MSEL for Expressive Language (EL). Informal assessments of the same skills were embedded in activities such as reading and cake-decorating. Outcomes and results: Standard scores on MSEL-A/ET VR and RL subtests ranged from ‘very low’ to ‘above average’. All children scored ‘very low’ on standard EL assessment. Informal assessments added information about EL, with children producing 1–3 word utterances and a range of communicative functions through an eye gaze device. Conclusions and implications: Combining low-tech augmentative and alternative communication, eye gaze technology, informal activities and formal assessment, yields greater insight into children's abilities. This is important in informing suitable support and education for the individual. © 2021 The Author(s)",Augmentative and alternative communication; Cognitive assessment; Eye tracking; Neurodevelopmental disorder; Receptive and expressive language assessment; Rett syndrome,Child; Cognition; Humans; Language; Language Development Disorders; Language Tests; Rett Syndrome; Article; child; clinical article; cognition assessment; communication skill; comparative study; daily life activity; eye tracking; female; gaze; human; language; language development; male; neurologic examination; pediatric patient; performance; reliability; Rett syndrome; visual system function; cognition; complication; developmental language disorder; language; language test; Rett syndrome,Article,Final,All Open Access; Green Open Access; Hybrid Gold Open Access,Scopus,2-s2.0-85108024483,Gaming / VR
Uludağ S.; Dorak F.; Vurgun N.; Yüzbaşioğlu Y.; Ateş E.,"Uludağ, Sevil (57189902145); Dorak, Ferudun (38961289600); Vurgun, Nilgün (55226644900); Yüzbaşioğlu, Yasin (57201403886); Ateş, Ercan (57226110755)",57189902145; 38961289600; 55226644900; 57201403886; 57226110755,Effects of 10 weeks of imagery and concentration training on visual focus and free-throw performance in basketball players.,2021,Journal of Physical Education and Sport,21,4,223,1761,1768,7.0,11,10.7752/jpes.2021.04223,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85110545105&doi=10.7752%2fjpes.2021.04223&partnerID=40&md5=4a5a5d82807574609ca3f72cb362ed14,"Problem Statement: Free throw shooting is one of the unique techniques applied in the game of basketball. It is a technique that needs special attention to develop due to the fact that it is a unique shot. As much as physiological characteristics prepare the body for the shot, it is important to acknowledge that psychological state of the player affects it, too. Free throw shots can be vital for winning games. Purpose: The purpose of the study was to examine the effects of 10 weeks of imagery and concentration training on visual focus and free-throw performance in basketball players. The participants consisted of 29 basketball players (11 female, 18 male) from different teams in Izmir that compete at the youth level (age 15.62 ±,09). Approach: The participants were divided equally into homogenous groups pre-test and were evaluated based on their success rate. While the study was not done with the control group, the implementation was done with the imagery and concentration group for 10 weeks, three times a week for 15 minutes a day. Concentration studies were done with yantra. In the imagery studies, the athlete was asked to watch the recording of the best shot they made, and to recreate it. Before the participants recreate it, they were advised to read the imagery script and then use the information provided to mentally recreate this material from their desired viewpoint and from an alternative perspective. At the end of 10 weeks, the athletes were tested again. The study investigated whether the mental training affected visual focus or performance or not.Results: The results of this study revealed that there was a significant difference between the pre-test and post-testing protocols of the concentration group, while the control and imagery groups revealed no significant difference (p<0.028). Conclusions: Present findings show that concentration trainings with yantra have an improving effect on visual focus. In order to better understand the effect of imagery and concentration training on performance, different tests can be done. Also, before investigating the effects of imagery trainings on performance, imagery skills of the athletes can be measured. © 2021, Editura Universitatii din Pitesti. All rights reserved.",Concentration; Eye tracking; Free-throw; Imagery; Visual focus,,Article,Final,All Open Access; Gold Open Access,Scopus,2-s2.0-85110545105,Gaming / VR
Hosp B.W.; Schultz F.; Honer O.; Kasneci E.,"Hosp, Benedikt W. (57202890167); Schultz, Florian (55589145900); Honer, Oliver (14026905700); Kasneci, Enkelejda (56059892600)",57202890167; 55589145900; 14026905700; 56059892600,Soccer goalkeeper expertise identification based on eye movements,2021,PLoS ONE,16,5-May,e0251070,,,,21,10.1371/journal.pone.0251070,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85106383248&doi=10.1371%2fjournal.pone.0251070&partnerID=40&md5=b4d103428b7c144aac7169f6220443b8,"By focusing on high experimental control and realistic presentation, the latest research in expertise assessment of soccer players demonstrates the importance of perceptual skills, especially in decision making. Our work captured omnidirectional in-field scenes displayed through virtual reality glasses to 12 expert players (picked by DFB), 10 regional league intermediate players, and13 novice soccer goalkeepers in order to assess the perceptual skills of athletes in an optimized manner. All scenes were shown from the perspective of the same natural goalkeeper and ended after the return pass to that goalkeeper. Based on the gaze behavior of each player, we classified their expertise with common machine learning techniques. Our results show that eye movements contain highly informative features and thus enable a classification of goalkeepers between three stages of expertise, namely elite youth player, regional league player, and novice, at a high accuracy of 78.2%. This research underscores the importance of eye tracking and machine learning in perceptual expertise research and paves the way for perceptual-cognitive diagnosis as well as future training systems. © 2021 Hosp et al. This is an open access article distributed under the terms of the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.",,Adolescent; Adult; Athletes; Athletic Performance; Cognition; Decision Making; Eye Movements; Eye-Tracking Technology; Germany; Humans; Machine Learning; Male; Perception; Soccer; Virtual Reality; Young Adult; accuracy; adolescent; Article; athletic performance; eye movement; gaze; human; human experiment; machine learning; perception; skill; soccer goalkeeper; soccer player; adult; athlete; classification; cognition; decision making; eye movement; Germany; male; physiology; psychology; soccer; virtual reality; young adult,Article,Final,All Open Access; Gold Open Access; Green Open Access,Scopus,2-s2.0-85106383248,Gaming / VR
Lu W.; He H.; Urban A.; Griffin J.,"Lu, Wenyi (57200881818); He, Hao (57210982273); Urban, Alex (57211249210); Griffin, Joe (57198599188)",57200881818; 57210982273; 57211249210; 57198599188,What the Eyes Can Tell: Analyzing Visual Attention with an Educational Video Game,2021,Eye Tracking Research and Applications Symposium (ETRA),PartF169257,,36,,,,8,10.1145/3448018.3459654,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85107565275&doi=10.1145%2f3448018.3459654&partnerID=40&md5=71abc061612b8813a2090954ac69ed6f,"3D video games show potential as educational tools that improve learner engagement. Integrating 3D games into school curricula, however, faces various challenges. One challenge is providing visualizations on learning dashboards for instructors. Such dashboards provide needed information so that instructors may conduct timely and appropriate interventions when students need it. Another challenge is identifying contributive learning predictors for a computational model, which can be the core algorithm used to make games more intelligent for tutoring and assessment purposes. Previous studies have found that students' visual-attention is a vital aspect of engagement during gameplay. However, few studies have examined whether attention visualization patterns can distinguish students from different performance groups. Complicating this research is the relatively nascent investigation into gaze metrics for learning-prediction models. In this exploratory study, we used eye-tracking data from an educational game, Mission HydroSci, to examine visual-attention pattern differences between low and high performers and how their self-reported demographics affect such patterns. Results showed different visual-attention patterns between low and high performers. Additionally, self-reported science, gaming, and navigational expertise levels were significantly correlated to several gaze metric features.  © 2021 ACM.",,Behavioral research; Eye tracking; Human computer interaction; Learning systems; Predictive analytics; Students; Visualization; Computational model; Educational game; Educational tools; Educational video games; Exploratory studies; Performance group; Prediction model; Visual Attention; Three dimensional computer graphics,Conference paper,Final,,Scopus,2-s2.0-85107565275,Gaming / VR
Asbee J.; Parsons T.D.,"Asbee, Justin (57201196275); Parsons, Thomas D. (57207899507)",57201196275; 57207899507,Exploratory Factor Analysis of the Virtual Reality Stroop Task,2021,Annual Review of CyberTherapy and Telemedicine,19,,,61,65,4.0,0,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85136320122&partnerID=40&md5=0c80fefffe983372315d0d3f2e99febf,"Executive functioning involves various controlled processing abilities such as cognitive workload, attention, and inhibition. A common test of executive functioning is the Stroop Color Word Interference Test. The dual process theory has been used to explain results from the Stroop test and states that automatic processing (e.g., word reading) is understood to be an overlearned behavior that requires little effort or direct control from the participant, while controlled processing (e.g., color-word interference) involves inhibition of prepotent (overlearned) responses. In the current study, 85 participants from a university in the southwestern United States (58% female; M age = 19.82, SD = 2.10) completed the Virtual Reality Stroop Task (VRST). To examine the underlying constructs of the VRST, a factor analysis was conducted. Both principal component analysis (PCA) and principal axis factoring (PAF) were conducted. Further, both oblique and orthogonal rotation methods were conducted. There were 4 factors with eigenvalues greater than one, which accounted for 78.46% of the total variance, however based on scree plots, parallel analysis, and the minimum average partial test, 3 factors were retained. Color naming and word reading loaded onto an automatic processing factor. Simple interference and complex interference were loaded onto a controlled processing factor. The VRST may also discriminate between participants’ response accuracy (to Stroop stimuli) relative to low and high stress environmental stimuli. © 2021, Interactive Media Institute. All rights reserved.",Human Performance; Human-computer interactions; Serious Games; Virtual Reality,adult; article; controlled study; exploratory factor analysis; female; human; human computer interaction; human experiment; major clinical study; male; orthogonal rotation; physiological stress; principal component analysis; reading; Stroop test; United States; virtual reality,Article,Final,,Scopus,2-s2.0-85136320122,Gaming / VR
Ahn S.; Santosa S.; Parent M.; Wigdor D.; Grossman T.; Giordano M.,"Ahn, Sunggeun (56413728800); Santosa, Stephanie (57196512534); Parent, Mark (57224004046); Wigdor, Daniel (6507569914); Grossman, Tovi (7003520062); Giordano, Marcello (57223998608)",56413728800; 57196512534; 57224004046; 6507569914; 7003520062; 57223998608,"Stickypie: A gaze-based, scale-invariant marking menu optimized for ar/vr",2021,Conference on Human Factors in Computing Systems - Proceedings,,,,,,,16,10.1145/3411764.3445297,https://www.scopus.com/inward/record.uri?eid=2-s2.0-105002573607&doi=10.1145%2f3411764.3445297&partnerID=40&md5=767ec19e80b46565d74679a67a383aa0,"This work explores the design of marking menus for gaze-based AR/VR menu selection by expert and novice users. It frst identifes and explains the challenges inherent in ocular motor control and current eye tracking hardware, including overshooting, incorrect selections, and false activations. Through three empirical studies, we optimized and validated design parameters to mitigate these errors while reducing completion time, task load, and eye fatigue. Based on the fndings from these studies, we derived a set of design guidelines to support gaze-based marking menus in AR/VR. To overcome the overshoot errors found with eye-based expert marking menu behaviour, we developed StickyPie, a marking menu technique that enables scale-independent marking input by estimating saccade landing positions. An evaluation of StickyPie revealed that StickyPie was easier to learn than the traditional technique (i.e., RegularPie) and was 10% more efcient after 3 sessions. © 2021 ACM.",Ar/vr; Eye gaze input; Head-worn display; Marking menu,Ar/vr; Experts and novice users; Eye gaze input; Eye-gaze; Head-worn displays; Marking Menu; Menu selection; Motor currents; Ocular motor controls; Scale-invariant; Eye tracking,Conference paper,Final,,Scopus,2-s2.0-105002573607,Gaming / VR
David-John B.; Peacock C.; Zhang T.; Murdison T.S.; Benko H.; Jonker T.R.,"David-John, Brendan (57205639875); Peacock, Candace (57200309816); Zhang, Ting (56739503800); Murdison, T. Scott (55810288000); Benko, Hrvoje (9737287100); Jonker, Tanya R. (55510841000)",57205639875; 57200309816; 56739503800; 55810288000; 9737287100; 55510841000,Towards gaze-based prediction of the intent to interact in virtual reality,2021,Eye Tracking Research and Applications Symposium (ETRA),PartF169257,,2,,,,83,10.1145/3448018.3458008,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85107586886&doi=10.1145%2f3448018.3458008&partnerID=40&md5=7559995e1522123353840b6085df3488,"With the increasing frequency of eye tracking in consumer products, including head-mounted augmented and virtual reality displays, gaze-based models have the potential to predict user intent and unlock intuitive new interaction schemes. In the present work, we explored whether gaze dynamics can predict when a user intends to interact with the real or digital world, which could be used to develop predictive interfaces for low-effort input. Eye-tracking data were collected from 15 participants performing an item-selection task in virtual reality. Using logistic regression, we demonstrated successful prediction of the onset of item selection. The most prevalent predictive features in the model were gaze velocity, ambient/focal attention, and saccade dynamics, demonstrating that gaze features typically used to characterize visual attention can be applied to model interaction intent. In the future, these types of models can be used to infer user's near-term interaction goals and drive ultra-low-friction predictive interfaces.  © 2021 ACM.",eye tracking; intent prediction; interaction; mixed reality; virtual reality,Behavioral research; Consumer products; Digital storage; Forecasting; Logistic regression; Virtual reality; Augmented and virtual realities; Digital world; Interaction schemes; Item selection; Model interaction; Ultra-low friction; Visual Attention; Eye tracking,Conference paper,Final,,Scopus,2-s2.0-85107586886,Gaming / VR
Cai T.; Sha S.S.H.A.; Meng Z.; Own C.-M.,"Cai, Tingting (57316205000); Sha, Shuhong Sha (57316029100); Meng, Zhaopeng (7201894900); Own, Chung-Ming (8546378100)",57316205000; 57316029100; 7201894900; 8546378100,Tangible vs. Multi-Touch: Comparing Potential to Enhance Learning for Preschool Children Using Eye-tracking,2021,ACM International Conference Proceeding Series,,,,1,6,5.0,0,10.1145/3478472.3478473,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85118221007&doi=10.1145%2f3478472.3478473&partnerID=40&md5=0fafaa499f839d2e01cf563ca385318c,"Studies have suggested the possible advantages of Tangible User Interfaces (TUIs) over Multi-Touch Interfaces (MTIs) in preschool education, but more objective evidence is lacking. In this study, we designed an math game called ""THE NUMBERS""for preschool children, using eye-tracking to assess cognitive differences between the TUI version and the MTI version to compare their potential to enhance learning. Results showed that when using the TUI version participants made more attempts, which was a significant predictor of learning outcomes; the TUI version had a lower cognitive load; participants' attention was more focused on areas containing critical content using the TUI version; and the TUI version had higher satisfaction than the MTI version because it was more accessible. These findings provided theoretical supports for the use of TUIs in preschool education.  © 2021 ACM.",Multi-Touch Interfaces; Preschool education; Tangible educational games; Tangible User Interfaces,User interfaces; Educational game; Enhance learning; Eye-tracking; Learning outcome; Multi-touch; Multitouch interface; Preschool children; Preschool education; Tangible educational game; Tangible user interfaces; Eye tracking,Conference paper,Final,,Scopus,2-s2.0-85118221007,Gaming / VR
Chen X.-L.; Hou W.-J.,"Chen, Xiao-Lin (57195074496); Hou, Wen-Jun (14627737100)",57195074496; 14627737100,Visual Fatigue Assessment Model Based on Eye-Related Data in Virtual Reality,2021,"International Conference on Virtual Rehabilitation, ICVR",2021-May,,9483841,262,268,6.0,2,10.1109/ICVR51878.2021.9483841,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85111425751&doi=10.1109%2fICVR51878.2021.9483841&partnerID=40&md5=9a65612549ae3c9eba0074d4a7e791a7,"With the widespread virtual reality headsets, the way humans experiencing the virtual environment is rapidly changing. Visual fatigue even extended while using Head-Mounted Displays (HMDs) with near-eye displays embedded. Eye tracker can be used to assess visual fatigue. Recently eye-tracking technology was included in virtual reality headsets such as the FOVE and HTC Vive Pro Eye, which provides a hardware basis for assessing visual fatigue of virtual reality HMDs using the eye tracker. This paper proposed using only HMD intergraded with eye-tracking (HTC Vive Pro Eye) to assess visual fatigue. As no measurement method for the visual fatigue caused by HMDs has been widely accepted, we use how long users use HMDs to demarcate visual fatigue. We implemented three classifications to employ a feature set that contains 13 eye-related features. Two of the models can provide an accurate visual fatigue level. © 2021 IEEE.",eye tracking; virtual reality; visual fatigue,Eye tracking; Helmet mounted displays; Eye trackers; Eye tracking technologies; Feature sets; Head mounted displays; Measurement methods; Virtual-reality headsets; Visual fatigue; Virtual reality,Conference paper,Final,,Scopus,2-s2.0-85111425751,Gaming / VR
Hu Z.; Bulling A.; Li S.; Wang G.,"Hu, Zhiming (57208101391); Bulling, Andreas (6505807414); Li, Sheng (56002421500); Wang, Guoping (7407150270)",57208101391; 6505807414; 56002421500; 7407150270,FixationNet: Forecasting Eye Fixations in Task-Oriented Virtual Environments,2021,IEEE Transactions on Visualization and Computer Graphics,27,5,9382883,2681,2690,9.0,60,10.1109/TVCG.2021.3067779,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85103251105&doi=10.1109%2fTVCG.2021.3067779&partnerID=40&md5=216cf8756fad33afadd5c6a0b28ffb3f,"Human visual attention in immersive virtual reality (VR) is key for many important applications, such as content design, gaze-contingent rendering, or gaze-based interaction. However, prior works typically focused on free-viewing conditions that have limited relevance for practical applications. We first collect eye tracking data of 27 participants performing a visual search task in four immersive VR environments. Based on this dataset, we provide a comprehensive analysis of the collected data and reveal correlations between users' eye fixations and other factors, i.e. users' historical gaze positions, task-related objects, saliency information of the VR content, and users' head rotation velocities. Based on this analysis, we propose FixationNet - a novel learning-based model to forecast users' eye fixations in the near future in VR. We evaluate the performance of our model for free-viewing and task-oriented settings and show that it outperforms the state of the art by a large margin of 19.8% (from a mean error of 2.93° to 2.35°) in free-viewing and of 15.1% (from 2.05° to 1.74°) in task-oriented situations. As such, our work provides new insights into task-oriented attention in virtual environments and guides future work on this important topic in VR research.  © 1995-2012 IEEE.",convolutional neural network; deep learning; Fixation forecasting; task-oriented attention; virtual reality; visual search,"Adolescent; Adult; Computer Graphics; Deep Learning; Female; Fixation, Ocular; Humans; Male; Models, Statistical; Neural Networks, Computer; Virtual Reality; Young Adult; Behavioral research; Eye tracking; Comprehensive analysis; Gaze-based interaction; Gaze-contingent; Human visual attention; Immersive virtual reality; Learning Based Models; State of the art; Viewing conditions; adolescent; adult; computer graphics; eye fixation; female; human; male; physiology; statistical model; virtual reality; young adult; Virtual reality",Article,Final,,Scopus,2-s2.0-85103251105,Gaming / VR
Lu T.; Lou Z.; Shao F.; You X.; Tang M.,"Lu, Tianjiao (57209296270); Lou, Zhenshan (57216563461); Shao, Feng (57216561437); You, Xuqun (36952116700); Tang, Menghan (57216561125)",57209296270; 57216563461; 57216561437; 36952116700; 57216561125,Attention allocation in pilots based on climbing and circling mission behavior,2021,Psychological Research,85,3,,1136,1145,9.0,5,10.1007/s00426-020-01324-1,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85083890959&doi=10.1007%2fs00426-020-01324-1&partnerID=40&md5=e6e7d12fa5f2018a477cad566489e029,"Objective: Exploration of changes in eye movement at different flight conditions can enrich scholarly understanding of situation awareness (SA) and inform new scanning behavior training techniques for efficient and effective pilot education. Background: The SA requirements for pilots vary from mission to mission. Eye tracking is often used to analyze various attention allocation and SA acquisition processes at work in different missions. Methods: Pilot eye movements were measured during a climbing task and circling task using a cockpit-based simulator. Results: Results of situation awareness rating technique (SART) tests show that there are significant differences between attention processes during climbing versus circling flight tasks. Fixation frequency during climbing is lower than in the circling task. Additionally, saccade frequency and average fixation time in the climbing task are markedly higher than those in the circling task. Wilcoxon test results show that the pilot has a higher fixation count and fixation time during the circling phase in out-view (OV) areas of interest (AOI) than during the climbing phase. Notably, the attention probability is higher in climbing task than in circling task when the current area of fixation (AOF) is in the head-up display (HUD) AOI and the next fixation area is in the instrumentation panel left (IPL); when the current AOF is in the out-view right (OVR) and the next AOF is HUD, the attention probability is higher in climbing task than in circling task; when the current fixation is in the IPL and the next fixation probability is to the out-view left (OVL), the attention probability is higher in task climbing task than in circling task. In terms of the Markov stationary distribution, the Wilcoxon test shows that, when IPL AOI is the area of the maximum probability of fixation in both tasks, the attention probability of HUD AOI and instrumentation panel right (IPR) is higher in climbing task than in circling task. Conclusion: Circling tasks require efficient eye movement patterns accompanied by strict attention distribution, which yields high SA level and flight performance when performed properly. Application: This paper summarizes the attention characteristics at different flight phases and various requirements of different tasks according to pilot eye movement tracking results. Similar activities, as routine training, can enhance the efficiency of a novice pilot’s attention distribution. © 2020, Springer-Verlag GmbH Germany, part of Springer Nature.",,Adult; Attention; Awareness; Computer Simulation; Eye Movements; Humans; Male; Pilots; Task Performance and Analysis; adult; airplane pilot; attention; awareness; computer simulation; eye movement; human; male; physiology; task performance,Article,Final,,Scopus,2-s2.0-85083890959,Gaming / VR
Kim S.J.; Laine T.H.; Suk H.J.,"Kim, Si Jung (57007273000); Laine, Teemu H. (11438860800); Suk, Hae Jung (56157231500)",57007273000; 11438860800; 56157231500,"Presence effects in virtual reality based on user characteristics: Attention, enjoyment, and memory",2021,Electronics (Switzerland),10,9,1051,,,,20,10.3390/electronics10091051,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85121676295&doi=10.3390%2felectronics10091051&partnerID=40&md5=5b593fc64989f9b6ed6ecff593eb99ab,"Presence refers to the emotional state of users where their motivation for thinking and acting arises based on the perception of the entities in a virtual world. The immersion level of users can vary when they interact with different media content, which may result in different levels of presence especially in a virtual reality (VR) environment. This study investigates how user characteristics, such as gender, immersion level, and emotional valence on VR, are related to the three elements of presence effects (attention, enjoyment, and memory). A VR story was created and used as an immersive stimulus in an experiment, which was presented through a head-mounted display (HMD) equipped with an eye tracker that collected the participants’ eye gaze data during the experiment. A total of 53 university students (26 females, 27 males), with an age range from 20 to 29 years old (mean 23.8), participated in the experiment. A set of pre-and post-questionnaires were used as a subjective measure to support the evidence of relationships among the presence effects and user characteristics. The results showed that user characteristics, such as gender, immersion level, and emotional valence, affected their level of presence, however, there is no evidence that attention is associated with enjoyment or memory. © 2021 by the authors. Licensee MDPI, Basel, Switzerland.",Eye-tracking; Immersion; Presence effects; User characteristics; Virtual reality,,Article,Final,All Open Access; Gold Open Access,Scopus,2-s2.0-85121676295,Gaming / VR
Moon J.; Ryu J.,"Moon, Jewoong (57203512389); Ryu, Jeeheon (55752010700)",57203512389; 55752010700,"The effects of social and cognitive cues on learning comprehension, eye-gaze pattern, and cognitive load in video instruction",2021,Journal of Computing in Higher Education,33,1,,39,63,24.0,42,10.1007/s12528-020-09255-x,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85084035210&doi=10.1007%2fs12528-020-09255-x&partnerID=40&md5=bf6bbb60d8f2d8c445d1250d0ac60db6,"Students experience challenges when understanding visual information in multimedia learning. Specifically, immersive multimedia environments, such as virtual reality increase the likelihood that students undergo distractions in which information seeking during system-paced instruction occurred. Although previous studies have reviewed various cue designs to yield students’ higher attention, skepticism still exists regarding which ways cue designs can support their learning comprehension in video instruction. For this study, we sampled a total of 64 undergraduates in a university. Using video instruction performed by an animated pedagogical agent (APA), this study examined the effect of social (i.e., an APA’s conversational gestures) and cognitive (i.e., visual cue) cues on students’ learning comprehension and eye-gaze data within types of visual information (text and pictorial). Also, this study investigated how both cues promoted students’ cognitive load overall. Specific to text information processing, the results of the study confirmed that the negative prime effect of social cues undermined students’ learning comprehension and increased their cognitive load, whereas cognitive cues appeared to be supportive in video instruction. Also, this study found that students’ different visual-attention patterns appeared in pictorial information processing. In terms of pictorial information processing, the study finding implies that whereas social cues caused visual distractions and lowered learning comprehension, cognitive cues as visual cues helped learners to integrate pictorial information via visuospatial clues. Conclusively, we reported several design implications derived from the study findings. © 2020, Springer Science+Business Media, LLC, part of Springer Nature.",Animated pedagogical agent; Cognitive load; Conversational gestures; Eye-tracking; Learning comprehension; Visual cues,,Article,Final,,Scopus,2-s2.0-85084035210,Gaming / VR
Teng Y.-Y.; Chou W.-C.; Cheng M.-T.,"Teng, Yuan-Yu (57219454312); Chou, Wen-Chi (55218947600); Cheng, Meng-Tzu (26029682400)",57219454312; 55218947600; 26029682400,"Learning immunology in a game: Learning outcomes, the use of player characters, immersion experiences and visual attention distributions",2021,Journal of Computer Assisted Learning,37,2,,475,486,11.0,12,10.1111/jcal.12501,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85092761587&doi=10.1111%2fjcal.12501&partnerID=40&md5=b7c100d031a710b8188d074669c78d9a,"This study contributed to the current body of literature on game-based learning by investigating the way playing an educational game, Humunology, affected learning about the immune system and examining further the association between game immersion and visual attention distribution. A total of 79 undergraduate and graduate students participated, and data were collected both in situ and ex situ. The results showed that the students learned through playing Humunology, and the analyses of the use of player characters indicated that the game design facilitated gameplay behaviours that are consistent with the science content. The use of the eye-tracking method also revealed that students who were more immersed in playing Humunology paid more attention to areas related to player characters. The interpretations and limitations are discussed further. © 2020 John Wiley & Sons Ltd",eye-tracking; game-based learning; gameplay behaviours; immersion; science learning,,Article,Final,,Scopus,2-s2.0-85092761587,Gaming / VR
Cucca A.; Di Rocco A.; Acosta I.; Beheshti M.; Berberian M.; Bertisch H.C.; Droby A.; Ettinger T.; Hudson T.E.; Inglese M.; Jung Y.J.; Mania D.F.; Quartarone A.; Rizzo J.-R.; Sharma K.; Feigin A.; Biagioni M.C.; Ghilardi M.F.,"Cucca, Alberto (56516963100); Di Rocco, Alessandro (57202693424); Acosta, Ikuko (6601964246); Beheshti, Mahya (57208333398); Berberian, Marygrace (57193143324); Bertisch, Hilary C. (11139088700); Droby, Amgad (56507456100); Ettinger, Tom (57221768502); Hudson, Todd E. (7203069715); Inglese, Matilde (7004371136); Jung, Yoon J. (57221767178); Mania, Daniella F. (57221763966); Quartarone, Angelo (6701444018); Rizzo, John-Ross (56527876700); Sharma, Kush (57208877002); Feigin, Andrew (7006373790); Biagioni, Milton C. (56232706500); Ghilardi, M. Felice (7004106833)",56516963100; 57202693424; 6601964246; 57208333398; 57193143324; 11139088700; 56507456100; 57221768502; 7203069715; 7004371136; 57221767178; 57221763966; 6701444018; 56527876700; 57208877002; 7006373790; 56232706500; 7004106833,Art therapy for Parkinson's disease,2021,Parkinsonism and Related Disorders,84,,,148,154,6.0,40,10.1016/j.parkreldis.2021.01.013,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85100120883&doi=10.1016%2fj.parkreldis.2021.01.013&partnerID=40&md5=6be93458a3fe8dfe430ff5c10c803cb1,"Objective: To explore the potential rehabilitative effect of art therapy and its underlying mechanisms in Parkinson's disease (PD). Methods: Observational study of eighteen patients with PD, followed in a prospective, open-label, exploratory trial. Before and after twenty sessions of art therapy, PD patients were assessed with the UPDRS, Pegboard Test, Timed Up and Go Test (TUG), Beck Depression Inventory (BDI), Modified Fatigue Impact Scale and PROMIS-Self-Efficacy, Montreal Cognitive Assessment, Rey-Osterrieth Complex Figure Test (RCFT), Benton Visual Recognition Test (BVRT), Navon Test, Visual Search, and Stop Signal Task. Eye movements were recorded during the BVRT. Resting-state functional MRI (rs-fMRI) was also performed to assess functional connectivity (FC) changes within the dorsal attention (DAN), executive control (ECN), fronto-occipital (FOC), salience (SAL), primary and secondary visual (V1, V2) brain networks. We also tested fourteen age-matched healthy controls at baseline. Results: At baseline, PD patients showed abnormal visual-cognitive functions and eye movements. Analyses of rs-fMRI showed increased functional connectivity within DAN and ECN in patients compared to controls. Following art therapy, performance improved on Navon test, eye tracking, and UPDRS scores. Rs-fMRI analysis revealed significantly increased FC levels in brain regions within V1 and V2 networks. Interpretation: Art therapy improves overall visual-cognitive skills and visual exploration strategies as well as general motor function in patients with PD. The changes in brain connectivity highlight a functional reorganization of visual networks. © 2021 The Author(s)",Art therapy; Eye tracking; Parkinson's disease; Rehabilitation; Rs-fMRI,"Aged; Art Therapy; Cognitive Dysfunction; Connectome; Eye-Tracking Technology; Female; Humans; Magnetic Resonance Imaging; Male; Middle Aged; Nerve Net; Neurological Rehabilitation; Outcome Assessment, Health Care; Parkinson Disease; aged; art therapy; Article; Beck Depression Inventory; Benton Visual Recognition Test; clinical article; cognition; controlled study; executive function; eye movement; female; functional connectivity; functional magnetic resonance imaging; human; male; Modified Fatigue Impact Scale; Montreal cognitive assessment; mood change; motor performance; Navon Test; observational study; occipitofrontal fasciculus; Parkinson disease; pegboard test; priority journal; prospective study; rating scale; Rey Osterrieth complex figure test; secondary visual cortex; self concept; skill; Unified Parkinson Disease Rating Scale; visual network; Visual Search, and Stop Signal Task; clinical trial; cognitive defect; complication; connectome; diagnostic imaging; middle aged; nerve cell network; neurorehabilitation; nuclear magnetic resonance imaging; Parkinson disease; pathophysiology",Article,Final,All Open Access; Hybrid Gold Open Access,Scopus,2-s2.0-85100120883,Gaming / VR
Javora O.; Hannemann T.; Volná K.; Děchtěrenko F.; Tetourová T.; Stárková T.; Brom C.,"Javora, Ondřej (57192101470); Hannemann, Tereza (57192197657); Volná, Kristina (57204480838); Děchtěrenko, Filip (55987134200); Tetourová, Tereza (57217144812); Stárková, Tereza (57192108200); Brom, Cyril (23090133700)",57192101470; 57192197657; 57204480838; 55987134200; 57217144812; 57192108200; 23090133700,Is contextual animation needed in multimedia learning games for children? An eye tracker study,2021,Journal of Computer Assisted Learning,37,2,,305,318,13.0,5,10.1111/jcal.12489,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85092063559&doi=10.1111%2fjcal.12489&partnerID=40&md5=54dfc4ceec125c451af1eb503a91381f,"The present study investigates affective-motivational, attention, and learning effects of unexplored emotional design manipulation: Contextual animation (animation of contextual elements) in multimedia learning game (MLGs) for children. Participants (N = 134; Mage = 9.25; Grades 3 and 4) learned either from an experimental version of the MLG with a high amount of contextual animation or from an identical MLG with no contextual animation (control). Children strongly preferred (χ2 = 87.04, p <.001) and found the experimental version more attractive (p <.001, d = −1.11). No significant differences in overall enjoyment and learning outcomes were found. Attention differences, measured by dwell times and fixation durations, were small and reached only borderline significance (p =.035; d = −0.39). The implication is that contextual animation in MLG for children increases such instructional materials' attractiveness without compromising cognitive processes needed for learning; however, it does not lead to their higher instructional efficiency. © 2020 John Wiley & Sons Ltd",animation; children; emotional design; eye tracker; games; learning,,Article,Final,,Scopus,2-s2.0-85092063559,Gaming / VR
Kapp S.; Barz M.; Mukhametov S.; Sonntag D.; Kuhn J.,"Kapp, Sebastian (57195483291); Barz, Michael (57189847803); Mukhametov, Sergey (57211779287); Sonntag, Daniel (12241487800); Kuhn, Jochen (55984409000)",57195483291; 57189847803; 57211779287; 12241487800; 55984409000,Arett: Augmented reality eye tracking toolkit for head mounted displays,2021,Sensors,21,6,2234,,,,94,10.3390/s21062234,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85102790558&doi=10.3390%2fs21062234&partnerID=40&md5=44063cb55782a4842be02ef315ce4c64,"Currently an increasing number of head mounted displays (HMD) for virtual and augmented reality (VR/AR) are equipped with integrated eye trackers. Use cases of these integrated eye trackers include rendering optimization and gaze-based user interaction. In addition, visual attention in VR and AR is interesting for applied research based on eye tracking in cognitive or educational sciences for example. While some research toolkits for VR already exist, only a few target AR scenarios. In this work, we present an open-source eye tracking toolkit for reliable gaze data acquisition in AR based on Unity 3D and the Microsoft HoloLens 2, as well as an R package for seamless data analysis. Furthermore, we evaluate the spatial accuracy and precision of the integrated eye tracker for fixation targets with different distances and angles to the user (n = 21). On average, we found that gaze estimates are reported with an angular accuracy of 0.83 degrees and a precision of 0.27 degrees while the user is resting, which is on par with state-of-the-art mobile eye trackers. © 2021 by the authors. Licensee MDPI, Basel, Switzerland.",Accuracy; Augmented reality; Eye tracking; Precision; Toolkit,Augmented Reality; Eye-Tracking Technology; Smart Glasses; Virtual Reality; Augmented reality; Behavioral research; Data acquisition; Helmet mounted displays; Educational science; Head mounted displays; Rendering optimizations; Spatial accuracy; State of the art; User interaction; Virtual and augmented reality; Visual Attention; virtual reality; Eye tracking,Article,Final,All Open Access; Gold Open Access; Green Open Access,Scopus,2-s2.0-85102790558,Gaming / VR
Casanova M.; Clavreul A.; Soulard G.; Delion M.; Aubin G.; Minassian A.T.; Seguier R.; Menei P.,"Casanova, Morgane (57222634199); Clavreul, Anne (6602967441); Soulard, Gwenaelle (57208444774); Delion, Matthieu (26421612400); Aubin, Ghislaine (7006702436); Minassian, Aram Ter (7004101530); Seguier, Renaud (6505976299); Menei, Philippe (56277676100)",57222634199; 6602967441; 57208444774; 26421612400; 7006702436; 7004101530; 6505976299; 56277676100,Immersive virtual reality and ocular tracking for brain mapping during awake surgery: Prospective evaluation study,2021,Journal of Medical Internet Research,23,3,e24373,,,,13,10.2196/24373,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85103573797&doi=10.2196%2f24373&partnerID=40&md5=ad6ee6f1200fd1827ad21e9803699f12,"Background: Language mapping during awake brain surgery is currently a standard procedure. However, mapping is rarely performed for other cognitive functions that are important for social interaction, such as visuospatial cognition and nonverbal language, including facial expressions and eye gaze. The main reason for this omission is the lack of tasks that are fully compatible with the restrictive environment of an operating room and awake brain surgery procedures. Objective: This study aims to evaluate the feasibility and safety of a virtual reality headset equipped with an eye-tracking device that is able to promote an immersive visuospatial and social virtual reality (VR) experience for patients undergoing awake craniotomy. Methods: We recruited 15 patients with brain tumors near language and/or motor areas. Language mapping was performed with a naming task, DO 80, presented on a computer tablet and then in 2D and 3D via the VRH. Patients were also immersed in a visuospatial and social VR experience. Results: None of the patients experienced VR sickness, whereas 2 patients had an intraoperative focal seizure without consequence; there was no reason to attribute these seizures to virtual reality headset use. The patients were able to perform the VR tasks. Eye tracking was functional, enabling the medical team to analyze the patients' attention and exploration of the visual field of the virtual reality headset directly. Conclusions: We found that it is possible and safe to immerse the patient in an interactive virtual environment during awake brain surgery, paving the way for new VR-based brain mapping procedures. © 2021 Journal of Medical Internet Research. All rights reserved.",Awake surgery; Brain mapping; Eye tracking; Mobile phone; Nonverbal language; Virtual reality; Visuospatial cognition,Brain Mapping; Brain Neoplasms; Female; Humans; Male; Prospective Studies; Virtual Reality; Wakefulness; anesthetic agent; local anesthetic agent; adult; aged; aphasia; Article; astrocytoma; brain mapping; brain metastasis; brain surgery; clinical article; controlled study; depth perception; device safety; electrostimulation; eye-tracking technology; feasibility study; female; focal epilepsy; frontal lobe; general anesthesia; glioblastoma; human; language; left handedness; left hemisphere; local anesthesia; male; neuronavigation; oligodendroglioma; open study; parietal lobe; peroperative complication; primary tumor; prospective study; right handedness; right hemisphere; temporoparietal junction; virtual reality; visual field; wakefulness; white matter; brain tumor; wakefulness,Article,Final,All Open Access; Gold Open Access; Green Open Access,Scopus,2-s2.0-85103573797,Gaming / VR
Bozki E.; Stark P.; Gao H.; Hasenbein L.; Hahn J.-U.; Kasneci E.; Gollner R.,"Bozki, Efe (57210111357); Stark, Philipp (57223937195); Gao, Hong (57217014766); Hasenbein, Lisa (57211289472); Hahn, Jens-Uwe (56115756900); Kasneci, Enkelejda (56059892600); Gollner, Richard (56661056800)",57210111357; 57223937195; 57217014766; 57211289472; 56115756900; 56059892600; 56661056800,Exploiting object-of-interest information to understand attention in VR classrooms,2021,"Proceedings - 2021 IEEE Conference on Virtual Reality and 3D User Interfaces, VR 2021",,,9417698,597,605,8.0,44,10.1109/VR50410.2021.00085,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85106485809&doi=10.1109%2fVR50410.2021.00085&partnerID=40&md5=d5e8191bde0b1ab343f29b66cb7b67e3,"Recent developments in computer graphics and hardware technology enable easy access to virtual reality headsets along with integrated eye trackers, leading to mass usage of such devices. The immersive experience provided by virtual reality and the possibility to control environmental factors in virtual setups may soon help to create realistic digital alternatives to conventional classrooms. The importance of such settings has become especially evident during the COVID-19 pandemic, forcing many schools and universities to provide the digital teaching. Researchers foresee that such transformations will continue in the future with virtual worlds becoming an integral part of education. Until now, however, students' behaviors in immersive virtual environments have not been investigated in depth. In this work, we study students' attention by exploiting object-of-interests using eye tracking in different classroom manipulations. More specifically, we varied sitting positions of students, visualization styles of virtual avatars, and hand-raising percentages of peer-learners. Our empirical evidence shows that such manipulations play an important role in students' attention towards virtual peer-learners, instructors, and lecture material. This research may contribute to understanding of how visual attention relates to social dynamics in the virtual classroom, including significant considerations for the design of virtual learning spaces. © 2021 IEEE.",Applied computing-Education-Computer -assisted instruction; Applied computing-Education-Interactive learning environments; Computing methodologies-Computer graphics-Graphics systems and interfaces-Virtual reality; Human-centered computing-Human computer interaction (HCI)-Empirical studies in HCI,Behavioral research; Computer aided instruction; Computer hardware; E-learning; Eye tracking; Object tracking; Students; User interfaces; Digital teachings; Environmental factors; Hardware technology; Immersive virtual environments; Sitting positions; Students' behaviors; Virtual Classroom; Virtual-reality headsets; Virtual reality,Conference paper,Final,,Scopus,2-s2.0-85106485809,Gaming / VR
Liu C.; Yu D.; Zhang J.; Xie S.,"Liu, Chang (37761868300); Yu, Dingguo (34769243400); Zhang, Jiefang (56643544600); Xie, Songyun (55750967300)",37761868300; 34769243400; 56643544600; 55750967300,A Utility Human Machine Interface Using Low Cost EEG Cap and Eye Tracker,2021,"9th IEEE International Winter Conference on Brain-Computer Interface, BCI 2021",,,9385304,,,,3,10.1109/BCI51272.2021.9385304,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85104834405&doi=10.1109%2fBCI51272.2021.9385304&partnerID=40&md5=811d8429851dc506f7f9d6dc8f35d359,"Electroencephalogram (EEG) based Brain Computer Interface (BCI) has a huge market with big potential and wide prospect, however, the acquisition equipment is too expensive to be popular with ordinary users, which makes the majority of applications are still in the laboratory. Inspired by the development of hybrid Human-Computer Interaction (HCI), a utility HCI using low-cost EEG cap and eye tracker are investigated in this paper. The validation experiment indicates that the proposed system is able to detect and classify multiple patterns of EEG signals and translate them into control commands to interact with the environment. Furthermore, an eye tracker enables subjects to freely observe the surrounding environment and select a target object under the naked eye VR technique. Comparing to the HCI only based on eye tracker, EEG signals are used to realize the motor function in this paper to reduce the fatigue caused by long-time fixation in traditional eye tracker application. Furthermore, compared to the BCI system at the same price, the lack of electrodes can be compensated by an eye tracker. In conclusion, the hybrid HCI proposed in this paper achieves superior performance through low-cost equipment, which may promote the development of the interaction between the human and environment based on the physiological signals.  © 2021 IEEE.",EEG cap; Electroencephalogram (EEG); eye tracker; hybrid human-computer interaction (HCI); low cost,Costs; Electroencephalography; Eye tracking; Human computer interaction; Object tracking; Acquisition equipments; Electro-encephalogram (EEG); Human computer interaction (HCI); Human Machine Interface; Low-cost equipment; Multiple patterns; Physiological signals; Surrounding environment; Brain computer interface,Conference paper,Final,,Scopus,2-s2.0-85104834405,Gaming / VR
Hu Z.,"Hu, Zhiming (57208101391)",57208101391,[DC] Eye fixation forecasting in task-oriented virtual reality,2021,"Proceedings - 2021 IEEE Conference on Virtual Reality and 3D User Interfaces Abstracts and Workshops, VRW 2021",,,9419161,707,708,1.0,3,10.1109/VRW52623.2021.00236,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85105966888&doi=10.1109%2fVRW52623.2021.00236&partnerID=40&md5=f1c5e70c67e30187665a531cc1408f81,"In immersive virtual reality (VR), users' visual attention is crucial for many important applications, including VR content design, gaze-based interaction, and gaze-contingent rendering. Especially, information on users' future eye fixations is key for intelligent user interfaces and has significant relevance for many areas, such as visual attention enhancement, dynamic event triggering, and human-computer interaction. However, previous works typically focused on free-viewing conditions and paid less attention to task-oriented attention. This paper aims at forecasting users' eye fixations in task-oriented virtual reality. To this end, a VR eye tracking dataset that corresponds to different users performing a visual search task in immersive virtual environments is built. A comprehensive analysis of users' eye fixations is performed based on the collected data. The analysis reveals that eye fixations are correlated with users' historical gaze positions, task-related objects, saliency information of the VR content, and head rotation velocities. Based on this analysis, a novel learning-based model is proposed to forecast users' eye fixations in the near future in immersive virtual environments. © 2021 IEEE.",Convolutional neural network; Eye tracking; Deep learning; Fixation forecasting; Virtual reality; Visual attention; Visual search,Abstracting; Behavioral research; Eye tracking; Forecasting; Human computer interaction; User interfaces; Comprehensive analysis; Gaze-based interaction; Immersive virtual environments; Immersive virtual reality; Intelligent User Interfaces; Learning Based Models; Viewing conditions; Visual Attention; Virtual reality,Conference paper,Final,,Scopus,2-s2.0-85105966888,Gaming / VR
Heyman G.M.; Moncaleano S.,"Heyman, Gene M. (7004825479); Moncaleano, Sebastian (57211435697)",7004825479; 57211435697,Behavioral psychology's matching law describes the allocation of covert attention: A choice rule for the mind.,2021,Journal of Experimental Psychology: General,150,2,,195,205,10.0,4,10.1037/xge0000919,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85089133679&doi=10.1037%2fxge0000919&partnerID=40&md5=6f1b1beb9cc39905cd2eddc8ccda6598,"The matching law describes the allocation of behavior over a wide range of settings, including laboratory experimental chambers, forest foraging patches, sports arenas, and board games. Interestingly, matching persists in settings in which economic analyses predict quite different distributions of behavior, and it also differs systematically from probability matching. We tested whether the matching law also describes the allocation of covert cognitive processes. Sixty-four participants viewed 2, small, vertically arranged adjacent stimuli that projected an image that fit within the fovea. A trial version of the reward contingencies used in matching law experiments determined which stimulus was the target. For example, in 1 condition, the top stimulus was the target 3 times more frequently than the bottom stimulus. However, the amount of time the stimuli were available was tailored to each participant so that they were not able to make use of the information in both stimuli even though an eye-tracking experiment confirmed that they saw both. The implication of this restriction is that participants had to decide which stimulus to attend to prior to each trial. The only available objective basis for this decision was the relative frequencies that a stimulus was the target. The matching law predicted the correlation between the relative frequency that a stimulus was the target and the proportion of trials that it was attended to. The results support the claim that the matching law is a general choice principle—one that describes the allocation of covert mental processes as well as overt behavioral responses. (PsycInfo Database Record (c) 2021 APA, all rights reserved) © 2020 American Psychological Association",attention allocation; choice; covert attention; matching law; visual attention,Adolescent; Attention; Female; Humans; Male; Photic Stimulation; Resource Allocation; Reward; Visual Perception; Young Adult; adolescent; attention; female; human; male; photostimulation; physiology; resource allocation; reward; vision; young adult,Article,Final,All Open Access; Green Open Access,Scopus,2-s2.0-85089133679,Gaming / VR
Luu W.; Zangerl B.; Kalloniatis M.; Palmisano S.; Kim J.,"Luu, Wilson (57215363922); Zangerl, Barbara (6603103909); Kalloniatis, Michael (7004108096); Palmisano, Stephen (6701694405); Kim, Juno (55784672000)",57215363922; 6603103909; 7004108096; 6701694405; 55784672000,Vision impairment provides new insight into self-motion perception,2021,Investigative Ophthalmology and Visual Science,62,2,e2772238,,,,9,10.1167/IOVS.62.2.4,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85101044754&doi=10.1167%2fIOVS.62.2.4&partnerID=40&md5=f2408ee5465e289053ddf1dd0af523aa,"PURPOSE. Leading causes of irreversible blindness such as age-related macular degeneration (AMD) and glaucoma can, respectively, lead to central or peripheral vision loss. The ability of sufferers to process visual motion information can be impacted even during early stages of eye disease. We used head-mounted display virtual reality as a tool to better understand how vision changes caused by eye diseases directly affect the processing of visual information critical for self-motion perception. METHODS. Participants with intermediate AMD or early manifest glaucoma with nearnormal visual acuities and visual fields were recruited for this study. We examined their experiences of self-motion in depth (linear vection), spatial presence, and cybersickness when viewing radially expanding patterns of optic flow simulating different speeds of self-motion in depth. Viewing was performed with the head stationary (passive condition) or while making lateral-sway head movements (active conditions). RESULTS. Participants with AMD (i.e., central visual field loss) were found to have greater vection strength and spatial presence, compared to participants with normal visual fields. However, participants with glaucoma (i.e., peripheral visual field loss) were found to have lower vection strength and spatial presence, compared to participants with normal visual fields. Both AMD and glaucoma groups reported reduced severity in cybersickness compared to healthy normals. CONCLUSIONS. These findings strongly support the view that perceived self-motion is differentially influenced by peripheral versus central vision loss, and that patients with different visual field defects are oppositely biased when processing visual cues to self-motion perception. © 2021 The Authors.",Age-related macular degeneration; Glaucoma; Presence; Vection; Virtual reality,"Aged; Aged, 80 and over; Cues; Eye Movements; Female; Humans; Illusions; Male; Middle Aged; Motion Perception; Self Concept; Vision Disorders; Visual Acuity; adult; age related macular degeneration; Article; auditory stimulation; best corrected visual acuity; binocular vision; biomechanics; cognition; computer language; contrast sensitivity; controlled study; cybersickness; disease simulation; eye movement; eye tracking; female; glaucoma; human; intraocular pressure; male; Montreal cognitive assessment; motion; movement perception; optic flow; optical coherence tomography; perimetry; retina blood vessel; retina fovea; retinal nerve fiber layer thickness; stereoscopic vision; test retest reliability; velocity; virtual reality; vision; visual acuity; visual field; visual impairment; visual information; visual stimulation; visual system parameters; aged; association; illusion; middle aged; movement perception; pathophysiology; physiology; self concept; very elderly; visual acuity; visual disorder",Article,Final,All Open Access; Gold Open Access; Green Open Access,Scopus,2-s2.0-85101044754,Gaming / VR
Rubo M.; Gamer M.,"Rubo, Marius (57196299847); Gamer, Matthias (8979946100)",57196299847; 8979946100,Stronger reactivity to social gaze in virtual reality compared to a classical laboratory environment,2021,British Journal of Psychology,112,1,,301,314,13.0,19,10.1111/bjop.12453,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85085685886&doi=10.1111%2fbjop.12453&partnerID=40&md5=0ef3ec58b6a514306db7cb467a290cf2,"People show a robust tendency to gaze at other human beings when viewing images or videos, but were also found to relatively avoid gaze at others in several real-world situations. This discrepancy, along with theoretical considerations, spawned doubts about the appropriateness of classical laboratory-based experimental paradigms in social attention research. Several researchers instead suggested the use of immersive virtual scenarios in eliciting and measuring naturalistic attentional patterns, but the field, struggling with methodological challenges, still needs to establish the advantages of this approach. Here, we show using eye-tracking in a complex social scenario displayed in virtual reality that participants show enhanced attention towards the face of an avatar at near distance and demonstrate an increased reactivity towards her social gaze as compared to participants who viewed the same scene on a computer monitor. The present study suggests that reactive virtual agents observed in immersive virtual reality can elicit natural modes of information processing and can help to conduct ecologically more valid experiments while maintaining high experimental control. © 2020 The Authors. British Journal of Psychology published by John Wiley & Sons Ltd on behalf of British Psychological Society",reactive virtual agents; social attention; social gaze; virtual reality,Attention; Emotions; Female; Humans; Laboratories; Virtual Reality; adult; article; attention; controlled study; eye tracking; female; gaze; human; human experiment; virtual reality; attention; emotion; laboratory,Article,Final,All Open Access; Green Open Access; Hybrid Gold Open Access,Scopus,2-s2.0-85085685886,Gaming / VR
Désiron J.C.; Bétrancourt M.; de Vries E.,"Désiron, Juliette C. (57196067428); Bétrancourt, Mireille (6602857958); de Vries, Erica (7201772092)",57196067428; 6602857958; 7201772092,Cross-Representational Signaling and Cohesion Support Inferential Comprehension of Text–Picture Documents,2021,Frontiers in Psychology,11,,592509,,,,7,10.3389/fpsyg.2020.592509,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85100540236&doi=10.3389%2ffpsyg.2020.592509&partnerID=40&md5=ef319c23d18349b96c6886eee59651ac,"Learning from a text–picture multimedia document is particularly effective if learners can link information within the text and across the verbal and the pictorial representations. The ability to create a mental model successfully and include those implicit links is related to the ability to generate inferences. Text processing research has found that text cohesion facilitates the generation of inferences, and thus text comprehension for learners with poor prior knowledge or reading abilities, but is detrimental for learners with good prior knowledge or reading abilities. Moreover, multimedia research has found a positive effect from adding visual representations to text information, particularly when implementing signaling, which consists of verbal or visual cues designed to guide attention to the pictorial representation of relevant information. We expected that, as with text-only documents, struggling readers would benefit from high text cohesion (Hypothesis 1) and that signaling would foster inference generation as well (Hypothesis 2). Further, we hypothesized that better learning outcomes would be observed when text cohesion was low and signaling was present (Hypothesis 3). Our first experimental study investigated the effect of those two factors (cohesion and signaling) on three levels of comprehension (text based, local inferences, global inferences). Participants were adolescents in prevocational schools (n = 95), where some of the students are struggling readers. The results showed a trend in favor of high cohesion, but with no significant effect, a significant positive effect of cross-representational signaling (CRS) on comprehension from local inferences, and no interaction effect. A second experiment focused on signaling only and attention toward the picture, with collection of eye-tracking data in addition to measures of offline comprehension. As this study was conducted with university students (n = 47), who are expected to have higher reading abilities and thus are less likely to benefit from high cohesion, the material was presented in its low cohesive version. The results showed no effect of conditions on comprehension performances but confirmed differences in processing behaviors. Participants allocated more attention to the pictorial representation in the CRS condition than in the no signaling condition. © Copyright © 2021 Désiron, Bétrancourt and de Vries.",comprehension; eye-tracking; inference; multimedia learning; signaling; text cohesion,,Article,Final,All Open Access; Gold Open Access; Green Open Access,Scopus,2-s2.0-85100540236,Gaming / VR
Dilini N.; Senaratne A.; Yasarathna T.; Warnajith N.; Seneviratne L.,"Dilini, Nimesha (57457188500); Senaratne, Asara (57439836000); Yasarathna, Tharindu (57221920037); Warnajith, Nalin (55364675700); Seneviratne, Leelanga (57209269648)",57457188500; 57439836000; 57221920037; 55364675700; 57209269648,Cheating Detection in Browser-based Online Exams through Eye Gaze Tracking,2021,"Proceedings of 6th International Conference on Information Technology Research: Digital Resilience and Reinvention, ICITR 2021",,,,,,,18,10.1109/ICITR54349.2021.9657277,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124808637&doi=10.1109%2fICITR54349.2021.9657277&partnerID=40&md5=a6fd630f07215b13e537932107fee66e,"Eye-tracking can detect and examine human visual attention, emotional conditions, latent cognitive processes such as efforts to recall a concept or the fear of running out of time, and so on. Hence, we can use eye-tracking to identify deviant behavior patterns in learning and problem-solving. At present, given the existence of a global pandemic, online exams are widely used by educational institutions to evaluate students' performance. However, identifying cheating is challenging due to the absence of a human (invigilator) monitoring students' behavior as done in exams held in a physical location. In an online environment, students' behavior, and attempts to cheat, can only be captured via a computer, thus requiring a mechanism for online proctoring with capabilities for cheating detection. In this research paper, we present a browser based cheating detection approach in online examinations through eye gaze tracking. We developed a browser plugin to track the eye gaze movements through the in-built web camera. Using the plugin, we generate an eye gaze dataset while a student faces an online examination. We then process and analyze this dataset to detect any misbehavior during an online examination. The underlying research work of this paper identifies different eye gaze patterns during online examinations and present a cheating detection mechanism. For anomaly detection in the eye gaze data, we use a One-Class Support Vector Machine (OCSVM). We then use these identified anomalies to predict cheating behaviors of the test takers. The given approach can be used for any web-based quiz examination such as academic institutions' exams, company recruitment exams, and overseas testing exams to detect any anomalous behaviors of the test takers during the examination period. The given eye tracking approach can also be applied to other research domains such as online gaming, and web usability studies to capture information related to user behaviors.  © 2021 IEEE.",anomaly detection; eye tracking; one-class support vector machine; Online proctoring; unsupervised outlier detection,Anomaly detection; Behavioral research; E-learning; Eye movements; Eye tracking; Students; Anomaly detection; Cheating detection; Eye-gaze; Eye-tracking; One-class support vector machine; Online examinations; Online proctoring; Outlier Detection; Support vectors machine; Unsupervised outlier detection; Support vector machines,Conference paper,Final,,Scopus,2-s2.0-85124808637,Gaming / VR
Ouyang Y.; Luo X.,"Ouyang, Yewei (57195718282); Luo, Xiaowei (35099137200)",57195718282; 35099137200,Using eye-tracking to compare the experienced safety supervisors and novice in identifying job site hazards under a VR environment,2021,"EG-ICE 2021 Workshop on Intelligent Computing in Engineering, Proceedings",,,,270,280,10.0,1,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85134268730&partnerID=40&md5=bbe1f78d426d42ee204a630ae9c142a5,"Hazard-identification experience is a kind of tacit knowledge which is difficult to be extracted from experienced subjects and to be described explicitly in the text. Researchers have applied eye-tracking technology in eliciting the cognitive processes of experienced workers while performing the hazard-identification task. However, the image-based tasks in previous studies are substantially different from how the hazards are perceived on the construction site. To improve the ecological validity of the hazard-identification task, this study develops panoramic VR scenarios of various job sites as the stimulus, and both experienced safety supervisors and students are invited to identify hazards in the virtual sites. Their performances and eye-movement data are compared. The results show the experienced allocate more attention to hazardous areas instead of unimportant things, and they inspect more details which are ignored by the novice. The identified differences may be incorporated into the training courses to educate the hazard-identification of the novice. © 2021 Universitätsverlag der Technischen Universität Berlin. All Rights Reserved.",,Eye movements; Eye tracking; Supervisory personnel; Virtual reality; Cognitive process; Construction sites; Ecological validity; Eye tracking technologies; Eye-tracking; Hazard identification; Image-based; Job sites; Tacit knowledge; Workers'; Hazards,Conference paper,Final,,Scopus,2-s2.0-85134268730,Gaming / VR
Heyselaar E.; Peeters D.; Hagoort P.,"Heyselaar, Evelien (42661623500); Peeters, David (53464091900); Hagoort, Peter (7003301986)",42661623500; 53464091900; 7003301986,Do we predict upcoming speech content in naturalistic environments?,2021,"Language, Cognition and Neuroscience",36,4,,440,461,21.0,20,10.1080/23273798.2020.1859568,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85097623456&doi=10.1080%2f23273798.2020.1859568&partnerID=40&md5=f73f156214c187cfb8905bec6749c41c,"The ability to predict upcoming actions is a hallmark of cognition. It remains unclear, however, whether the predictive behaviour observed in controlled lab environments generalises to rich, everyday settings. In four virtual reality experiments, we tested whether a well-established marker of linguistic prediction (anticipatory eye movements) replicated when increasing the naturalness of the paradigm by means of immersing participants in naturalistic scenes (Experiment 1), increasing the number of distractor objects (Experiment 2), modifying the proportion of predictable noun-referents (Experiment 3), and manipulating the location of referents relative to the joint attentional space (Experiment 4). Robust anticipatory eye movements were observed for Experiments 1–3. The anticipatory effect disappeared, however, in Experiment 4. Our findings suggest that predictive processing occurs in everyday communication if the referents are situated in the joint attentional space. Methodologically, our study confirms that ecological validity and experimental control may go hand-in-hand in the study of human predictive behaviour. © 2020 The Author(s). Published by Informa UK Limited, trading as Taylor & Francis Group.",eye tracking; language comprehension; Prediction; virtual reality; visual world paradigm,adult; article; comprehension; ecological validity; eye tracking; female; human; human experiment; language; male; prediction; speech; virtual reality,Article,Final,All Open Access; Green Open Access; Hybrid Gold Open Access,Scopus,2-s2.0-85097623456,Gaming / VR
Li J.; Fu R.,"Li, Junxuan (57226784701); Fu, Rongrong (57203101786)",57226784701; 57203101786,Design of Traditional Brand H5 Game Advertisement Based on EEG and Eye Movement Analysis: Example of MAXAM,2021,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),12780 LNCS,,,613,626,13.0,0,10.1007/978-3-030-78224-5_42,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85132701793&doi=10.1007%2f978-3-030-78224-5_42&partnerID=40&md5=14d84dc8e069c6798be66dd2e9b975e6,"The acceptance of traditional brands is gradually decreasing among young people. Aiming at the problem, this research explores the promotion of recognition and acceptance of these brands through the design of H5 game advertisements. This paper takes MAXAM brand as an example to study its present situation. H5 game is designed by taking the Elemental Tetrad model as guidance and also integrating story, mechanism, aesthetics and technology. The design principle comprises of shaping a sense of mission and immersion, optimizing information feedback, and strengthening brand information output. An optimal design scheme is proposed by utilizing AARRR theory. After completing design, the transmission effect is evaluated via EEG combined with eye movement experiment and IPA analysis method. The results indicate that H5 game advertisements can enhance users’ brain arousal to the brand, and also can improve somewhat the attention of target users to MAXAM hand cream and a stronger willingness to buy. H5 game advertisements can be used as a method of mobile propagation in the process of MAXAM brand renaissance according to the analysis of IPA effective sample data, and also provides a basic and effective reference for China’s time-honored brands, similar to MAXAM, to update positioning and establish brand recognition, familiarity and audience satisfaction. © Springer Nature Switzerland AG 2021.",EEG and eye movement; H5 game; MAXAM,Big data; Eye tracking; Marketing; Design Principles; EEG and eye movement; Eye movement analysis; H5 game; Information feedback; MAXAM; Optimal design; Present situation; Story mechanisms; Young peoples; Eye movements,Conference paper,Final,,Scopus,2-s2.0-85132701793,Gaming / VR
Cao S.; Zhao X.; Qin B.; Li J.; Xiang Z.,"Cao, Shihao (57219797003); Zhao, Xinbo (55352149700); Qin, Beibei (57221152419); Li, Junjie (59790635200); Xiang, Zheng (57296381600)",57219797003; 55352149700; 57221152419; 59790635200; 57296381600,A Monocular Reflection-Free Head-Mounted 3D Eye Tracking System,2021,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),12890 LNCS,,,659,672,13.0,1,10.1007/978-3-030-87361-5_54,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85117076445&doi=10.1007%2f978-3-030-87361-5_54&partnerID=40&md5=4368d46cfaf8759e17022a665fa88524,"Head-mounted eye tracking has significant potential for gaze baesd application such as consumer attention monitoring, human-computer interaction, or virtual reality (VR). Existing methods, however, either use pupil center-corneal reflection (PCCR) vectors as gaze directions or require complex hardware setups and use average physiological parameters of the eye to obtain gaze directions. In view of this situation, we propose a novel method which uses only a single camera to obtain gaze direction by fitting a 3D eye model based on the motion trajectory of pupil contour. Then a 3D to 2D mapping model is proposed based on the fitting model, so the complex structure of hardware and the use of average parameters for the eyes are avoided. The experimental results show that the method can improve the gaze accuracy and simplify the hardware structure. © 2021, Springer Nature Switzerland AG.",3D gaze estimation; Head-mounted device; Mapping model; Pupil contour; Single camera,3D modeling; Cameras; Computer hardware; Eye movements; Eye tracking; Human computer interaction; Mapping; Physiological models; 3d gaze estimation; Eye tracking systems; Free-head; Gaze direction; Gaze estimation; Head-mounted device; Head-mounted eye tracking; Mapping modeling; Pupil contour; Single cameras; Virtual reality,Conference paper,Final,,Scopus,2-s2.0-85117076445,Gaming / VR
Llanes-Jurado J.; Marín-Morales J.; Moghaddasi M.; Khatri J.; Guixeres J.; Alcañiz M.,"Llanes-Jurado, Jose (57218430050); Marín-Morales, Javier (57191977544); Moghaddasi, Masoud (57211442960); Khatri, Jaikishan (57218446760); Guixeres, Jaime (26423690300); Alcañiz, Mariano (36921902100)",57218430050; 57191977544; 57211442960; 57218446760; 26423690300; 36921902100,Comparing eye tracking and head tracking during a visual attention task in immersive virtual reality,2021,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),12763 LNCS,,,32,43,11.0,4,10.1007/978-3-030-78465-2_3,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85120689596&doi=10.1007%2f978-3-030-78465-2_3&partnerID=40&md5=4ec89d01dd455a227a4c4ab75c97df9a,"The use of eye tracking (ET) and head tracking (HT) in head-mounted displays allows for the study of a subject’s attention in virtual reality environments, expanding the possibility to develop experiments in areas such as health or consumer behavior research. ET is a more precise technique than HT, but many commercial devices do not include ET systems. One way to study visual attention is to segment the space in areas of interest (AoI). However, the ET and HT responses could be similar depending on the size of the studied area in the virtual environment. Therefore, understanding the differences between ET and HT based on AoI size is critical in order to enable the use of HT to assess human attention. The purpose of this study was to perform a comparison between ET and HT technologies through the study of multiple sets of AoI in an immersive virtual environment. To do that, statistical techniques were developed with the objective of measuring the differences between the two technologies. This study found that with HT, an accuracy of 75.37% was obtained when the horizontal and vertical angular size of the AoIs was 25°. Moreover, the results suggest that horizontal movements of the head are much more similar to eye movements than vertical movements. Finally, this work presents a guide for future researchers to measure the precision of HT against ET, considering the dimensions of the AoI defined in a virtual scenario. © Springer Nature Switzerland AG 2021.",Area of interest; Eye-tracking; Head-mounted display; Head-tracking; Virtual-reality,Consumer behavior; Eye movements; Eye tracking; Virtual reality; Area of interest; Commercial Devices; Eye tracking systems; Eye-tracking; Head tracking; Head-mounted-displays; Immersive virtual reality; Tracking response; Virtual-reality environment; Visual Attention; Helmet mounted displays,Conference paper,Final,,Scopus,2-s2.0-85120689596,Gaming / VR
,,,"9th International Conference on Cognitive Sciences, Intercognsci 2020",2021,Advances in Intelligent Systems and Computing,1358 AIST,,,,,736.0,0,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85105857665&partnerID=40&md5=8fddad66768577be00d31597e6cfa79a,"The proceedings contain 84 papers. The special focus in this conference is on Cognitive Sciences. The topics include: The Effectiveness of Metacognitive Hints in Insight Problem Solving; effects of Online Repetitive Transcranial Magnetic Stimulation on the Frequency of Insights During Anagram Solving; the Mirror Neuron System Activity is Higher with Personal Direct Interaction; gender Differences in Object and Spatial Inattentional Blindness Under Working Memory Load; effect of ‘Dry’ Immersion on Visual Illusions; auditory Mechanisms for Analyzing Conspecific Movement; probability of Visually Perceiving Emotional Expression During Saccade is Rising, not Being Suppressed; Eye Movements and EEG During Reading as Markers of Interest; age Features of Eye Movements in Adolescents When Reading from Various Electronic Devices; trends and Perspectives in Cognitive Research; dependence of Eye Movement Parameters During Sight-Reading on Pianist’s Skill and Complexity of Musical Notation; linear and Non-linear Patterns of Eye Movements in Lexical Search: Expert Versus Novice Language Learners; eye Movements in Visual Semantic Search: Scanning Patterns and Cognitive Processing Across Three Cultures; saccade Trajectories in the Presence of Emotional and Non-emotional Distractors; cognitive Mechanisms of Ambiguity Resolution; psychophysiological Interactions Underlying Meaning Selection in Ambiguity Resolution; phonetic Ambiguity Resolution: To Be or not to Be Aware; stroop Effect: Conflict Detection and Control Strategy Factors; Alpha-Band Effective Connectivity During Cued Versus Implicit Modality-Specific Anticipatory Attention: EEG-Source Analysis; the Selected Profession as Determinant of the Flynn Effect: Specificity of Changes in the Intelligence Structure of University Students; affective Priming and Decision-Making in the Economic Game; concreteness/Abstractness Concept: State of the Art.",,,Conference review,Final,,Scopus,2-s2.0-85105857665,Gaming / VR
Rooijackers P.; van Silfhout G.; Schuurs U.; Mulders I.; van den Bergh H.,"Rooijackers, Patrick (39963135200); van Silfhout, Gerdineke (56067641000); Schuurs, Uriël (55447837000); Mulders, Iris (57186539300); van den Bergh, Huub (7005230156)",39963135200; 56067641000; 55447837000; 57186539300; 7005230156,Reading texts and answering questions; an eye-tracking study among Dutch tenth grade preacademic pupils; [Lezen en antwoorden bij de tekst met vragen geobserveerd Een eye-trackstudie onder vwo 4-leerlingen],2021,Pedagogische Studien,97,1,,42,58,16.0,2,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85127156155&partnerID=40&md5=edde413d6087a48fab3f54c858c1aa79,"To measure and train the reading skills of students often the traditional ‘text with questions’ is used in Dutch secondary education. With this task it is often assumed that students who read the text well will answer more questions correctly. In this study the relationship between reading and answering process in this particular task is investigated by means of the analysis of eye-movements. Starting point is an assumption in the Construction-Integration theory of Kintsch (1998): to build an adequate comprehension model readers have to give more attention to topical sentences than to detail sentences. It was investigated whether Dutch tenth grade preacademic students (N = 16) spent more reading time on topical sentences, and to what extent this is related to the answering process. Results show that participants paid more attention (longer reading times) to topical sentences, even if we control for differences in sentence length. However, on a deeper comprehension level a relationship between reading and answering process could not be established. The analysis indicates that participants initially read linearly and only in the answering process constructed a deeper comprehension of the text. The question is therefore to what extent the ‘text with questions’ in school subject Dutch fosters better text comprehension. © 2021 Vereniging voor Onderwijsresearch (VOR). All rights reserved.",eye-tracking; preacademic; school subject Dutch; text comprehension; text with questions,,Article,Final,,Scopus,2-s2.0-85127156155,Gaming / VR
Krebs C.; Falkner M.; Niklaus J.; Persello L.; Klöppel S.; Nef T.; Urwyler P.,"Krebs, Christine (57202361407); Falkner, Michael (57222653246); Niklaus, Joel (57205354879); Persello, Luca (57222660022); Klöppel, Stefan (23100462800); Nef, Tobias (15726063900); Urwyler, Prabitha (36702218000)",57202361407; 57222653246; 57205354879; 57222660022; 23100462800; 15726063900; 36702218000,Application of eye tracking in puzzle games for adjunct cognitive markers: Pilot observational study in older adults,2021,JMIR Serious Games,9,1,e24151,,,,14,10.2196/24151,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85103642332&doi=10.2196%2f24151&partnerID=40&md5=981e7b3f62640f81119a87bec1bfea1b,"Background: Recent studies suggest that computerized puzzle games are enjoyable, easy to play, and engage attentional, visuospatial, and executive functions. They may help mediate impairments seen in cognitive decline in addition to being an assessment tool. Eye tracking provides a quantitative and qualitative analysis of gaze, which is highly useful in understanding visual search behavior. Objective: The goal of the research was to test the feasibility of eye tracking during a puzzle game and develop adjunct markers for cognitive performance using eye-tracking metrics. Methods: A desktop version of the Match-3 puzzle game with 15 difficulty levels was developed using Unity 3D (Unity Technologies). The goal of the Match-3 puzzle was to find configurations (target patterns) that could be turned into a row of 3 identical game objects (tiles) by swapping 2 adjacent tiles. Difficulty levels were created by manipulating the puzzle board size (all combinations of width and height from 4 to 8) and the number of unique tiles on the puzzle board (from 4 to 8). Each level consisted of 4 boards (ie, target patterns to match) with one target pattern each. In this study, the desktop version was presented on a laptop computer setup with eye tracking. Healthy older subjects were recruited to play a full set of 15 puzzle levels. A paper-pencil–based assessment battery was administered prior to the Match-3 game. The gaze behavior of all participants was recorded during the game. Correlation analyses were performed on eye-tracking data correcting for age to examine if gaze behavior pertains to target patterns and distractor patterns and changes with puzzle board size (set size). Additionally, correlations between cognitive performance and eye movement metrics were calculated. Results: A total of 13 healthy older subjects (mean age 70.67 [SD 4.75] years; range 63 to 80 years) participated in this study. In total, 3 training and 12 test levels were played by the participants. Eye tracking recorded 672 fixations in total, 525 fixations on distractor patterns and 99 fixations on target patterns. Significant correlations were found between executive functions (Trail Making Test B) and number of fixations on distractor patterns (P=.01) and average fixations (P=.005). Conclusions: Overall, this study shows that eye tracking in puzzle games can act as a supplemental source of data for cognitive performance. The relationship between a paper-pencil test for executive functions and fixations confirms that both are related to the same cognitive processes. Therefore, eye movement metrics might be used as an adjunct marker for cognitive abilities like executive functions. However, further research is needed to evaluate the potential of the various eye movement metrics in combination with puzzle games as visual search and attentional marker. © Christine Krebs, Michael Falkner, Joel Niklaus, Luca Persello, Stefan Klöppel, Tobias Nef, Prabitha Urwyler.",Aging; Attention; Cognition; Cognitive assessment; Executive functions; Eye tracking; Fixations; Puzzle games; Visual search,,Article,Final,All Open Access; Gold Open Access; Green Open Access,Scopus,2-s2.0-85103642332,Gaming / VR
Mastrocola V.M.,"Mastrocola, Vicente Martin (57210183325)",57210183325,Horror Ludens: Using Fear to Construct Meaning in Video Games,2021,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),12789 LNCS,,,89,98,9.0,0,10.1007/978-3-030-77277-2_7,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85112140538&doi=10.1007%2f978-3-030-77277-2_7&partnerID=40&md5=be0bd1587ee6d64b9bc083e88be5563d,"In this article, we discuss how fear can be a powerful element to construct meaning in some specific video games. Titles like Phantaruk, Alan Wake, Here they lie, and many others help us to find some answers in this scenario, but herein we intend to focus our attention on the game Rapid Eye Movement (PC, 2020–2021). Created by an independent Brazilian studio named Abysstrakt Games, and scheduled to be launched in late 2021, the game sets its action in a dreamlike ambient where the player has the role of a person inside a nightmare, looking for clues to set the time on different clocks, trying to wake up. As a methodological process to understand how it is possible to create meaning using fear in video games, we have employed a formal analysis of gameplay that “is based on studying a game independently of context, that is, without regarding which specific people are playing a specific instance of the game” [1]. We have observed a group of players of Rapid Eye Movement in order to study how moments of horror and terror create an atmosphere of fear and, consequently, the meaning of the gaming experience. In this work, we present these impressions as a qualitative research, with the objective of identifying the main points inside the fearful experience of playing Rapid Eye Movement, in order to comprehend how terror, horror, anxiety and despair could be used to support the game design process. © 2021, Springer Nature Switzerland AG.",Horror; Indie game; Video game,Eye movements; Wakes; Formal analysis; Game design; Gameplay; Gaming experiences; Qualitative research; Rapid eye movement; Video game; Wake up; Human computer interaction,Conference paper,Final,,Scopus,2-s2.0-85112140538,Gaming / VR
Lin H.; Hsiao M.-Y.; Hsieh Y.-C.; Huang K.-L.; Tsai C.-W.; Lin W.,"Lin, Hsuan (53984887700); Hsiao, Ming-Yu (57205604185); Hsieh, Yu-Chen (56937694300); Huang, Kuo-Liang (56153107200); Tsai, Chia-Wen (57208515161); Lin, Wei (56237618100)",53984887700; 57205604185; 56937694300; 56153107200; 57208515161; 56237618100,A Preliminary Study on the Effect of Somatosensory Games upon Children’s Activity Space and Bodily Movements,2021,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),12771 LNCS,,,127,140,13.0,1,10.1007/978-3-030-77074-7_10,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85112159865&doi=10.1007%2f978-3-030-77074-7_10&partnerID=40&md5=1a411019449be6ce94db782f3207ef44,"With the progress of game technology, the interactions between people and input devices have become more diverse and versatile. Meanwhile, with the introduction of somatosensory games, the mode of interaction is no longer restricted to manual operation. Players can input operational instructions into the game through gestures or bodily movements; in addition, the demand for activity space is different from that in the past. When operating game consoles, players not only pay attention to the distance between their eyes and monitors but also need a proper space to move around. Therefore, besides exploring the correlation between bodily activity space and virtual space, this study probes the influence of somatosensory dance games on children’s bodily movements. The findings show that by performing dances with the help of somatosensory games, children can understand the dance movements in the games and improve their spatial presence. Moreover, the ages of children have a close relationship with the execution of basic bodily movements. Lastly, children in the youngest group mainly employ the upper body while dancing. The findings can be used as a reference for game development. © 2021, Springer Nature Switzerland AG.",Dance; Natural mapping; Spatial presence; Virtual space,Eye movements; Product design; Activity spaces; Bodily movement; Dance movement; Game development; Game technologies; Input devices; Manual operations; Virtual spaces; Software design,Conference paper,Final,,Scopus,2-s2.0-85112159865,Gaming / VR
Amat A.Z.; Zhao H.; Swanson A.; Weitlauf A.S.; Warren Z.; Sarkar N.,"Amat, Ashwaq Z. (57203133051); Zhao, Huan (56963017900); Swanson, Amy (53265026200); Weitlauf, Amy S. (57191707622); Warren, Zachary (25032282500); Sarkar, Nilanjan (7201361624)",57203133051; 56963017900; 53265026200; 57191707622; 25032282500; 7201361624,"Design of an Interactive Virtual Reality System, InViRS, for Joint Attention Practice in Autistic Children",2021,IEEE Transactions on Neural Systems and Rehabilitation Engineering,29,,,1866,1876,10.0,47,10.1109/TNSRE.2021.3108351,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85115432533&doi=10.1109%2fTNSRE.2021.3108351&partnerID=40&md5=f890fcefb7d1f7d5fa99f0fb0109fb73,"Many children with Autism Spectrum Disorder (ASD) exhibit atypical gaze behaviors related to joint attention, a fundamental social-communication skill. Specifically, children with ASD show differences in the skills of gaze sharing and gaze following. In this work we present a novel virtual reality (VR)-based system, called InViRS, in which children with ASD play games allowing them to practice gaze sharing and gaze following. InViRS has three main design contributions: (i) a closed-loop joint attention paradigm with real-time tracking of the participant's eye gaze and game performance measures, (ii) an assistive feedback mechanism that provides guidance and hints in real time, and (iii) a controller that adaptively changes the avatar's gaze prompts according to the performance measures. Results from a pilot study to evaluate the feasibility of InViRS with 9 autistic1 children and 9 typically developing (TD) children offered preliminary support for the feasibility of successful gameplay as well as positive impacts on the targeted skills of gaze sharing and gaze following.  © 2001-2011 IEEE.",Autism; autonomous systems; gaze tracking; human computer interaction; Intelligent system; joint attention; virtual reality,"Attention; Autism Spectrum Disorder; Autistic Disorder; Child; Fixation, Ocular; Humans; Pilot Projects; Virtual Reality; Diseases; Eye tracking; Human computer interaction; Intelligent systems; Atypicals; Autism spectrum disorders; Autistic children; Autonomous system; Children with autisms; Gaze-tracking; Interactive virtual reality; Joint attention; Performance measure; Virtual reality system; adolescent; Article; astrocyte; attention; autism; Autism Diagnostic Observation Schedule; child; clinical article; communication skill; computer assisted tomography; controlled study; eye tracking; facial recognition; feasibility study; feedback system; female; gaze; head movement; human; human computer interaction; imagery; joint; learning disorder; male; perception; skill; social adaptation; Social Communication Questionnaire; Social Responsivenesss scale; task performance; training; video game; virtual reality; visual acuity; attention; eye fixation; pilot study; Virtual reality",Article,Final,All Open Access; Gold Open Access; Green Open Access,Scopus,2-s2.0-85115432533,Gaming / VR
Beitner J.; Helbing J.; Draschkow D.; Võ M.L.H.,"Beitner, Julia (57203870149); Helbing, Jason (57214221531); Draschkow, Dejan (56270096800); Võ, Melissa L.- H. (9634349400)",57203870149; 57214221531; 56270096800; 9634349400,Get your guidance going: Investigating the activation of spatial priors for efficient search in virtual reality,2021,Brain Sciences,11,1,44,1,17,16.0,19,10.3390/brainsci11010044,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85099750756&doi=10.3390%2fbrainsci11010044&partnerID=40&md5=f74394516f98bb165ab9f796bd284374,"Repeated search studies are a hallmark in the investigation of the interplay between memory and attention. Due to a usually employed averaging, a substantial decrease in response times occurring between the first and second search through the same search environment is rarely discussed. This search initiation effect is often the most dramatic decrease in search times in a series of sequential searches. The nature of this initial lack of search efficiency has thus far remained unexplored. We tested the hypothesis that the activation of spatial priors leads to this search efficiency profile. Before searching repeatedly through scenes in VR, participants either (1) previewed the scene, (2) saw an interrupted preview, or (3) started searching immediately. The search initiation effect was present in the latter condition but in neither of the preview conditions. Eye movement metrics revealed that the locus of this effect lies in search guidance instead of search initiation or decision time, and was beyond effects of object learning or incidental memory. Our study suggests that upon visual processing of an environment, a process of activating spatial priors to enable orientation is initiated, which takes a toll on search time at first, but once activated it can be used to guide subsequent searches. © 2021 by the authors. Licensee MDPI, Basel, Switzerland.",Incidental memory; Repeated search; Virtual reality; Visual search,adult; article; eye movement; human; learning; memory; virtual reality; vision,Article,Final,All Open Access; Gold Open Access; Green Open Access,Scopus,2-s2.0-85099750756,Gaming / VR
Lee J.; Kim W.; Kim J.; Lee S.,"Lee, Jeonghaeng (57536763400); Kim, Woojae (56939502800); Kim, Jinwoo (58105917500); Lee, Sanghoon (55746172800)",57536763400; 56939502800; 58105917500; 55746172800,A Study on Virtual Reality Sickness and Visual Attention,2021,"2021 Asia-Pacific Signal and Information Processing Association Annual Summit and Conference, APSIPA ASC 2021 - Proceedings",,,,1465,1469,4.0,7,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85126724127&partnerID=40&md5=2e470132d0955c22cdc481c0d26be670,"It is a well-known fact that virtual reality (VR) sickness is an obstacle to an immersive VR experience, however, an objective analysis of the physiological responses for VR sickness has been insufficient. In this study, our analysis uncovers how the users' visual attention varies with the level of VR sickness and how the level of VR sickness influences the center-bias tendency. Toward this, we first conduct a large-scale eye-tracking experiment of 21 inexperienced users while they experience VR sickness-oriented database VR-SP [15]. Then, we quantify the tendency of visual behavior according to VR sickness. To do this, we newly define a visual entropy measurement of VR visual attention. The experimental results clearly suggest that the center-bias effect becomes stronger as the degree of VR sickness increases. In other words, this implies that the users' explorativeness in VR content may be restricted by the VR sickness and this leads to the restraint of the immersive experience. For a more clear demonstration, we also show the visual entropy can be used to predict VR sickness with an accuracy of 80% on the VR-SP database.  © 2021 APSIPA.",,Behavioral research; Diseases; Entropy; Eye tracking; Physiological models; Center bias; Entropy measurement; Eye-tracking; Immersive virtual reality; Large-scales; Objective analysis; Physiological response; Virtual reality experiences; Visual Attention; Visual behavior; Virtual reality,Conference paper,Final,,Scopus,2-s2.0-85126724127,Gaming / VR
Shojaee A.; Kim H.W.; Cook-Chennault K.; Alarcon I.V.,"Shojaee, Amirbahador (57435037300); Kim, Hyeon Woo (57435175500); Cook-Chennault, Kimberly (25651701700); Alarcon, Idalis Villanueva (56740595400)",57435037300; 57435175500; 25651701700; 56740595400,What you see is what you get? - Relating eye-tracking metrics to students' attention to game elements,2021,"Proceedings - Frontiers in Education Conference, FIE",2021-October,,,,,,5,10.1109/FIE49875.2021.9637372,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123540923&doi=10.1109%2fFIE49875.2021.9637372&partnerID=40&md5=57c2d3e5a5cbce041aba77f2494856b6,"Though engineering digital game inclusion in undergraduate classrooms has steadily increased over the last two decades for in-person courses, their use has exponentially increased in remote and contactless higher education learning environments. Studies exploring student technological acceptance of and content mastery from the use of engineering digital games have provided mixed results in terms of student enjoyment, engagement, and game effectiveness. The majority of these studies have relied on pre- and post-questionnaires to assess differences in students' gaming experiences and performance in the game and learning environment. However, quantitative methods such as the measurement of physiological responses during gameplay have been less explored for the exploration of student engagement and education. The goal of this work is to explore how a set of eye - tracking metrics can be related to gamer attention to in-game stimuli and game interface areas of interest.  © 2021 IEEE.",digital game; engineering education game; eye tracking; mixed methods analysis; undergraduate,Computer aided instruction; E-learning; Engineering education; Eye tracking; Physiological models; Students; Surveys; Digital games; Education game; Engineering education game; Eye-tracking; Game elements; Learning environments; Method analysis; Mixed method; Mixed method analyse; Undergraduate; Computer games,Conference paper,Final,,Scopus,2-s2.0-85123540923,Gaming / VR
Yousefzadeh C.; Van Rynbach A.; Bryant D.; Bos P.,"Yousefzadeh, Comrun (57200619221); Van Rynbach, Andre (36571257200); Bryant, Doug (7202225605); Bos, Philip (35570104600)",57200619221; 36571257200; 7202225605; 35570104600,Continuous high-efficiency beam deflector for ar/vr devices,2021,Digest of Technical Papers - SID International Symposium,52,1,,402,405,3.0,2,10.1002/sdtp.14701,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85113830574&doi=10.1002%2fsdtp.14701&partnerID=40&md5=69b027f610ca1eb8fab23da776ee76c4,"The use of Augmented/Virtual Reality (AR/VR) and near-eye displays (NED) can cause eye fatigue due to the mismatch of the accommodation and convergence (AC) distances. Maxwellian view displays (retina-projection display) have the potential to bypass AC mismatch by projecting the image directly onto the retina. In this approach, the final image always stays in focus at the retina by converging the rays at the eye pupil, therefore, making tthe focus depth independent. However, the narrow eyebox size can hinder the employment of this approach. Therefore, there is a need for a beam steering device with highefficiency, continuous steering capabilities that, in combination with an eye-tracking system, could expand the application of AR/VR systems. In this paper, we represent a novel approach to a fully tunable beam steering device based on the PancharatnamBerry phase in a liquid crystal. This device maintains a high efficiency that, along with an eye-tracking system, can potentially address the limitations in Maxwellian view systems. © 2021 SID.",AR/VR devices; Beam steering device; Fringe Field Switching; Liquid-crystal beam steering; Maxwellian view systems; Opto-Electronic device; Pancharatnam-Berry’s phase,Efficiency; Liquid crystals; Augmented/virtual reality; Beam deflectors; Beam steering devices; Continuous steering; Eye tracking systems; High-efficiency; Pancharatnam-Berry phase; Projection displays; Eye tracking,Conference paper,Final,,Scopus,2-s2.0-85113830574,Gaming / VR
Sedghikhanshir A.; Zhu Y.,"Sedghikhanshir, Alireza (57441099700); Zhu, Yimin (57208037171)",57441099700; 57208037171,Exploring the Impact of Visual Properties of Natural Objects on Attention in Both Real and Virtual Office Environment: A Pilot Study,2021,Computing in Civil Engineering 2021 - Selected Papers from the ASCE International Conference on Computing in Civil Engineering 2021,,,,1384,1392,8.0,3,10.1061/9780784483893.169,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85132583727&doi=10.1061%2f9780784483893.169&partnerID=40&md5=24e8dd3cc0d1f191691ee24002e8e0c2,"Human visual attention to natural objects could change their stress levels in indoor environments. However, the impact of objects' properties as visual stimuli on people in virtual environments has been not understood yet. Consequently, the lack of understanding may prevent researchers from using virtual environments as a reliable tool to support the design of indoor environments with a combination of natural and non-natural objects. We hypothesize that attention to the natural elements differs among objects with different visual properties. Accordingly, we conducted an experimental study to investigate the influences of visual properties (type of natural objects, size, and location). Twenty participants were assigned to experience one of the two comparable office settings (real versus virtual), including three natural objects. The fixation time was measured by the eye-tracking method for the natural objects in conjunction with non-natural objects. The initial analysis showed that the fixation time of natural objects in VR was not different from the real condition. Additionally, different fixation times of objects varied in visual properties suggested that visual properties affect human attention to various objects. © 2021 Computing in Civil Engineering 2021 - Selected Papers from the ASCE International Conference on Computing in Civil Engineering 2021. All rights reserved.",,Behavioral research; Eye tracking; Fixation time; Human visual attention; Indoor environment; Natural objects; Object property; Office environments; Pilot studies; Stress levels; Virtual office; Visual properties; Virtual reality,Conference paper,Final,,Scopus,2-s2.0-85132583727,Gaming / VR
Lalwani R.; Chouhan A.; John V.; Sonar P.; Mahajan A.; Pendyala N.; Streicher A.; Prabhune A.,"Lalwani, Riya (57501184700); Chouhan, Ashish (57211662283); John, Varun (57302204600); Sonar, Prashant (57499309900); Mahajan, Aakash (57506771600); Pendyala, Naresh (57493664300); Streicher, Alexander (37162259500); Prabhune, Ajinkya (56534978900)",57501184700; 57211662283; 57302204600; 57499309900; 57506771600; 57493664300; 37162259500; 56534978900,I-Mouse: A Framework for Player Assistance in Adaptive Serious Games,2021,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),12749 LNAI,,,234,238,4.0,2,10.1007/978-3-030-78270-2_42,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85125084491&doi=10.1007%2f978-3-030-78270-2_42&partnerID=40&md5=60e21d5d1fb205514d6bbc376ae923c2,"A serious game is an educational digital game created to entertain and achieve characterizing goal to promote learning. However, a serious game’s major challenge is capturing and sustaining player attention and motivation, thus restricting learning abilities. Adaptive frameworks in serious games (Adaptive serious games) tackle the challenge by automatically assisting players in balancing boredom and frustration. The current state-of-the-art in Adaptive serious games targets modeling a player’s cognitive states by considering eye-tracking characteristics like gaze, fixation, pupil diameter, or mouse tracking characteristics such as mouse positions. However, a combination of eye and mouse tracking characteristics has seldom been used. Hence, we present I-Mouse, a framework for predicting the need for player assistance in educational serious games through a combination of eye and mouse-tracking data. I-Mouse framework comprises four steps: (a) Feature generation for identifying cognitive states, (b) Partition clustering for player state modeling, (c) Data balancing of the clustered data, and (d) Classification to predict the need for assistance. We evaluate the framework using a real game data set to predict the need for assistance, and Random Forest is the best performing model with an accuracy of 99% amongst the trained classification models. © Springer Nature Switzerland AG 2021.",Adaptivity; Eye and mouse tracking; Serious games,Classification (of information); Decision trees; Eye tracking; Learning systems; Mammals; Serious games; 'current; Adaptive framework; Adaptive serious games; Adaptivity; Cognitive state; Digital games; Eye and mouse tracking; Learning abilities; State of the art; Target model; Forecasting,Conference paper,Final,,Scopus,2-s2.0-85125084491,Gaming / VR
Bruni L.E.; Dini H.; Simonetti A.,"Bruni, Luis Emilio (36338567900); Dini, Hossein (57210580632); Simonetti, Aline (57211819253)",36338567900; 57210580632; 57211819253,Narrative Cognition in Mixed Reality Systems: Towards an Empirical Framework,2021,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),12770 LNCS,,,3,17,14.0,9,10.1007/978-3-030-77599-5_1,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85112139725&doi=10.1007%2f978-3-030-77599-5_1&partnerID=40&md5=da105c16d034490f6ce39ebc290fd9e0,"In this paper, we propose an interdisciplinary theoretical and empirical framework to investigate the particular faculties related to human “narrative cognition”, in general, and in relation to MRT in particular. In order to contextualize our approach, we shortly review the cognitive turn in narratology, as well as state of the art in different domains that have undertaken psychophysiological studies that either characterize aspects that are relevant to narrative cognition, or which investigate mixed reality experiences. The idea is to bring together knowledge and insights from narratology, different branches of semiotics and cognitive sciences, with empirical strategies that bridge the gap between first-person phenomenological approaches and psychophysiological and behavioural methods. We propose a rationale in order to combine tools and techniques from MRT/VR/AR, interactive digital narratives and storytelling, with a suite of integrated psychophysiological methods (such as EEG, HR, GSR and eye tracking) and phenomenological-subjective approaches. © 2021, Springer Nature Switzerland AG.",Electroencephalogram (EEG); Interactive narratives; Mixed reality technologies; Narrative cognition; Phenomenology; Psychophysiology; VR,Eye tracking; Mixed reality; Psychophysiology; Contextualize; Different domains; Electroencephalogram; Interactive narrative; Mixed reality systems; Mixed reality technologies; Narrative cognition; Phenomenology; State of the art; VR; Electroencephalography,Conference paper,Final,,Scopus,2-s2.0-85112139725,Gaming / VR
Wanyu P.E.I.; Xiangmin G.U.O.; Tiantian L.O.,"Wanyu, P.E.I. (57223113785); Xiangmin, G.U.O. (36571734100); Tiantian, L.O. (57223112124)",57223113785; 36571734100; 57223112124,Detecting virtual perception based on multi-dimensional biofeedback: A method to pre-evaluate architectural design objectives,2021,"Projections - Proceedings of the 26th International Conference of the Association for Computer-Aided Architectural Design Research in Asia, CAADRIA 2021",2,,,183,192,9.0,3,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85104850928&partnerID=40&md5=8f0959cb1d33bee7d6c50d8595eaa159,"In the information age, the attention to architectural design has gradually shifted from spatial aesthetics to the human's spatial experience. The situation of human perception becomes essential feedback information that designers can use to improve the design schemes. This research proposes an auxiliary method for pre-evaluating the architectural design goals and providing recommendations for architects to optimize the scheme. Specifically, by aggregating and quantitative analyzing electrophysiological signals and eye-tracking data, this research obtained the user's spatial perception with little effect of subjective consciousness as their feedback on the architectural environment. We took the campus outdoor space of an International School of Design as the research sample. By combining the architect's design concept and objectives, we constructed the contrast spatial schemes in virtual reality (VR) for users to experience and analyzed the usability of this method when pre-evaluate design objectives in a practical project. © 2021 and published by the Association for Computer-Aided Architectural Design Research in Asia (CAADRIA), Hong Kong.",Architectural design objectives; Multi-dimensional biofeedback; Pre-evaluation; Virtual reality,Biofeedback; Electrophysiology; Eye tracking; User experience; Architectural environment; Design objectives; Feed back information; Human perception; Multi dimensional; Practical projects; Quantitative analyzing; Spatial perception; Architectural design,Conference paper,Final,,Scopus,2-s2.0-85104850928,Gaming / VR
Sakamoto K.; Hashimoto S.; Tguchi M.; Hase K.,"Sakamoto, Kenta (57824700100); Hashimoto, Shingo (57215191565); Tguchi, Meguru (57824302400); Hase, Kimitaka (7006073396)",57824700100; 57215191565; 57824302400; 7006073396,Development and expand of 3D rehabilitation system using Mixed Reality technology,2021,Transactions of Japanese Society for Medical and Biological Engineering,Annual59,Proc,,808,810,2.0,0,10.11239/jsmbe.Annual59.808,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85135198553&doi=10.11239%2fjsmbe.Annual59.808&partnerID=40&md5=baf51f7bea30d8fbe10b02521cc04328,"Patients often refuse desk training and test of cognitive function that hasn't changed since about 50 years ago. On the other hand, the tasks using Mixed Reality (MR) have the advantage and maintain patient's motivation easily. We have been developed cognitive rehabilitation tasks using MR in collaboration with Kansai Medical University. At present, we develop a novel rehabilitation system by using Microsoft HoloLens2, based on previous experience and knowledge. We created ""numbers cancellation task"" that imitated the Trail Making Test, ""flower path task"" for dual tasks, and ""search task"" based on daily life. We devised a select method for many clinical situations. This system enables us to move the cursor by head tracking or eye tracking, and we can select the objects by tapping or gazing. Additionally, there are also two-mode, ""touch"" and ""pinch"", as the hand tracking function. Therapists can confirm the patient's first-person view (real space and virtual reality object) on the PC in real-time. Thus, when they are performing the task, it is possible to understand whether or not the patients are correctly instructed by advice. © 2021, Japan Soc. of Med. Electronics and Biol. Engineering. All rights reserved.",cognitive function; HoloLens2; Mixed reality; rehabilitation,Brain; Eye tracking; Patient rehabilitation; Cognitive functions; Cognitive rehabilitation; Dual-tasks; Hololens2; MicroSoft; Mixed reality; Mixed reality technologies; Rehabilitation System; Search tasks; Trail making tests; adult; article; cancellation test; cognition; cognitive rehabilitation; eye tracking; flower; human; rehabilitation; touch; trail making test; virtual reality; Mixed reality,Article,Final,,Scopus,2-s2.0-85135198553,Gaming / VR
Tuah N.M.; Nizam D.N.M.; Sani Z.H.A.,"Tuah, Nooralisa Mohd (36663223300); Nizam, Dinna Nina Mohd (54929854900); Sani, Zaidatol Haslinda A. (46461837200)",36663223300; 54929854900; 46461837200,Modelling the Player and Avatar Attachment based on Student’s Engagement and Attention in Educational Games,2021,International Journal of Advanced Computer Science and Applications,12,7,,353,360,7.0,2,10.14569/IJACSA.2021.0120740,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85112179669&doi=10.14569%2fIJACSA.2021.0120740&partnerID=40&md5=8a2c354dbe244362c052de3f5f93884e,"The Player and Avatar attachment help to motivate a student to strengthen their engagement in gameplay. The different types of avatar designs deployed in a game have an impact on students’ engagements. The avatars are designed with different roles, wherein each role offers varying motivational effects on students’ engagement. Several research in human and computer interaction have assessed user engagement and user attention in a computer or system application as well as in gameplay. Among the usual approaches to assess user engagement are using questionnaire and eye-tracking. Investigating the possible use of these approaches in determining the player and avatar attachment, particularly the attachment that associated with the various avatar designs and their effect on students’ engagement are inconclusive and remains untapped. Essentially, studying students’ engagement and attention perception while learning enriches one’s comprehension about engagement in the education segment. As such, this study proposes a new model of player and avatar attachment based on the students’ engagement and focus attention on the gameplay of digital educational games (DEGs). The model is developed follows a stepwise approach consisting component identification, relationship of the components, model development, and model validation. Several components were scrutinized, summarized, and developed into the model proposed in this study. A significant attachment can determine the avatar design that may influence a student’s engagement in gameplay. Hence, this study offers several constructive recommendations for future avatars in game design for education purpose, which may validate the user’s engagement based on his or her focus attention. © 2021. All Rights Reserved.",attention; Avatar; digital educational games; engagement,Computer games; E-learning; Education computing; Eye tracking; Game design; Human computer interaction; Interactive computer graphics; Attention; Avatar; Avatar designs; Digital educational game; Educational game; Engagement; Gameplay; Humaninteraction; Student engagement; User engagement; Students,Article,Final,All Open Access; Gold Open Access,Scopus,2-s2.0-85112179669,Gaming / VR
Macari S.; Milgramm A.; Reed J.; Shic F.; Powell K.K.; Macris D.; Chawarska K.,"Macari, Suzanne (23009558600); Milgramm, Anna (57210980167); Reed, Jessa (58416610300); Shic, Frederick (6507802882); Powell, Kelly K. (57077873300); Macris, Deanna (42561440300); Chawarska, Katarzyna (6505972503)",23009558600; 57210980167; 58416610300; 6507802882; 57077873300; 42561440300; 6505972503,Context-Specific Dyadic Attention Vulnerabilities During the First Year in Infants Later Developing Autism Spectrum Disorder,2021,Journal of the American Academy of Child and Adolescent Psychiatry,60,1,,166,175,9.0,37,10.1016/j.jaac.2019.12.012,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85086396014&doi=10.1016%2fj.jaac.2019.12.012&partnerID=40&md5=7eaed1eb958d6d500dc76ee079f12b6f,"Objective: Although some eye-tracking studies demonstrate atypical attention to faces by 6 months of age in autism spectrum disorder (ASD), behavioral studies in early infancy return largely negative results. We examined the effects of context and diagnosis on attention to faces during face-to-face live interactions in infants at high familial risk (HR) and low familial risk (LR) for ASD. Method: Participants were 6-, 9-, and 12-month-old siblings of children with ASD who were later determined to have ASD (n = 21), other developmental challenges (HR-C; n = 74), or typical development (TD) (HR-TD; n = 32), and low-risk, typically developing controls (LR-TD; n = 49). Infants were administered the social orienting probes task, consisting of five conditions: dyadic bid, song, peek-a-boo, tickle, and toy play. Attention to an unfamiliar examiner's face was coded by blinded raters from video recordings. Results: At all ages, the ASD group spent less time looking at the examiner's face than the HR-C, HR-TD, and LR-TD groups during the Dyadic Bid and Tickle conditions (all p <.05), but not during the Song, Peek-a-Boo, or Toy Play conditions (all p >.23). Lower attention to faces during Dyadic Bid and Tickle conditions was significantly correlated with higher severity of autism symptoms at 18 months. Conclusion: During the prodromal stages of the disorder, infants with ASD exhibited subtle impairments in attention to faces of interactive partners during interactions involving eye contact and child-directed speech (with and without physical contact), but not in contexts involving singing, familiar anticipatory games, or toy play. Considering the convergence with eye-tracking findings on limited attention to faces in infants later diagnosed with ASD, reduced attention to faces of interactive partners in specific contexts may constitute a promising candidate behavioral marker of ASD in infancy. © 2020 American Academy of Child and Adolescent Psychiatry",attention; autism; infancy; social behavior; social interaction,Autism Spectrum Disorder; Child; Humans; Infant; Risk; Siblings; Article; attention disturbance; autism; Autism Diagnostic Observation Schedule; child; child development; cognition assessment; controlled study; diagnostic procedure; dyadic bid; eye tracking; face; female; game; human; infancy; infant; longitudinal study; major clinical study; male; Mullen Scales of Early Learning; peek a boo; priority journal; sibling; singing; social evolution; social interaction; tickle; toy play; visual attention; risk,Article,Final,All Open Access; Green Open Access,Scopus,2-s2.0-85086396014,Gaming / VR
Arjun S.; Reddy G.S.R.; Mukhopadhyay A.; Vinod S.; Biswas P.,"Arjun, Somnath (57193513088); Reddy, G.S. Rajshekar (57377147400); Mukhopadhyay, Abhishek (57205506014); Vinod, Sanjana (57441219000); Biswas, Pradipta (14007579800)",57193513088; 57377147400; 57205506014; 57441219000; 14007579800,Evaluating Visual Variables in a Virtual Reality Environment,2021,"34th British Human Computer Interaction Conference Interaction Conference, BCS HCI 2021",,,,133,138,5.0,7,10.14236/ewic/HCI2021.1,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124103432&doi=10.14236%2fewic%2fHCI2021.1&partnerID=40&md5=372374c732025a157ba5d3f745846334,"Large amount of multi-dimensional data can be difficult to visualize in standard 2D display. Virtual Reality and the associated 3rd dimension may be useful for data analysis; however, 3D charts may often be confusing to users rather conveying information. This paper investigated and evaluated graphical primitives of 3D charts in a Virtual Reality (VR) environment. We compared six different 3D graphs involving two graph types and five visual variables. We analysed ocular and EEG parameters of users while they undertook representative data interpretation tasks using 3D graphs. Our analysis found significant differences in fixation rate, alpha and low-beta EEG bands among different graphs and a bar chart using different sizes of columns for different data values found to be preferred among users in terms of correct response. We also found that colour makes it easier to interpret nominal data as compared to shape and size variable reduces the time required for processing numerical data as compared to orientation or opacity. Our results can be used to develop 3D sensor dashboard and visualization techniques for VR environments. © Arjun et al. Published by BCS Learning and Development Ltd.",Cognitive load; Evaluation; Eye tracking; Virtual reality; Visual variables; Visualization,Data handling; Data visualization; Eye tracking; Three dimensional computer graphics; Virtual reality; 3d graphs; Cognitive loads; Evaluation; Eye-tracking; Graphical primitives; Large amounts; Multidimensional data; Two-graphs; Virtual-reality environment; Visual variables; Visualization,Conference paper,Final,,Scopus,2-s2.0-85124103432,Gaming / VR
Koirala A.; Yu Z.; Schiltz H.; Van Hecke A.; Armstrong B.; Zheng Z.,"Koirala, Ankit (57209797744); Yu, Zhiwei (57209807755); Schiltz, Hillary (57190307874); Van Hecke, Amy (16314144900); Armstrong, Brian (7202332773); Zheng, Zhi (55804315700)",57209797744; 57209807755; 57190307874; 16314144900; 7202332773; 55804315700,A Preliminary Exploration of Virtual Reality-Based Visual and Touch Sensory Processing Assessment for Adolescents with Autism Spectrum Disorder,2021,IEEE Transactions on Neural Systems and Rehabilitation Engineering,29,,9371715,619,628,9.0,18,10.1109/TNSRE.2021.3064148,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85102625234&doi=10.1109%2fTNSRE.2021.3064148&partnerID=40&md5=5d1631218725969abce06f8a5e7ddf6f,"Sensory abnormalities are experienced by 90 - 95% of individuals with Autism Spectrum Disorder (ASD), a developmental disorder that impacts at least 1 in 132 children worldwide. Virtual reality (VR) technologies can precisely present sensory stimuli and be integrated with human sensing technologies to automatically detect sensory responses, and thus has a potential to improve sensory assessment objectiveness and sensitivity, compared to traditional questionnaire-based methods. However, there is a lack of evidence to demonstrate this potential. Therefore, we designed and developed a preliminary sensory assessment VR system (SAVR) to objectively and precisely evaluate the visual and touch sensory processing differences between adolescents with ASD and their typically developing (TD) peers through game playing. A controlled experiment was conducted with 12 adolescents with ASD and 12 TD adolescents. Participants' sensory pattern was assessed by SAVR and a widely used traditional questionnaire-the Adult/Adolescent Sensory Profile (AASP). We hypothesized that: 1) compared to AASP, SAVR can find more significant differences between the two participant groups, and 2) there are significant and strong correlations between the SAVR results and the AASP results. Statistical analyses of the experimental data supported the hypotheses. The implication and limitations of this preliminary exploration as well as future works are discussed. © 2001-2011 IEEE.",Autism spectrum disorder; sensory assessment; virtual reality,Adolescent; Adult; Autism Spectrum Disorder; Child; Cognition; Humans; Perception; Touch; Virtual Reality; Diseases; Surveys; cytochrome c oxidase; Autism spectrum disorders; Controlled experiment; Developmental disorders; Sensory abnormalities; Sensory processing; Sensory profiles; Sensory stimulus; Strong correlation; Article; autism; behavior; correlation analysis; correlation coefficient; data visualization; eye tracking; facial expression; facial recognition; gaze; human; information processing; questionnaire; robotics; sensation; sensitivity analysis; sensory analysis; sensory evoked potential; speech perception; touch; touch sensory processing; velocity; virtual reality; visual information; visual sensory processing; adolescent; adult; autism; child; cognition; perception; Virtual reality,Article,Final,,Scopus,2-s2.0-85102625234,Gaming / VR
,,,"18th International Conference on Engineering Psychology and Cognitive Ergonomics, EPCE 2021, held as part of the 23rd International Conference, HCI International 2020",2021,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),12767 LNAI,,,,,456.0,0,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85112145268&partnerID=40&md5=eed2e22f3f53b0264685c7db9728ba90,"The proceedings contain 34 papers. The special focus in this conference is on Engineering Psychology and Cognitive Ergonomics. The topics include: Implementation Digital Tower for Apron Control on a Large-Scale of International Airport; how Metacognitive Monitoring Feedback Influences Workload in a Location-Based Augmented Reality Environment; effect of Height Perception on State Self-Esteem and Cognitive Performance in Virtual Reality; the Influence of Gender on Human’s Cognitive Ability and the Correlation Research of Different Cognitive Dimensions; neurophysiological Visual Classification Indicators in the Brain-Computer Interface; an Evaluation of Two-Dimensional Digital Input Models for Mathematical Structure: Effects on Working Memory, Cognitive Load, and Efficiency; evaluation of Relationship Quality Within Dyads Through the Performance in Dual-Player Cooperative Tasks; culture’s Consequences on the Categorisation of Causal Factors in Aviation Accident Reports; Cognitive Activity Recognition Based on Self-supervised Learning from EEG Signals; a Qualitative Study on the Workload of High-Speed Railway Dispatchers; computer Aided Search Tasks in a Naturally Occurring Environment; effect of Predictive Next-Letter Highlighting and Its Delays on the Bare-Handed Input in Virtual Reality; methodology to Quantify Accuracy for Procedure Execution Analysis; human Factors Analysis for Aviation Accidents and Incidents in Singapore; preliminary Analysis of Human Error Prediction Model by Using Biological Information; Using Eye Tracking to Analyze the Effects of Spatial Contiguity in MOOC Video Subtitles; system Performance and Empathetic Design Enhance User Experience for Fault Diagnosis Expert System; the Effect of Diagonal Bar Position in Prohibition Signs on Recognition Efficiency of the Signs; a Survey Study of Factors Influencing Smart Phone Fluency; foreword.",,,Conference review,Final,,Scopus,2-s2.0-85112145268,Gaming / VR
Stahlke S.N.; Bellyk J.D.; Meier O.R.; Mirza-Babaei P.; Kapralos B.,"Stahlke, Samantha N. (57148217700); Bellyk, Josh D. (57222495314); Meier, Owen R. (57222493996); Mirza-Babaei, Pejman (39061915500); Kapralos, Bill (55946696900)",57148217700; 57222495314; 57222493996; 39061915500; 55946696900,Frontiers of Immersive Gaming Technology: A Survey of Novel Game Interaction Design and Serious Games for Cognition,2021,Intelligent Systems Reference Library,196,,,523,536,13.0,2,10.1007/978-3-030-59608-8_28,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85102885054&doi=10.1007%2f978-3-030-59608-8_28&partnerID=40&md5=c2e78aacad7de73c711531e44454ecf5,"This chapter presents an overview of novel game interaction design using brain computer interface (BCI), electroencephalography (EEG) and eye tracking.Our main goal is to highlight particular applications of these novel interfaces in digital games and accessible computing technology. We also investigate commercial offerings within these areas, such as mass-market “brain-training” games. Given the growing popularity and the relative novelty of these interfaces, this chapter reviews the current state of the art to gain an understanding of how the field may look moving forward. © 2021, Springer Nature Switzerland AG.",Brain computer interface (BCI); Digital games; Immersive technology; Interaction design; Rehabilitation,,Book chapter,Final,,Scopus,2-s2.0-85102885054,Gaming / VR
Chihming W.; Zexin J.; Yuxin L.; Songqing H.; Zhongwei Y.,"Chihming, Wu (57222383247); Zexin, Ji (57222391322); Yuxin, Lin (57191491986); Songqing, He (57222382405); Zhongwei, Yu (57222384982)",57222383247; 57222391322; 57191491986; 57222382405; 57222384982,Investigation on the eye-tracking technology in hazard identification of building construction engineering,2020,"2nd IEEE International Conference on Architecture, Construction, Environment and Hydraulics 2020, ICACEH 2020",,,9366265,32,35,3.0,6,10.1109/ICACEH51803.2020.9366265,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85102582757&doi=10.1109%2fICACEH51803.2020.9366265&partnerID=40&md5=9134c5b5bb58f27791387309085cd6c5,"The application of eye-tracking technology in construction safety management hazards is studied to accurately understand the personnel's cognition of construction hazards. Based on OSHA's construction safety training materials, this study aims to apply the eye-tracking technology to construction safety into virtual reality (VR) scene. The eye-tracking technology is combined with focus time, trajectory analysis, and other indicators. The tester validated the technology on the construction hazard. Finally, the research results are compared to OSHA construction hazard identification training materials, and feasible development suggestions and directions in the future are suggested. © 2020 IEEE.",building construction; eye-tracking technology; hazard identification; safety management,Hazardous materials; Hazards; Human resource management; Hydraulics; Building construction; Construction hazards; Construction safety; Eye tracking technologies; Hazard identification; Research results; Training material; Trajectory analysis; Eye tracking,Conference paper,Final,,Scopus,2-s2.0-85102582757,Gaming / VR
,,,"13th Annual Information Systems and Neuroscience, NeuroIS 2021",2021,Lecture Notes in Information Systems and Organisation,52 LNISO,,,,,246.0,0,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85119434414&partnerID=40&md5=8e5e115fd33f79f0f304594a3c523096,"The proceedings contain 27 papers. The special focus in this conference is on Information Systems and Neuroscience. The topics include: Predicting In-Field Flow Experiences Over Two Weeks from ECG Data: A Case Study; an Inward Focus of Attention During Information Security Decision Making: Electrophysiological Evidence; EyeTC: Attentive Terms and Conditions of Internet-Based Services with Webcam-Based Eye Tracking; detecting Flow Experiences in the Field Using Video-Based Head and Face Activity Recognition: A Pilot Study; understanding the Potential of Augmented Reality in Manufacturing Environments; on How Mind Wandering Facilitates Creative Incubation While Using Information Technology: A Research Agenda for Robust Triangulation; Consumers Prefer Abstract Design in Digital Signage: An Application of Fuzzy-Trace Theory in NeuroIS; topographic Analysis of Cognitive Load in Tacit Coordination Games Based on Electrophysiological Measurements; Active Learning Techniques for Preparing NeuroIS Researchers; towards a Psychophysiological Investigation of Perceived Trustworthiness and Risk in Online Pharmacies: Results of a Pre-study; examining the Impact of Social Video Game Tournaments on Gamers’ Mental Well-Being; Continuing Doctoral Student Training for NeuroIS and EEG During a Pandemic: A Distance Hands-On Learning Syllabus; design Mode, Color, and Button Shape: A Pilot Study on the Neural Effects of Website Perception; does Media Richness Influence the User Experience of Chatbots: A Pilot Study; development of a New Dynamic Personalised Emotional Baselining Protocol for Human-Computer Interaction; mediators of the Relationship Between Self-control and Pathological Technology Use: Negative Affect and Cognitive Failures, but not Self-efficacy; high Fidelity Vibrokinetic Stimulation Augments Emotional Reactivity and Interhemispheric Coherence During Passive Multimedia Interaction.",,,Conference review,Final,,Scopus,2-s2.0-85119434414,Gaming / VR
Andari E.; Massa N.M.; Fargotstein M.D.; Taylor N.B.; Halverson D.M.; Owens A.V.; Currin D.L.; Bhattacharya A.; Gitman D.; Cuthbert B.C.; Young L.J.; Duncan E.J.,"Andari, Elissar (36027394400); Massa, Nicholas M. (57192712188); Fargotstein, Molly D. (57190222664); Taylor, Nicholas B. (57201032335); Halverson, David M. (57222409617); Owens, Andrew V. (57212087267); Currin, Danielle L. (57225261987); Bhattacharya, Arpita (57188632145); Gitman, Dmitriy (57222409069); Cuthbert, Bruce C. (57222409511); Young, Larry J. (7403664944); Duncan, Erica J. (7006067592)",36027394400; 57192712188; 57190222664; 57201032335; 57222409617; 57212087267; 57225261987; 57188632145; 57222409069; 57222409511; 7403664944; 7006067592,Effects of Oxytocin on Emotion Recognition in Schizophrenia: A Randomized Double-Blind Pilot Study,2021,Journal of Clinical Psychopharmacology,41,2,,103,113,10.0,10,10.1097/JCP.0000000000001367,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85102657362&doi=10.1097%2fJCP.0000000000001367&partnerID=40&md5=1f62e9f9cb3acfd3fc97e9b81a09627f,"Background Schizophrenia (SCZ) is a neurodevelopmental disorder that leads to poor social function. Oxytocin (OXT), a neuropeptide involved in social cognition, is a potential therapeutic agent for alleviating social dysfunction. Therefore, we investigated the effects of intranasal oxytocin (IN-OXT) on emotional processes in experimental interactive social contexts in individuals with SCZ. Methods In a male-only parallel randomized placebo-controlled double-blind trial, we investigated the effects of IN-OXT (24 IU) on visual fixation on pictures of faces and emotion recognition in an interactive ball-tossing game that probed processing of social and nonsocial stimuli. Results Intranasal oxytocin enhanced the recognition of emotions during an emotion-based ball-tossing game. This improvement was specific to the game that included social cue processing. Intranasal oxytocin did not affect eye gaze duration or gaze dwell time on faces in these patients. Conclusions An acute low dose of IN-OXT had a modest effect on social cue processing and was limited to emotion recognition. Higher doses and long-term trials targeting emotional processing in SCZ may lead to improved social function.  © Wolters Kluwer Health, Inc. All rights reserved.",emotion recognition; oxytocin; schizophrenia; social attention,"Administration, Intranasal; Adult; Case-Control Studies; Dose-Response Relationship, Drug; Double-Blind Method; Emotions; Fixation, Ocular; Humans; Male; Middle Aged; Oxytocin; Pilot Projects; Recognition, Psychology; Schizophrenia; Social Perception; oxytocin; adult; case control study; controlled study; dose response; double blind procedure; drug effect; emotion; eye fixation; human; intranasal drug administration; male; middle aged; perception; pilot study; randomized controlled trial; schizophrenia",Article,Final,,Scopus,2-s2.0-85102657362,Gaming / VR
Haskins A.J.; Mentch J.; Botch T.L.; Robertson C.E.,"Haskins, Amanda J. (57201212494); Mentch, Jeff (57210840182); Botch, Thomas L. (57217186010); Robertson, Caroline E. (57196949630)",57201212494; 57210840182; 57217186010; 57196949630,"Active vision in immersive, 360° real-world environments",2020,Scientific Reports,10,1,14304,,,,29,10.1038/s41598-020-71125-4,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85089976369&doi=10.1038%2fs41598-020-71125-4&partnerID=40&md5=14bb25c9caebae711dac19a098c28917,"How do we construct a sense of place in a real-world environment? Real-world environments are actively explored via saccades, head turns, and body movements. Yet, little is known about how humans process real-world scene information during active viewing conditions. Here, we exploited recent developments in virtual reality (VR) and in-headset eye-tracking to test the impact of active vs. passive viewing conditions on gaze behavior while participants explored novel, real-world, 360° scenes. In one condition, participants actively explored 360° photospheres from a first-person perspective via self-directed motion (saccades and head turns). In another condition, photospheres were passively displayed to participants while they were head-restricted. We found that, relative to passive viewers, active viewers displayed increased attention to semantically meaningful scene regions, suggesting more exploratory, information-seeking gaze behavior. We also observed signatures of exploratory behavior in eye movements, such as quicker, more entropic fixations during active as compared with passive viewing conditions. These results show that active viewing influences every aspect of gaze behavior, from the way we move our eyes to what we choose to attend to. Moreover, these results offer key benchmark measurements of gaze behavior in 360°, naturalistic environments. © 2020, The Author(s).",,"Adolescent; Adult; Attention; Eye Movements; Eye-Tracking Technology; Female; Humans; Male; Virtual Reality; Vision, Ocular; Young Adult; adult; article; attention; controlled study; exploratory behavior; eye tracking; female; gaze; human; human experiment; information seeking; major clinical study; male; motion; saccadic eye movement; virtual reality; vision; adolescent; attention; eye movement; young adult",Article,Final,All Open Access; Gold Open Access; Green Open Access,Scopus,2-s2.0-85089976369,Gaming / VR
Fortin-Guichard D.; Laflamme V.; Julien A.-S.; Trottier C.; Grondin S.,"Fortin-Guichard, Daniel (57008328500); Laflamme, Vincent (55965789900); Julien, Anne-Sophie (57078441200); Trottier, Christiane (25951656900); Grondin, Simon (7004026519)",57008328500; 55965789900; 57078441200; 25951656900; 7004026519,Decision-making and dynamics of eye movements in volleyball experts,2020,Scientific Reports,10,1,17288,,,,18,10.1038/s41598-020-74487-x,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85092559219&doi=10.1038%2fs41598-020-74487-x&partnerID=40&md5=5d8e07a8d187b9578f3f9a89cc9cb89a,"Key decision-makers among experts in a given field can sometimes be identified based on their role and responsibilities. The aim of the study is to compare perceptual-cognitive skills of experts with decisional responsibilities (setters in volleyball) with that of other volleyball experts. Eighty-two participants (26 setters, 36 other players and 20 controls) viewed 50 volleyball video sequences. Sequences stopped 120 ms before ball contact and participants, whose eye movements were recorded, had to predict the ball direction. Generalized Estimating Equations analysis revealed that setters and controls made more but shorter fixations than other players. However, both expert groups made better predictions than controls. Dynamics analyses of eye movements over time show that, right before ball contact, opposing players’ upper body is a most relevant attentional cue in all game situations. Results are discussed in terms of decision-making responsibilities to identify key decision-makers in volleyball and in general. They point towards specific perceptual-cognitive abilities found in setters and support the idea that they constitute a subgroup of experts, but that they are not “better” than other players in anticipating the game. © 2020, The Author(s).",,Adolescent; Adult; Athletes; Athletic Performance; Attention; Cognition; Decision Making; Eye Movements; Female; Humans; Male; Video Recording; Volleyball; Young Adult; adult; article; decision making; eye movement; female; human; human experiment; major clinical study; male; prediction; responsibility; skill; videorecording; volleyball; adolescent; athlete; athletic performance; attention; cognition; physiology; psychology; volleyball; young adult,Article,Final,All Open Access; Gold Open Access; Green Open Access,Scopus,2-s2.0-85092559219,Gaming / VR
Munz T.; Schäfer N.; Blascheck T.; Kurzhals K.; Zhang E.; Weiskopf D.,"Munz, Tanja (56369779800); Schäfer, Noel (57217115390); Blascheck, Tanja (55338748700); Kurzhals, Kuno (55390097400); Zhang, Eugene (37076152100); Weiskopf, Daniel (6603960393)",56369779800; 57217115390; 55338748700; 55390097400; 37076152100; 6603960393,Comparative visual gaze analysis for virtual board games,2020,ACM International Conference Proceeding Series,,,3430038,,,,1,10.1145/3430036.3430038,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85098454025&doi=10.1145%2f3430036.3430038&partnerID=40&md5=4dd5bb25c2b8496a0a3919dace556a4e,"We introduce an approach for the visual analysis of eye movement data from two people playing competitive virtual board games. Our approach provides methods to temporally synchronize and spatially register gaze and mouse recordings from two eye tracking devices. Analysts can examine such fused data visually with a combination of techniques: attention maps and gaze plots as well as a temporal summary of the distance between gaze positions and mouse events of the two players. We show different game scenarios from the competitive game Go, which is especially complex for analyzing strategies of individual players, to demonstrate our methods. In general, our visual analysis approach can provide analysts with insights into strategies, learning processes, and means of communication between people.  © 2020 ACM.",board games; comparative analysis; eye tracking; visual analysis,Data visualization; Eye movements; Visual communication; Board games; Competitive games; Eye movement datum; Eye tracking devices; Game scenario; Gaze analysis; Learning process; Visual analysis; Eye tracking,Conference paper,Final,,Scopus,2-s2.0-85098454025,Gaming / VR
Lee H.; Cho H.; Choe Y.S.; Seo S.W.; Joo E.Y.,"Lee, Hanul (57210752496); Cho, Hyunjin (59439026900); Choe, Yeong Sim (57210807810); Seo, Sang Won (16305669100); Joo, Eun Yeon (7003659894)",57210752496; 59439026900; 57210807810; 16305669100; 7003659894,Association Between Amyloid Accumulation and Sleep in Patients With Idiopathic REM Sleep Behavior Disorder,2020,Frontiers in Neurology,11,,547288,,,,7,10.3389/fneur.2020.547288,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85097812613&doi=10.3389%2ffneur.2020.547288&partnerID=40&md5=82d2d9fec5d07e13cbc3187421273ff4,"Background and Objectives: Amyloid-beta protein may lead to sleep disturbance and eventually develop cognitive impairment. Idiopathic rapid eye movement (REM) sleep behavior disorder (iRBD) is a predictor of neurodegeneration, yet there have been limited studies evaluating the relationship between cognitive decline and amyloid accumulation in iRBD patients. The aim of this study is to investigate the clinical and sleep characteristics of iRBD patients and its association with amyloid deposition. Methods: We enroll 23 iRBD patients (mean age, 65.8 years; male, 73.9%), and their mean history of clinically suspected RBD was 6.5 years. All underwent 18F-flutemetamol amyloid PET completed polysomnography (PSG) and questionnaires. Patients were classified into two groups according to amyloid deposition as amyloid positive and negative. Clinical and sleep parameters were compared between groups and were correlated with amyloid deposition, calculated as a standardized uptake value ratio (SUVR). Results: Four patients (17.4%) were revealed to be amyloid positive, and they showed increased percentage of wake after sleep onset (WASO), stage N1, and stage N2 sleep and worse on the Stroop Word Color Test compared to amyloid negative patients. Global SUVR was correlated with total sleep time, sleep efficiency, WASO, and N1 sleep, and these sleep parameters were associated with a part of default mode network of brains such as orbitofrontal, dorsolateral pre-frontal, and left temporal areas. Conclusion: iRBD patients with amyloid deposition have worse sleep quality than patients without amyloid. Relationship between fragmented sleep and amyloid deposition in the default mode network may be crucial to elucidate the disease progress of iRBD. © Copyright © 2020 Lee, Cho, Choe, Seo and Joo.",amyloid; cognition; default mode network; idiopathic REM sleep behavior disorder; sleep,amyloid protein; flutemetamol f 18; adult; aged; Article; bioaccumulation; clinical article; clinical feature; cognition; controlled study; default mode network; disease association; dorsolateral prefrontal cortex; female; human; idiopathic disease; left hemisphere; male; medical history; medical parameters; middle aged; orbital cortex; parasomnia; patient; polysomnography; positron emission tomography; questionnaire; regression analysis; sleep; sleep efficiency; sleep parameters; sleep stage; sleep time; standardization; StroopWord Color Test; temporal cortex; wake after sleep onset,Article,Final,All Open Access; Gold Open Access; Green Open Access,Scopus,2-s2.0-85097812613,Gaming / VR
Lee T.L.; Yeung M.K.; Sze S.L.; Chan A.S.,"Lee, Tsz Lok (57194725942); Yeung, Michael K. (56400385600); Sze, Sophia L. (7005368099); Chan, Agnes S. (7403167842)",57194725942; 56400385600; 7005368099; 7403167842,Computerized eye-tracking training improves the saccadic eye movements of children with attention-deficit/hyperactivity disorder,2020,Brain Sciences,10,12,1016,1,9,8.0,28,10.3390/brainsci10121016,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85098777319&doi=10.3390%2fbrainsci10121016&partnerID=40&md5=d1d430e7d712c53c1169b7f775222c36,"Abnormal saccadic eye movements, such as longer anti-saccade latency and lower pro-saccade accuracy, are common in children with attention-deficit/hyperactivity disorder (ADHD). The present study aimed to investigate the effectiveness of computerized eye-tracking training on improving saccadic eye movements in children with ADHD. Eighteen children with ADHD (mean age = 8.8 years, 10 males) were recruited and assigned to either the experimental (n = 9) or control group (n = 9). The experimental group underwent an accumulated 240 min of eye-tracking training within two weeks, whereas the control group engaged in web game playing for the same amount of time. Saccadic performances were assessed using the anti-and pro-saccade tasks before and after training. Compared to the baseline, only the children who underwent the eye-tracking training showed significant improvements in saccade latency and accuracy in the anti-and pro-saccade tasks, respectively. In contrast, the control group exhibited no significant changes. These preliminary findings support the use of eye-tracking training as a safe non-pharmacological intervention for improving the saccadic eye movements of children with ADHD. © 2020 by the authors. Licensee MDPI, Basel, Switzerland.",ADHD; Cognitive training; Eye-tracking; Fixation; Saccade,accuracy; amplitude modulation; anti saccade task; Article; attention deficit disorder; attention network; behavior change; calibration; child; clinical article; clinical assessment; cognition; computer analysis; computerized eye tracking training; conners parent rating scale revised short form; controlled study; demography; eye tracking; female; frontal lobe; head movement; human; intelligence quotient; intervention study; latent period; male; preschool child; pro saccade task; psychologist; questionnaire; reaction time; saccadic eye movement; school child; task performance; temporal analysis; Wechsler preschool and primary scale of intelligence,Article,Final,All Open Access; Gold Open Access; Green Open Access,Scopus,2-s2.0-85098777319,Gaming / VR
Derick L.-R.; Gabriel G.-S.; Maximo L.-S.; Olivia F.-D.; Noe C.-S.; Juan O.-R.,"Derick, Lagunes-Ramirez (57220986461); Gabriel, Gonzalez-Serna (57219268651); Maximo, Lopez-Sanchez (56358151300); Olivia, Fragoso-Diaz (57221005931); Noe, Castro-Sanchez (57221002401); Juan, Olivares-Rojas (57220984999)",57220986461; 57219268651; 56358151300; 57221005931; 57221002401; 57220984999,Study of the User's Eye Tracking to Analyze the Blinking Behavior while Playing a Video Game to Identify Cognitive Load Levels,2020,"2020 IEEE International Autumn Meeting on Power, Electronics and Computing, ROPEC 2020",,,9258693,,,,7,10.1109/ROPEC50909.2020.9258693,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85097986842&doi=10.1109%2fROPEC50909.2020.9258693&partnerID=40&md5=7c98b6db0a1c96f5df1c9bc18dc67773,"As eye tracking studies are becoming more common, eye behavior is being explored in different scientific fields as it can be analyzed to detect changes in the mental state of users. In this way the development of new products or technologies could include eye tracking as an objective way of obtaining feedback from users. This paper presents an experiment that compares the eye behavior of users' blinks during interaction with two different levels of difficulty in a video game. The results of the experiment show that the participants' blink rate per minute was significantly reduced, according to the state-of-The-Art normal blink behavior, during the two tasks. However, it is also shown that there is no significant difference between the samples taken, which is attributable to the fact that interaction with video games, in any difficulty, represents high cognitive load.  © 2020 IEEE.",cognitive workload; Eye tracking; human-computer interaction,Human computer interaction; Blink rates; Cognitive loads; Eye-tracking studies; Mental state; Scientific fields; State of the art; Video game; Eye tracking,Conference paper,Final,,Scopus,2-s2.0-85097986842,Gaming / VR
Pöysä-Tarhonen J.; Awwal N.; Häkkinen P.; Otieno S.,"Pöysä-Tarhonen, Johanna (36680518000); Awwal, Nafisa (56110791600); Häkkinen, Päivi (55917698700); Otieno, Suzanne (57212026197)",36680518000; 56110791600; 55917698700; 57212026197,From monitoring to sharing of attention in dyadic interaction: The affordances of gaze data to better understand social aspects of remote collaborative problem solving,2020,"ICCE 2020 - 28th International Conference on Computers in Education, Proceedings",1,,,109,118,9.0,2,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85099447499&partnerID=40&md5=ecc79bbacab282b2ca321004a6f2f010,"This paper aims to better understand the social aspects of collaborative problem solving (CPS) through studying joint attention behaviour (JAB) in an online game-like environment. To capture these behaviours and exemplify how 'jointness' is achieved in CPS in remote dyadic interaction, event-related measures are utilised based on the following multiple interaction data: (1) individuals' gaze data from CPS task completion and (2) automatically generated log files (i.e. chats and actions) from dyadic interactions. The results give empirical evidence of the detached, individualistic attention experiences (i.e. monitoring and common attention) and of bidirectional relations (i.e. mutual and shared attention) in which partners adopt an engaged approach towards one another to solve the task together. It is also observed how lower level attention in CPS can be a precursor to a higher level; that is, during interaction, there is a move from monitoring the partner's actions towards common attention experience. In addition, it is noticed that richer second-person relations may come in degrees. In methodological terms, the gaze data can provide access to better uncover dyadic processes during remote CPS, but without the information embedded in the log data, they would not provide sufficient contextual details of the real interaction to fully understand social connotations related to CPS. © ICCE 2020 - 28th International Conference on Computers in Education, Proceedings. All rights reserved.",Collaborative problem solving; Joint attention behaviour; Live eye tracking; Process-orientated research,Affordances; Automatically generated; Collaborative problem solving; Dyadic interaction; Joint attention; Multiple interactions; On-line games; Real interactions; Social aspects,Conference paper,Final,,Scopus,2-s2.0-85099447499,Gaming / VR
Ghani U.; Signal N.; Niazi I.K.; Taylor D.,"Ghani, Usman (59597564900); Signal, Nada (56079751100); Niazi, Imran Khan (56658453700); Taylor, Denise (37860944700)",59597564900; 56079751100; 56658453700; 37860944700,A novel approach to validate the efficacy of single task ERP paradigms to measure cognitive workload,2020,International Journal of Psychophysiology,158,,,9,15,6.0,17,10.1016/j.ijpsycho.2020.09.007,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85092536778&doi=10.1016%2fj.ijpsycho.2020.09.007&partnerID=40&md5=4ad662d9dbbc2c22b2cedd73979e58ae,"The present study examined the utility of a single-task paradigm to evaluate cognitive workload. The cognitive workload from twenty-five healthy participants was measured during a tilt-ball game while tones were presented in the background to generate event-related potentials (ERPs) in electroencephalographic (EEG) data. In the game, participants were instructed to move the ball to highlighted targets and avoid moving obstacles. The game's difficulty level was manipulated (easy, medium, hard) by adjusting the number and speed of the moving obstacles. The difficulty levels were presented in a random order during multiple short runs to minimize the effects of habituation, fatigue, and boredom. The behavioral results showed that greater task difficulty resulted in a significant decrease (p < 0.001) in game performance, i.e., participants achieved few targets with a high collision rate. To evaluate cognitive workload, we measured the amplitude of early ERP components (N1, P1, and P2) corresponding to the involuntary attention orienting response. The amplitude of the N1 component decreased significantly (p = 0.029) with an increase in cognitive workload. These findings suggest that the early ERP component, specifically the N1, corresponds to attention orienting response, and that the task difficulty modulates it. This study provided evidence that the inverse relationship between ERP components and cognitive workload can be reliably assessed by controlling for other factors such as habituation or boredom during a single task paradigm. © 2020 Elsevier B.V.",Cognitive workload; Electroencephalographic (EEG); Event-related potential (ERP); Habituation; Single stimulus; Single-stimulus,Attention; Cognition; Electroencephalography; Evoked Potentials; Humans; Psychomotor Performance; adult; Article; attention; boredom; cognition; electroencephalography; event related potential; fatigue; female; game; habituation; human; human experiment; male; normal human; orienting response; task performance; workload; cognition; evoked response; psychomotor performance,Article,Final,,Scopus,2-s2.0-85092536778,Gaming / VR
Delvigne V.; Wannous H.; Vandeborre J.-P.; Ris L.; Dutoit T.,"Delvigne, Victor (57219434009); Wannous, Hazem (23391125300); Vandeborre, Jean-Philippe (6507497277); Ris, Laurence (6602720245); Dutoit, Thierry (36022249200)",57219434009; 23391125300; 6507497277; 6602720245; 36022249200,Attention Estimation in Virtual Reality with EEG based Image Regression,2020,"Proceedings - 2020 IEEE International Conference on Artificial Intelligence and Virtual Reality, AIVR 2020",,,9319046,10,16,6.0,11,10.1109/AIVR50618.2020.00012,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85100002706&doi=10.1109%2fAIVR50618.2020.00012&partnerID=40&md5=5df4cb7f8d0060642d7143ae3d7ba48d,"Attention Deficit Hyperactivity Disorder (ADHD) is a neurodevelopmental disorder affecting a certain amount of children and their way of living. A novel method to treat this disorder is to use Brain-Computer Interfaces (BCI) throughout the patient learns to self-regulate his symptoms by herself. In this context, researches have led to tools aiming to estimate the attention toward these interfaces. In parallel, the democratization of virtual reality (VR) headset, and the fact that it produces valid environments for several aspects: safe, flexible and ecologically valid have led to an increase of its use for BCI application. Another point is that Artificial Intelligence (AI) is more and more developed in different domain among which medical application. In this paper, we present an innovative method aiming to estimate attention from the measurement of physiological signals: Electroencephalogram (EEG), gaze direction and head movement. This framework is developed to assess attention in VR environments. We propose a novel approach for feature extraction and a dedicated Machine Learning model. The pilot study has been applied on a set of volunteer and our approach presents a lower error rate in comparison with the state of the art methods.  © 2020 IEEE.",Brain-Compute Interface; Eye-tracking; Machine Learning; Virtual Reality,Artificial intelligence; Brain computer interface; Electroencephalography; Medical applications; Medical computing; Attention deficit hyperactivity disorder; Attention estimations; Different domains; Electro-encephalogram (EEG); Innovative method; Machine learning models; Physiological signals; State-of-the-art methods; Virtual reality,Conference paper,Final,All Open Access; Green Open Access,Scopus,2-s2.0-85100002706,Gaming / VR
Richter R.; Günther T.; Groh R.,"Richter, Robert (57206332538); Günther, Tobias (57555264400); Groh, Rainer (25031087800)",57206332538; 57555264400; 25031087800,Using Multiple Perspective Projections to Guide Visual Attention in Glyph-based Data Visualisations in VR,2020,"Proceedings of the ACM Symposium on Virtual Reality Software and Technology, VRST",,,,,,,0,10.1145/3385956.3422096,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85095825551&doi=10.1145%2f3385956.3422096&partnerID=40&md5=8e740a9b2d94f419036dcd0828c0c152,"An important goal of glyph based data visualization is to detect anomalies in data sets or to manage complex search tasks by guiding the users attention to regions of interest. Previous studies suggest that the user's attention is tied to regions where different perspective projections meet. We describe a method for perspective distortion of glyphs within a virtual scene, while the rest of the scene is projected according to the common model of the computer graphics camera. The result is a multi-perspective image that directs the user's attention. The perspective distortion is only applied if the user focuses on an irrelevant area of the data visualization. As soon as the gaze moves towards the relevant glyph, the perspective distortion is gradually removed. Therefore we evaluate eye tracking data in our prototypical implementation. © 2020 Owner/Author.",Eye tracking; gaze detection; glyph visualization; perspective projection,Behavioral research; Eye tracking; Virtual reality; Visualization; Complex searches; Multi-perspective image; Multiple perspectives; Perspective distortion; Perspective projections; Prototypical implementation; Regions of interest; Visual Attention; Data visualization,Conference paper,Final,,Scopus,2-s2.0-85095825551,Gaming / VR
Krishnappa Babu P.R.; Lahiri U.,"Krishnappa Babu, Pradeep Raj (57204900977); Lahiri, Uttama (35119157400)",57204900977; 35119157400,Multiplayer interaction platform with gaze tracking for individuals with autism,2020,IEEE Transactions on Neural Systems and Rehabilitation Engineering,28,11,9205891,2443,2450,7.0,17,10.1109/TNSRE.2020.3026655,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85095861373&doi=10.1109%2fTNSRE.2020.3026655&partnerID=40&md5=c2634b716b63c0ce465502a7191fb319,"Deficits in interpersonal communication along with difficulty in putting oneself into the shoes of others characterizes individuals with Autism Spectrum Disorder (ASD). Additionally, they exhibit atypical looking pattern causing them to miss aspects related to understanding other's preference for a context that is crucial for effective social communication. Prior research studies show the use of multiplayer platforms can improve interaction among these individuals. However, these multiplayer platforms do not demand players to understand each other's preference, important for effective social interaction. In this work, we have developed a multiplayer interaction platform using virtual reality augmented with eye-tracking technology. Thirty-six participants comprising of individuals with ASD (n = 18; GroupASD) and typically developing (TD) individuals (n = 18; GroupTD) interacted in pairs within each participant group using our platform. Results indicate that both GroupASD and GroupTD showed improvement in performance across the tasks with the GroupTD performing better than the GroupASD. Additionally, the eye-gaze data indicated an underlying relationship between one's looking pattern and task performance that was differentiated between the GroupASD and GroupTD. The current results indicate a potential of our multiplayer interaction platform to serve as a complementary tool in the hands of the interventionist promoting social reciprocity and interaction among individuals with ASD.  © 2001-2011 IEEE.",autism; eye-tracking; fixation duration; fixation frequency; Multiplayer social interaction,"Autism Spectrum Disorder; Autistic Disorder; Eye-Tracking Technology; Fixation, Ocular; Humans; Virtual Reality; Diseases; Economic and social effects; Autism spectrum disorders; Complementary tools; Eye tracking technologies; Inter-personal communications; Interaction platform; Research studies; Social communications; Social interactions; Article; autism; base pairing; clinical article; eye fixation; eye tracking; facial recognition; gaze; head movement; human; learning; social interaction; task performance; training; virtual reality; visual acuity; visual attention; visual feedback; visual stimulation; virtual reality; Eye tracking",Article,Final,,Scopus,2-s2.0-85095861373,Gaming / VR
Corrigan J.-P.; Hanna D.; Dyer K.F.W.,"Corrigan, John-Paul (57217178950); Hanna, Donncha (24066993000); Dyer, Kevin F.W. (25648925100)",57217178950; 24066993000; 25648925100,Investigating predictors of trauma induced data-driven processing and its impact on attention bias and free recall,2020,Behavioural and Cognitive Psychotherapy,48,6,,646,657,11.0,4,10.1017/S135246582000048X,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85092802385&doi=10.1017%2fS135246582000048X&partnerID=40&md5=1bd65a665064e94c49e03c6ef54c428f,"Whilst data-driven processing (DDP) during trauma has been shown to play a role in poor memory integration and is associated with post-traumatic stress disorder (PTSD) re-experiencing symptoms, the pre-trauma risk factors and related cognitive mechanisms are uncertain.Aims: This experimental study aimed to investigate predictors of peri-traumatic DDP, as well as its role in attention bias to threat and free recall.Method: A virtual reality video was used to simulate an analogue trauma. Questionnaires, a free recall task, and an eye-tracking measure assessed cognitive changes after exposure.Results: Regression analysis demonstrated that trait dissociation at pre-exposure to trauma significantly predicted DDP. Attention bias towards threat-related images was found. Results showed that DDP and poorer free recall predicted attention bias to threat images and higher levels of DDP actually predicted higher overall scores in the free recall task.Conclusions: This study showed that DDP is strongly linked to dissociative traits, and along with memory disintegration it may predict attention changes after exposure to a trauma. © ",attentional bias; memory; PTSD; virtual reality,"Attention; Attentional Bias; Dissociative Disorders; Humans; Mental Recall; Stress Disorders, Post-Traumatic; attention; attentional bias; dissociative disorder; human; posttraumatic stress disorder; recall",Article,Final,,Scopus,2-s2.0-85092802385,Gaming / VR
Lui M.; McEwen R.; Mullally M.,"Lui, Michelle (54993944900); McEwen, Rhonda (24470147200); Mullally, Martha (23486065200)",54993944900; 24470147200; 23486065200,Immersive virtual reality for supporting complex scientific knowledge: Augmenting our understanding with physiological monitoring,2020,British Journal of Educational Technology,51,6,,2180,2198,18.0,45,10.1111/bjet.13022,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85089782866&doi=10.1111%2fbjet.13022&partnerID=40&md5=aab5e0bc45d47b6a2576f0404426b3d0,"Educators are recognizing the potential power of immersive virtual reality (IVR) to allow learners to experience previously intangible firsthand phenomena, such as atoms and molecules. In this study, an IVR simulation of a complex gene regulation system was co-designed with an undergraduate microbiology course instructor. The course, with 234 students, was taught using active learning strategies, including peer instruction and exposure to a two-dimensional computer simulation. Thirty-four students from the course participated in an interactive IVR experience using head-mounted displays. We assess students' conceptual understanding using tests, multimodal data collected during the IVR sessions (including video analysis in combination with physiological sensor data and eye-tracking data) as well as semi-structured interviews. We found that students who were seated while in IVR demonstrated significantly higher conceptual understanding of gene regulation at the end of the course and higher overall course outcomes, as compared to students who experienced the course as originally designed (control). However, students who experienced IVR in a standing position performed similarly to the control group. In addition, learning gain appears to be influenced by a combination of prior knowledge and how IVR is experienced (ie, sitting vs. standing). Learning implications for the connections between sensorimotor systems and cognition in IVR are discussed. © 2020 British Educational Research Association",,Eye tracking; Genes; Helmet mounted displays; Learning systems; Patient monitoring; Physiology; Students; User experience; Active learning strategies; Conceptual understanding; Head mounted displays; Immersive virtual reality; Physiological monitoring; Physiological sensors; Semi structured interviews; Sensorimotor systems; Virtual reality,Article,Final,,Scopus,2-s2.0-85089782866,Gaming / VR
Cardoso Da Silva A.; Sierra-Franco C.A.; Silva-Calpa G.F.M.; Carvalho F.; Raposo A.B.,"Cardoso Da Silva, Abner (57215290796); Sierra-Franco, Cesar A. (57210573981); Silva-Calpa, Greis Francy M. (57200627100); Carvalho, Felipe (23396260500); Raposo, Alberto Barbosa (6603721426)",57215290796; 57210573981; 57200627100; 23396260500; 6603721426,Eye-tracking Data Analysis for Visual Exploration Assessment and Decision Making Interpretation in Virtual Reality Environments,2020,"Proceedings - 2020 22nd Symposium on Virtual and Augmented Reality, SVR 2020",,,9262683,39,46,7.0,9,10.1109/SVR51698.2020.00022,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85099530045&doi=10.1109%2fSVR51698.2020.00022&partnerID=40&md5=bf2f147b7a108bbeef8a560f53faf170,"This study presents an eye-tracking data analysis using ten different metrics aiming to complement the evaluation process of visual attention and decision making. We developed a virtual reality environment and used an eye-tracking device incorporated in a Head-Mounted Display to collect and analyze data. In the virtual environment, we included an activity that requires attention in the decision making process. 10 subjects participated in the activity that consists of solving a set of Raven's Progressive Matrices questions. The data generated in the questions solving process were analyzed. Results suggest that metrics such as spatial density, on-target fixation and time to first fixation provide insights that may help to improve the visual attention analysis, and thus, the decision making process.  © 2020 IEEE.",eye-tracking; virtual reality; visual attention,Augmented reality; Behavioral research; Data handling; Decision making; Helmet mounted displays; Information analysis; Virtual reality; Decision making process; Eye tracking devices; Head mounted displays; Spatial densities; Virtual-reality environment; Visual Attention; Visual exploration; Eye tracking,Conference paper,Final,,Scopus,2-s2.0-85099530045,Gaming / VR
Rubin M.; Minns S.; Muller K.; Tong M.H.; Hayhoe M.M.; Telch M.J.,"Rubin, Mikael (56459512900); Minns, Sean (57202513002); Muller, Karl (57204293789); Tong, Matthew H. (16426328500); Hayhoe, Mary M. (7004513666); Telch, Michael J. (7006365823)",56459512900; 57202513002; 57204293789; 16426328500; 7004513666; 7006365823,Avoidance of social threat: Evidence from eye movements during a public speaking challenge using 360°- video,2020,Behaviour Research and Therapy,134,,103706,,,,23,10.1016/j.brat.2020.103706,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85090416158&doi=10.1016%2fj.brat.2020.103706&partnerID=40&md5=be751a5ad373870e533ceaa7c6445af0,"Social anxiety (SA) is thought to be maintained in part by avoidance of social threat, which exacerbates fear of negative evaluation. Yet, relatively little research has been conducted to evaluate the connection between social anxiety and attentional processes in realistic contexts. The current pilot study examined patterns of attention (eye movements) in a commonly feared social context – public speaking. Participants (N = 84) with a range of social anxiety symptoms gave an impromptu five-minute speech in an immersive 360°-video environment, while wearing a virtual reality headset equipped with eye-tracking hardware. We found evidence for the expected interaction between fear of public speaking and social threat (uninterested vs. interested audience members). Consistent with prediction, participants with greater fear of public speaking looked fewer times at uninterested members of the audience (high social threat) compared to interested members of the audience (low social threat) b = 0.418, p = 0.046, 95% CI [0.008, 0.829]. Analyses of attentional indices over the course of the speech revealed that the interaction between fear of public speaking and gaze on audience members was only significant in the first three-minutes. Our results provide support for theoretical models implicating avoidance of social threat as a maintaining factor in social anxiety. Future research is needed to test whether guided attentional training targeting in vivo attentional avoidance may improve clinical outcomes for those presenting with social anxiety. © 2020 Elsevier Ltd",Avoidance; Eye movements; Social anxiety; Virtual reality; Visual attention,"Adolescent; Adult; Anxiety; Avoidance Learning; Eye Movement Measurements; Female; Fixation, Ocular; Humans; Male; Middle Aged; Phobia, Social; Pilot Projects; Speech; Virtual Reality; Young Adult; adult; anxiety; Article; attention; avoidance behavior; eye movement; eye tracking; fear; female; gaze; human; immersion; in vivo study; Liebowitz Social Anxiety Scale; male; outcome assessment; pilot study; prediction; public speaking; risk factor; social anxiety; theoretical model; threat; virtual reality; adolescent; eye fixation; middle aged; oculography; pathophysiology; psychology; social phobia; speech; young adult",Article,Final,All Open Access; Green Open Access,Scopus,2-s2.0-85090416158,Gaming / VR
Shi Y.; Du J.; Zhu Q.,"Shi, Yangming (57189999728); Du, Jing (57219889677); Zhu, Qi (57209806243)",57189999728; 57219889677; 57209806243,The impact of engineering information format on task performance: Gaze scanning pattern analysis,2020,Advanced Engineering Informatics,46,,101167,,,,26,10.1016/j.aei.2020.101167,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85090331127&doi=10.1016%2fj.aei.2020.101167&partnerID=40&md5=7a3324aaff6899572e7cee45665daaa7,"The emergence of new visualization technologies such as Virtual Reality (VR) and Augmented Reality (AR) had been widely implemented in the Architecture, Engineering, and Construction (AEC) industry. Although cumulative evidence pointed out a positive impact of these visualization technologies on construction task performance, there is still an obvious disagreement on the benefits or implications of these new visualization technologies, due to the lack of understanding of the mechanisms in which the visualization affects cognitive processes related to information processing. To obtain more evidence, this paper presents a human-subject experiment (n = 90) to investigate the impact of information format on the performance of an industrial pipeline maintenance task. The investigation centers around how different engineering information formats affect the attention patterns as a potential explanation for the changes in performance. A between-group experiment design was used where the participants were randomly assigned to one of the three groups (2D group, 3D group, and VR group) depending on what type of information was given to review the pipe operation instruction. After the review session, the participants were asked to perform the operation task in the virtual environment based on their memory. The results showed that the 3D and VR groups outperformed the 2D group in task performance. The analysis of eye-tracking data further indicated that the information format significantly changed the gaze scanning pattern when participants were reviewing the operational instructions. We also found that the task performance was correlated with eye-tracking features including gaze movement and pupil dilation. Our findings provided more evidence about the mechanisms in which new visualization technologies affect the attention patterns, helped resolve the current disagreement within the literature. In addition, a prediction model was proposed to use eye-tracking features to predict construction task performance. © 2020 Elsevier Ltd",Eye-tracking; Information format; Virtual reality; Working memory,"Augmented reality; Eye movements; Predictive analytics; Virtual reality; Visualization; Architecture , engineering , and constructions; Cumulative evidence; Engineering information; Experiment design; Human subject experiments; Information format; Pipeline maintenance; Visualization technologies; Eye tracking",Article,Final,All Open Access; Bronze Open Access,Scopus,2-s2.0-85090331127,Gaming / VR
,,,ICMI 2020 - Proceedings of the 2020 International Conference on Multimodal Interaction,2020,ICMI 2020 - Proceedings of the 2020 International Conference on Multimodal Interaction,,,,,,919.0,0,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85096643902&partnerID=40&md5=08c9421be790cb0e8a70ca335c984b2a,The proceedings contain 125 papers. The topics discussed include: advanced multi-instance learning method with multi-features engineering and conservative optimization for engagement intensity prediction; a multi-modal system to assess cognition in children from their physical movements; a neural architecture for detecting user confusion in eye-tracking data; analysis of face-touching behavior in large scale social interaction dataset; attention sensing through multimodal user modeling in an augmented reality guessing game; bring the environment to life: a sonification module for people with visual impairments to improve situation awareness; combining auditory and mid-air haptic feedback for a light switch button; and depression severity assessment for adolescents at high risk of mental disorders.,,,Conference review,Final,,Scopus,2-s2.0-85096643902,Gaming / VR
Syrjämäki A.H.; Isokoski P.; Surakka V.; Pasanen T.P.; Hietanen J.K.,"Syrjämäki, Aleksi H. (57199743520); Isokoski, Poika (6507262881); Surakka, Veikko (6602844678); Pasanen, Tytti P. (56254344300); Hietanen, Jari K. (7006296964)",57199743520; 6507262881; 6602844678; 56254344300; 7006296964,Eye contact in virtual reality – A psychophysiological study,2020,Computers in Human Behavior,112,,106454,,,,28,10.1016/j.chb.2020.106454,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85086498840&doi=10.1016%2fj.chb.2020.106454&partnerID=40&md5=efa94b4683877bdcce8ab43595e3b968,"This experiment investigated whether eye contact would evoke similar attention and emotion related psychophysiological responses in virtual reality (VR) as in a face-to-face interaction. Participants viewed a confederate in a live interaction (Live condition) and a confederate's avatar in VR (VR condition). In both conditions, the confederate/avatar was portraying direct and laterally averted gaze. Heart rate deceleration responses reflecting attention orienting were greater to direct gaze compared to averted gaze, and the effect was not significantly different between Live and VR conditions. However, skin conductance responses reflecting physiological arousal were larger in response to direct than averted gaze only in the Live condition. These results suggest that while eye contact with a live person evokes substantial attention and emotion related psychophysiological responses, the physiological effects of eye contact are diminished in VR. © 2020 Elsevier Ltd",Arousal; Attention; Avatar; Eye gaze; Mediated communication,Physiology; Virtual reality; Eye contact; Face-to-face interaction; Heart rates; Physiological effects; Skin conductance; adult; arousal; article; attention; controlled study; deceleration; electrodermal response; female; gaze; heart rate; human; human experiment; male; virtual reality; Physiological models,Article,Final,All Open Access; Green Open Access,Scopus,2-s2.0-85086498840,Gaming / VR
Putze F.; Küster D.; Urban T.; Zastrow A.; Kampen M.,"Putze, Felix (22036416700); Küster, Dennis (15064259400); Urban, Timo (57220055588); Zastrow, Alexander (57220046354); Kampen, Marvin (57220055117)",22036416700; 15064259400; 57220055588; 57220046354; 57220055117,Attention Sensing through Multimodal User Modeling in an Augmented Reality Guessing Game,2020,ICMI 2020 - Proceedings of the 2020 International Conference on Multimodal Interaction,,,,33,40,7.0,4,10.1145/3382507.3418865,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85096675360&doi=10.1145%2f3382507.3418865&partnerID=40&md5=b31fbf3ac59f0288b7cb189846204695,"We developed an attention-sensitive system that is capable of playing the children's guessing game ""I spy with my litte eye""with a human user. In this game, the user selects an object from a given scene and provides the system with a single-sentence clue about it. For each trial, the system tries to guess the target object. Our approach combines top-down and bottom-up machine learning for object and color detection, automatic speech recognition, natural language processing, a semantic database, eye tracking, and augmented reality. Our evaluation demonstrates performance significantly above chance level, and results for most of the individual machine learning components are encouraging. Participants reported very high levels of satisfaction and curiosity about the system. The collected data shows that our guessing game generates a complex and rich data set. We discuss the capabilities and challenges of our system and its components with respect to multimodal attention sensing.  © 2020 ACM.",attention; augmented reality; gamification; top-down and bottom-up modeling,Augmented reality; Eye tracking; Interactive computer systems; Machine learning; Natural language processing systems; Object detection; Semantics; Speech recognition; Automatic speech recognition; Color detection; Multi-modal; NAtural language processing; Semantic database; Sensitive systems; Target object; User Modeling; Object tracking,Conference paper,Final,,Scopus,2-s2.0-85096675360,Gaming / VR
Moinnereau M.-A.; Oliveira A.; Falk T.H.,"Moinnereau, Marc-Antoine (57204068344); Oliveira, Alcyr (55231999300); Falk, Tiago H. (7004897891)",57204068344; 55231999300; 7004897891,Saccadic Eye Movement Classification Using ExG Sensors Embedded into a Virtual Reality Headset,2020,"Conference Proceedings - IEEE International Conference on Systems, Man and Cybernetics",2020-October,,9283019,3494,3498,4.0,7,10.1109/SMC42975.2020.9283019,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85098873798&doi=10.1109%2fSMC42975.2020.9283019&partnerID=40&md5=a3cdc59a59c0c1e3f810d1636611374a,"Measuring saccadic eye movements when wearing a virtual reality (VR) head-mounted display (HMD) has recently gained a lot of attention, as it allows for enriched user experiences. This has led to an increase in devices showcasing camera-based eye tracking capabilities. Such devices, however, can be orders of magnitude more expensive than conventional off-the-shelf HMDs. In this study, we explore the use of low-cost sensors embedded directly into the faceplate of the HMD to measure electroencephalography (EEG) and electrooculography (EOG) signals. In a ""do-it-yourself"" manner, we rely on the openBCI biosignal amplifier for data acquisition. A 7-channel system was tested on four participants who attended visually to a moving target in their field-of-view that moved every 10 degrees over a circumference. Time series and handcrafted features were extracted from the measured ExG signals and served as input to two different classifiers: support vector machine (SVM) and a multilayer perceptron (MLP). A hierarchical classification approach was proposed and found to achieve the best results with the fusion of both features sets, resulting in an average accuracy of 76.51% with an SVM. The results are encouraging and suggest that accurate, low-cost classification of saccadic eye movements may be possible. © 2020 IEEE.",electroencephalography; Electrooculography; eye movement; head-mounted display; MLP; SVM; virtual reality,Biomedical signal processing; Costs; Data acquisition; Electroencephalography; Electrophysiology; Eye movements; Eye tracking; Helmet mounted displays; Multilayer neural networks; Support vector machines; User experience; Bio-signal amplifier; Head mounted displays; Hierarchical classification; Low-cost sensors; Multi layer perceptron; Orders of magnitude; Saccadic eye movements; Virtual-reality headsets; Virtual reality,Conference paper,Final,,Scopus,2-s2.0-85098873798,Gaming / VR
Pfeuffer K.; Mecke L.; Delgado Rodriguez S.; Hassib M.; Maier H.; Alt F.,"Pfeuffer, Ken (36141954200); Mecke, Lukas (57190245974); Delgado Rodriguez, Sarah (57212167536); Hassib, Mariam (56150903100); Maier, Hannah (57219863654); Alt, Florian (27267528900)",36141954200; 57190245974; 57212167536; 56150903100; 57219863654; 27267528900,Empirical Evaluation of Gaze-enhanced Menus in Virtual Reality,2020,"Proceedings of the ACM Symposium on Virtual Reality Software and Technology, VRST",,,,,,,42,10.1145/3385956.3418962,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85095819416&doi=10.1145%2f3385956.3418962&partnerID=40&md5=37861db4cbabe1e28ede7cf4e2361ae2,"Many user interfaces involve attention shifts between primary and secondary tasks, e.g., when changing a mode in a menu, which detracts the user from their main task. In this work, we investigate how eye gaze input affords exploiting the attention shifts to enhance the interaction with handheld menus. We assess three techniques for menu selection: dwell time, gaze button, and cursor. Each represents a different multimodal balance between gaze and manual input. We present a user study that compares the techniques against two manual baselines (dunk brush, pointer) in a compound colour selection and line drawing task. We show that user performance with the gaze techniques is comparable to pointer-based menu selection, with less physical effort. Furthermore, we provide an analysis of the trade-off as each technique strives for a unique balance between temporal, manual, and visual interaction properties. Our research points to new opportunities for integrating multimodal gaze in menus and bimanual interfaces in 3D environments. © 2020 ACM.",Design; Gaze; Manual input; Menu; Pointing; Virtual Reality,Economic and social effects; Virtual reality; 3-D environments; Attention shifts; Bi-manual interface; Empirical evaluations; Menu selection; Secondary tasks; User performance; Visual interaction; User interfaces,Conference paper,Final,,Scopus,2-s2.0-85095819416,Gaming / VR
Ninaus M.; Kiili K.; Wood G.; Moeller K.; Kober S.E.,"Ninaus, Manuel (55797096600); Kiili, Kristian (8304180400); Wood, Guilherme (8646361800); Moeller, Korbinian (23019055400); Kober, Silvia Erika (36701518800)",55797096600; 8304180400; 8646361800; 23019055400; 36701518800,To Add or Not to Add Game Elements? Exploring the Effects of Different Cognitive Task Designs Using Eye Tracking,2020,IEEE Transactions on Learning Technologies,13,4,9234529,847,860,13.0,27,10.1109/TLT.2020.3031644,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85093665975&doi=10.1109%2fTLT.2020.3031644&partnerID=40&md5=97f096659d632bcf76368d794d1c47ae,"Research on instructional design provides inconsistent results on the use of game elements in cognitive tasks or learning. Cognitive load theory suggests that game elements increase extraneous cognitive load and, thus, may distract the users. In contrast, from an emotional design perspective, the use of game elements is argued to increase performance by providing a more interesting and motivating task environment. To contribute to this debate, the current study investigated the effect of game elements on behavioral performance, attention, and motivation. We designed two versions of the number line estimation task - one with game elements and one without. Participants completed both versions of the task while their eye-fixation behavior was recorded. Results indicated that participants paid attention to game elements, that is, they fixated them, although they were not necessary to complete the task. However, no difference in estimation accuracy was observed between the two task versions. Moreover, the task version with game elements was rated to be more attractive, stimulating, and novel, and participants reported experiencing greater flow. In sum, these data indicate that game elements seem to capture attention but also increase motivational aspects of learning tasks rather than decreasing performance.  © 2008-2011 IEEE.",Educational games; emotional design; eye tracking; game elements; number line estimation; seductive details.,Education computing; Motivation; Behavioral performance; Cognitive load theory; Cognitive loads; Emotional design; Instructional designs; Learning tasks; Line estimation; Task environment; Eye tracking,Article,Final,All Open Access; Green Open Access,Scopus,2-s2.0-85093665975,Gaming / VR
Ma M.; Gong J.; Li W.; Huang L.; Ma X.; Li Y.,"Ma, Mingming (59865477200); Gong, Jianhua (59157665500); Li, Wenhang (12786923500); Huang, Lin (57192088936); Ma, Xiaohui (59791133400); Li, Yabin (57211756228)",59865477200; 59157665500; 12786923500; 57192088936; 59791133400; 57211756228,Layout Optimization of the Directional Emergency Evacuation Signs Based on Virtual Reality Eye-Tracking Experiment; [基于虚拟眼动实验的指向型应急疏散标识布局优化方法],2020,Wuhan Daxue Xuebao (Xinxi Kexue Ban)/Geomatics and Information Science of Wuhan University,45,9,,1386,1394,8.0,8,10.13203/j.whugis20190409,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85092332639&doi=10.13203%2fj.whugis20190409&partnerID=40&md5=f83c1b6aabcc95394c6642e2cf2a235e,"The emergency evacuation sign is one of the key factors that determine whether the personnel can escape safely. The rationality of its layout needs further quantitative optimization research. This paper takes the office building as the experimental area, with the help of the virtual reality eye-tracking experiment, analyzes the unreasonable layout of the directional emergency evacuation signs in the experimental area, and designs an effective modification and optimization scheme to improve the directional function of the emergency evacuation signs. Two groups of virtual eye movement evacuation experiments are designed. Each experiment consists of two experiments, in which the experimental group 1 uses the real evacuation sign layout, and the experimental group 2 uses the modified evacuation sign layout as the verification experiment. Compared with the experimental data, the evacuation time of the two experiments in experimental groups 1 and 2 was reduced by 30.28% and 30.99%, and the expected evacuation distance was reduced by 42.67% and 40.06%, so the evacuation time and the expected evacuation distance were significantly improved. The experimental results and data analysis prove the effectiveness of the modification method and scheme of directional emergency evacuation signs in the experimental area. At the same time, a new optimization method and process of directional emergency evacuation signs layout is established. © 2020, Editorial Board of Geomatics and Information Science of Wuhan University. All right reserved.",Evacuation time; Expected evacuation distance; Layout optimization; Spatial cognition; Virtual geographic experiments; Virtual reality eye-tracking experiment,Accident prevention; Eye movements; Office buildings; Virtual reality; Emergency evacuation; Evacuation experiment; Experimental groups; Layout optimization; Modification methods; Optimization method; Optimization researches; Optimization scheme; architectural design; experiment; eye; optimization; safety; spatial cognition; virtual reality; Eye tracking,Article,Final,,Scopus,2-s2.0-85092332639,Gaming / VR
Shi Y.; Zhu Y.; Mehta R.K.; Du J.,"Shi, Yangming (57189999728); Zhu, Yibo (57209917661); Mehta, Ranjana K. (55413685300); Du, Jing (57219889677)",57189999728; 57209917661; 55413685300; 57219889677,A neurophysiological approach to assess training outcome under stress: A virtual reality experiment of industrial shutdown maintenance using Functional Near-Infrared Spectroscopy (fNIRS),2020,Advanced Engineering Informatics,46,,101153,,,,69,10.1016/j.aei.2020.101153,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85089076628&doi=10.1016%2fj.aei.2020.101153&partnerID=40&md5=3c3d85c3f17b118a4868f9dc1d207db9,"Shutdown maintenance, i.e., turning off a facility for a short period for renewal or replacement operations is a highly stressful task. With the limited time and complex operation procedures, human stress is a leading risk. Especially shutdown maintenance workers often need to go through excessive and stressful on-site trainings to digest complex operation information in limited time. The challenge is that workers’ stress status and task performance are hard to predict, as most trainings are only assessed after the shutdown maintenance operation is finished. A proactive assessment or intervention is needed to evaluate workers’ stress status and task performance during the training to enable early warning and interventions. This study proposes a neurophysiological approach to assess workers’ stress status and task performance under different virtual training scenarios. A Virtual Reality (VR) system integrated with the eye-tracking function was developed to simulate the power plant shutdown maintenance operations of replacing a heat exchanger in both normal and stressful scenarios. Meanwhile, a portable neuroimaging device – Functional Near-Infrared Spectroscopy (fNIRS) was also utilized to collect user's brain activities by measuring hemodynamic responses associated with neuron behavior. A human–subject experiment (n = 16) was conducted to evaluate participants’ neural activity patterns and physiological metrics (gaze movement) related to their stress status and final task performance. Each participant was required to review the operational instructions for a pipe maintenance task for a short period and then perform the task based on their memory in both normal and stressful scenarios. Our experiment results indicated that stressful training had a strong impact on participants’ neural connectivity patterns and final performance, suggesting the use of stressors during training to be an important and useful control factors. We further found significant correlations between gaze movement patterns in review phase and final task performance, and between the neural features and final task performance. In summary, we proposed a variety of supervised machine learning classification models that use the fNIRS data in the review session to estimate individual's task performance. The classification models were validated with the k-fold (k = 10) cross-validation method. The Random Forest classification model achieved the best average classification accuracy (80.38%) in classifying participants’ task performance compared to other classification models. The contribution of our study is to help establish the knowledge and methodological basis for an early warning and estimating system of the final task performance based on the neurophysiological measures during the training for industrial operations. These findings are expected to provide more evidence about an early performance warning and prediction system based on a hybrid neurophysiological measure method, inspiring the design of a cognition-driven personalized training system for industrial workers. © 2020 Elsevier Ltd",Eye-tracking; fNIRS; Shutdown maintenance training; Virtual reality,Behavioral research; Brain; Decision trees; E-learning; Eye tracking; Infrared devices; Maintenance; Near infrared spectroscopy; Neurons; Occupational risks; Plant shutdowns; Supervised learning; Virtual reality; Classification accuracy; Cross-validation methods; Functional near-infrared spectroscopy (fnirs); Maintenance operations; Neural activity patterns; Neurophysiological measures; Random forest classification; Supervised machine learning; Functional neuroimaging,Article,Final,All Open Access; Bronze Open Access,Scopus,2-s2.0-85089076628,Gaming / VR
Davis R.; Sikorskii A.,"Davis, Rebecca (24279725800); Sikorskii, Alla (8554152100)",24279725800; 8554152100,Eye Tracking Analysis of Visual Cues during Wayfinding in Early Stage Alzheimer's Disease,2020,Dementia and Geriatric Cognitive Disorders,49,1,,91,97,6.0,37,10.1159/000506859,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85086835982&doi=10.1159%2f000506859&partnerID=40&md5=94e304dce15524177e518f92ae98b64a,"Introduction: Persons with Alzheimer's disease (AD) have profound impairment in wayfinding, potentially related to a deficit in visual attention and selection of relevant environmental information. This study sought to determine differences in visual attention to salient visual cues and nonsalient cues (building features) in older adults with and without AD during active wayfinding in a large-scale, virtual reality spatial task. Methods: Fifteen subjects (7 with AD and 8 controls without AD) were asked to find their way repeatedly during 10 trials in a virtual simulation of a senior retirement community. Subjects wore eye tracking glasses to capture visual fixations while wayfinding. The least square means (LSMs) and their standard errors (SEs) for percentage of fixations and duration of fixations on salient and nonsalient cues were estimated from the linear mixed effects models and compared by group (AD or control) and cue type. Results: The group by cue type interaction was significant for both percentage of fixations (F(1, 13) = 6.79, p = 0.02) and duration of fixations (F(1, 13) = 4.87, p = 0.04). The AD group had significantly lower percentages of fixations on salient cues, LSM = 57.91 (SE = 2.44), compared to controls, LSM = 66.40 (SE = 2.19); p = 0.03. Persons with AD had a higher percentage of fixations on building features, LSM = 31.65 (SE = 2.18), than controls, LSM = 24.54 (SE = 1.95); p = 0.02. Shorter durations of fixations on salient cues were experienced by the AD group, LSM = 38.89 (SE = 1.69), than the control group, LSM = 44.69 (SE = 1.55); p = 0.02. Discussion/Conclusion: Individuals with AD may have difficulty selecting relevant information for wayfinding as compared to normally aging individuals and attend more frequently than controls to irrelevant information. This may help explain the wayfinding difficulties seen in AD. © 2020 S. Karger AG, Basel. Copyright: All rights reserved.",Alzheimer's disease; Eye tracking; Wayfinding,"Aged; Alzheimer Disease; Cues; Early Diagnosis; Eye Movement Measurements; Eye-Tracking Technology; Female; Fixation, Ocular; Humans; Male; Spatial Navigation; Spatial Processing; Virtual Reality; aged; Alzheimer disease; Article; association; clinical article; clinical assessment; clinical evaluation; clinical feature; comparative study; controlled study; eye tracking; female; human; information processing; least square analysis; male; outcome assessment; personal experience; priority journal; retirement; sex difference; spatial analysis; task performance; virtual reality; visual attention; wayfinding; Alzheimer disease; association; early diagnosis; eye fixation; oculography; psychology; spatial behavior; spatial orientation",Article,Final,All Open Access; Green Open Access,Scopus,2-s2.0-85086835982,Gaming / VR
Ding D.; Neerincx M.A.; Brinkman W.-P.,"Ding, Ding (57195589909); Neerincx, Mark A (9133405200); Brinkman, Willem-Paul (8707580500)",57195589909; 9133405200; 8707580500,"The Effect of an Adaptive Simulated Inner Voice on User's Eye-gaze Behaviour, Ownership Perception and Plausibility Judgement in Virtual Reality",2020,Interacting with Computers,32,5,,510,523,13.0,2,10.1093/iwcomp/iwab008,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85110968349&doi=10.1093%2fiwcomp%2fiwab008&partnerID=40&md5=ef3a9998ee71522aba12170aaa2885d4,"Virtual cognitions (VCs) are a stream of simulated thoughts people hear while emerged in a virtual environment, e.g. by hearing a simulated inner voice presented as a voice over. They can enhance people's self-efficacy and knowledge about, for example, social interactions as previous studies have shown. Ownership and plausibility of these VCs are regarded as important for their effect, and enhancing both might, therefore, be beneficial. A potential strategy for achieving this is the synchronization of the VCs with people's eye fixation using eye-tracking technology embedded in a head-mounted display. Hence, this paper tests this idea in the context of a pre-therapy for spider and snake phobia to examine the ability to guide people's eye fixation. An experiment with 24 participants was conducted using a within-subjects design. Each participant was exposed to two conditions: one where the VCs were adapted to eye gaze of the participant and the other where they were not adapted, i.e. the control condition. The findings of a Bayesian analysis suggest that credibly more ownership was reported and more eye-gaze shift behaviour was observed in the eye-gaze-adapted condition than in the control condition. Compared to the alternative of no or negative mediation, the findings also give some more credibility to the hypothesis that ownership, at least partly, positively mediates the effect eye-gaze-adapted VCs have on eye-gaze shift behaviour. Only weak support was found for plausibility as a mediator. These findings help improve insight into how VCs affect people.  © 2021 The Author(s) 2021. Published by Oxford University Press on behalf of The British Computer Society.",eye tracking; eye-gaze adaptive; snake phobia; spider phobia; virtual cognitions; virtual reality exposure,Audition; Helmet mounted displays; Virtual reality; Bayesian Analysis; Exposed to; Eye fixations; Eye tracking technologies; Head mounted displays; Paper tests; Self efficacy; Social interactions; Eye tracking,Article,Final,All Open Access; Green Open Access; Hybrid Gold Open Access,Scopus,2-s2.0-85110968349,Gaming / VR
Dzwilewski K.L.C.; Merced-Nieves F.M.; Aguiar A.; Korrick S.A.; Schantz S.L.,"Dzwilewski, Kelsey L.C. (56786380200); Merced-Nieves, Francheska M. (57212309332); Aguiar, Andrea (7005379000); Korrick, Susan A. (6603926592); Schantz, Susan L. (7101834759)",56786380200; 57212309332; 7005379000; 6603926592; 7101834759,Characterization of performance on an automated visual recognition memory task in 7.5-month-old infants,2020,Neurotoxicology and Teratology,81,,106904,,,,10,10.1016/j.ntt.2020.106904,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85086595177&doi=10.1016%2fj.ntt.2020.106904&partnerID=40&md5=4837468fdc89a876afd7722da7330571,"Infant looking behaviors measured during visual assessment paradigms may be more reliable predictors of long-term cognitive outcomes than standard measures such as the Bayley Scales of Infant Development typically used in environmental epidemiology. Infrared eye tracking technology offers an innovative approach to automate collection and processing of looking behavior data, making it possible to efficiently assess large numbers of infants. The goals of this study were to characterize infant looking behavior measures including side preference, fixation duration, and novelty preference using eye tracking and an automated version of an established visual recognition memory paradigm that includes both human faces and geometric figures as stimuli. An ancillary goal was to assess the feasibility of obtaining a precise measure of looking to the eye region of faces from the eye-tracking data. In this study, 309 7.5-month-old infants from a prospective birth cohort were assessed using a visual recognition memory (VRM) paradigm. Infrared eye tracking was used to record looking time as infants were shown nine blocks of trials with a pair of identical faces or shapes followed by two trials in which the familiar stimulus was paired with a novel one. Infants were assessed in one of four conditions: in conditions A and B, stimulus set 1 were the familiar stimuli and set 2 were novel; in conditions C and D, set 2 were familiar and set 1 novel. The novel stimuli were presented on the right first in conditions A and C and on the left first in conditions B and D. We observed a significant right side preference, which has not been reported before (57% of looking time spent looking at right side stimulus, p-value < 0.0001). Infants showed a preference for the novel stimuli similar to that published in prior studies (57–60% of looking time spent looking at the novel stimulus, p-value < 0.0001), as well as average fixation durations similar to previous studies. Infants also showed a strong preference for the eyes versus the rest of the face (p-value < 0.0001). Novelty preference was significantly higher when set 2 stimuli were novel (p-value < 0.0001), suggesting a preference among infants for set 2 stimuli compared to set 1 stimuli. The pattern of novelty preference across trials was significantly different between infants who saw the novel stimuli on the left first and those who saw them on the right first (p-value < 0.0001) but the overall mean novelty preference was not significantly different between these groups. There were also significant differences in average fixation duration and eyes preference measures across stimuli (p-values < 0.05). These findings show that VRM assessment can be automated for use in large-scale epidemiological studies using infrared eye tracking with looking behavior measure results similar to those obtained with standard non-automated methods, and that side and stimulus preferences are important modifiers of looking behavior that are critical to consider in this type of assessment. © 2020 Elsevier Inc.",Attention; Eye tracking; Infant cognition; Information processing speed; Visual recognition memory,"Attention; Cognition; Female; Humans; Infant; Male; Memory; Pattern Recognition, Visual; Prospective Studies; Recognition, Psychology; article; attention; clinical assessment; cohort analysis; controlled study; eye tracking; face; feasibility study; female; human; human experiment; infant; infrared radiation; male; memory assessment; prospective study; velocity; visual memory; attention; cognition; memory; pattern recognition; physiology",Article,Final,All Open Access; Green Open Access,Scopus,2-s2.0-85086595177,Gaming / VR
Chen N.; Zhao M.; Gao K.; Zhao J.,"Chen, Na (44360905500); Zhao, Ming (57218531881); Gao, Kun (57218530135); Zhao, Jun (57103965100)",44360905500; 57218531881; 57218530135; 57103965100,The physiological experimental study on the effect of different color of safety signs on a virtual subway fire escape—an exploratory case study of zijing mountain subway station,2020,International Journal of Environmental Research and Public Health,17,16,5903,1,19,18.0,29,10.3390/ijerph17165903,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85089438825&doi=10.3390%2fijerph17165903&partnerID=40&md5=a27d9287eda183423d809c43b3c061da,"Safety signs play a very important role in people’s evacuation during emergencies. In order to explore the appropriate color for subway safety signs, four safety signs of different color combinations are designed, and the virtual reality, eye-tracking technology, and physiological indicator measurement are used in a virtual subway fire escape experiment. A total of 96 participants with equal distribution in gender and four different color combination groups were recruited. Participants’ eye-tracking and physiological data (heart rate, skin conductance) were real-time recorded through ErgoLAB V3.0 in the whole experiment. The relationship between Color_of_safety_sign and escape performance, eye-tracking indicators, and physiological indicators is discussed respectively through SPSS. The results show that “Green and black” group has the best evacuation escape performance, low cognitive load, high search efficiency on safety signs, and the highest stress level and immersion and “Green and black” can be the most appropriate color for safety sign. This research is of certain significance for improving the function of subway fire-fighting infrastructure and the resilience of the metro system. Moreover, it can provide references and advice on risk management, emergency evacuation, and so on. © MDPI AG. All rights reserved.",Eye-tracking; Physiological experiment; Safety sign color; Subway fire escape; Virtual reality,Color; Emergencies; Fires; Humans; Railroads; Safety Management; Transportation Facilities; China; color; experimental study; fire; metro system; physiology; risk assessment; transportation safety; virtual reality; adult; article; controlled study; experimental study; exploratory research; eye tracking; female; gender; heart rate; human; human experiment; immersion; major clinical study; male; risk management; skin conductance; stress; virtual reality; color; emergency; fire; railway; safety; traffic and transport,Article,Final,All Open Access; Gold Open Access; Green Open Access,Scopus,2-s2.0-85089438825,Gaming / VR
Duchowski A.T.,"Duchowski, Andrew T. (6701824388)",6701824388,"Eye-based interaction in graphical systems: 20 years later gaze applications, analytics, & interaction",2020,"ACM SIGGRAPH 2020 Courses, SIGGRAPH 2020",,,,,,,2,10.1145/3388769.3407492,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85092007296&doi=10.1145%2f3388769.3407492&partnerID=40&md5=13cf2b4a3e9dce03b9c85eef6e6736de,"The course starts with an overview of eye-tracking applications, distinguishing eye movement analysis from synthesis in virtual reality, games, and other venues including mobile eye tracking. The focus is on four forms of applications: diagnostic (off-line measurement), active (selection, look to shoot), passive (foveated rendering, a.k.a. gaze-contingent displays), and expressive (gaze synthesis). The course covers basic eye movement analytics, e.g., fixation count and dwell time within AOIs, as well as advanced analysis using ambient/focal attention modeling. The course concludes with an overview and a demo of how to build an interactive application using Python. © 2020 Owner/Author.",,Eye movements; Interactive computer graphics; Motion analysis; Advanced analysis; Attention model; Eye movement analysis; Gaze synthesis; Gaze-contingent displays; Interactive applications; Line measurements; Mobile eye-tracking; Eye tracking,Conference paper,Final,,Scopus,2-s2.0-85092007296,Gaming / VR
Kim H.-C.; Jin S.; Jo S.; Lee J.-H.,"Kim, Hyun-Chul (57194876917); Jin, Sangsoo (57217161623); Jo, Sungman (58673297700); Lee, Jong-Hwan (36065360400)",57194876917; 57217161623; 58673297700; 36065360400,A naturalistic viewing paradigm using 360° panoramic video clips and real-time field-of-view changes with eye-gaze tracking: Naturalistic viewing paradigm based on 360° panoramic video and real-time eye gaze,2020,NeuroImage,216,,116617,,,,20,10.1016/j.neuroimage.2020.116617,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85086523933&doi=10.1016%2fj.neuroimage.2020.116617&partnerID=40&md5=bc3127fe51565b934a81cab9368480a5,"The naturalistic viewing of a video clip enables participants to obtain more information from the clip compared to conventional viewing of a static image. Because changing the field-of-view (FoV) allows new visual information to be obtained, we were motivated to investigate whether naturalistic viewing with varying FoV based on active eye movement can enhance the viewing experience of natural stimuli, such as those found in a video clip with a 360° FoV in an MRI scanner. To this end, we developed a novel naturalistic viewing paradigm based on real-time eye-gaze tracking while participants were watching a 360° panoramic video during fMRI acquisition. The gaze position of the participants was recorded using an eye-tracking computer and then transmitted to a stimulus presentation computer via a TCP/IP connection. The identified gaze position was then used to alter the participants' FoV of the video clip in real-time, so the participants could change their FoV to fully explore the 360° video clip (referred to in this paper as active viewing). The gaze position of one participant while watching a video was used to change the FoV of the same video clip for a paired participant (referred to as yoked or passive viewing). Four 360° panoramic videos were used as stimuli, divided into categories based on the brightness level (i.e., bright vs. dark) and location (i.e., nature vs. city). Each of the subjects participated in the active viewing of one of the two nature videos and one of the two city videos and then engaged in the passive viewing of the other video in each category, followed by conventional viewing with a fixed FoV (referred to as fixed viewing) after each of the active or passive viewings. Forty-eight healthy volunteers participated in the study, and data from 42 of these participants were used in the analysis. Representational similarity analysis (RSA) was conducted in a multiple regression framework using representational dissimilarity matrix (RDM) codes to accommodate all of the information regarding neuronal activations from fMRI analysis and the participants' subjective ratings of their viewing experience with the four video clips and with the two contrasting viewing conditions (i.e., “active–fixed” and “passive–fixed”). It was found that the participants' naturalistic viewing experience of the video clips was substantially more immersive with active viewing than with passive and fixed viewing. The RSA using the RDM codes revealed the brain regions associated with the viewing experience, including eye movement and spatial navigation in the superior frontal area (of Brodmann's area 6) and the inferior/superior parietal areas, respectively. Brain regions potentially associated with cognitive and affective processing during the viewing of the video, such as the default-mode networks and insular/Rolandic operculum areas, were also identified. To the best of our knowledge, this is the first study that has used the participants' eye movements to interactively change their FoV for 360° panoramic video clips in real-time. Our method of utilizing the MRI environment can be further extended to other environments such as electroencephalography and behavioral research. It would also be feasible to apply our method to virtual reality and/or augmented reality systems to maximize user experience based on their eye movement. © 2020 The Authors",360° panoramic video; Eye movements; Functional MRI; Naturalistic viewing; Real-time eye gaze; Representational similarity analysis,Adult; Brain Mapping; Cerebral Cortex; Eye Movements; Eye-Tracking Technology; Female; Humans; Magnetic Resonance Imaging; Male; Motion Pictures; Nerve Net; Visual Perception; Young Adult; adult; angular gyrus; anterior cingulate; Article; brain region; brightness; Brodmann area 6; cell activation; cerebellum; cerebellum vermis; cognition; controlled study; electroencephalography; eye movement; eye tracking; female; functional magnetic resonance imaging; gaze; human; human experiment; inferior temporal gyrus; male; middle temporal gyrus; nerve cell; normal human; panoramic video; posterior cingulate; precuneus; priority journal; stimulus response; superior temporal gyrus; supplementary motor area; thalamus; videorecording; visual information; visual stimulation; brain cortex; brain mapping; diagnostic imaging; movie; nerve cell network; nuclear magnetic resonance imaging; physiology; procedures; vision; young adult,Article,Final,All Open Access; Gold Open Access,Scopus,2-s2.0-85086523933,Gaming / VR
Juarez D.; Tur-Viñes V.; Mengual A.,"Juarez, David (55790655400); Tur-Viñes, Victoria (55377110400); Mengual, Ana (57196009252)",55790655400; 55377110400; 57196009252,Neuromarketing Applied to Educational Toy Packaging,2020,Frontiers in Psychology,11,,2077,,,,22,10.3389/fpsyg.2020.02077,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85090757177&doi=10.3389%2ffpsyg.2020.02077&partnerID=40&md5=a6fc9ab5f955e85b4389de05e2b05f0d,"This research work comes in response to the question of which aspects are more relevant for consumers when purchasing educational toys as opposed to other toys that are focused solely on leisure. This empirical research focuses on an educational toy distributed in Spain by the Educa brand (Conector family, reference “I learn English”), which is the product best-selling product of its brand in this area, and analyses how consumers make decisions concerning this product in relation to other products designed by competitors. The research looks into customer reactions while looking at these products, measuring brain activity generated by different aspects of product design and its influence on choice. The aim of the present study was to propose a model that optimizes the design of educational toy packaging. Through the use of neuromarketing techniques –attention through eye tracking, and emotion using galvanic skin response– as well as qualitative research techniques, the objective of this research is to determine the motivations in the processes of buying educational toys. The packaging design elements analyzed are brand, product family, toy name, recommended age, game image, number of questions/topics, and additional texts. The results suggest that the most important elements are the graphic details of the packaging, obtaining a perception of a higher educational level as more questions are addressed by the game. The simultaneous combination of qualitative techniques monitored with galvanic skin response (neuro-qualitative study) allows additional conclusions to be aligned with the end user of the product, including a prominent social component when the product is purchased as a gift for a third party. © Copyright © 2020 Juarez, Tur-Viñes and Mengual.",educational toy; eye tracking; galvanic skin response; marketing; neuromarketing; packaging; psychology,,Article,Final,All Open Access; Gold Open Access; Green Open Access,Scopus,2-s2.0-85090757177,Gaming / VR
Delvigne V.; Ris L.; Dutoit T.; Wannous H.; Vandeborre J.-P.,"Delvigne, Victor (57219434009); Ris, Laurence (6602720245); Dutoit, Thierry (36022249200); Wannous, Hazem (23391125300); Vandeborre, Jean-Philippe (6507497277)",57219434009; 6602720245; 36022249200; 23391125300; 6507497277,VERA: Virtual Environments Recording Attention,2020,"2020 IEEE 8th International Conference on Serious Games and Applications for Health, SeGAH 2020",,,9201699,,,,7,10.1109/SeGAH49190.2020.9201699,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85092717391&doi=10.1109%2fSeGAH49190.2020.9201699&partnerID=40&md5=fe6ece87a0a6839b21da34a964d0e4c4,"Children with Attention Deficit Hyperactivity Disorder (ADHD), present different symptoms binding for everyday life, e.g. difficulty to be focused, impulsiveness, difficulty to regulate motor functions, etc. The most commonly prescribed treatment is the medication that can present side effects. Another solution is behavioural treatment that does not seem to present better results than medication for a higher cost. A novel method with growing interest is the use of neurofeedback (NF) to teach the patient to self-regulate symptoms by herself, through the visualisation of the brain activity in an understandable form. Moreover, virtual reality (VR) is a supportive environment for NF in the context of ADHD. However, before proceeding the NF, it is important to determine the features of the physiological signals corresponding to the symptoms' appearance. We present here a novel framework based on the joint measurement of electroencephalogram (EEG) and sight direction by equipment that can be embedded in VR headset, the goals being to estimate attentional state. In parallel to the signal acquisition, attentional tasks are performed to label the physiological signals. Features have been extracted from the signals and machine learning (ML) models have been applied to retrieve the attentional state. Encouraging results have been provided from the pilot study with the ability to make the right classification in multiple scenarios. Moreover, a dataset with the labelled physiological signals is under development. It will help to have a better understanding of the mechanism behind ADHD symptoms.  © 2020 IEEE.",Brain-Computer Interface; Eye-tracking; Machine Learning; Virtual-Reality,Brain; Electroencephalography; Serious games; Signal processing; Attention deficit hyperactivity disorder; Brain activity; Electro-encephalogram (EEG); Joint measurement; Motor function; Physiological signals; Pilot studies; Signal acquisitions; Virtual reality,Conference paper,Final,All Open Access; Green Open Access,Scopus,2-s2.0-85092717391,Gaming / VR
Horan B.; Heckenberg R.; Maruff P.; Wright B.,"Horan, Ben (57202873469); Heckenberg, Rachael (57191573704); Maruff, Paul (7005861856); Wright, Bradley (23973917600)",57202873469; 57191573704; 7005861856; 23973917600,"Development of a new virtual reality test of cognition: Assessing the test-retest reliability, convergent and ecological validity of CONVIRT",2020,BMC Psychology,8,1,61,,,,15,10.1186/s40359-020-00429-x,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85086603306&doi=10.1186%2fs40359-020-00429-x&partnerID=40&md5=ec148b5ad6c5084985265702bec99be9,"Background: Technological advances provide an opportunity to refine tools that assess central nervous system performance. This study aimed to assess the test-retest reliability and convergent and ecological validity of a newly developed, virtual-reality, concussion assessment tool, 'CONVIRT', which uses eye-tracking technology to assess visual processing speed, and manual reaction time (pushing a button on a riding crop) to assess attention and decision-making. CONVIRT was developed for horse jockeys, as of all sportspersons, they are most at risk of concussion. Methods: Participants (N = 165), were assessed with CONVIRT, which uses virtual reality to give the user the experience of riding a horse during a horserace. Participants were also assessed with standard Cogstate computer-based concussion measures in-between two completions of the CONVIRT battery. The physiological arousal induced by the test batteries were assessed via measures of heart rate and heart rate variability (LF/HF ratio). Results: Satisfactory test-retest reliability and convergent validity with Cogstate attention and decision-making subtests and divergent validity in visual processing speed measures were observed. CONVIRT also increased heart rate and LF/HF ratio, which may better approximate participant arousal levels in their workplace. Conclusions: CONVIRT may be a reliable and valid tool to assess elements of cognition and CNS disruption. The increased ecological validity may also mean better informed 'return-to-play' decisions and stronger industry acceptance due to the real-world meaningfulness of the assessment. However, before this can be achieved, the sensitivity of the CONVIRT battery needs to be demonstrated.  © 2020 The Author(s).",Assessment; Concussion; Eye-tracking; Mild traumatic brain injury; VR,Adolescent; Adult; Attention; Brain Concussion; Cognition; Female; Humans; Male; Neuropsychological Tests; Reaction Time; Reproducibility of Results; Virtual Reality; Young Adult; adolescent; adult; attention; brain concussion; cognition; female; human; male; neuropsychological test; physiology; reaction time; reproducibility; virtual reality; young adult,Article,Final,All Open Access; Gold Open Access; Green Open Access,Scopus,2-s2.0-85086603306,Gaming / VR
Friehs M.A.; Dechant M.; Vedress S.; Frings C.; Mandryk R.L.,"Friehs, Maximilian Achim (57198442354); Dechant, Martin (57194897794); Vedress, Sarah (57219113115); Frings, Christian (34568737400); Mandryk, Regan Lee (6506898492)",57198442354; 57194897794; 57219113115; 34568737400; 6506898492,Effective gamification of the stop-signal task: Two controlled laboratory experiments,2020,JMIR Serious Games,8,3,e17810,,,,53,10.2196/17810,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85097501965&doi=10.2196%2f17810&partnerID=40&md5=e01986f8cebc61ab018d656c1de27a5d,"Background: A lack of ability to inhibit prepotent responses, or more generally a lack of impulse control, is associated with several disorders such as attention-deficit/hyperactivity disorder and schizophrenia as well as general damage to the prefrontal cortex. A stop-signal task (SST) is a reliable and established measure of response inhibition. However, using the SST as an objective assessment in diagnostic or research-focused settings places significant stress on participants as the task itself requires concentration and cognitive effort and is not particularly engaging. This can lead to decreased motivation to follow task instructions and poor data quality, which can affect assessment efficacy and might increase drop-out rates. Gamification—the application of game-based elements in nongame settings—has shown to improve engaged attention to a cognitive task, thus increasing participant motivation and data quality. Objective: This study aims to design a gamified SST that improves participants’ engagement and validate this gamified SST against a standard SST. Methods: We described the design of our gamified SST and reported on 2 separate studies that aim to validate the gamified SST relative to a standard SST. In study 1, a within-subject design was used to compare the performance of the SST and a stop-signal game (SSG). In study 2, we added eye tracking to the procedure to determine if overt attention was affected and aimed to replicate the findings from study 1 in a between-subjects design. Furthermore, in both studies, flow and motivational experiences were measured. Results: In contrast, the behavioral performance was comparable between the tasks (P<.87; BF01=2.87), and the experience of flow and intrinsic motivation were rated higher in the SSG group, although this difference was not significant. Conclusions: Overall, our findings provide evidence that the gamification of SST is possible and that the SSG is enjoyed more. Thus, when participant engagement is critical, we recommend using the SSG instead of the SST. © 2020 JMIR Publications. All Rights Reserved.",Cognition; Experimental; Games; Motivation; Proof of concept study; Psychology; Video games,,Article,Final,All Open Access; Gold Open Access; Green Open Access,Scopus,2-s2.0-85097501965,Gaming / VR
Krukar J.; Mavros P.; Hoelscher C.,"Krukar, Jakub (57190400520); Mavros, Panagiotis (55617109300); Hoelscher, Christoph (7005742397)",57190400520; 55617109300; 7005742397,Towards capturing focal/ambient attention during dynamic wayfinding,2020,Eye Tracking Research and Applications Symposium (ETRA),,,3391417,,,,8,10.1145/3379157.3391417,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85086313485&doi=10.1145%2f3379157.3391417&partnerID=40&md5=72a3a8482bca3b30238adcf7eb018062,"This work-in-progress paper reports on an ongoing experiment in which mobile eye-tracking is used to evaluate different wayfinding support systems. Specifically, it tackles the problem of detecting and isolating attentional demands of building layouts and signage systems in wayfinding tasks. The coefficient K has been previously established as a measure of focal/ambient attention for eye-tracking data. Here, we propose a novel method to compute coefficient K using eye-tracking from virtual reality experiments. We detail challenges associated with transforming a two-dimensional coefficient K concept to three-dimensional data, and the debatable theoretical equivalence of the concept after such a transformation. We present a preliminary implementation to experimental data and explore the possibilities of the method for novel insight in architectural analyses. © 2020 Owner/Author.",attention; eye-tracking; usability; virtual reality; wayfinding,Metadata; Architectural analysis; Building layout; Mobile eye-tracking; Support systems; Three-dimensional data; Way-finding; Wayfinding tasks; Work in progress; Eye tracking,Conference paper,Final,All Open Access; Bronze Open Access; Green Open Access,Scopus,2-s2.0-85086313485,Gaming / VR
Lee-Cultura S.; Sharma K.; Papavlasopoulou S.; Retalis S.; Giannakos M.,"Lee-Cultura, Serena (57203855458); Sharma, Kshitij (55903734200); Papavlasopoulou, Sofia (57063398000); Retalis, Symeon (6602278764); Giannakos, Michail (36462343600)",57203855458; 55903734200; 57063398000; 6602278764; 36462343600,Using sensing technologies to explain children's self-representation in motion-based educational games,2020,"Proceedings of the Interaction Design and Children Conference, IDC 2020",,,,541,555,14.0,35,10.1145/3392063.3394419,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85087391022&doi=10.1145%2f3392063.3394419&partnerID=40&md5=010d91cbde5c5e0f7abd98988350b311,"Motion-Based Touchless Games (MBTG) are being investigated as a promising interaction paradigm in children's learning experiences. Within these games, children's digital persona (i.e, avatar), enables them to efficiently communicate their motion-based interactivity. However, the role of children's Avatar Self-Representation (ASR) in educational MBTG is rather under-explored. We present an in-situ within subjects study where 46 children, aged 8 - 12, played three MBTG with different ASRs. Each avatar had varying visual similarity and movement congruity (synchronisation of movement in digital and physical spaces) to the child. We automatically and continuously monitored children's experiences using sensing technology (eye-trackers, facial video, wristband data, and Kinect skeleton data). This allowed us to understand how children experience the different ASRs, by providing insights into their affective and behavioural processes. The results showed that ASRs have an effect on children's stress, arousal, fatigue, movement, visual inspection (focus) and cognitive load. By exploring the relationship between children's degree of self-representation and their affective and behavioural states, our findings help shape the design of future educational MBTG for children, and emphasises the need for additional studies to investigate how ASRs impacts children's behavioural, interaction, cognitive and learning processes. © 2020 ACM.",avatar; educational technologies; embodied interaction; embodied learning; motion-based games; multimodal data,Digital persona; Educational game; Interaction paradigm; Learning experiences; Learning process; Sensing technology; Visual inspection; Visual similarity; User experience,Conference paper,Final,,Scopus,2-s2.0-85087391022,Gaming / VR
Zhang B.; Liu S.; Hu C.; Luo Z.; Huang S.; Sui J.,"Zhang, Bao (55720902000); Liu, Shuhui (57203900474); Hu, Cenlou (57214724177); Luo, Ziwen (57214725883); Huang, Sai (49861379200); Sui, Jie (57194470269)",55720902000; 57203900474; 57214724177; 57214725883; 49861379200; 57194470269,Enhanced memory-driven attentional capture in action video game players,2020,Computers in Human Behavior,107,,106271,,,,8,10.1016/j.chb.2020.106271,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85078972430&doi=10.1016%2fj.chb.2020.106271&partnerID=40&md5=72767f5c54cc058218663e20a0689244,"Action video game players (AVGPs) have been shown to have an enhanced cognitive control ability to reduce stimulus-driven attentional capture (e.g., from an exogenous salient distractor) compared with non-action video game players (NVGPs). Here we examined whether these benefits could extend to the memory-driven attentional capture (i.e., working memory (WM) representations bias visual attention toward a matching distractor). AVGPs and NVGPs were instructed to complete a visual search task while actively maintaining 1, 2 or 4 items in WM. There was a robust advantage to the memory-driven attentional capture in reaction time and first eye movement fixation in the AVGPs compared to the NVGPs when they had to maintain one item in WM. Moreover, the effect of memory-driven attentional capture was maintained in the AVGPs when the WM load was increased, but it was eliminated in the NVGPs. The results suggest that AVGPs may devote more attentional resources to sustaining the cognitive control rather than to suppressing the attentional capture driven by the active WM representations. © 2020 Elsevier Ltd",Action video game players; Cognitive promotion; Memory-driven attentional capture; Working memory,Behavioral research; Eye movements; Action video games; Cognitive control; Cognitive promotion; Non-action; Visual Attention; Visual search; Working memory; article; executive function; eye movement; human; reaction time; video game; visual attention; working memory; Human computer interaction,Article,Final,,Scopus,2-s2.0-85078972430,Gaming / VR
Singh R.; Miller T.; Newn J.; Velloso E.; Vetere F.; Sonenberg L.,"Singh, Ronal (56415316800); Miller, Tim (7403948057); Newn, Joshua (57188820341); Velloso, Eduardo (53364337000); Vetere, Frank (10039716000); Sonenberg, Liz (55882943000)",56415316800; 7403948057; 57188820341; 53364337000; 10039716000; 55882943000,Combining gaze and AI planning for online human intention recognition,2020,Artificial Intelligence,284,,103275,,,,47,10.1016/j.artint.2020.103275,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85082649576&doi=10.1016%2fj.artint.2020.103275&partnerID=40&md5=2d250108481166a4a329c695b6567be5,"Intention recognition is the process of using behavioural cues, such as deliberative actions, eye gaze, and gestures, to infer an agent's goals or future behaviour. In artificial intelligence, one approach for intention recognition is to use a model of possible behaviour to rate intentions as more likely if they are a better ‘fit’ to actions observed so far. In this paper, we draw from literature linking gaze and visual attention, and we propose a novel model of online human intention recognition that combines gaze and model-based AI planning to build probability distributions over a set of possible intentions. In human-behavioural experiments (n=40) involving a multi-player board game, we demonstrate that adding gaze-based priors to model-based intention recognition improved the accuracy of intention recognition by 22% (p<0.05), determined those intentions ≈90 seconds earlier (p<0.05), and at no additional computational cost. We also demonstrate that, when evaluated in the presence of semi-rational or deceptive gaze behaviours, the proposed model is significantly more accurate (9% improvement) (p<0.05) compared to a model-based or gaze only approaches. Our results indicate that the proposed model could be used to design novel human-agent interactions in cases when we are unsure whether a person is honest, deceitful, or semi-rational. © 2020 Elsevier B.V.",Gaze; Intention recognition; Planning,Planning; Probability distributions; Computational costs; Gaze; Gaze behaviours; Human agent interactions; Human intentions; Intention recognition; Model-based OPC; Visual Attention; Behavioral research,Article,Final,All Open Access; Bronze Open Access,Scopus,2-s2.0-85082649576,Gaming / VR
Soret R.; Charras P.; Khazar I.; Hurter C.; Peysakhovich V.,"Soret, Rebai (57210112083); Charras, Pom (24483147000); Khazar, Ines (57217114395); Hurter, Christophe (24740874700); Peysakhovich, Vsevolod (56644829800)",57210112083; 24483147000; 57217114395; 24740874700; 56644829800,Eye-tracking and Virtual Reality in 360-degrees: Exploring two ways to assess attentional orienting in rear space,2020,Eye Tracking Research and Applications Symposium (ETRA),,,3391418,,,,5,10.1145/3379157.3391418,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85086300866&doi=10.1145%2f3379157.3391418&partnerID=40&md5=302f8d305f2e3834d8d8b5f340f70736,"The Posner cueing task is a classic experimental paradigm in cognitive science for measuring visual attention orienting abilities. Recently, it was suggested that this paradigm can be adapted in virtual reality (e.g. in an immersive and ecological environment) to evaluate the effectiveness of perceptual stimuli in directing attention and by extension to study the underlying cognitive processes. In this study, auditory and visual endogenous cue were used to voluntary orient attention at 360°. Two groups of participants (N=33 and N=28) equipped with a virtual reality headset including integrated eye-tracking performed a modified version of the Posner cueing task in a 360° immersive environment. In this task, participants had to destroy space objects, as quickly as possible, through eye interaction. Predictive visual or auditory informed participants about target location. The results show that these endogenous cues significantly improve performance even if the object to be destroyed occurred outside the visual field or through a mirror. This experiment provides one of the first demonstrations that attentional orienting mechanism can improve performances of visual information processing in an immersive and ecological 360° environment where information can appear in rear space. © 2020 ACM.",attentional orienting; cueing; endogenous; front space; Posner task; rear space; virtual reality,Behavioral research; Ecology; Virtual reality; Cognitive process; Cognitive science; Ecological environments; Immersive environment; Improve performance; Virtual-reality headsets; Visual Attention; Visual information processing; Eye tracking,Conference paper,Final,All Open Access; Green Open Access,Scopus,2-s2.0-85086300866,Gaming / VR
Porras-Garcia B.; Ferrer-Garcia M.; Yilmaz L.; Sen Y.O.; Olszewska A.; Ghita A.; Serrano-Troncoso E.; Treasure J.; Gutiérrez-Maldonado J.,"Porras-Garcia, Bruno (57201200642); Ferrer-Garcia, Marta (13405944200); Yilmaz, Lena (57215591068); Sen, Yigit O. (57215578711); Olszewska, Agata (57215592067); Ghita, Alexandra (57200515877); Serrano-Troncoso, Eduardo (40262408400); Treasure, Janet (7006097367); Gutiérrez-Maldonado, José (9334173600)",57201200642; 13405944200; 57215591068; 57215578711; 57215592067; 57200515877; 40262408400; 7006097367; 9334173600,Body-related attentional bias as mediator of the relationship between body mass index and body dissatisfaction,2020,European Eating Disorders Review,28,4,,454,464,10.0,29,10.1002/erv.2730,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85081219061&doi=10.1002%2ferv.2730&partnerID=40&md5=4077fc6590aec8200d16c4150fb0e9c6,"Body image disturbance, consisting of an affective (body dissatisfaction) and perceptual (body distortion) component, is not only found in eating disorders, but is also present in healthy individuals, affecting their psychological well-being and everyday life. A higher body mass index is associated with higher body dissatisfaction, whereas results in relation to body distortion are mixed. Furthermore, body dissatisfaction is associated with a weight-related attentional bias. This study aimed to investigate the mediating role of a weight-related attentional bias in the relationship between body mass index and body image disturbance. Forty-one college women took part in a virtual reality and eye tracking procedure, in which the illusion of owning a virtual avatar with their body measurements was induced. During this procedure, body-related attentional bias was measured and afterwards body image disturbance was assessed. Mediation analysis revealed that weight-related attentional bias mediated the relationship between body mass index and body dissatisfaction (but not distortion). These findings suggest that modifying weight-related attentional bias would be a useful treatment target for improving body dissatisfaction. In addition, virtual reality technology could serve as an innovative method for modifying attentional bias in an ecologically valid way. Highlights: This Virtual Reality and Eye-Tracking study expands our knowledge about the relation between body mass index, body-related attention and body image disturbances. The results suggest that attentional bias towards weight-related body parts mediates the relation between BMI and body dissatisfaction. On the contrary, the relation between BMI and body distortion was not significant. © 2020 John Wiley & Sons, Ltd and Eating Disorders Association",body dissatisfaction; body mass index; body-related attention; eye-tracking; virtual reality,Adult; Attentional Bias; Body Dissatisfaction; Body Mass Index; Female; Humans; Young Adult; adult; Article; attentional bias; body dissatisfaction; body image; body mass; body size; body weight; college student; controlled study; eye tracking; female; human; human experiment; normal human; physical attractiveness; virtual reality; psychology; young adult,Article,Final,,Scopus,2-s2.0-85081219061,Gaming / VR
Swanenburg J.; Büchi F.; Straumann D.; Weber K.P.; de Bruin E.D.,"Swanenburg, Jaap (6507130634); Büchi, Fabienne (57218107177); Straumann, Dominik (7006199777); Weber, Konrad P. (7402657086); de Bruin, Eling D. (7004058816)",6507130634; 57218107177; 7006199777; 7402657086; 7004058816,Exergaming With Integrated Head Turn Tasks Improves Compensatory Saccade Pattern in Some Patients With Chronic Peripheral Unilateral Vestibular Hypofunction,2020,Frontiers in Neurology,11,,601,,,,2,10.3389/fneur.2020.00601,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85087867052&doi=10.3389%2ffneur.2020.00601&partnerID=40&md5=9a3dfa7556e278d53006bc523ad84f99,"Background: This study aimed to determine whether vestibular rehabilitation using active video games (Exergames), including promoted head turns and unsupported locomotion, may facilitate vestibular compensation and gait in subjects with one-sided chronic peripheral vestibular hypofunction (cPVH). Methods: 12 patients with cPVH (mean age of 65 ± 12 years, 8 male) were recruited for this study. The study consisted of a four-week baseline control period T1-T2 followed by a four-week intervention period T2-T3. The intervention included exergames that required physical tasks such as steps, weight shifts or balance control to cognitive challenges, in a virtual environment to play the game. The subjects participated in a total of 176 min of exergaming in eight sessions. Because of the changing projection direction of the game to the wall, the subjects had to turn their heads constantly while playing the game. Dynamic visual acuity (DVA) was assessed. Vestibulo-Ocular reflex (VOR) gain deficit and cumulative overt saccade amplitude (COSA) were measured with the video head-impulse test. Additionally, the functional gait assessment (FGA), Extended Timed Get-Up-and-Go (ETGUG), and the Dizziness handicap inventory (DHI), were assessed. Results: DVA showed no significant group level change (p = 0.475, z = −0.714, d = 0.295) with a small effect size and improvements in five out of 12 subjects. Ipsilesional VOR gain did not improve (p = 0.157, z = −1.414, d = 0.481) on group level while there was an intermediate effect size and improvements in six out of 12 subjects. COSA got significant smaller (p = 0.006, z = −2.746, d = 1.354) with improvements in seven out of 12 subjects. The contralesional sides did not change. The FGA for the group significantly improved with an intermediate effect size (p < 0.001, z = −3.08, d = 1.617) and five individuals showed clinically relevant improvements. The ETGUG group value improved significantly with a strong effect size (p < 0.001, z = −2.67, d = 1.030), with seven individuals contributing to this change. The DHI showed no change (p = 0.172, z = −1.381, d = 0.592) neither on the group nor on the individuals' level. The game scores of the subjects improved during the intervention period of the intervention for every game. Conclusion: The results of this study demonstrate that exergaming with promoted head turns facilitates vestibular compensation in some subjects with cPVH. This is the first study that shows an improvement in cumulative overt saccade amplitude after exergaming in chronic vestibular subjects. © Copyright © 2020 Swanenburg, Büchi, Straumann, Weber and de Bruin.",dynamic visual acuity; exergaming; head turns; saccades; vestibular loss,aged; Article; chronic peripheral vestibular hypofunction; clinical article; cognition; controlled study; Dizziness handicap inventory; Extended Timed Get Up and Go; female; gait; head impulse test; head movement; human; locomotion; male; saccadic eye movement; task performance; vestibular disorder; vestibular test; vestibuloocular reflex; video game; visual acuity,Article,Final,All Open Access; Gold Open Access; Green Open Access,Scopus,2-s2.0-85087867052,Gaming / VR
Paletta L.; Pszeida M.; Dini A.; Russegger S.; Schuessler S.; Jos A.; Schuster E.; Steiner J.; Fellner M.,"Paletta, Lucas (6602696802); Pszeida, Martin (56160335300); Dini, Amir (57193756837); Russegger, Silvia (56507463100); Schuessler, Sandra (56136359700); Jos, Anna (57217115597); Schuster, Eva (57209337615); Steiner, Josef (57202940223); Fellner, Maria (23476948600)",6602696802; 56160335300; 57193756837; 56507463100; 56136359700; 57217115597; 57209337615; 57202940223; 23476948600,MIRA-A Gaze-based Serious Game for Continuous Estimation of Alzheimer's Mental State,2020,Eye Tracking Research and Applications Symposium (ETRA),,,3391989,,,,11,10.1145/3379157.3391989,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85086309320&doi=10.1145%2f3379157.3391989&partnerID=40&md5=de2d01d418be8f86d91af8c1f1f8b381,"Persons with Alzheimer's disease demonstrate a dysfunctionality in the continuous tracking of stimuli and are characterized with a significant impairment of their inhibitory functionality of eye movements. In previous work several methodologies of attention analytics were developed with laboratory based eye tracking technology but there is still a lack in providing opportunity for pervasive and continuous tracking of mental state for people still living at home. This work proposes a playful cognitive assessment method based on the antisaccade task. The performance scores of the serious game were analyzed in a field trial with 15 participants being diagnosed with light degree of Alzheimer's disease within a period of 10 weeks. The results present a statistically significant correlation between the game outcome scores and the Montreal Cognitive Assessment (MoCA) score, the golden standard for the analysis of executive functions in early Alzheimer's disease. This indicates first successful steps towards the daily use of serious games for pervasive assessment of Alzheimer's mental state. © 2020 Owner/Author.",dementia care; Eye tracking; playful decision support,Eye movements; Eye tracking; Neurodegenerative diseases; Alzheimer's; Alzheimer's disease; Antisaccade task; Cognitive assessments; Continuous tracking; Executive function; Eye tracking technologies; Mental state; Serious games,Conference paper,Final,,Scopus,2-s2.0-85086309320,Gaming / VR
Cohen M.A.; Botch T.L.; Robertson C.E.,"Cohen, Michael A. (55477529200); Botch, Thomas L. (57217186010); Robertson, Caroline E. (57196949630)",55477529200; 57217186010; 57196949630,"The limits of color awareness during active, real-world vision",2020,Proceedings of the National Academy of Sciences of the United States of America,117,24,,13821,13827,6.0,62,10.1073/pnas.1922294117,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85086681534&doi=10.1073%2fpnas.1922294117&partnerID=40&md5=1a8aeed01dbe9f538c6c9d1fe4da40af,"Color ignites visual experience, imbuing the world with meaning, emotion, and richness. As soon as an observer opens their eyes, they have the immediate impression of a rich, colorful experience that encompasses their entire visual world. Here, we show that this impression is surprisingly inaccurate. We used head-mounted virtual reality (VR) to place observers in immersive, dynamic realworld environments, which they naturally explored via saccades and head turns. Meanwhile, we monitored their gaze with inheadset eye tracking and then systematically altered the visual environments such that only the parts of the scene they were looking at were presented in color and the rest of the scene (i.e., the visual periphery) was entirely desaturated. We found that observers were often completely unaware of these drastic alterations to their visual world. In the most extreme case, almost a third of observers failed to notice when less than 5% of the visual display was presented in color. This limitation on perceptual awareness could not be explained by retinal neuroanatomy or previous studies of peripheral visual processing using more traditional psychophysical approaches. In a second study, we measured color detection thresholds using a staircase procedure while a set of observers intentionally attended to the periphery. Still, we found that observers were unaware when a large portion of their field of view was desaturated. Together, these results show that during active, naturalistic viewing conditions, our intuitive sense of a rich, colorful visual world is largely incorrect. © 2020 National Academy of Sciences. All rights reserved.",Attention; Color; Scenes; Virtual reality; Vision,"Adolescent; Adult; Attention; Awareness; Color; Female; Humans; Male; Virtual Reality; Vision, Ocular; Young Adult; adult; analytical error; Article; awareness; biological monitoring; color vision; eye tracking; female; gaze; head movement; human; human experiment; male; neuroanatomy; normal human; priority journal; psychophysics; saccadic eye movement; virtual reality; visual attention; visual discrimination; visual nervous system; visual threshold; adolescent; attention; awareness; color; vision; young adult",Article,Final,All Open Access; Green Open Access; Hybrid Gold Open Access,Scopus,2-s2.0-85086681534,Gaming / VR
Liu C.; Plopski A.; Orlosky J.,"Liu, Chang (57207047627); Plopski, Alexander (56023098100); Orlosky, Jason (55641218100)",57207047627; 56023098100; 55641218100,OrthoGaze: Gaze-based three-dimensional object manipulation using orthogonal planes,2020,Computers and Graphics (Pergamon),89,,,1,10,9.0,19,10.1016/j.cag.2020.04.005,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85084676499&doi=10.1016%2fj.cag.2020.04.005&partnerID=40&md5=c4c7731e155cf4b0818dee97105e57f4,"In virtual and augmented reality, gaze-based methods have been explored for decades as effective user interfaces for hands-free interaction. Though several well-known gaze-based methods exist for simple interactions such as selection, no solutions exist for 3D manipulation tasks requiring a higher degree of freedom (DoF). In this paper, we introduce OrthoGaze, a novel user interface that allows users to intuitively manipulate the three-dimensional position of a virtual object using only their eye or head gaze. Our approach makes use of three selectable, orthogonal planes, where each plane not only helps guide the user's gaze in an arbitrary virtual space, but also allows for 2-DoF manipulations of object position. To evaluate our method, we conducted two user studies involving aiming and docking tasks in virtual reality to evaluate the fundamental characteristics of sustained gaze aiming and to determine which type of gaze-based control performs best when combined with OrthoGaze. Results showed that eye gaze was more accurate than head gaze for sustained aiming. Additionally, eye and head gaze-based control for 3D manipulations achieved 78% and 96% performance, respectively, in comparison with a hand-held controller. Subjective results also suggest that gaze-based manipulation can comprehensively cause more fatigue than controller-based. From the experimental results, we expect OrthoGaze to become an effective method for pure hands-free object manipulation in head-mounted displays. © 2020 Elsevier Ltd",Eye tracking; Human-computer interaction; Object manipulation; User interface,Augmented reality; Controllers; Degrees of freedom (mechanics); Helmet mounted displays; User interfaces; 3d manipulation tasks; Degree of freedom (dof); Dimensional position; Fundamental characteristics; Hands-free interactions; Head mounted displays; Three-dimensional object; Virtual and augmented reality; Virtual reality,Article,Final,,Scopus,2-s2.0-85084676499,Gaming / VR
Putze F.; Vourvopoulos A.; Lécuyer A.; Krusienski D.; Bermúdez i Badia S.; Mullen T.; Herff C.,"Putze, Felix (22036416700); Vourvopoulos, Athanasios (48762198300); Lécuyer, Anatole (6601917415); Krusienski, Dean (6602223823); Bermúdez i Badia, Sergi (6506360007); Mullen, Timothy (40461848600); Herff, Christian (55351767300)",22036416700; 48762198300; 6601917415; 6602223823; 6506360007; 40461848600; 55351767300,Editorial: Brain-Computer Interfaces and Augmented/Virtual Reality,2020,Frontiers in Human Neuroscience,14,,144,,,,40,10.3389/fnhum.2020.00144,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85085298940&doi=10.3389%2ffnhum.2020.00144&partnerID=40&md5=9527152b5a567ff93a46b0585c6d9685,[No abstract available],augmented reality; BCI; EEG; fNIRS; virtual reality,adaptation; attention; augmented reality; Editorial; electroencephalography; eye tracking; functional magnetic resonance imaging; functional near-infrared spectroscopy; human; imagery; mass medium; nervous system function; neurorehabilitation; steady state visual evoked potential; stroke rehabilitation; virtual reality; visual evoked potential; workload,Editorial,Final,All Open Access; Gold Open Access; Green Open Access,Scopus,2-s2.0-85085298940,Gaming / VR
Jeong J.H.; Park H.-J.; Yeo S.-H.; Kim H.,"Jeong, Ji Hyeok (57217088732); Park, Hyun-Jung (57217089457); Yeo, Sang-Hoon (26322215300); Kim, Hyungmin (57072753200)",57217088732; 57217089457; 26322215300; 57072753200,A multimodal analysis combining behavioral experiments and survey-based methods to assess the cognitive effect of video game playing: Good or evil?,2020,Sensors (Switzerland),20,11,3219,,,,3,10.3390/s20113219,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85086144796&doi=10.3390%2fs20113219&partnerID=40&md5=eada0783058fd57dd6d85cc2eb8a9db0,"This study aims to bridge the gap between the discrepant views of existing studies in different modalities on the cognitive effect of video game play. To this end, we conducted a set of tests with different modalities within each participant: (1) Self-Reports Analyses (SRA) consisting of five popular self-report surveys, and (2) a standard Behavioral Experiment (BE) using pro- and antisaccade paradigms, and analyzed how their results vary between Video Game Player (VGP) and Non-Video Game Player (NVGP) participant groups. Our result showed that (1) VGP scored significantly lower in Behavioral Inhibition System (BIS) than NVGP (p = 0.023), and (2) VGP showed significantly higher antisaccade error rate than NVGP (p = 0.005), suggesting that results of both SRA and BE support the existing view that video game play has a maleficent impact on the cognition by increasing impulsivity. However, the following correlation analysis on the results across individual participants found no significant correlation between SRA and BE, indicating a complex nature of the cognitive effect of video game play. © 2020 by the authors. Licensee MDPI, Basel, Switzerland. .",Antisaccade; Impulsivity; Internet gaming disorder; Prosaccade; Response inhibition; Video game addiction,Cognition; Eye-Tracking Technology; Humans; Impulsive Behavior; Self Report; Surveys and Questionnaires; Video Games; Cognitive systems; Interactive computer graphics; Modal analysis; Surveys; Behavioral experiment; Cognitive effects; Complex nature; Correlation analysis; Error rate; Multimodal analysis; Video game; Video game playing; cognition; human; impulsiveness; questionnaire; self report; video game; Human computer interaction,Article,Final,All Open Access; Gold Open Access; Green Open Access,Scopus,2-s2.0-85086144796,Gaming / VR
Schenk S.; Bellebaum C.; Lech R.K.; Heinen R.; Suchan B.,"Schenk, Sabrina (57191410095); Bellebaum, Christian (8223849100); Lech, Robert K. (55773554000); Heinen, Rebekka (56490408100); Suchan, Boris (6602608617)",57191410095; 8223849100; 55773554000; 56490408100; 6602608617,Play to Win: Action Video Game Experience and Attention Driven Perceptual Exploration in Categorization Learning,2020,Frontiers in Psychology,11,,933,,,,10,10.3389/fpsyg.2020.00933,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85085471756&doi=10.3389%2ffpsyg.2020.00933&partnerID=40&md5=31e791f69e1963804da4eaca0132d43c,"Categorization learning is a fundamental and complex cognitive ability. The present EEG study examined how much action video gamers differ from non-gamers in the usage of visual exploration and attention driven perceptual analyses during a categorization learning task. Seventeen healthy right-handed non-gamers and 16 healthy right-handed action video gamers performed a visual categorization task with 14 ring stimuli, which were divided into two categories. All stimuli had the same structure but differed with respect to their color combinations and were forming two categories including a prototype, five typical stimuli and one exception. The exception shared most similarities with the prototype of the opposite group. Prototypes and typical stimuli were correctly categorized at an early stage of the experiment, whereas the successful categorization of exceptions occurred later. The behavioral data yield evidence that action video gamers perform correct categorizations of exceptions earlier than non-gamers. Additionally, groups differed with respect to differential expressions of the attention related P150 ERP component (early perceptual analysis) and the N170 ERP component, which reflected differential processing demands for the stimulus material. In comparison to non-gamers, the analyses of the eye movements yield for action video gamers different, more central fixations possibly indicating covert peripheral processing. For both groups fixations as well as saccades decrease and in the case of exceptions, one of the two segments that are decisive for correct categorization shows higher fixation rates at the end of the experiment. These findings indicate for both groups a learning process regarding the stimulus material. Regarding the group differences, we interpret the results to indicate that action video gamers show a different stimulus exploration, use an enhanced early perceptual analysis of the stimulus material and therefore may detect changes in objects faster and learned the belonging of the stimuli to their categories in an earlier trial phase. © Copyright © 2020 Schenk, Bellebaum, Lech, Heinen and Suchan.",abstraction-based strategy; action video games; categorization learning; exception-based strategy; perceptual processing; visual attention,,Article,Final,All Open Access; Gold Open Access; Green Open Access,Scopus,2-s2.0-85085471756,Gaming / VR
McNamara A.; Mehta R.,"McNamara, Ann (35253845600); Mehta, Ranjana (55413685300)",35253845600; 55413685300,Additional insights: Using eye tracking and brain sensing in virtual reality,2020,Conference on Human Factors in Computing Systems - Proceedings,,,3375060,,,,4,10.1145/3334480.3375060,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85090214793&doi=10.1145%2f3334480.3375060&partnerID=40&md5=0acaa881ddb50f590012f171b0430141,"Virtual Reality has the potential to transform the way we work, rest and play. We are seeing use cases as diverse as education and pain management, with new applications being imagined every day. Virtual Reality technology comes with new challenges, and many obstacles need to be overcome to ensure good user experience. Recently many new Virtual Reality systems with integrated eye-tracking have become available. At the same time, many research labs are using Functional Near-Infrared Spectroscopy (fNIRS) to non-invasively measure brain activity and serve as a brain-computer interface. This course presents timely, relevant information on how Virtual Reality can leverage eye-tracking and brain activity data to optimize the user experience and to alleviate usability issues surrounding many challenges in immersive VEs. The integration of these sensors allows us to determine additional insights into human behavior, including where the viewer is focusing their attention, and monitor cognitive load. Advancing these approaches could make the Virtual Reality experience more comfortable, safe and effective for the user, and open a new world to facilitate new experimentation for Human Computer Interaction (and Brain Computer Interaction) researchers. © 2020 Owner/Author.",Brain computer interface; Eye tracking; Fnirs; Virtual reality,Behavioral research; Brain; Brain computer interface; Functional neuroimaging; Human computer interaction; Human engineering; Infrared devices; Near infrared spectroscopy; Neurophysiology; User experience; Virtual reality; Brain computer interactions; Cognitive loads; Functional near-infrared spectroscopy (fnirs); Human behaviors; New applications; Virtual reality experiences; Virtual reality system; Virtual reality technology; Eye tracking,Conference paper,Final,,Scopus,2-s2.0-85090214793,Gaming / VR
Geronazzo M.; Vieira L.S.; Nilsson N.C.; Udesen J.; Serafin S.,"Geronazzo, Michele (36720522500); Vieira, Luis S. (58298137400); Nilsson, Niels Christian (54993660100); Udesen, Jesper (8724602800); Serafin, Stefania (6603367536)",36720522500; 58298137400; 54993660100; 8724602800; 6603367536,Superhuman Hearing - Virtual Prototyping of Artificial Hearing: A Case Study on Interactions and Acoustic Beamforming,2020,IEEE Transactions on Visualization and Computer Graphics,26,5,8998401,1912,1922,10.0,9,10.1109/TVCG.2020.2973059,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85079662779&doi=10.1109%2fTVCG.2020.2973059&partnerID=40&md5=c15f18da3296af90574900c8ee9e18e5,"Directivity and gain in microphone array systems for hearing aids or hearable devices allow users to acoustically enhance the information of a source of interest. This source is usually positioned directly in front. This feature is called acoustic beamforming. The current study aimed to improve users' interactions with beamforming via a virtual prototyping approach in immersive virtual environments (VEs). Eighteen participants took part in experimental sessions composed of a calibration procedure and a selective auditory attention voice-pairing task. Eight concurrent speakers were placed in an anechoic environment in two virtual reality (VR) scenarios. The scenarios were a purely virtual scenario and a realistic 360° audio-visual recording. Participants were asked to find an individual optimal parameterization for three different virtual beamformers: (i) head-guided, (ii) eye gaze-guided, and (iii) a novel interaction technique called dual beamformer, where head-guided is combined with an additional hand-guided beamformer. None of the participants were able to complete the task without a virtual beamformer (i.e., in normal hearing condition) due to the high complexity introduced by the experimental design. However, participants were able to correctly pair all speakers using all three proposed interaction metaphors. Providing superhuman hearing abilities in the form of a dual acoustic beamformer guided by head and hand movements resulted in statistically significant improvements in terms of pairing time, suggesting the task-relevance of interacting with multiple points of interests. © 2020 IEEE.",Acoustic beamforming; Artificial hearing; Multi-speaker scenario; Sonic interactions; Virtual prototyping; Virtual reality,"Acoustic Stimulation; Acoustics; Adult; Auditory Perception; Equipment Design; Female; Hearing; Hearing Aids; Humans; Male; Signal Processing, Computer-Assisted; Virtual Reality; Young Adult; Acoustics; Array processing; Beamforming; Hearing aids; Job analysis; Microphones; Virtual prototyping; Virtual reality; Acoustic beamforming; Array signal processing; Auditory systems; Multi-speaker scenario; Sonic interactions; Task analysis; acoustics; adult; auditory stimulation; devices; equipment design; female; hearing; hearing aid; human; male; physiology; signal processing; virtual reality; young adult; Audition",Article,Final,All Open Access; Green Open Access,Scopus,2-s2.0-85079662779,Gaming / VR
Xiao W.; Cheng J.,"Xiao, Wangqun (35779408300); Cheng, Jianxin (55803715700)",35779408300; 55803715700,Perceptual design method for smart industrial robots based on virtual reality and synchronous quantitative physiological signals,2020,International Journal of Distributed Sensor Networks,16,5,,,,,8,10.1177/1550147720917646,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85084685456&doi=10.1177%2f1550147720917646&partnerID=40&md5=0310abe29667b1d99c0beb1f17a5b016,"In the research of industrial robot design, designing using only the perceptual thinking and creativity of an industrial designer or overemphasizing the intervention of quantitative data research in the field of emotional cognition is relatively one sided. In this article, research on how to combine the above two aspects effectively will be conducted. The aim is to present a design method which provides artistic creativity and scientific support for industrial robot design. Therefore, a method for representing perceptual image spaces of industrial robots through pictures and semantics by evaluating the perceptual images and using statistical approaches such as factor analysis will be proposed. Perceptual design elements of industrial robots are decomposed from the perspective of style and color. After the quantitative type I analysis, the numerical relationships between the semantics of images and design elements are identified. Also, a method for mapping relationships between the perceptual image spaces and design elements of industrial robots is developed. After three-dimensional modeling and simulation, the semantic difference methods are used in combination with the emotional evaluation and measurement methods for physiological experiments such as eye tracking, skin conductance, heart rate, and electroencephalography experiments with the aid of virtual reality. Finally, a perceptual design method is extracted for smart industrial robots based on virtual reality and synchronous quantitative physiological signals. © The Author(s) 2020.",emotion recognition; Industrial design; industrial robots; Kansei image; physiological techniques,Electroencephalography; Electrophysiology; Eye tracking; Image analysis; Industrial research; Machine design; Physiological models; Semantics; Virtual reality; Artistic creativity; Industrial designers; Mapping relationships; Measurement methods; Physiological signals; Semantic difference method; Statistical approach; Three-dimensional model; Industrial robots,Article,Final,All Open Access; Gold Open Access,Scopus,2-s2.0-85084685456,Gaming / VR
Tang W.; Wu S.; Vigier T.; Da Silva M.P.,"Tang, Wei (57217851876); Wu, Shiyi (57201740826); Vigier, Toinon (55206851100); Da Silva, Matthieu Perreira (24337440300)",57217851876; 57201740826; 55206851100; 24337440300,Influence of Emotions on Eye Behavior in Omnidirectional Content,2020,"2020 12th International Conference on Quality of Multimedia Experience, QoMEX 2020",,,9123126,,,,9,10.1109/QoMEX48832.2020.9123126,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85087689905&doi=10.1109%2fQoMEX48832.2020.9123126&partnerID=40&md5=a580d3b4b6b3a2ec5663028a7a516a8b,"The recent development of Virtual Reality (VR) technologies and interactive visual content faces some important challenges in multimedia processing and Quality of Experience (QoE). Considering omnidirectional, also called 360-degree, content, one major research topic is the development of reliable visual attention models. In this paper, we present a new dataset to study the influence of emotions on eye behavior in omnidirectional content. This dataset is based on an eye-tracking experiment where 19 observers have assessed emotional valence and arousal dimensions in 360-degree images. Several analyses are then conducted to compare fixation and saccade features, inter-observer saliency congruency and spatial oculomotor biases in positive, neutral and negative content. Results show a significant impact of negative images on visual attention, with more visual agitation and avoidance behavior from larger, longer and faster saccades. However, no obvious difference of eye behavior is found between positive and neutral stimulis on this dataset. © 2020 IEEE.",emotions; eye behavior; eye features; omnidirectional images; saliency,Behavioral research; Eye movements; Multimedia systems; Quality of service; Avoidance behavior; Emotional valences; Multimedia processing; Quality of experience (QoE); Research topics; Visual Attention; Visual attention model; Visual content; Eye tracking,Conference paper,Final,All Open Access; Green Open Access,Scopus,2-s2.0-85087689905,Gaming / VR
Ahn J.Y.; Han S.Y.; Jo J.; Lee S.H.; Cho N.I.,"Ahn, Joon Young (57189522011); Han, Sang Yoon (57193417343); Jo, Junho (57210977136); Lee, Sang Hwa (36063845300); Cho, Nam Ik (7201718669)",57189522011; 57193417343; 57210977136; 36063845300; 7201718669,A VR/AR Interface Design based on Unaligned Hand Position and Gaze Direction,2020,IEIE Transactions on Smart Processing and Computing,9,2,,92,103,11.0,0,10.5573/IEIESPC.2020.9.2.092,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85087449664&doi=10.5573%2fIEIESPC.2020.9.2.092&partnerID=40&md5=534173728be67b6246393f2b5323c8ca,"Abstract: In conventional natural user interface schemes that use hand gestures without holding a sensor device, users usually need to hold up their hands to align them to the gaze direction where the (virtual) object is displayed. Also, users need to move their hands in the designated space for quite a long time, which may cause fatigue and decrease the performance of gesture recognition. Hence, we propose a new interface scheme that alleviates this problem by freeing the hand position from the gaze direction such that users can even put their hands on a desk. For this, we need to have more robust hand detection and an effective gesture recognition scheme, because the hands can be placed anywhere in the space, and hand motion is not calibrated to the virtual space. For robust hand detection, we propose an algorithm based on a new hand pose model, which is implemented using a wide-angle camera. For efficient gaze calibration and tracking, we use a three-dimensional eyeball model from our previous work. Because the gaze and hand motion/positions are not aligned, we designed a finite state machine for robust gesture flow. Based on this scheme, we defined six interactive interfaces for the target object: click, double click, drag and drop, zoom in, zoom out, and return home. More complicated interactions can be implemented based on these six basic interfaces. The proposed interface scheme is implemented on a handheld mobile device, which is shown to work robustly for a wide range of hand positions. © 2020 Institute of Electronics and Information Engineers. All rights reserved.",Eye gaze tracking; Hand gesture recognition; Natural user interface (NUI),Gesture recognition; User interfaces; Calibration and tracking; Handheld mobile devices; Interactive interfaces; Interface designs; Interface schemes; Natural user interfaces; Virtual spaces; Wide angle cameras; Palmprint recognition,Article,Final,,Scopus,2-s2.0-85087449664,Gaming / VR
Sidenmark L.; Clarke C.; Zhang X.; Phu J.; Gellersen H.,"Sidenmark, Ludwig (57210111157); Clarke, Christopher (59887381800); Zhang, Xuesong (57218825004); Phu, Jenny (57219116481); Gellersen, Hans (6701531333)",57210111157; 59887381800; 57218825004; 57219116481; 6701531333,Outline Pursuits: Gaze-assisted Selection of Occluded Objects in Virtual Reality,2020,Conference on Human Factors in Computing Systems - Proceedings,,,3376438,,,,80,10.1145/3313831.3376438,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85091288497&doi=10.1145%2f3313831.3376438&partnerID=40&md5=da31c18d4295d6aad41e1b6acc5e1c71,"In 3D environments, objects can be difficult to select when they overlap, as this affects available target area and increases selection ambiguity. We introduce Outline Pursuits which extends a primary pointing modality for gaze-assisted selection of occluded objects. Candidate targets within a pointing cone are presented with an outline that is traversed by a moving stimulus. This affords completion of the selection by gaze attention to the intended target's outline motion, detected by matching the user's smooth pursuit eye movement. We demonstrate two techniques implemented based on the concept, one with a controller as the primary pointer, and one in which Outline Pursuits are combined with head pointing for hands-free selection. Compared with conventional raycasting, the techniques require less movement for selection as users do not need to reposition themselves for a better line of sight, and selection time and accuracy are less affected when targets become highly occluded. © 2020 ACM.",eye tracking; occlusion; smooth pursuits; virtual reality,Human engineering; Virtual reality; 3-D environments; Candidate target; Hands-free; Line of Sight; Occluded objects; Raycasting; Smooth pursuit eye movement; Eye movements,Conference paper,Final,All Open Access; Green Open Access,Scopus,2-s2.0-85091288497,Gaming / VR
Rumpf C.; Boronczyk F.; Breuer C.,"Rumpf, Christopher (53985162800); Boronczyk, Felix (57200645418); Breuer, Christoph (8859518000)",53985162800; 57200645418; 8859518000,Predicting consumer gaze hits: A simulation model of visual attention to dynamic marketing stimuli,2020,Journal of Business Research,111,,,208,217,9.0,46,10.1016/j.jbusres.2019.03.034,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85063897092&doi=10.1016%2fj.jbusres.2019.03.034&partnerID=40&md5=fa6b0d05b7518b2b39761eda8b898a4e,"The purpose of the present study is to build and test a simulation model for the prediction of gaze hits in the context of dynamic marketing stimuli. Forecasting the attentional effect of dynamic stimuli is of particular interest when it comes to indirect forms of marketing communication such as sponsorship, product placement, or in-game-advertising. Based on large-scale eye tracking data an artificial neural network was trained, providing high predictive accuracy. The model's business applicability is demonstrated with the case of a soccer sponsorship, using media data and color features as model input. The study highlights the value of eye tracking data for the ex-ante valuation of visual communication stimuli which benefits marketing management at the initiation, implementation, and evaluation stages. © 2019 Elsevier Inc.",Artificial neural network; Eye tracking; Indirect marketing; Simulation model; Visual attention,,Article,Final,,Scopus,2-s2.0-85063897092,Gaming / VR
Shi Y.; Du J.; Ragan E.,"Shi, Yangming (57189999728); Du, Jing (57219889677); Ragan, Eric (26667185300)",57189999728; 57219889677; 26667185300,Review visual attention and spatial memory in building inspection: Toward a cognition-driven information system,2020,Advanced Engineering Informatics,44,,101061,,,,33,10.1016/j.aei.2020.101061,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85079853656&doi=10.1016%2fj.aei.2020.101061&partnerID=40&md5=e417f2425719114073f62ce4a35a3233,"With the increasing complexity of modern buildings, it is becoming more challenging for the professionals in the Architecture, Engineering, and Construction (AEC) industry to effectively digest complex engineering and design information and develop an accurate spatial memory that is critical to their daily tasks. As emerging visualization technologies, such as Virtual Reality, are considered as a promising solution, there is a pressing need to understand the mechanism by which different information visualization methods affect AEC task performance. Cognition literature has discovered a strong relationship between attention and memory development, but little has been done to understand how the visual attention patterns during the design documents review affect the effectiveness of spatial memory in AEC tasks. To fill the knowledge gap, this paper presents a human-subject experiment (n = 63) to test how spatial knowledge is acquired in a building inspection task and how the different visual attention patterns affect the development of spatial memory. Participants were asked to review the design information of a real building on campus. To trigger different attention patterns, they were randomly assigned to one of the three groups based on the forms of information given in the review session, including 2D, 3D, and VR groups. After a brief review session, participants were asked to go to the real building to identify discrepancies (based on memory) that were intentionally inserted by the authors. The inspection performance was used to evaluate the spatial memory development. The results indicate that in general there is a positive relationship between test subjects’ visual attention (fixation time) and spatial memory, but the increasing rate varies across the three groups, suggesting that visual context plays a critical role in the development efficiency of spatial memory. The findings also indicate that the visual attention – spatial memory relationship may be mediated by the use of different spatial knowledge acquisition strategies. This study is expected to contribute to the construction information technology literature by setting the cornerstone of a cognition-driven information system that tailors into the spatial cognitive process of AEC professionals. © 2020 Elsevier Ltd",Attention; Eye-tracking; Spatial knowledge; Spatial memory; Virtual reality,"Architectural design; Eye tracking; Information systems; Information use; Inspection; Virtual reality; Visualization; Architecture , engineering , and constructions; Attention; Human subject experiments; Information visualization; Spatial knowledge; Spatial knowledge acquisitions; Spatial memory; Visualization technologies; Behavioral research",Article,Final,All Open Access; Bronze Open Access,Scopus,2-s2.0-85079853656,Gaming / VR
Hu Z.; Li S.; Gai M.,"Hu, Zhiming (57208101391); Li, Sheng (56002421500); Gai, Meng (46461184000)",57208101391; 56002421500; 46461184000,Temporal continuity of visual attention for future gaze prediction in immersive virtual reality,2020,Virtual Reality and Intelligent Hardware,2,2,,142,152,10.0,15,10.1016/j.vrih.2020.01.002,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85083026056&doi=10.1016%2fj.vrih.2020.01.002&partnerID=40&md5=e176b9bdafecf06511e550ccd33903f0,"Background: Eye tracking technology is receiving increased attention in the field of virtual reality. Specifically, future gaze prediction is crucial in pre-computation for many applications such as gaze-contingent rendering, advertisement placement, and content-based design. To explore future gaze prediction, it is necessary to analyze the temporal continuity of visual attention in immersive virtual reality. Methods: In this paper, the concept of temporal continuity of visual attention is presented. Subsequently, an autocorrelation function method is proposed to evaluate the temporal continuity. Thereafter, the temporal continuity is analyzed in both free-viewing and task-oriented conditions. Results: Specifically, in free-viewing conditions, the analysis of a free-viewing gaze dataset indicates that the temporal continuity performs well only within a short time interval. A task-oriented game scene condition was created and conducted to collect users' gaze data. An analysis of the collected gaze data finds the temporal continuity has a similar performance with that of the free-viewing conditions. Temporal continuity can be applied to future gaze prediction and if it is good, users' current gaze positions can be directly utilized to predict their gaze positions in the future. Conclusions: The current gaze's future prediction performances are further evaluated in both free-viewing and task-oriented conditions and discover that the current gaze can be efficiently applied to the task of short-term future gaze prediction. The task of long-term gaze prediction still remains to be explored. © 2019 Beijing Zhongke Journal Publishing Co. Ltd",Autocorrelation analysis; Gaze prediction; Temporal continuity; Virtual reality; Visual attention,Autocorrelation; Behavioral research; Eye tracking; Virtual reality; 'current; Autocorrelation analysis; Condition; Eye tracking technologies; Gaze prediction; Immersive virtual reality; Task-oriented; Temporal continuity; Viewing conditions; Visual Attention; Forecasting,Article,Final,All Open Access; Gold Open Access,Scopus,2-s2.0-85083026056,Gaming / VR
Prestori F.; Montagna I.; D’angelo E.; Mapelli L.,"Prestori, Francesca (6506844054); Montagna, Ileana (57208242115); D’angelo, Egidio (57223020904); Mapelli, Lisa (14045787300)",6506844054; 57208242115; 57223020904; 14045787300,The optogenetic revolution in cerebellar investigations,2020,International Journal of Molecular Sciences,21,7,2494,,,,13,10.3390/ijms21072494,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85083022892&doi=10.3390%2fijms21072494&partnerID=40&md5=624fbbb710e92867f0e3600b37a93e25,"The cerebellum is most renowned for its role in sensorimotor control and coordination, but a growing number of anatomical and physiological studies are demonstrating its deep involvement in cognitive and emotional functions. Recently, the development and refinement of optogenetic techniques boosted research in the cerebellar field and, impressively, revolutionized the methodological approach and endowed the investigations with entirely new capabilities. This translated into a significant improvement in the data acquired for sensorimotor tests, allowing one to correlate single-cell activity with motor behavior to the extent of determining the role of single neuronal types and single connection pathways in controlling precise aspects of movement kinematics. These levels of specificity in correlating neuronal activity to behavior could not be achieved in the past, when electrical and pharmacological stimulations were the only available experimental tools. The application of optogenetics to the investigation of the cerebellar role in higher-order and cognitive functions, which involves a high degree of connectivity with multiple brain areas, has been even more significant. It is possible that, in this field, optogenetics has changed the game, and the number of investigations using optogenetics to study the cerebellar role in non-sensorimotor functions in awake animals is growing. The main issues addressed by these studies are the cerebellar role in epilepsy (through connections to the hippocampus and the temporal lobe), schizophrenia and cognition, working memory for decision making, and social behavior. It is also worth noting that optogenetics opened a new perspective for cerebellar neurostimulation in patients (e.g., for epilepsy treatment and stroke rehabilitation), promising unprecedented specificity in the targeted pathways that could be either activated or inhibited. © 2020 by the authors. Licensee MDPI, Basel, Switzerland.",Cerebellum; Non-sensorimotor functions; Optogenetics; Sensorimotor system,Animals; Biomechanical Phenomena; Cerebellum; Cognition; Humans; Optogenetics; Single-Cell Analysis; absence; associative learning; blood pressure regulation; cerebellum; clinical feature; cognition; conditioning; decision making; emotion; eye movement; eyelid reflex; Haplorhini; human; kinematics; motor dysfunction; nonhuman; optogenetics; photostimulation; Review; reward; schizophrenia; sensorimotor integration; social behavior; temporal lobe epilepsy; voluntary movement; working memory; animal; biomechanics; cerebellum; optogenetics; physiology; procedures; single cell analysis,Review,Final,All Open Access; Gold Open Access; Green Open Access,Scopus,2-s2.0-85083022892,Gaming / VR
Peterson M.; Tober B.; Littlejohn D.; Hill M.,"Peterson, Matthew (57189226800); Tober, Brad (56884596000); Littlejohn, Deborah (55873562900); Hill, Mac (59207485300)",57189226800; 56884596000; 55873562900; 59207485300,Anticipating Gaze-Based HCI Applications with the Tech Receptivity Interval: Eye Tracking as Input,2020,Visible Language,54,2-Jan,,98,127,29.0,2,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85127304584&partnerID=40&md5=5a7bf2a511b12b32f5cc36f74a2d589f,"HCI researchers have repurposed diagnostic eye tracking technology as a mode of user input. Existing applications are numerous, but primarily address severe motor disability, with a recent increase in gaming enhancement. As noted by cognitive psychologist Nadiya Slobodenyuk, gaze-based HCI represents a fundamental change to the human–computer relationship if adopted for general interaction and information design purposes. A gaze-responsive system can make inferences on a user’s mental state and respond rapidly without explicit user commands. The implications of such a system are significant, and are difficult to imagine and anticipate. We introduce the tech receptivity interval (TRI) as a framework to guide speculative design investigations that imagine potential applications of nascent technology. TRI distinguishes infancy and maturity conditions of receptivity, emphasizing the need for users to adapt to technologies before technological affordances can be fully realized. We provide case reports on gaze-based interaction, using TRI and conducted in an academic design studio. The case reports suggest applications not yet addressed in the literature. The case reports also suggest gaze-responsive changes to information structures in the form of temporal hierarchy and temporal text, which break from the long tradition of language representation in static lines and paragraphs. © 2020, University of Cincinnati. All rights reserved.",diegetic prototypes; embodied cognition; extended mind; gaze-controlled interfaces; speculative design; temporal hierarchy,,Article,Final,,Scopus,2-s2.0-85127304584,Gaming / VR
Murthy L.R.D.,"Murthy, L.R.D. (57205505055)",57205505055,Multimodal interaction for real and virtual environments,2020,"International Conference on Intelligent User Interfaces, Proceedings IUI",,,,29,30,1.0,4,10.1145/3379336.3381506,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85082168765&doi=10.1145%2f3379336.3381506&partnerID=40&md5=07cada36f153b60a0d0c9dbde92799a6,"Multimodal interfaces can leverage the information from multiple modalities to provide robust and error-free interaction. Early multimodal interfaces demonstrate the feasibility of building such systems but focused on specific applications. The challenge in building adaptive systems is lack of techniques for input data fusion. In this direction, we have developed a multimodal head and eye gaze interface and evaluated it in two scenarios. In aviation scenario, our interface has reduced the task time and perceived cognitive load significantly from the existing interface. We have also studied the effect of various output conditions on user's performance in a Virtual Reality (VR) task. Further, we are making our proposed interface to include additional modalities and building novel haptic and multimodal output systems for VR. © 2020 International Conference on Intelligent User Interfaces, Proceedings IUI. All rights reserved.",Machine Learning; Multimodal Interfaces; Virtual Reality,Data fusion; Interactive computer systems; Learning systems; Virtual reality; Cognitive loads; Eye-gaze interface; In-buildings; Multi-modal; Multi-Modal Interactions; Multi-modal interfaces; Multimodal output; Multiple modalities; Haptic interfaces,Conference paper,Final,,Scopus,2-s2.0-85082168765,Gaming / VR
Hoffman H.G.; Boe D.A.; Rombokas E.; Khadra C.; LeMay S.; Meyer W.J.; Patterson S.; Ballesteros A.; Pitt S.W.,"Hoffman, Hunter G. (7201677607); Boe, David A. (57204659571); Rombokas, Eric (45061160300); Khadra, Christelle (56428016900); LeMay, Sylvie (57192675585); Meyer, Walter J. (55444517900); Patterson, Sam (57216980222); Ballesteros, Ann (57216980111); Pitt, Stephen W. (57216981220)",7201677607; 57204659571; 45061160300; 56428016900; 57192675585; 55444517900; 57216980222; 57216980111; 57216981220,"Virtual reality hand therapy: A new tool for nonopioid analgesia for acute procedural pain, hand rehabilitation, and VR embodiment therapy for phantom limb pain",2020,Journal of Hand Therapy,33,2,,254,262,8.0,40,10.1016/j.jht.2020.04.001,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85085605245&doi=10.1016%2fj.jht.2020.04.001&partnerID=40&md5=0114c55a7d41363f7c2687c47434effa,"Introduction: Affordable virtual reality (VR) technology is now widely available. Billions of dollars are currently being invested into improving and mass producing VR and augmented reality products. Purpose of the Study: The purpose of the present study is to explore the potential of immersive VR to make physical therapy/occupational therapy less painful, more fun, and to help motivate patients to cooperate with their hand therapist. Discussion: The following topics are covered: a) psychological influences on pain perception, b) the logic of how VR analgesia works, c) evidence for reduction of acute procedural pain during hand therapy, d) recent major advances in VR technology, and e) future directions—immersive VR embodiment therapy for phantom limb (chronic) pain. Conclusion: VR hand therapy has potential for a wide range of patient populations needing hand therapy, including acute pain and potentially chronic pain patients. Being in VR helps reduce the patients’ pain, making it less painful for patients to move their hand/fingers during hand therapy, and gamified VR can help motivate the patient to perform therapeutic hand exercises, and make hand therapy more fun. In addition, VR camera–based hand tracking technology may be used to help therapists monitor how well patients are doing their hand therapy exercises, and to quantify whether adherence to treatment increases long-term functionality. Additional research and development into using VR as a tool for hand therapist is recommended for both acute pain and persistent pain patient populations. © 2020 Hanley & Belfus",Acute pain; Hand therapy; Immersive virtual reality; Pain distraction phantom limb pain; Persistent pain,Acute Pain; Analgesia; Chronic Pain; Exercise Therapy; Hand; Humans; Video Games; Virtual Reality; analgesia; Article; burn; chronic pain; eye tracking; human; nociception; occupational therapy; phantom pain; physiotherapy; procedural pain; psychological aspect; range of motion; rehabilitation care; task performance; virtual reality; virtual reality hand therapy; wound care; analgesia; chronic pain; hand; kinesiotherapy; pain; video game,Article,Final,All Open Access; Green Open Access,Scopus,2-s2.0-85085605245,Gaming / VR
Burch M.,"Burch, Michael (36927991800)",36927991800,Teaching eye tracking visual analytics in computer and data science bachelor courses,2020,Eye Tracking Research and Applications Symposium (ETRA),,,,,,,7,10.1145/3379155.3391331,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85086182984&doi=10.1145%2f3379155.3391331&partnerID=40&md5=d0c97da5709f13d00046a2a01215b5f9,"Making students aware of eye tracking technologies can have a great benefit on the entire application field since they may build the next generation of eye tracking researchers. On the one hand students learn the usefulness and benefits of this technique for different scientific purposes like user evaluation to find design flaws or visual attention strategies, gaze-assisted interaction to enhance and augment traditional interaction techniques, or as a means to improve virtual reality experiences. However, on the other hand, the large amount of recorded data means a challenge for data analytics in order to find rules, patterns, but also anomalies in the data, finally leading to insights and knowledge to understand or predict eye movement patterns which can have synergy effects for both disciplines - eye tracking and visual analytics. In this paper we will describe the challenges of teaching eye tracking combined with visual analytics in a computer and data science bachelor course with 42 students in an active learning scenario following four teaching stages. Some of the student project results are shown to demonstrate learning outcomes with respect to eye tracking data analysis and visual analytics techniques. © 2020 ACM.",education; Eye tracking; information visualization; teaching; visual analytics,Behavioral research; Data Analytics; Data Science; Eye movements; Students; Teaching; Visualization; Application fields; Eye movement patterns; Eye tracking technologies; Interaction techniques; Learning outcome; Virtual reality experiences; Visual analytics; Visual attention strategy; Eye tracking,Conference paper,Final,,Scopus,2-s2.0-85086182984,Gaming / VR
de Mooij S.M.M.; Kirkham N.Z.; Raijmakers M.E.J.; van der Maas H.L.J.; Dumontheil I.,"de Mooij, Susanne M.M. (57200189331); Kirkham, Natasha Z. (7005119363); Raijmakers, Maartje E.J. (7004134241); van der Maas, Han L.J. (6701525820); Dumontheil, Iroise (12787193300)",57200189331; 7005119363; 7004134241; 6701525820; 12787193300,Should online math learning environments be tailored to individuals’ cognitive profiles?,2020,Journal of Experimental Child Psychology,191,,104730,,,,19,10.1016/j.jecp.2019.104730,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85075275556&doi=10.1016%2fj.jecp.2019.104730&partnerID=40&md5=06baea134161edf939dd21a3f6c1d5f7,"Online learning environments are well-suited for tailoring the learning experience of children individually and on a large scale. An environment such as Math Garden allows children to practice exercises adapted to their specific mathematical ability; this is thought to maximize their mathematical skills. In the current experiment, we investigated whether learning environments should also consider the differential impact of cognitive load on children's math performance depending on their individual verbal working memory (WM) and inhibitory control (IC) capacity. A total of 39 children (8–11 years old) performed a multiple-choice computerized arithmetic game. Participants were randomly assigned to two conditions where the visibility of time pressure, a key feature in most gamified learning environments, was manipulated. Results showed that verbal WM was positively associated with arithmetic performance in general but that higher IC predicted better performance only when the time pressure was not visible. This effect was mostly driven by the younger children. Exploratory analyses of eye-tracking data (N = 36) showed that when time pressure was visible, children attended more often to the question (e.g., 6 × 8). In addition, when time pressure was visible, children with lower IC, in particular younger children, attended more often to answer options representing operant confusion (e.g., 9 × 4 = 13) and visited more answer options before responding. These findings suggest that tailoring the visibility of time pressure, based on a child's individual cognitive profile, could improve arithmetic performance and may in turn improve learning in online learning environments. © 2019 The Authors",Arithmetic; Eye tracking; Individual differences; Inhibitory control; Time perception; Working memory,"Attention; Child; Education, Distance; Executive Function; Eye-Tracking Technology; Female; Humans; Inhibition, Psychological; Male; Mathematics; Memory, Short-Term; Problem Solving; arithmetic; article; child; clinical article; controlled study; exploratory research; eye tracking; female; human; human experiment; learning environment; male; randomized controlled trial; school child; time perception; visibility; working memory; attention; education; executive function; mathematics; physiology; problem solving; short term memory",Article,Final,All Open Access; Green Open Access; Hybrid Gold Open Access,Scopus,2-s2.0-85075275556,Gaming / VR
Kelly C.; Bernardet U.; Kessler K.,"Kelly, Cliona (57216933692); Bernardet, Ulysses (6507620426); Kessler, Klaus (8905196800)",57216933692; 6507620426; 8905196800,"A Neuro-VR toolbox for assessment and intervention in Autism: Brain responses to non-verbal, gaze and proxemics behaviour in Virtual Humans.",2020,"Proceedings - 2020 IEEE Conference on Virtual Reality and 3D User Interfaces, VRW 2020",,,9090451,565,566,1.0,2,10.1109/VRW50115.2020.00134,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85085365083&doi=10.1109%2fVRW50115.2020.00134&partnerID=40&md5=c7744fd7964d308446f64e0f68fa5bd5,"This research plans to investigate non-verbal social interaction in a virtual environment. Participants are required to interact with a virtual human (VH) in order to complete a puzzle using only eye-gaze. The project will focus on joint attention (JA) that can be categorised into two processes, initiation (IJA) and response (RJA). Firstly, we will investigate these categories by recording behaviours such as accuracy, completion times and the participants' eye-tracking data to influence JA behaviour of the VH. This will later progress to investigate neural substrates using electrophysiological recordings of the brain (EEG). Based on previous research [1; 5], we expect differences to manifest in the neural signatures that are representative of each process. Manipulations will centre around the proximity of the character and the use of distractions; objects randomly appearing in the scene, eliciting overt and sometimes, covert gaze responses. The final stage of the project will incorporate a closed-loop system that is moderated by the EEG signals. This, in turn, will modulate the behaviour of the VH i.e. the proximity and/or extent of cooperation throughout the task. © 2020 IEEE.",ASD; EEG; Eye-gaze; Joint Attention; VR,Closed loop systems; Electrophysiology; Eye tracking; User interfaces; Completion time; Electrophysiological recordings; Joint attention; Neural signatures; Neural substrates; Research plans; Social interactions; Virtual humans; Virtual reality,Conference paper,Final,,Scopus,2-s2.0-85085365083,Gaming / VR
Adhanom I.B.; Lee S.C.; Folmer E.; MacNeilage P.,"Adhanom, Isayas B. (57209396333); Lee, Samantha C. (57217014475); Folmer, Eelke (9840498500); MacNeilage, Paul (10240387800)",57209396333; 57217014475; 9840498500; 10240387800,GazeMetrics: An open-source tool for measuring the data quality of HMD-based eye trackers,2020,Eye Tracking Research and Applications Symposium (ETRA),,,40,,,,19,10.1145/3379156.3391374,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85085734037&doi=10.1145%2f3379156.3391374&partnerID=40&md5=cb3362ce211b29ec6fa26946ffa3dff4,"As virtual reality (VR) garners more attention for eye tracking research, knowledge of accuracy and precision of head-mounted display (HMD) based eye trackers becomes increasingly necessary. It is tempting to rely on manufacturer-provided information about the accuracy and precision of an eye tracker. However, unless data is collected under ideal conditions, these values seldom align with on-site metrics. Therefore, best practices dictate that accuracy and precision should be measured and reported for each study. To address this issue, we provide a novel open-source suite for rigorously measuring accuracy and precision for use with a variety of HMD-based eye trackers. This tool is customizable without having to alter the source code, but changes to the code allow for further alteration. The outputs are available in real time and easy to interpret, making eye tracking with VR more approachable for all users. © 2020 ACM.",Accuracy; Eye movements; Eye tracker data quality; Eye tracking; Precision; Virtual reality,Codes (symbols); Helmet mounted displays; Open Data; Open source software; Virtual reality; Accuracy and precision; Best practices; Customizable; Eye trackers; Head mounted displays; Measuring accuracy; Open source tools; Open sources; Eye tracking,Conference paper,Final,All Open Access; Green Open Access,Scopus,2-s2.0-85085734037,Gaming / VR
Zhang J.; Mullikin M.; Li Y.; Mei C.,"Zhang, Jingjing (57211521701); Mullikin, Meg'n (57216937024); Li, Yi (55363372400); Mei, Chao (56421487500)",57211521701; 57216937024; 55363372400; 56421487500,A Methodology of Eye Gazing Attention Determination for VR Training,2020,"Proceedings - 2020 IEEE Conference on Virtual Reality and 3D User Interfaces, VRW 2020",,,9090559,138,141,3.0,5,10.1109/VRW50115.2020.00029,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85085356353&doi=10.1109%2fVRW50115.2020.00029&partnerID=40&md5=7eee4070139b5c1f1e21dacd0b8190ac,"Many systems have successfully used virtual reality (VR), eye tracking system and applied behavioral analysis in Autism Spectrum Disorders (ASD) therapies. Recognizing eye gazing patterns is critical. When people look at different objects, their eyes may stay on some objects for a long time or glance at some objects. This paper proposes a methodology of eye gazing attention determination which uses different time thresholds to track the eyes pattern of autistic children in adaptive virtual environments therapy systems. Moreover, by setting up hierarchical structure of virtual objects, we increase the reliability and immersion of the eye gaze attention determination methodology in VR attention training. © 2020 IEEE.",Autism Spectrum Disorders (ASD); Eye tracking system; H.5.2 [Information Interfaces and Presentation]:User Interfaces - Evaluation / methodology; Time thresholds; Virtual reality,Eye tracking; User interfaces; Adaptive virtual environments; Autism spectrum disorders; Autistic children; Behavioral analysis; Eye tracking systems; Hierarchical structures; Time thresholds; Virtual objects; Virtual reality,Conference paper,Final,,Scopus,2-s2.0-85085356353,Gaming / VR
Burch M.; Kurzhals K.,"Burch, Michael (36927991800); Kurzhals, Kuno (55390097400)",36927991800; 55390097400,Visual analysis of eye movements during game play,2020,Eye Tracking Research and Applications Symposium (ETRA),,,55,,,,6,10.1145/3379156.3391839,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85085736073&doi=10.1145%2f3379156.3391839&partnerID=40&md5=42315b3ae57e01a6ae22e3ddf5f6a646,"Eye movements indicate visual attention and strategies during game play, regardless of whether in board, sports, or computer games. Additional factors such as individual vs. group play and active playing vs. observing game play further differentiate application scenarios for eye movement analysis. Visual analysis has proven to be an effective means to investigate and interpret such highly dynamic spatio-temporal data. In this paper, we contribute a classification strategy for different scenarios for the visual analysis of gaze data during game play. Based on an initial sample of related work, we derive multiple aspects comprising data sources, game mode, player number, player state, analysis mode, and analysis goal. We apply this classification strategy to describe typical analysis scenarios and research questions as they can be found in related work. We further discuss open challenges and research directions for new application scenarios of eye movements in game play. © 2020 ACM.",Eye tracking; Games; Interaction; Visual analysis,Behavioral research; Computer games; Eye tracking; Analysis modes; Application scenario; Eye movement analysis; New applications; Research questions; Spatio-temporal data; Visual analysis; Visual Attention; Eye movements,Conference paper,Final,,Scopus,2-s2.0-85085736073,Gaming / VR
Khokhar A.; Yoshimura A.; Borst C.W.,"Khokhar, Adil (57210910170); Yoshimura, Andrew (57210910262); Borst, Christoph W. (9736479200)",57210910170; 57210910262; 9736479200,Modified Playback of Avatar Clip Sequences Based on Student Attention in Educational VR,2020,"Proceedings - 2020 IEEE Conference on Virtual Reality and 3D User Interfaces, VRW 2020",,,9090598,851,852,1.0,0,10.1109/VRW50115.2020.00276,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85085379484&doi=10.1109%2fVRW50115.2020.00276&partnerID=40&md5=8d5fadd751a92f108e88537d853b363a,We demonstrate a system that sequences teacher avatar clips considering student eye tracking. We are investigating subjective suitability of avatar responses to student misunderstandings or inattention. Three different avatar behaviors are demonstrated to allow a teacher pedagogical agent to behave more appropriately to student attention or distraction. An in-game mobile device provides an experiment control mechanism for 2 levels of distractions. © 2020 IEEE.,Human-centered computing; Visualization,Eye tracking; Students; Virtual reality; Control mechanism; Pedagogical agents; User interfaces,Conference paper,Final,,Scopus,2-s2.0-85085379484,Gaming / VR
"Al-Ghamdi N.A.; Meyer W.J., III; Atzori B.; Alhalabi W.; Seibel C.C.; Ullman D.; Hoffman H.G.","Al-Ghamdi, Najood A. (36503686800); Meyer, Walter J. (55444517900); Atzori, Barbara (56195819000); Alhalabi, Wadee (35316856000); Seibel, Clayton C. (57214805510); Ullman, David (57224705119); Hoffman, Hunter G. (7201677607)",36503686800; 55444517900; 56195819000; 35316856000; 57214805510; 57224705119; 7201677607,Virtual Reality Analgesia With Interactive Eye Tracking During Brief Thermal Pain Stimuli: A Randomized Controlled Trial (Crossover Design),2020,Frontiers in Human Neuroscience,13,,467,,,,32,10.3389/fnhum.2019.00467,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85079162616&doi=10.3389%2ffnhum.2019.00467&partnerID=40&md5=29ff1fa60a185ed08af52edfe6e159c3,"In light of growing concerns about opioid analgesics, developing new non-pharmacologic pain control techniques has become a high priority. Adjunctive virtual reality can help reduce acute pain during painful medical procedures. However, for some especially painful medical procedures such as burn wound cleaning, clinical researchers recommend that more distracting versions of virtual reality are needed, to further amplify the potency of virtual reality analgesia. The current study with healthy volunteers explores for the first time whether interacting with virtual objects in Virtual Reality (VR) via “hands free” eye-tracking technology integrated into the VR helmet makes VR more effective/powerful than non-interactive/passive VR (no eye-tracking) for reducing pain during brief thermal pain stimuli. Method: Forty eight healthy volunteers participated in the main study. Using a within-subject design, each participant received one brief thermal pain stimulus during interactive eye tracked virtual reality, and each participant received another thermal pain stimulus during non-interactive VR (treatment order randomized). After each pain stimulus, participants provided subjective 0–10 ratings of cognitive, sensory and affective components of pain, and rated the amount of fun they had during the pain stimulus. Results: As predicted, interactive eye tracking increased the analgesic effectiveness of immersive virtual reality. Compared to the passive non-interactive VR condition, during the interactive eye tracked VR condition, participants reported significant reductions in worst pain (p < 0.001) and pain unpleasantness (p < 0.001). Participants reported a significantly stronger illusion of presence (p < 0.001), and significantly more fun in VR (p < 0.001) during the interactive condition compared to during passive VR. In summary, as predicted by our primary hypothesis, in the current laboratory acute pain analog study with healthy volunteers, increasing the immersiveness of the VR system via interactive eye tracking significantly increased how effectively VR reduced worst pain during a brief thermal pain stimulus. Although attention was not directly measured, the pattern of pain ratings, presence ratings, and fun ratings are consistent with an attentional mechanism for how VR reduces pain. Whether the current results generalize to clinical patient populations is another important topic for future research. Additional research and development is recommended. © Copyright © 2020 Al-Ghamdi, Meyer, Atzori, Alhalabi, Seibel, Ullman and Hoffman.",analgesia; distraction; non-pharmacologic analgesic techniques; opioid analgesia; pain; virtual reality,adult; affect; analgesia; Article; clinical effectiveness; cognition; controlled study; crossover procedure; device therapy; eye movement; female; human; human experiment; illusion; immersion; normal human; pain assessment; pain severity; prediction; randomized controlled trial; sensory system; thermal stimulation; unpleasant sensation; virtual reality; volunteer,Article,Final,All Open Access; Gold Open Access; Green Open Access,Scopus,2-s2.0-85079162616,Gaming / VR
Jogeshwar A.K.; Diaz G.J.; Farnand S.P.; Pelz J.B.,"Jogeshwar, Anjali K. (57193727303); Diaz, Gabriel J. (55436582600); Farnand, Susan P. (6506684949); Pelz, Jeff B. (7007018556)",57193727303; 55436582600; 6506684949; 7007018556,The cone model: Recognizing gaze uncertainty in virtual environments,2020,IS and T International Symposium on Electronic Imaging Science and Technology,2020,9,288,,,,3,10.2352/ISSN.2470-1173.2020.9.IQSP-288,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85086303720&doi=10.2352%2fISSN.2470-1173.2020.9.IQSP-288&partnerID=40&md5=332fa176cecc51d879bb0e18d17bbc05,"Eye tracking is used by psychologists, neurologists, vision researchers, and many others to understand the nuances of the human visual system, and to provide insight into a person's allocation of attention across the visual environment. When tracking the gaze behavior of an observer immersed in a virtual environment displayed on a head-mounted display, estimated gaze direction is encoded as a three-dimensional vector extending from the estimated location of the eyes into the 3D virtual environment. Additional computation is required to detect the target object at which gaze was directed. These methods must be robust to calibration error or eye tracker noise, which may cause the gaze vector to miss the target object and hit an incorrect object at a different distance. Thus, the straightforward solution involving a single vector-to-object collision could be inaccurate in indicating object gaze. More involved metrics that rely upon an estimation of the angular distance from the ray to the center of the object must account for an object's angular size based on distance, or irregularly shaped edges - information that is not made readily available by popular game engines (e.g. Unity © 2020 Society for Imaging Science and Technology.",,Helmet mounted displays; Object detection; Object tracking; 3-D virtual environment; Allocation of attentions; Calibration error; Head mounted displays; Human Visual System; Object collision; Three-dimensional vectors; Visual environments; Eye tracking,Conference paper,Final,,Scopus,2-s2.0-85086303720,Gaming / VR
Reichenberger J.; Pfaller M.; Mühlberger A.,"Reichenberger, Jonas (56674467400); Pfaller, Michael (55258589800); Mühlberger, Andreas (6507375232)",56674467400; 55258589800; 6507375232,Gaze Behavior in Social Fear Conditioning: An Eye-Tracking Study in Virtual Reality,2020,Frontiers in Psychology,11,,35,,,,38,10.3389/fpsyg.2020.00035,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85079138491&doi=10.3389%2ffpsyg.2020.00035&partnerID=40&md5=f5b6a10d326f745cca7465cbe1a075ab,"The vigilance-avoidance hypothesis of selective attention assumes that socially anxious persons initially direct their attention toward fear-related stimuli and subsequently avoid these social stimuli to reduce emotional distress. New technical developments provide tools to implicit measure overt attention on fear-related stimuli via eye-tracking in ecological valid virtual environments presented via a head-mounted display. We examined in 27 low (LSA) and 26 high socially anxious (HSA) individuals fear ratings, physical behavior (duration of approach), hypervigilance (time to first fixation), and attentional avoidance (count of fixations) toward virtual female and male agents (CS) during social fear conditioning (SFC) and extinction in virtual reality (VR). As hypothesized, generally SFC was successfully induced and extinguished concerning the fear ratings. Our findings partly support the vigilance-avoidance hypothesis as HSA directed especially at the first half of the fear acquisition their initial attention more at CS+ than CS− agents, and avoided subsequently the CS+ more than the CS− agents during the fear acquisition. In contrast, in LSA participants initial and sustained attention did not differ between CS+ and CS− agents during fear acquisition. We conclude that HSA individuals guide their initial attention to emotionally threatening stimuli and subsequently avoid the threatening stimuli to possibly reduce their emotional distress, whereas LSA individuals regulate themselves less in their (fear) responses during SFC. Measuring implicit gaze behavior within a well-controlled virtual environment is an interesting innovative tool to in deeply investigate the impact of attention on emotional learning processes. © Copyright © 2020 Reichenberger, Pfaller and Mühlberger.",eye-tracking; social anxiety disorder; social fear conditioning; vigilance-avoidance hypothesis; virtual reality,,Article,Final,All Open Access; Gold Open Access; Green Open Access,Scopus,2-s2.0-85079138491,Gaming / VR
Zhang R.; Walshe C.; Liu Z.; Guan L.; Muller K.S.; Whritner J.A.; Zhang L.; Hayhoe M.M.; Ballard D.H.,"Zhang, Ruohan (57184546900); Walshe, Calen (57219542563); Liu, Zhuode (57204286138); Guan, Lin (59830568100); Muller, Karl S. (57204293789); Whritner, Jake A. (57194013348); Zhang, Luxin (57208268090); Hayhoe, Mary M. (7004513666); Ballard, Dana H. (57220533693)",57184546900; 57219542563; 57204286138; 59830568100; 57204293789; 57194013348; 57208268090; 7004513666; 57220533693,Atari-HEAD: Atari human eye-tracking and demonstration dataset,2020,AAAI 2020 - 34th AAAI Conference on Artificial Intelligence,,,,6811,6820,9.0,37,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85093216340&partnerID=40&md5=e3b9f44c65afd3e483795fdf3333b06c,"Large-scale public datasets have been shown to benefit research in multiple areas of modern artificial intelligence. For decision-making research that requires human data, high-quality datasets serve as important benchmarks to facilitate the development of new methods by providing a common reproducible standard. Many human decision-making tasks require visual attention to obtain high levels of performance. Therefore, measuring eye movements can provide a rich source of information about the strategies that humans use to solve decision-making tasks. Here, we provide a large-scale, high-quality dataset of human actions with simultaneously recorded eye movements while humans play Atari video games. The dataset consists of 117 hours of gameplay data from a diverse set of 20 games, with 8 million action demonstrations and 328 million gaze samples. We introduce a novel form of gameplay, in which the human plays in a semi-frame-by-frame manner. This leads to near-optimal game decisions and game scores that are comparable or better than known human records. We demonstrate the usefulness of the dataset through two simple applications: predicting human gaze and imitating human demonstrated actions. The quality of the data leads to promising results in both tasks. Moreover, using a learned human gaze model to inform imitation learning leads to an 115% increase in game performance. We interpret these results as highlighting the importance of incorporating human visual attention in models of decision making and demonstrating the value of the current dataset to the research community. We hope that the scale and quality of this dataset can provide more opportunities to researchers in the areas of visual attention, imitation learning, and reinforcement learning. Copyright © 2020, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.",,Behavioral research; Benchmarking; Decision making; Eye movements; Human computer interaction; Large dataset; Reinforcement learning; Human actions; Human decision making; Human visual attention; Imitation learning; Multiple areas; Near-optimal; Research communities; Visual Attention; Eye tracking,Conference paper,Final,,Scopus,2-s2.0-85093216340,Gaming / VR
Günther U.; Harrington K.I.S.; Dachselt R.; Sbalzarini I.F.,"Günther, Ulrik (56661947000); Harrington, Kyle I. S. (19638432100); Dachselt, Raimund (6507253418); Sbalzarini, Ivo F. (8650204800)",56661947000; 19638432100; 6507253418; 8650204800,Bionic Tracking: Using Eye Tracking to Track Biological Cells in Virtual Reality,2020,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),12535 LNCS,,,280,297,17.0,4,10.1007/978-3-030-66415-2_18,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85101327963&doi=10.1007%2f978-3-030-66415-2_18&partnerID=40&md5=97377cdcf3d5c4f0728a32ebe62b176e,"We present Bionic Tracking, a novel method for solving biological cell tracking problems with eye tracking in virtual reality using commodity hardware. Using gaze data, and especially smooth pursuit eye movements, we are able to track cells in time series of 3D volumetric datasets. The problem of tracking cells is ubiquitous in developmental biology, where large volumetric microscopy datasets are acquired on a daily basis, often comprising hundreds or thousands of time points that span hours or days. The image data, however, is only a means to an end, and scientists are often interested in the reconstruction of cell trajectories and cell lineage trees. Reliably tracking cells in crowded three-dimensional space over many time points remains an open problem, and many current approaches rely on tedious manual annotation or curation. In the Bionic Tracking approach, we substitute the usual 2D point-and-click interface for annotation or curation with eye tracking in a virtual reality headset, where users follow cells with their eyes in 3D space in order to track them. We detail the interaction design of our approach and explain the graph-based algorithm used to connect different time points, also taking occlusion and user distraction into account. We demonstrate Bionic Tracking using examples from two different biological datasets. Finally, we report on a user study with seven cell tracking experts, highlighting the benefits and limitations of Bionic Tracking compared to point-and-click interfaces. © 2020, Springer Nature Switzerland AG.",,Bionics; Cells; Computer vision; Cytology; Eye movements; Graph algorithms; Graphic methods; Large dataset; Trees (mathematics); Virtual reality; Developmental biology; Graph-based algorithms; Point-and-click interface; Smooth pursuit eye movement; Three dimensional space; Tracking approaches; Virtual-reality headsets; Volumetric data sets; Eye tracking,Conference paper,Final,,Scopus,2-s2.0-85101327963,Gaming / VR
Kim N.; Lee H.,"Kim, Nayeon (57216898685); Lee, Hyunsoo (54393374900)",57216898685; 54393374900,Evaluating Visual Perception by Tracking Eye Movement in Architectural Space During Virtual Reality Experiences,2020,Advances in Intelligent Systems and Computing,1152 AISC,,,302,308,6.0,9,10.1007/978-3-030-44267-5_45,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85085196680&doi=10.1007%2f978-3-030-44267-5_45&partnerID=40&md5=c235f23ac62610ef12d2f6cda5eeac6d,"This study quantitatively evaluated subjects’ visual perceptions and experiences of an architectural environment in virtual reality. This study was conducted to empirically determine which spatial elements subjects were most interested in in virtual architectural space; to analyze the sequence of objects that captured subjects’ visual attention, how long each object captured their attention for, and the number of objects in each area of interest that captured their attention; to analyze subjects’ emotional responses to virtual architectural spaces. A head-mounted display with an integrated eye-tracking device was used to measure the visual attention that subjects paid to spatial elements of a virtual environment. The result showed which architectural and interior elements subjects paid the most attention to in an area of interest in the virtual architectural space. This study’s findings provide useful information and practical guidelines to architects and designers about which built environment elements attract occupants’ visual attention and what types of physiological responses the elements cause. The conclusions drawn from evaluating visual perception with state-of-the-art VR integrated eye-tracking technology can inform architectural design processes to improve user experiences. © The Editor(s) (if applicable) and The Author(s), under exclusive license to Springer Nature Switzerland AG 2020.",Architectural space; Evaluation; Eye tracking; Head-Mounted display; Virtual reality; Visual perception,Architectural design; Behavioral research; Eye movements; Eye tracking; Helmet mounted displays; Physiological models; User experience; Architectural environment; Architectural space; Eye tracking devices; Eye tracking technologies; Head mounted displays; Physiological response; Practical guidelines; Virtual reality experiences; Virtual reality,Conference paper,Final,,Scopus,2-s2.0-85085196680,Gaming / VR
Pei W.; Lo T.; Guo X.,"Pei, Wanyu (57219209543); Lo, TianTian (56039940300); Guo, Xiangmin (36571734100)",57219209543; 56039940300; 36571734100,A Biofeedback Process: Detecting Architectural Space with the Integration of Emotion Recognition and Eye-tracking Technology,2020,"RE: Anthropocene, Design in the Age of Humans - Proceedings of the 25th International Conference on Computer-Aided Architectural Design Research in Asia, CAADRIA 2020",2,,,263,272,9.0,2,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85091706710&partnerID=40&md5=dccda5ce9602dc6518298ca0028ff998,"This paper coincides with the conference theme that people have gradually become a vital force influencing the environmental system. In the future, it is necessary to study the influence of not only the built environment on people but also people's feedback on environmental design. This study explores the processes of interactive design using both emotion recognition and eye-tracking of users. By putting on wearable devices to roam and perceive in a virtual reality space, the physiological data of the users are collected in real-time and used to analyze their emotional responses and visual attention to the spaces. This method will provide an auxiliary way for non-architectural professional users to participate in architectural space design. At present, there is a lack of research on the comprehensive application of eye movement knowledge and emotional feedback in architectural space design. This integration will help professional designers to optimize the design of architectural space. For this paper, we review existing research and proposing an interactive design workflow that integrates eye tracking and emotion recognition. This workflow will help with the next stage of research to understand the design of a new International School of Design building. © 2020 and published by the Association for Computer-Aided Architectural Design Research in Asia (CAADRIA), Hong Kong.",Architectural space environment; Interactive design; Perception detection; Virtual reality,Behavioral research; Biofeedback; Eye movements; Eye tracking; Physiological models; Speech recognition; Architectural space; Architectural space designs; Emotion recognition; Emotional feedback; Environmental systems; Eye tracking technologies; Interactive design; Professional designers; Architectural design,Conference paper,Final,,Scopus,2-s2.0-85091706710,Gaming / VR
Piszczek M.; Pomianek M.; Maciejewski M.; Jodlowski L.; Krukowski P.,"Piszczek, Marek (12241215700); Pomianek, Mateusz (57195680192); Maciejewski, Marcin (57195297120); Jodlowski, Leon (6603096761); Krukowski, Piotr (57217385926)",12241215700; 57195680192; 57195297120; 6603096761; 57217385926,The importance of monitoring vergence eye movements for solutions using virtual technologies,2020,Photonics Letters of Poland,12,2,,40,42,2.0,0,10.4302/plp.v12i2.1015,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85087162781&doi=10.4302%2fplp.v12i2.1015&partnerID=40&md5=b3f10f80b8d558835f88d3d4fd893cc4,"Eyesight monitoring systems are of particular interest to virtual technology research teams and concern mainly three key aspects: physiological, technical and functional. The first one is to solve the so-called conflict of vergence and accommodation. In contrast to typical gaming applications, e.g. vision therapy requires the ability to smoothly transfer attention between the points of near and far. The technical aspect is primarily the search for new measurement methods, which is particularly important in the case of mobile goggles with limited computing capabilities of the processor. The last aspect concerns the possibility of creating effective HMI (Human Machine Interface) solutions. © 2020 Photonics Society of Poland.",,Magnetic materials; Photonics; Computing capability; Gaming applications; Human Machine Interface; Monitoring system; New Measurement Method; Technical aspects; Vergence eye movements; Virtual technology; Eye movements,Article,Final,All Open Access; Gold Open Access,Scopus,2-s2.0-85087162781,Gaming / VR
Lueckenhoff A.; Wessels C.; Kyrarini M.; Makedon F.,"Lueckenhoff, Alexis (57220054298); Wessels, Callen (57219945746); Kyrarini, Maria (56786567700); Makedon, Fillia (7003437865)",57220054298; 57219945746; 56786567700; 7003437865,Towards game-based assessment of executive functions in children,2020,CEUR Workshop Proceedings,2844,,,15,18,3.0,0,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85104585970&partnerID=40&md5=c620641e68417bd9465fe73662b3abdc,"Executive Functions are very important mental skills that help us to coordinate, plan, pay attention, organize, and multitask, among others. Weak executive functions may affect school or work performance. Therefore, there is a need of identifying executive function deficits early during childhood and enable interventions that could improve executive functioning skills. In this work, we present a game-based assessment system of executive functions in children that could be performed at home. The proposed system utilizes machine learning techniques to detect and track head and eye movements from image frames and fuses this data with game performance. A novel variation of the Flanker task has been developed as a game to measure engagement, attention, working memory, and processing speed. In the future, the proposed system will be evaluated in a real-world study on children between 6 and 14 years old. Copyright © 2020 for this paper by its authors. Use permitted under Creative Commons License Attribution 4.0 International (CC BY 4.0).",Executive function; Eye gaze; Flanker task; Game-based assessment,Eye movements; Learning systems; Assessment system; Executive function; Image frames; Machine learning techniques; Processing speed; Real-world; Work performance; Working memory; Artificial intelligence,Conference paper,Final,,Scopus,2-s2.0-85104585970,Gaming / VR
Javed Y.; Al Qahtani E.; Shehab M.,"Javed, Yousra (55399009300); Al Qahtani, Elham (57755525800); Shehab, Mohamed (23025577500)",55399009300; 57755525800; 23025577500,The Impact of Advertisements on User Attention During Permission Authorization,2020,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),12423 LNCS,,,722,739,17.0,0,10.1007/978-3-030-60114-0_47,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85092893627&doi=10.1007%2f978-3-030-60114-0_47&partnerID=40&md5=e93e0673519b4ef3fbdb0dafe1931e7e,"Advertisements are an integral part of websites and play an important role in generating revenue. Many websites also present users with permission authorization windows. For instance, Facebook’s third-party application authorization window is displayed when playing a game on various gaming websites. Although researchers have investigated the impact of advertisements on user frustration and their ability to process content, the extent to which advertisements distract the users during permission authorization has not been explored. This paper investigates the impact of advertisement’s presence and its content type on user’s attention during permission authorization. We conducted a between-subjects experiment on the mockup of a popular gaming website that contained banner advertisements. The control group was presented with no advertisements above the permission authorization dialog. Whereas, the treatment group was presented with static or animated advertisements. Eye-gaze tracking was performed while participants interacted with the applications. We observed that the presence of animated advertisements that contain sound significantly distracted participants away from the permission authorization window. Our findings suggest that increasing the number of distraction elements (e.g., animation and sound) increases the likelihood of users ignoring text on important windows such as permission authorization windows. Moreover, the use of shopping and politics related advertisements, can attract more attention compared to food and sports related advertisements. © 2020, Springer Nature Switzerland AG.",Advertisements; Distractions; Eye-gaze tracking; Permission authorization windows; User attention,Animation; Eye tracking; Human computer interaction; Websites; Control groups; Eye gaze tracking; Facebook; Integral part; Third party application (Apps); Treatment group; User attention; User experience,Conference paper,Final,,Scopus,2-s2.0-85092893627,Gaming / VR
,,,"7th International Conference on Learning and Collaboration Technologies, LCT 2020, held as part of the 22nd International Conference on Human-Computer Interaction, HCII 2020",2020,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),12206 LNCS,,,,,1249.0,0,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85089004395&partnerID=40&md5=e4c135e842bf3cbc3b48e0a6f5239339,The proceedings contain 86 papers. The special focus in this conference is on Learning and Collaboration Technologies. The topics include: Semantically annotated learning media for reduced cognitive load; learner’s mental state estimation with pc built-in camera; virtual reality as a stress reduction measure – chilling out on the beach in my living room; brain activation in virtual reality for attention guidance; visualizing students’ eye movement data to understand their math problem-solving processes; a co-design approach for the development and classroom integration of embodied learning apps; play to learn! nurturing fundamental digital skills of romanian preschoolers by developing edutainment applications; teachers’ adoption of embodied learning digital games with an inclusive education approach: Lessons learnt from the inteled project in spain; do individual differences modulate the effect of agency on learning outcomes with a serious game?; co-design for a competency self-assessment chatbot and survey in science education; agency affects learning outcomes with a serious game; experiential learning and stem in modern education: Incorporating educational escape rooms in parallel to classroom learning; edugame4city. a gamification for architecture students. viability study applied to urban design; game4city. gamification for citizens through the use of virtual reality made available to the masses. viability study in two public events; stickandclick – sticking and composing simple games as a learning activity; facilitating ideation and knowledge sharing in workplaces: The design and use of gamification in virtual platforms; immersive telepresence framework for remote educational scenarios; web-based teleoperation system for learning of 3d prototype designing and printing.,,,Conference review,Final,,Scopus,2-s2.0-85089004395,Gaming / VR
Adama V.S.; Schindler B.; Schmid T.,"Adama, V. Sophie (57192248776); Schindler, Benjamin (58386129400); Schmid, Thomas (57196539641)",57192248776; 58386129400; 57196539641,Using Time Domain and Pearson’s Correlation to Predict Attention Focus in Autistic Spectrum Disorder from EEG P300 Components,2020,IFMBE Proceedings,76,,,1890,1893,3.0,9,10.1007/978-3-030-31635-8_230,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85075898802&doi=10.1007%2f978-3-030-31635-8_230&partnerID=40&md5=d24543a8ee337b993bb3212c6d17aafd,"Patients with Autistic Spectrum Discorder are known to have deficits in interpreting others’ intentions from gaze-direction or other social attention. Here, we use electroencephalography data recorded in virtual reality experiments with patients to predict one out of eight objects that was focused on. Correct labels for these objects were known from parallel eye-tracking measurements. We extracted features from the time domain and from Pearson’s correlation and applied both statistical and neuro-inspired supervised machine learning algorithms. Using a multi-layer perceptron, we achieved 65.4% accuracy on the validation data set and 70.0% accuracy on the test data set. © 2020, Springer Nature Switzerland AG.",Brain-computer interface; Decision tree; Electroencephalogram; Event-related potentials; Feature extraction; Machine learning; Neural network; P300; Random forest; Support vector machine,Biochemical engineering; Brain computer interface; Decision trees; Electroencephalography; Electrophysiology; Eye tracking; Feature extraction; Learning algorithms; Learning systems; Medical computing; Neural networks; Statistical tests; Supervised learning; Support vector machines; Virtual reality; Autistic spectrum disorders; Event related potentials; Eye-tracking measurements; Multi layer perceptron; P300; Random forests; Supervised machine learning; Validation data; Machine learning,Conference paper,Final,,Scopus,2-s2.0-85075898802,Gaming / VR
Lee K.I.; Jeon J.H.; Song B.C.,"Lee, Kang Il (57217629614); Jeon, Jung Ho (57211625894); Song, Byung Cheol (36062099900)",57217629614; 57211625894; 36062099900,Deep Learning-Based Pupil Center Detection for Fast and Accurate Eye Tracking System,2020,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),12364 LNCS,,,36,52,16.0,36,10.1007/978-3-030-58529-7_3,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85097309459&doi=10.1007%2f978-3-030-58529-7_3&partnerID=40&md5=c4609c2c6646ff73561941bbe1280b1f,"In augmented reality (AR) or virtual reality (VR) systems, eye tracking is a key technology and requires significant accuracy as well as real-time operation. Many techniques for detecting pupil centers with error range of iris radius have been developed, but few techniques have precise performance with error range of pupil radius. In addition, the conventional methods rarely guarantee real-time pupil center detection in a general-purpose computer environment due to high complexity. Thus, we propose more accurate pupil center detection by improving the representation quality of the network in charge of pupil center detection. This is realized by representation learning based on mutual information. Also, the latency of the entire system is greatly reduced by using non-local block and self-attention block with large receptive field, which makes it accomplish real-time operation. The proposed system not only shows real-time performance of 52 FPS in a general-purpose computer environment but also provides state-of-the-art accuracy in terms of fine level index of 96.71%, 99.84% and 96.38% for BioID, GI4E and Talking Face Video datasets, respectively. © 2020, Springer Nature Switzerland AG.",Mobile applications; Remote eye tracking,Augmented reality; Computer vision; Eye tracking; General purpose computers; Real time systems; Conventional methods; Eye tracking systems; Key technologies; Mutual informations; Real time performance; Real-time operation; Receptive fields; State of the art; Deep learning,Conference paper,Final,,Scopus,2-s2.0-85097309459,Gaming / VR
Johnson M.E.; Case L.; Jensen C.; Salkowski A.,"Johnson, Marit E. (57196025917); Case, Laura (57216620583); Jensen, Christopher (57216617060); Salkowski, Abigail (57216614020)",57196025917; 57216620583; 57216617060; 57216614020,Ergonomic risks utilizing a hands-free input device compared to traditional input devices,2020,"Proceedings of the 2016 Industrial and Systems Engineering Research Conference, ISERC 2016",,,,1168,1173,5.0,0,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85084083930&partnerID=40&md5=8cfc4e6852f7cf721bf9155064742d1e,"Today's technology is demanding more from the human body in forms of extended hours in computing and input, increasing risks of injuries and costs to companies. Is there a way to reduce this impact while maintaining a healthy computing environment? This study is aimed at comparing ergonomic risks between use of an eye tracking device and traditional computer input devices for typical input tasks (ie. clicking, dragging, typing). Surface EMG and inclinometer data were compiled for upper extremity muscle recruitment patterns and cervicothoracic position, respectively, during these typical computing tasks. Participants (n=22, age range 19-43, mean age =24.4) completed 3 tests (simple gaming, form completion, and typing) and a self-assessment utilizing mouse and keyboard input, followed by input via a commercially available eye tracking input device. Results in the preliminary data visually showed changes at the cervicothoracic junction position, while increased upper extremity recruitment developed as testing proceeded. Overwhelmingly, participants rated themselves more proficient using traditional input versus the eye tracking device. These findings point to cautions of fatigue in the short duration and early learning phase, which may contribute to risks of hands-free computing, solely utilizing the peripheral eye tracker. © 2016 Proceedings of the 2016 Industrial and Systems Engineering Research Conference, ISERC 2016. All rights reserved.",Computer; EMG; Ergonomic exposure; Eye tracker; Input,Data visualization; Engineering research; Ergonomics; Industrial research; Knobs; Mammals; Systems engineering; Computing environments; Computing-task; Eye tracking devices; Muscle recruitment patterns; Self assessment; Short durations; Traditional computers; Upper extremity; Eye tracking,Conference paper,Final,,Scopus,2-s2.0-85084083930,Gaming / VR
Xi Z.; Newton O.; McGowin G.; Sukthankar G.; Fiore S.; Oden K.,"Xi, Zerong (57219597778); Newton, Olivia (57190797973); McGowin, Greg (57219601540); Sukthankar, Gita (6507708407); Fiore, Steve (59804837100); Oden, Kevin (57188972018)",57219597778; 57190797973; 57219601540; 6507708407; 59804837100; 57188972018,Predicting Student Flight Performance with Multimodal Features,2020,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),12268 LNCS,,,277,287,10.0,3,10.1007/978-3-030-61255-9_27,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85094164069&doi=10.1007%2f978-3-030-61255-9_27&partnerID=40&md5=a18f57f11c55101da2386f5f67310955,"This paper investigates the problem of predicting student flight performance in a training simulation from multimodal features, including flight controls, visual attention, and knowledge acquisition tests. This is a key component of developing next generation training simulations that can optimize the usage of student training time. Two types of supervised machine learning classifiers (random forest and support vector machines) were trained to predict the performance of twenty-three students performing simple flight tasks in virtual reality. Our experiments reveal the following: 1) features derived from gaze tracking and knowledge acquisition tests can serve as an adequate substitute for flight control features; 2) data from the initial portion of the flight task is sufficient to predict the final outcome; 3) our classifiers perform slightly better at predicting student failure than success. These results indicate the feasibility of using machine learning for early prediction of student failures during flight training. © 2020, Springer Nature Switzerland AG.",Gaze tracking; Intelligent training simulations; Multimodal features; Pilot training; Supervised learning,Behavioral research; Decision trees; Eye tracking; Forecasting; Knowledge acquisition; Learning systems; Random forests; Students; Support vector machines; Early prediction; Flight control; Flight training; Multimodal features; Student training; Supervised machine learning; Training simulation; Visual Attention; Flight simulators,Conference paper,Final,,Scopus,2-s2.0-85094164069,Gaming / VR
Woodworth J.W.; Broussard D.; Borst C.W.,"Woodworth, Jason W. (57192676048); Broussard, David (57215364589); Borst, Christoph W. (9736479200)",57192676048; 57215364589; 9736479200,Designing Tools To Improve Collaborative Interaction in a VR Environment for Teaching Geosciences Interpretation,2020,"Lecture Notes in Informatics (LNI), Proceedings - Series of the Gesellschaft fur Informatik (GI)",,,,,,,1,10.18420/muc2020-ws122-326,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85138826193&doi=10.18420%2fmuc2020-ws122-326&partnerID=40&md5=888d59243c9c74b38b179fbaf6af3998,"We discuss practical and theoretical solutions to problems that arose during the development of a collaborative VR application in which a teacher guides students through visualization and interactive interpretation of a geological dataset. To provide access to a large number of tools, we introduced a dashboard-style menu that rotates and moves to follow the user through the environment. We expect users to need good awareness of each other in the virtual environment, and especially to understand each other's attention to specific terrain surface features or annotations. For this, we display an eye gaze cue on the visualized terrain and visually tether a nametag widget on the dashboard to each user's avatar. Results of an initial usability review, involving an expert geologist guiding students, show promise for sharing eye gaze with a gaze trail as a basic method for understanding attention. Other tested indicators of avatar location or view appeared less important during the terrain feature presentation and interpretation. We additionally summarize ongoing work to enhance collaborative awareness through other eye tracking metrics and physiological data. © Proceedings of the Mensch und Computer 2020 Workshop on «Workshop on Virtual and Augmented Reality in Everyday Context (VARECo)». Copyright held by the owner/author(s)",collaboration; collaborativce VR; education; eye gaze; eye tracking; geological visualization; menu interaction; virtual reality,Augmented reality; Geology; Landforms; Virtual reality; Visualization; Collaboration; Collaborativce VR; Collaborative interaction; Designing tools; Eye-gaze; Eye-tracking; Geological visualization; Geosciences; Menu interaction; Practical solutions; Eye tracking,Conference paper,Final,,Scopus,2-s2.0-85138826193,Gaming / VR
Gauthier A.; Porayska-Pomsta K.; Mareschal D.,"Gauthier, Andrea (56605912700); Porayska-Pomsta, Kaśka (7801505406); Mareschal, Denis (7004069522)",56605912700; 7801505406; 7004069522,Using Eye-Tracking and Click-Stream Data to Design Adaptive Training of Children’s Inhibitory Control in a Maths and Science Game,2020,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),12164 LNAI,,,103,108,5.0,2,10.1007/978-3-030-52240-7_19,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85088563054&doi=10.1007%2f978-3-030-52240-7_19&partnerID=40&md5=87ab00205bd91c9a3ce989ba76d84aa4,"Computerised educational neuroscience interventions that train within-domain inhibitory control (IC) can improve children’s counterintuitive reasoning. However, the HCI or adaptive design of such environments often receive less attention. Eye-tracking and click data were used to compare four versions of an IC-training game in terms of their HCI design and potential for supporting adaptive feedback. Our results provide insights for developing an adaptive system to scaffold pupils’ transition towards using IC in un-cued, self-regulated scenarios. © 2020, Springer Nature Switzerland AG.",Adaptive support; Counterintuitive reasoning; Educational neuroscience; Game-based learning; Inhibitory control,Adaptive systems; Artificial intelligence; Data streams; Feedback; Integrated circuits; Scaffolds; Adaptive designs; Adaptive feedback; Adaptive training; Clickstreams; HCI design; Eye tracking,Conference paper,Final,,Scopus,2-s2.0-85088563054,Gaming / VR
Li T.-H.; Suzuki H.; Ohtake Y.,"Li, Ting-Hao (57218311579); Suzuki, Hiromasa (59983213300); Ohtake, Yutaka (7005292455)",57218311579; 59983213300; 7005292455,Visualization of user's attention on objects in 3d environment using only eye tracking glasses,2020,Journal of Computational Design and Engineering,7,2,,228,237,9.0,12,10.1093/JCDE/QWAA019,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85100857330&doi=10.1093%2fJCDE%2fQWAA019&partnerID=40&md5=a3a1835bb96e336bd6474682b945abf6,"Eye tracking technology is widely applied to detect user's attention in a 2D field, such as web page design, package design, and shooting games. However, because our surroundings primarily consist of 3D objects, applications will be expanded if there is an effective method to obtain and display user's 3D gaze fixation. In this research, a methodology is proposed to demonstrate the user's 3D gaze fixation on a digital model of a scene using only a pair of eye tracking glasses. The eye tracking glasses record user's gaze data and scene video. Thus, using image-based 3D reconstruction, a 3D model of the scene can be reconstructed from the frame images; simultaneously, the transformation matrix of each frame image can be evaluated to find 3D gaze fixation on the 3D model. In addition, a method that demonstrates multiple users' 3D gaze fixation on the same digital model is presented to analyze gaze distinction between different subjects. With this preliminary development, this approach shows potential to be applied to a larger environment and conduct a more reliable investigation. © 2020 Society for Computational Design and Engineering. All rights reserved.",3D gaze visualization; Eye tracking technology; Gaze distinction; Image-based 3D reconstruction,3D modeling; Eye tracking; Glass; Linear transformations; Object tracking; Websites; 3-D environments; 3D reconstruction; Digital model; Eye tracking technologies; Multiple user; Package designs; Transformation matrices; Web page design; digital image; electronic equipment; eye; image analysis; three-dimensional modeling; tracking; visualization; detection method; experimental study; methodology; model; perception; Three dimensional computer graphics,Article,Final,,Scopus,2-s2.0-85100857330,Gaming / VR
Kim N.; Lee H.,"Kim, Nayeon (57216898685); Lee, Hyunsoo (54393374900)",57216898685; 54393374900,Visual attention in retail environments: Design analysis using HMD based VR system integrated eye-tracking,2020,"RE: Anthropocene, Design in the Age of Humans - Proceedings of the 25th International Conference on Computer-Aided Architectural Design Research in Asia, CAADRIA 2020",1,,,631,640,9.0,5,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85091333282&partnerID=40&md5=c87124948ab612442b70020bc79a432b,"The goal of this study is to understand the spatial experience of users in retail environments in an immersive virtual reality setting. This study measures the visual attention and visual merchandising cognition of users via a quantitative method. The study was conducted to assess users' visual perception arising from the visual merchandising in-store environment during virtual reality experiences. The experiment was conducted using eye-tracking methodology in a virtual reality environment. After the experiment, participants responded to questionnaire surveys to assess visual merchandising cognition in retail environments. The experiment stimuli were provided in the virtual simulation of a retail store. During the experiment, each participant wearing a head-mounted display device was asked to experience the virtual retail space. The result shows the quantitative analysis of user behavior in the retail space and which design elements attract their attention. Unlike the precedent eye-tracking studies, this research analyzes visual attention during the spatial experience of retailing in its use of virtual reality technology. The approach and findings of this research provide useful information and practical guidelines to retailers and designers who are interested in improving the retail environment in consideration of customer visual attention and spatial elements. © 2020 and published by the Association for Computer-Aided Architectural Design Research in Asia (CAADRIA), Hong Kong.",Eye-tracking; HMD (Head-Mounted Display); Retail Environment; Virtual Reality; Visual Attention,Architectural design; Behavioral research; Eye tracking; Helmet mounted displays; Retail stores; Sales; Surveys; User experience; Eye-tracking studies; Head mounted displays; Immersive virtual reality; Practical guidelines; Questionnaire surveys; Virtual reality experiences; Virtual reality technology; Virtual-reality environment; Virtual reality,Conference paper,Final,,Scopus,2-s2.0-85091333282,Gaming / VR
Dhamodharan T.; Thomas M.; Ramdoss S.; JothiKumar K.; Saravana Sundharam S.N.S.; Muthuramalingam B.D.; Hussainalikhan N.N.; Ravichandran S.; Vadivel V.S.; Suresh P.; Buddhan S.; Madhusudanan A.,"Dhamodharan, Tamilselvi (35119754800); Thomas, Manju (57215781442); Ramdoss, Sathiyaprakash (36606813200); JothiKumar, Karthikeyan (57215777490); Saravana Sundharam, Sai Naveena Sri (57215772245); Muthuramalingam, Bhavani Devi (57215773320); Hussainalikhan, Nilofar Nisa (57215780194); Ravichandran, Sugirtha (57215773907); Vadivel, Vaibhava Shivani (57215773003); Suresh, Pavika (57215779894); Buddhan, Sasikumar (57215775143); Madhusudanan, Ajith (57215779897)",35119754800; 57215781442; 36606813200; 57215777490; 57215772245; 57215773320; 57215780194; 57215773907; 57215773003; 57215779894; 57215775143; 57215779897,Cognitive rehabilitation for autism children mental status observation using virtual reality based interactive environment,2020,Advances in Intelligent Systems and Computing,1131 AISC,,,1213,1218,5.0,5,10.1007/978-3-030-39512-4_185,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85081936097&doi=10.1007%2f978-3-030-39512-4_185&partnerID=40&md5=3e26423d021cfb0d4fe39bd0730258bc,"Proposed Cognitive Rehabilitation Therapy is to observe autism children mental status using virtual reality based interactive environment. Interactive environment is designed to analyze three levels of cognitive impairment decreasing functioning such as attention, reasoning, emotion, social behavior, decision making and language understanding. All the three levels were designed for mild autism children to have virtual walk through using HMD (Head Mount Device). We modeled a house and the behavior of the autism children were analyzed with day to day activities with the simple query to observe children language understanding. Observing the 5 autism children (3 boys and 2 girls in the age between 7 to 11) with the support of therapist gives promising improvement over the behavior in the virtual interactive environment and enhance their cognitive rehabilitation. Each level the therapist repeat the level, wherever children not responding. Observation results gives the children interact with virtual world with happy and positive emotions. In all the levels we incorporated children native language (Tamil Language) for the instruction, query and appreciation while they interact well. In future it is proposed to track their face, eye ball movement and predict the mental status of the children in detail to improve their overall mental status of autism children. © Springer Nature Switzerland AG 2020.",Assessment; Autism children; Mental state observation; Rehabilitation; Virtual reality,Behavioral research; Decision making; Diseases; Eye movements; Integration; Intelligent systems; Virtual reality; Assessment; Autism children; Cognitive impairment; Cognitive rehabilitation; Interactive Environments; Language understanding; Mental state; Virtual walk-through; Patient rehabilitation,Conference paper,Final,,Scopus,2-s2.0-85081936097,Gaming / VR
Sánchez-Morales A.; Durand-Rivera J.A.; Martínez-González C.L.,"Sánchez-Morales, A. (57217990566); Durand-Rivera, J.A. (57217988161); Martínez-González, C.L. (35520036000)",57217990566; 57217988161; 35520036000,Usability evaluation of a tangible user interface and serious game for identification of cognitive deficiencies in preschool children,2020,International Journal of Advanced Computer Science and Applications,11,6,,486,493,7.0,16,10.14569/IJACSA.2020.0110661,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85087825208&doi=10.14569%2fIJACSA.2020.0110661&partnerID=40&md5=139b508a66d8fbc8b437bb2b89961dea,"Detecting deficits in reading and writing literacy skills has been of great interest in the scientific community to correlate executive functions with future academic skills. In the present study, a prototype of a serious multimedia runner-type game was developed, Play with SID, designed to detect deficiencies in cognitive abilities in preschool children (sustained attention, memory, working memory, visuospatial abilities, and reaction time), before learning to read and write. Usability tests are used in Human-Computer Interaction to determine the feasibility of a system; it is the proof of concepts before the development of real systems. The aim of this paper was to evaluate the usability of the interface of the serious game, as well as the tangible user interface, a teddy bear with motion sensors. A usability study using the Wizard of Oz technique was conducted with 18 neurotypical preschool participants, ages 4 to 6. Concepts related to interactivity (interaction, the fulfillment of the activity objective, reaction to stimuli, and game time without distraction) were observed, as well as eye-tracking to assess attention and the Usability Scale System (SUS) to measure usability. According to the usability evaluation (confidence interval between 74.74% and 90.47%), the prototype has good to excellent usability, with no statistically significant differences between the age groups. The observed concept with the highest score was the game time without distraction. This characteristic will allow evaluating sustained attention. Also, we found out that the tangible interface use leads to the observation of laterality development, which will be added to the design of the serious game. The use of observation-based usability assessment techniques is useful for obtaining information from the participants when their communication skills are developing, and the expression of their perception in detail is limited. © Science and Information Organization.",HCI; Input device; Usability; User interface; Wizard of Oz,Eye tracking; Human computer interaction; Real time systems; Serious games; Usability engineering; Cognitive ability; Executive function; Input devices; Preschool children; Scientific community; Sustained attention; Tangible user interfaces; Usability evaluation; Wizard of Oz; Working memory; User interfaces,Article,Final,All Open Access; Gold Open Access,Scopus,2-s2.0-85087825208,Gaming / VR
Jeelani I.; Albert A.; Han K.,"Jeelani, Idris (57193824458); Albert, Alex (54894250700); Han, Kevin (56287693000)",57193824458; 54894250700; 56287693000,"Improving safety performance in construction using eye-tracking, visual data analytics, and virtual reality",2020,"Construction Research Congress 2020: Safety, Workforce, and Education - Selected Papers from the Construction Research Congress 2020",,,,395,404,9.0,12,10.1061/9780784482872.043,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85096941033&doi=10.1061%2f9780784482872.043&partnerID=40&md5=d3bba1f12985a8a718ed3e7ee6493411,"Globally, construction is among the most dangerous industries. Among others, research has demonstrated that construction workers and professionals fail to recognize and manage an unacceptable number of safety hazards. To address this, past research has focused on examining several factors including training, education, and management support that may indirectly influence hazard identification and management performance. However, proximal factors associated with poor hazard identification and management at the work interface has received very little attention. This paper summarizes our research aimed at (1) understanding why workers fail to identify and manage safety hazards as they participate in hazard recognition and management efforts and; (2) developing evidence-based interventions to counter poor hazard identification and management performance in the construction industry. This includes examining hazard recognition as a visual search task and understanding how search pattern (i.e., how workers examine the work environment) affects hazard recognition performance. More specifically, the research used eye-tracking technology to examine the relationship between how workers examine the workplace and the resulting hazard identification performance. Based on this new knowledge generated, new interventions were developed to improve hazard recognition and management performance. These include two interventions. First, an immersive hyper-realistic mixed reality training environment that was developed using stereoscopic visual data captured from real construction workplaces. The testing of the intervention with 56 participants suggested that the intervention can significantly improve hazard identification and management performance. Second, an AI-based system that detects hazardous conditions and objects in real-time to assist workers and managers. The system uses the live video captured by a wearable camera to localize the workers on a pre-built global map, detect any hazard present around the worker, and warns them in real-time. The system is tested in indoor and outdoor construction environments, which indicate 93% accuracy in detecting workers' proximity to hazards. © 2020 American Society of Civil Engineers.",,Accident prevention; Construction industry; Data Analytics; Eye tracking; Hazardous materials; Hazards; Human resource management; Mixed reality; Object detection; Stereo image processing; Construction environment; Construction workers; Eye tracking technologies; Hazard identification; Management efforts; Management support; Safety performance; Work environments; Occupational risks,Conference paper,Final,,Scopus,2-s2.0-85096941033,Gaming / VR
Zhu Q.; Wei P.; Shi Y.; Du J.,"Zhu, Qi (57209806243); Wei, Paul (57220105731); Shi, Yangming (57189999728); Du, Jing (57219889677)",57209806243; 57220105731; 57189999728; 57219889677,Cognitive benefits of human-robot collaboration in complex industrial operations: A virtual reality experiment,2020,Construction Research Congress 2020: Infrastructure Systems and Sustainability - Selected Papers from the Construction Research Congress 2020,,,,129,138,9.0,9,10.1061/9780784482858.015,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85096935836&doi=10.1061%2f9780784482858.015&partnerID=40&md5=94e12279b816bdd0ad98452f756abba9,"As a hallmark of Industry 4.0, collaborative robots (cobots) have been applied in various established industries such as manufacturing. In the construction industry, there is a growing interest and expectation regarding the use of cobots to improve productivity and safety. Although evidence from other industrial applications supports the benefits of cobots in performance improvement, the underlying mechanism remains unclear, let alone the evaluation and validation in construction operations. In this study, we hypothesize that the presence of cobots releases construction workers from less productive motor actions and enables workers to focus on work planning activities, which is more closely related to productivity. To test the hypothesis, we implemented a prototype of human-robot collaboration for a valve manipulation task that is commonly seen in industrial facility turnaround shutdown. A virtual reality (VR) model was created as the experiment platform, where participants (n=20) were asked to operate 24 valves in a given sequence according to the provided instructional information. The efficiency and task performance of test subjects were examined to quantify the benefits of robotic assistance, which was measured with an eye tracker. The results indicate that with the physical assistance of cobots, task performance was improved, and the user cognitive patterns were more preferred as they could allocate more neural resources and attention time on activity planning instead of repetitive motor actions. The findings are expected to set a foundation for better visibility, enhanced responsiveness, and better collaboration in the labor-intensive industry. © 2020 American Society of Civil Engineers.",,Accident prevention; Construction industry; Eye tracking; Industrial robots; Productivity; Sustainable development; Virtual reality; Collaborative robots; Construction operations; Construction workers; Experiment platforms; Human-robot collaboration; Industrial facilities; Industrial operations; Instructional information; Social robots,Conference paper,Final,,Scopus,2-s2.0-85096935836,Gaming / VR
Kobylinski P.; Pochwatko G.,"Kobylinski, Pawel (57194490703); Pochwatko, Grzegorz (55014223400)",57194490703; 55014223400,Visual attention convergence index for virtual reality experiences,2020,Advances in Intelligent Systems and Computing,1018,,,310,316,6.0,1,10.1007/978-3-030-25629-6_48,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069997665&doi=10.1007%2f978-3-030-25629-6_48&partnerID=40&md5=215c60f51ba53fe3db68804d3b2cc858,"The paper introduces a novel quantitative method in the domain of eye tracking (ET) for virtual reality (VR). The method might be of interest to researchers on the human factor in VR, behavioral psychologists, and designers of VR experiences. Several mathematical formulas describing a novel index quantifying convergence of visual attention are introduced. The index is based on recently developed distance variance [1], a function of distances between observations in metric spaces. An aggregated version of the visual attention convergence index introduced in the paper allows to measure the effectiveness of any system of attentional cues employed by a designer to guide the attention of VR experience participants along an intended narration line. An individual version of the index allows to capture individual differences in the convergence of visual attention across participants. Possibilities for real-life and academic usage of the index are discussed and example results of application to real VR ET data are summarized. © 2020, Springer Nature Switzerland AG.",Attention; Behavior; Distance variance; Energy statistics; Eye tracking; Human-technology interaction; Narration; Positional tracking; Psychology; Research methodology; Social sciences; User experience; Virtual Reality,Eye tracking; Social sciences; Social sciences computing; Virtual reality; Attention; Behavior; Distance variances; Energy statistics; Human-technology interaction; Narration; Psychology; Research methodologies; User experience; Behavioral research,Conference paper,Final,,Scopus,2-s2.0-85069997665,Gaming / VR
Shi Y.; Zheng Y.; Du J.; Zhu Q.; Liu X.,"Shi, Yangming (57189999728); Zheng, Yingfei (57220105990); Du, Jing (57219889677); Zhu, Qi (57209806243); Liu, Xin (57220101169)",57189999728; 57220105990; 57219889677; 57209806243; 57220101169,The impact of engineering information complexity on working memory development of construction workers: An eye-tracking investigation,2020,Construction Research Congress 2020: Infrastructure Systems and Sustainability - Selected Papers from the Construction Research Congress 2020,,,,89,98,9.0,3,10.1061/9780784482858.011,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85096932299&doi=10.1061%2f9780784482858.011&partnerID=40&md5=06bcfa2ff4258f56c04e74071fda7433,"Owing to the increasing complexity of construction tasks and operations performed in confined workplaces, workers rely progressively on working memory, i.e., the short-term and temporary storage of information pertaining to near future events, to ensure the seamless execution of construction tasks. Although literature has discovered a strong relationship between engineering information formats and the quality of working memory, there is still a clear theoretical disagreement on the implications of the complexity of engineering information in the development of working memory. This study addresses the knowledge gap with a human-subject experiment (n=60). Participants were required to review one of the two instructions for a pipe maintenance task: a simple 2D isometric drawing with bulletins (2D-simple) and a complex 2D isometric drawing with rich text (2D-complex). After the review session, the participants were asked to perform the pipe maintenance task in a virtual reality (VR) environment. Collected data include participants' task performance (accuracy and time), pupillary dilations, and gaze movements. The results show that the 2D-simple group outperformed the 2D-complex group in terms of both accuracy and time. An attention pattern analysis using approximate entropy (ApEn) of gaze movements suggests that a higher ApEn in the vertical axis, i.e., a more irregular and complex gaze movement between instructions, may result in a more efficient use of working memory and thus contributes to a better performance. This study provides preliminary evidence regarding the impact of engineering information complexity on the working memory development of construction workers. © 2020 American Society of Civil Engineers.",,Computational complexity; Digital storage; Professional aspects; Sustainable development; Approximate entropy; Construction workers; Engineering information; Human subject experiments; Maintenance tasks; Pattern analysis; Pupillary dilation; Temporary storage; Eye tracking,Conference paper,Final,,Scopus,2-s2.0-85096932299,Gaming / VR
Chen M.; Jin Y.; Goodall T.; Yu X.; Bovik A.C.,"Chen, Meixu (57212195714); Jin, Yize (57225874348); Goodall, Todd (55513291000); Yu, Xiangxu (57203982855); Bovik, Alan Conrad (56984291600)",57212195714; 57225874348; 55513291000; 57203982855; 56984291600,Study of 3D Virtual Reality Picture Quality,2020,IEEE Journal on Selected Topics in Signal Processing,14,1,,89,102,13.0,59,10.1109/JSTSP.2019.2956408,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85076160873&doi=10.1109%2fJSTSP.2019.2956408&partnerID=40&md5=efdb4e9b39d805b12bbbca6643069594,"Virtual Reality (VR) and its applications have attracted significant and increasing attention. However, the requirements of much larger file sizes, different storage formats, and immersive viewing conditions pose significant challenges to the goals of acquiring, transmitting, compressing and displaying high quality VR content. Towards meeting these challenges, it is important to be able to understand the distortions that arise and that can affect the perceived quality of displayed VR content. It is also important to develop ways to automatically predict VR picture quality. Meeting these challenges requires basic tools in the form of large, representative subjective VR quality databases on which VR quality models can be developed and which can be used to benchmark VR quality prediction algorithms. Towards making progress in this direction, here we present the results of an immersive 3D subjective image quality assessment study. In the study, 450 distorted images obtained from 15 pristine 3D VR images modified by 6 types of distortion of varying severities were evaluated by 42 subjects in a controlled VR setting. Both the subject ratings as well as eye tracking data were recorded and made available as part of the new database, in hopes that the relationships between gaze direction and perceived qualitymight be better understood.We also evaluated several publicly available IQA models on the new database, and also report a statistical evaluation of the performances of the compared IQA models.  © 2019 IEEE.",full reference; human perception; Image quality assessment; immersive image database; virtual reality,Benchmarking; Database systems; Digital storage; Eye tracking; Image quality; Quality control; 3D virtual reality; Full references; Human perception; Image database; Image quality assessment; Immersive; Immersive image database; Perceived quality; Picture quality; Signal-processing; Virtual reality,Article,Final,All Open Access; Green Open Access,Scopus,2-s2.0-85076160873,Gaming / VR
Mañas-Viniegra L.; Veloso A.-I.; Sierra-Sánchez J.,"Mañas-Viniegra, Luis (57197747570); Veloso, Ana-Isabel (35422995200); Sierra-Sánchez, Javier (55579423600)",57197747570; 35422995200; 55579423600,Immersive content with violence: A research using eye tracking with university students in Spain and Portugal; [Contenidos inmersivos violentos: Investigación con eye tracking en jóvenes universitarios en España y Portugal],2020,Profesional de la Informacion,29,1,e290108,,,,12,10.3145/epi.2020.ene.08,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85082006353&doi=10.3145%2fepi.2020.ene.08&partnerID=40&md5=d7fe9943d150b422edf3f51f1adafbbd,"Newspapers have been affected by a progressive loss of readership over the last decade, especially among those under 24 years old. This group continues to stay informed about topics of their interest, but through social networks. For this reason, the press has incorporated a playful, immersive, and participatory journalism based on technologies such as virtual reality or 360 videos. Through quantitative and qualitative analysis of data obtained from eye tracking, and by subsequently performing semi-structured interviews, this research verifies the attention paid by Spanish and Portuguese university students to violent, immersive, journalistic content. Students’ understanding and awareness of this informative material with violent events is analyzed. The capability of immersive journalism to capture the attention of the public under 24 years old and to raise awareness about social problems based on visualization of violent content is highlighted. © 2020, El Profesional de la Informacion. All rights reserved.",360 video; Digital journalism; Eye tracking; Immersive journalism; Portugal; Serious games; Spain; Users studies; UX (user experience); Violence; Virtual reality; VR; Young people,,Article,Final,,Scopus,2-s2.0-85082006353,Gaming / VR
Guilei S.,"Guilei, Sun (57209601640)",57209601640,Research on Location of Emergency Sign Based on Virtual Reality and Eye Tracking Technology,2020,Advances in Intelligent Systems and Computing,973,,,401,408,7.0,1,10.1007/978-3-030-20476-1_40,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85068228505&doi=10.1007%2f978-3-030-20476-1_40&partnerID=40&md5=2cf418d8b50f5cf6579e334f65f9a033,"In order to analyze the attention to exit sign during the emergency state, virtual reality technology (VR) was used to simulate escape scene and 3D Max was used to design the scene. To obtain the number of gaze points and the gaze duration of subjects, eye tracker was utilized to get data and spss 21.0 was taken advantage of to analyze the data. Results show that the position of emergency exit sign has significant influence on the recognition. And the exit sign of the height of 1.0 m on the front of the observer&#x2019;s line of sight is the most beneficial to discovery and identification. Moreover, the height and the position of exit sign have no significant influence on reading time. &#x00A9; 2020, Springer Nature Switzerland AG.",Exit sign; Eye tracking; Fixation time; Height; Identification,Human engineering; Identification (control systems); Virtual reality; Wearable technology; Emergency exit; Emergency state; Exit sign; Eye tracking technologies; Fixation time; Height; Line of Sight; Virtual reality technology; Eye tracking,Conference paper,Final,,Scopus,2-s2.0-85068228505,Gaming / VR
Li Z.; Akkil D.; Raisamo R.,"Li, Zhenxing (57205733604); Akkil, Deepak (56023371300); Raisamo, Roope (35610443700)",57205733604; 56023371300; 35610443700,Gaze-based Kinaesthetic Interaction for Virtual Reality,2020,Interacting with Computers,32,1,,17,32,15.0,9,10.1093/iwcomp/iwaa002,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85091319137&doi=10.1093%2fiwcomp%2fiwaa002&partnerID=40&md5=0dcad3f2afa855badb91e248549b56b4,"Kinaesthetic interaction using force-feedback devices is promising in virtual reality. However, the devices are currently not suitable for interactions within large virtual spaces because of their limited workspace. We developed a novel gaze-based kinaesthetic interface that employs the user’s gaze to relocate the device workspace. The workspace switches to a new location when the user pulls the mechanical arm of the device to its reset position and gazes at the new target. This design enables the robust relocating of device workspace, thus achieving an infinite interaction space, and simultaneously maintains a flexible hand-based kinaesthetic exploration. We compared the new interface with the scaling-based traditional interface in an experiment involving softness and smoothness discrimination. Our results showed that the gaze-based interface performs better than the traditional interface, in terms of efficiency and kinaesthetic perception. It improves the user experience for kinaesthetic interaction in virtual reality without increasing eye strain. © The Author(s) 2020. Published by Oxford University Press on behalf of The British Computer Society. All rights reserved. For Permissions, please email: journals.permissions@oup.com",Fatigue; Force-feedback device; Gaze tracking; Kinaesthetic interaction; Virtual reality; Workspace,User experience; Eye strain; Force feedback devices; Limited workspaces; Mechanical arm; Virtual spaces; Virtual reality,Article,Final,,Scopus,2-s2.0-85091319137,Gaming / VR
Jin S.; Qing C.; Xu X.; Wang Y.,"Jin, Shan (57215415341); Qing, Chunmei (24336280700); Xu, Xiangmin (55706178700); Wang, Yang (54934443300)",57215415341; 24336280700; 55706178700; 54934443300,Emotion recognition using eye gaze based on shallow cnn with identity mapping,2020,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),11691 LNAI,,,65,75,10.0,3,10.1007/978-3-030-39431-8_7,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85080938035&doi=10.1007%2f978-3-030-39431-8_7&partnerID=40&md5=5627a03e0087f3a9aef720cc7e4fe49d,"Machine recognition of human emotions has attracted more and more attention for its wide application in recent years. As a spontaneous signal of human behavior, eye gaze is utilized for emotion recognition. Compared with electroencephalogram (EEG) signals, eye gaze data is more available, and it can be used in many practical applications, such as virtual reality (VR). In this paper, a new method of human emotion recognition based on eye gaze is proposed. Firstly, a new set of eye gaze features is proposed which consists of eye gaze sequences, extracted statistical feature sequences and spectral feature sequences. Then, the eye gaze feature set is input into the convolutional layer to extract high-level features. Finally, these high-level features and the eye gaze feature set are combined to complete the mapping of features to human emotions. Experiments on MAHNOB-HCI dataset demonstrate the effectiveness of this method. © Springer Nature Switzerland AG 2020.",Arousal; Emotion recognition; Eye gaze; Shallow CNN with identity mapping; Valence,Biomedical signal processing; Brain; Cognitive systems; Electroencephalography; Mapping; Speech recognition; Virtual reality; Arousal; Emotion recognition; Eye-gaze; Identity mappings; Valence; Behavioral research,Conference paper,Final,,Scopus,2-s2.0-85080938035,Gaming / VR
de Oliveira A.; Khamis M.; Esteves A.,"de Oliveira, Ana (57221084814); Khamis, Mohamed (35243028400); Esteves, Augusto (36674891300)",57221084814; 35243028400; 36674891300,"Using a VR field study to assess the effects of visual and haptic cues in ""in-the-wild"" locomotion",2020,CEUR Workshop Proceedings,2779,,,,,,1,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85098187877&partnerID=40&md5=170c8784d33c91b8735b20cd3ad71a8f,"This work aims to assess the effect of visual and haptic cues in users with gait impairments; not only in performance, but also in terms of usability, perceived cognitive load, and safety. These haptic cues were delivered via wrist-worn devices, with the goal of supporting these users while out in-the-wild - three types of haptic cues were tested. To further assess the impact of haptic and visual cues outside of a laboratory environment, we used a Virtual Reality Field Study to safely assess the impact of these cues in users' awareness of their surroundings (measured via gaze hits and dwell). Despite conducting a preliminary study with participants not suffering from gait impairments (N=6), our results seem to indicate a positive effect of the haptic cues in regards to participant cadence, step length, and general awareness of their surroundings when compared to the visual cue. One of the simpler haptic cues was also the preferred stimulus by all participants. Copyright © 2020 for this paper by its authors.",Attention; Eye-tracking; Gait; Haptic Cues; Parkinson's Disease; Usability; Virtual Field Study; Virtual-reality; Visual cues,Cognitive loads; Field studies; Laboratory environment; Step length; Visual cues,Conference paper,Final,,Scopus,2-s2.0-85098187877,Gaming / VR
Huang Y.; Richter E.; Kleickmann T.; Scheiter K.; Richter D.,"Huang, Yizhen (57214226921); Richter, Eric (57204350319); Kleickmann, Thilo (36992400200); Scheiter, Katharina (6507519353); Richter, Dirk (36195062400)",57214226921; 57204350319; 36992400200; 6507519353; 36195062400,"Body in motion, attention in focus: A virtual reality study on teachers' movement patterns and noticing",2023,Computers and Education,206,,104912,,,,8,10.1016/j.compedu.2023.104912,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85169820757&doi=10.1016%2fj.compedu.2023.104912&partnerID=40&md5=e8a7020b55f07dd8ec8f70829bf24209,"When people navigate a space to perform tasks, their body and eye movements are closely linked. Within the classroom context, characteristics of teachers' body movements may be related to the noticing of relevant classroom events, in particular, visual attention to student disruptions. In the current study, we investigated this relationship in an immersive virtual reality (IVR) classroom that offered a standardized environment for tracking teachers' body and eye movements. Based on time series data collected during a short teaching task with 21 preservice teachers, we conducted K-means clustering with body movement features. We identified three distinctive patterns, which we labeled as immobile, anchored, and dynamic (body) movement patterns. Teachers with dynamic movement patterns venture away from the teacher's desk to far corners of the room; they don't dwell in one location for long but rather move continuously to various parts of the classroom, creating a dispersed movement. Dynamic movement patterns were associated with the best visual attention performance, defined as the number, speed, and duration of fixations on a classroom disruption. Our findings demonstrate the existence of unique and differentiable movement patterns among preservice teachers that have implications for teacher noticing, teacher–student interaction, and instructional quality. © 2023 The Authors",Augmented and virtual reality; Data science applications in education; Improving classroom teaching; Simulations; Teacher professional development,Behavioral research; E-learning; K-means clustering; Students; Virtual reality; Application in education; Augmented and virtual realities; Body movements; Data science application in education; Improving classroom teaching; Movement pattern; Science applications; Simulation; Teacher professional development; Teachers'; Eye movements,Article,Final,All Open Access; Hybrid Gold Open Access,Scopus,2-s2.0-85169820757,Gaming / VR
Huizeling E.; Alday P.M.; Peeters D.; Hagoort P.,"Huizeling, Eleanor (57218448505); Alday, Phillip M. (56046419200); Peeters, David (53464091900); Hagoort, Peter (7003301986)",57218448505; 56046419200; 53464091900; 7003301986,Combining EEG and 3D-eye-tracking to study the prediction of upcoming speech in naturalistic virtual environments: A proof of principle,2023,Neuropsychologia,191,,108730,,,,2,10.1016/j.neuropsychologia.2023.108730,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85177855696&doi=10.1016%2fj.neuropsychologia.2023.108730&partnerID=40&md5=db93a14a5e53c8d23f91d77f8ea7bfde,"EEG and eye-tracking provide complementary information when investigating language comprehension. Evidence that speech processing may be facilitated by speech prediction comes from the observation that a listener's eye gaze moves towards a referent before it is mentioned if the remainder of the spoken sentence is predictable. However, changes to the trajectory of anticipatory fixations could result from a change in prediction or an attention shift. Conversely, N400 amplitudes and concurrent spectral power provide information about the ease of word processing the moment the word is perceived. In a proof-of-principle investigation, we combined EEG and eye-tracking to study linguistic prediction in naturalistic, virtual environments. We observed increased processing, reflected in theta band power, either during verb processing - when the verb was predictive of the noun - or during noun processing - when the verb was not predictive of the noun. Alpha power was higher in response to the predictive verb and unpredictable nouns. We replicated typical effects of noun congruence but not predictability on the N400 in response to the noun. Thus, the rich visual context that accompanied speech in virtual reality influenced language processing compared to previous reports, where the visual context may have facilitated processing of unpredictable nouns. Finally, anticipatory fixations were predictive of spectral power during noun processing and the length of time fixating the target could be predicted by spectral power at verb onset, conditional on the object having been fixated. Overall, we show that combining EEG and eye-tracking provides a promising new method to answer novel research questions about the prediction of upcoming linguistic input, for example, regarding the role of extralinguistic cues in prediction during language comprehension. © 2023 The Authors",EEG; Eye-tracking; Language comprehension; Language prediction; Virtual reality; Visual world paradigm,Comprehension; Electroencephalography; Evoked Potentials; Eye-Tracking Technology; Female; Humans; Male; Speech; Speech Perception; adult; article; attention; controlled study; electroencephalogram; electroencephalography; eye tracking; eye-tracking technology; female; gaze; human; human experiment; language processing; male; normal human; prediction; speech; virtual reality; word processing; young adult; comprehension; evoked response; eye-tracking technology; physiology; speech; speech perception,Article,Final,All Open Access; Green Open Access; Hybrid Gold Open Access,Scopus,2-s2.0-85177855696,Gaming / VR
Mehringer W.; Stoeve M.; Krauss D.; Ring M.; Steussloff F.; Güttes M.; Zott J.; Hohberger B.; Michelson G.; Eskofier B.,"Mehringer, Wolfgang (57211242822); Stoeve, Maike (57201075454); Krauss, Daniel (58537835700); Ring, Matthias (55546847500); Steussloff, Fritz (58537810500); Güttes, Moritz (58537860400); Zott, Julia (58537835800); Hohberger, Bettina (23008815800); Michelson, Georg (7006760310); Eskofier, Bjoern (26428080900)",57211242822; 57201075454; 58537835700; 55546847500; 58537810500; 58537860400; 58537835800; 23008815800; 7006760310; 26428080900,Virtual reality for assessing stereopsis performance and eye characteristics in Post-COVID,2023,Scientific Reports,13,1,13167,,,,3,10.1038/s41598-023-40263-w,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85168066473&doi=10.1038%2fs41598-023-40263-w&partnerID=40&md5=7d410f52d23f376ed8e2d7026ca64ccf,"In 2019, we faced a pandemic due to the coronavirus disease (COVID-19), with millions of confirmed cases and reported deaths. Even in recovered patients, symptoms can be persistent over weeks, termed Post-COVID. In addition to common symptoms of fatigue, muscle weakness, and cognitive impairments, visual impairments have been reported. Automatic classification of COVID and Post-COVID is researched based on blood samples and radiation-based procedures, among others. However, a symptom-oriented assessment for visual impairments is still missing. Thus, we propose a Virtual Reality environment in which stereoscopic stimuli are displayed to test the patient’s stereopsis performance. While performing the visual tasks, the eyes’ gaze and pupil diameter are recorded. We collected data from 15 controls and 20 Post-COVID patients in a study. Therefrom, we extracted features of three main data groups, stereopsis performance, pupil diameter, and gaze behavior, and trained various classifiers. The Random Forest classifier achieved the best result with 71% accuracy. The recorded data support the classification result showing worse stereopsis performance and eye movement alterations in Post-COVID. There are limitations in the study design, comprising a small sample size and the use of an eye tracking system. © 2023, Springer Nature Limited.",,COVID-19; Depth Perception; Eye Movements; Humans; Virtual Reality; Vision Disorders; coronavirus disease 2019; depth perception; eye movement; human; physiology; virtual reality; visual disorder,Article,Final,All Open Access; Gold Open Access; Green Open Access,Scopus,2-s2.0-85168066473,Gaming / VR
Zinina A.A.; Zaidelman L.Y.; Kotov A.A.; Velichovsky B.M.,"Zinina, A.A. (57034067600); Zaidelman, L. Ya. (57203319992); Kotov, A.A. (57033665400); Velichovsky, B.M. (58921232600)",57034067600; 57203319992; 57033665400; 58921232600,"Reflex or Reflection? The Oculomotor Behavior of the Companion Robot, Creating the Impression of Communicating with an Emotional Being",2023,Scientific and Technical Information Processing,50,5,,500,511,11.0,0,10.3103/S0147688223050179,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85186757383&doi=10.3103%2fS0147688223050179&partnerID=40&md5=11f28beb49c535da8ee2f45f6d4c7efc,"Abstract: The control system of the F-2 companion robot implements a competitive system of rules (scenarios) to model the robot’s reactions to a wide range of events. The system is designed in such a way as to provide balanced responses by the robot to speech utterances and other events recognized by the computer vision system (orientation of the user’s face and gaze, events in the tangram game), as well as to the user’s touches. In this experiment, we apply this system to evaluate two robots that are able to determine the orientation of a person’s face and the direction of the gaze and respond differently to his attention. The implicit reactions of a person to the robot’s gaze and the problems of differences between reflexive and reflex behavior in eye movements in comparison with other communicative actions are considered. © Allerton Press, Inc. 2023.",attention; emotional agents; emotional interfaces; eye-to-eye contact; human-machine interaction; social gaze,Computer games; Human robot interaction; Speech recognition; Attention; Communicative actions; Companion robot; Computer vision system; Emotional agents; Emotional interface; Eye-to-eye contact; Human machine interaction; Social gaze; Speech utterance; Eye movements,Article,Final,,Scopus,2-s2.0-85186757383,Gaming / VR
Mendez-Encinas D.; Sujar A.; Bayona S.; Delgado-Gomez D.,"Mendez-Encinas, David (58027691200); Sujar, Aaron (57202589141); Bayona, Sofia (24821639800); Delgado-Gomez, David (16202707400)",58027691200; 57202589141; 24821639800; 16202707400,Attention and impulsivity assessment using virtual reality games,2023,Scientific Reports,13,1,13689,,,,9,10.1038/s41598-023-40455-4,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85168656530&doi=10.1038%2fs41598-023-40455-4&partnerID=40&md5=55c64b4b5901dcc35f6ff53ad95104af,"The assessment of cognitive functions is mainly based on standardized neuropsychological tests, widely used in various fields such as personnel recruitment, education, or health. This paper presents a virtual reality game that allows collecting continuous measurements of both the performance and behaviour of the subject in an immersive, controllable, and naturalistic experience. The application registers variables related to the user’s eye movements through the use of virtual reality goggles, as well as variables of the game performance. We study how virtual reality can provide data to help predict scores on the Attention Control Scale Test and the Barratt Impulsiveness Scale. We design the application and test it with a pilot group. We build a random forest regressor model to predict the attention and impulsivity scales’ total score. When evaluating the performance of the model, we obtain a positive correlation with attention (0.434) and with impulsivity (0.382). In addition, our model identified that the most significant variables are the time spent looking at the target or at distractors, the eye movements variability, the number of blinks and the pupil dilation in both attention and impulsivity. Our results are consistent with previous results in the literature showing that it is possible to use data collected in virtual reality to predict the degree of attention and impulsivity. © 2023, Springer Nature Limited.",,Cognition; Educational Status; Eye Movements; Impulsive Behavior; Virtual Reality; cognition; educational status; eye movement; impulsiveness; virtual reality,Article,Final,All Open Access; Gold Open Access; Green Open Access,Scopus,2-s2.0-85168656530,Gaming / VR
Seligman L.D.; Geers A.L.; Kramer L.; Clemens K.S.; Pituch K.A.; Colagiuri B.; Marusak H.A.; Rabinak C.A.; Turner N.; Nedley M.,"Seligman, Laura D. (7003523797); Geers, Andrew L. (7006117410); Kramer, Lauren (58078217100); Clemens, Kelly S. (57217280687); Pituch, Keenan A. (9938791000); Colagiuri, Ben (24511982300); Marusak, Hilary A. (55812452900); Rabinak, Christine A. (21835019800); Turner, Natalie (58079230400); Nedley, Michael (6603097181)",7003523797; 7006117410; 58078217100; 57217280687; 9938791000; 24511982300; 55812452900; 21835019800; 58079230400; 6603097181,Study protocol of an investigation of attention and prediction error as mechanisms of action for latent inhibition of dental fear in humans,2023,BMC Psychology,11,1,23,,,,4,10.1186/s40359-023-01054-0,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85146878863&doi=10.1186%2fs40359-023-01054-0&partnerID=40&md5=2acd37c6b7e588e99c5d296b7f213d3e,"Background: Evidence suggests that dental anxiety and phobia are frequently the result of direct associative fear conditioning but that pre-exposure to dental stimuli prior to conditioning results in latent inhibition of fear learning. The mechanisms underlying the pre-exposure effect in humans, however, are poorly understood. Moreover, pain sensitivity has been linked to dental fear conditioning in correlational investigations and theory suggests it may moderate the latent inhibition effect, but this hypothesis has not been directly tested. These gaps in our understanding are a barrier to the development of evidence-based dental phobia prevention efforts. Methods: Healthy volunteers between the ages of 6 and 35 years will be enrolled across two sites. Participants will complete a conditioning task in a novel virtual reality environment, allowing for control over pre-exposure and the examination of behaviour. A dental startle (a brief, pressurized puff of air to a tooth) will serve as the unconditioned stimulus. Using a within-subjects experimental design, participants will experience a pre-exposed to-be conditioned stimulus, a non-pre-exposed to-be conditioned stimulus, and a neutral control stimulus. Two hypothesized mechanisms, changes in prediction errors and attention, are expected to mediate the association between stimulus condition and fear acquisition, recall, and retention. To ascertain the involvement of pain sensitivity, this construct will be measured through self-report and the cold pressor task. Discussion: Dental phobia negatively affects the dental health and overall health of individuals. This study aims to determine the mechanisms through which pre-exposure retards conditioned dental fear acquisition, recall, and retention. A randomized control trial will be used to identify these mechanisms so that they can be precisely targeted and maximally engaged in preventative efforts. © 2023, The Author(s).",Dental phobia; Eye tracking; Fear learning; Latent inhibition; Pain sensitivity; Pre-exposure; Virtual reality,Adolescent; Adult; Attention; Child; Dental Anxiety; Humans; Learning; Memory; Pain; Young Adult; adolescent; adult; attention; child; dental anxiety; human; learning; memory; pain; young adult,Article,Final,All Open Access; Gold Open Access; Green Open Access,Scopus,2-s2.0-85146878863,Gaming / VR
Chen J.; Shi Y.; Li N.,"Chen, Jieyu (57037186900); Shi, Yin (59446148700); Li, Nan (56643518400)",57037186900; 59446148700; 56643518400,The role of selective attention in emergency wayfinding: An eye tracking-integrated virtual reality experiment,2023,Safety Science,168,,106320,,,,19,10.1016/j.ssci.2023.106320,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85172688336&doi=10.1016%2fj.ssci.2023.106320&partnerID=40&md5=1f87ff0aede927134bc7b07adcc36bed,"Emergencies, such as fire and earthquake, highly jeopardize human safety in indoor environments. In emergencies, acute stress activated by the threat of hazards influences human wayfinding efficiency and safety. However, the underlying cognitive basis of the influence of acute stress on the wayfinding process remains unclear. To narrow this knowledge gap, we focused on the core cognitive process, namely selective attention during emergency wayfinding, and conducted a virtual reality (VR)-based wayfinding experiment with a within-subject design. Three stress level settings were designed to induce participants’ stress. For each trial, participants were required to finish the wayfinding task within five minutes, during which a VR version of the Eriksen flanker task (EFT) was set to evaluate selective attention with the eye tracking function embedded in the VR helmet. The result demonstrated the positive effect of stress on the information process speed in selective attention measured by response time and the wayfinding performance measured by net distance. In addition, we found that the response time in selective attention plays a mediation role in the relationship between stress in the sympathetic nervous system (SNS) and wayfinding performance. This study provides researchers with a new pathway to investigate the cognitive mechanism in emergency wayfinding with VR and eye tracking technologies. The findings are expected to advance the understanding of the underlying cognitive mechanism in emergency wayfinding, which has significant implications for developing evacuation simulation considering cognitive processes, as well as for evacuation planning of complex indoor environments and crowd evacuation management during indoor emergencies. © 2023 Elsevier Ltd",Attention; Eriksen flanker task; Eye tracking; Stress; Virtual reality; Wayfinding,Cognitive systems; Response time (computer systems); Virtual reality; hydrocortisone; Attention; Cognitive mechanisms; Cognitive process; Eriksen flanker task; Eye-tracking; Human safety; Indoor environment; Performance; Selective attention; Way finding; acute stress; adrenergic system; adult; Article; emergency evacuation; emergency status; eye fixation; eye tracking; female; gaze; human; human experiment; male; mental stress; normal human; processing speed; reaction time; selective attention; spatial learning; spatial orientation; virtual reality; Eye tracking,Article,Final,,Scopus,2-s2.0-85172688336,Gaming / VR
Knipschild R.; Klip H.; van Leeuwaarden D.; van Onna M.J.R.; Lindauer R.J.L.; Staal W.G.; Bicanic I.A.E.; de Jongh A.,"Knipschild, Rik (57208594394); Klip, Helen (57057534900); van Leeuwaarden, Doenja (58561088100); van Onna, Mariken J. R. (58561297000); Lindauer, Ramon J. L. (57190078383); Staal, Wouter G. (6603635034); Bicanic, Iva A. E. (14059392100); de Jongh, Ad (7007023691)",57208594394; 57057534900; 58561088100; 58561297000; 57190078383; 6603635034; 14059392100; 7007023691,"Treatment of multiple traumatized adolescents by enhancing regulation skills and reducing trauma related symptoms: rationale, study design, and methods of randomized controlled trial (the Mars-study)",2023,BMC Psychiatry,23,1,644,,,,3,10.1186/s12888-023-05073-4,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85169698863&doi=10.1186%2fs12888-023-05073-4&partnerID=40&md5=09ed37120b8e74f8f9b0c47bf7c5fc69,"Background: There is ongoing debate regarding the treatment of severe and multiple traumatized children and adolescents with post-traumatic stress disorder (PTSD). Many clinicians favor a phase-based treatment approach (i.e., a stabilization phase prior to trauma-focused therapy) over immediate trauma-focused psychological treatment, despite the lack of scientific evidence. Research on the effects of different treatment approaches is needed for children and adolescents with (symptoms of complex) PTSD resulting from repeated sexual and/or physical abuse during childhood. Objective: This paper describes the rationale, study design, and methods of the MARS-study, a two-arm randomized controlled trial (RCT) that aims to compare the results of phase-based treatment with those of immediate trauma-focused treatment and determine whether immediate trauma-focused treatment is not worse than phase-based treatment in reducing PTSD symptoms. Methods: Participants are individuals between 12 and 18 years who meet the diagnostic criteria for PTSD due to repeated sexual abuse, physical abuse, or domestic violence during childhood. Participants will be blindly allocated to either the phase-based or immediate trauma-focused treatment condition. In the phase-based treatment condition, participants receive 12 sessions of the Dutch version of Skill Training in Affective and Interpersonal Regulation (STAIR-A), followed by 12 sessions of EMDR therapy. In the immediate trauma-focused condition, the participants receive 12 sessions of EMDR therapy. The two groups are compared for several outcome variables before treatment, mid-treatment (only in the phase-based treatment condition), after 12 trauma-focused treatment sessions (post-treatment), and six months post-treatment (follow-up). The main parameter is the presence and severity of PTSD symptoms (Clinician-Administered PTSD Scale for Children and Adolescents, CAPS-CA). The secondary outcome variables are the severity of complex PTSD symptoms (Interpersonal Problems as measured by the Experiences in Close Relationship-Revised, ECR-RC; Emotion Regulation as measured by the Difficulties in Emotion Regulation Scale, DERS; Self Esteem as measured by the Rosenberg Self Esteem Scale, RSES), changes in anxiety and mood symptoms (Revised Anxiety and Depression Scale; RCADS), changes in posttraumatic cognitions (Child Posttraumatic Cognitions Inventory, CPTCI), changes in general psychopathology symptoms (Child Behavior Checklist, CBCL), and Quality of Life (Youth Outcome Questionnaire, Y-OQ-30). Furthermore, parental stress (Opvoedingsvragenlijst, OBVL) and patient-therapist relationship (Feedback Informed Treatment, FIT) will be measured, whereas PTSD symptoms will be monitored in each session during both treatment conditions (Children’s Revised Impact of Event Scale, CRIES-13). Discussion: Treating (symptoms of complex) PTSD in children and adolescents with a history of repeated sexual and/or physical abuse during childhood is of great importance. However, there is a lack of consensus among trauma experts regarding the optimal treatment approach. The results of the current study may have important implications for selecting effective treatment options for clinicians working with children and adolescents who experience the effects of exposure to multiple interpersonal traumatic events during childhood. Trial registrations: The study was registered on the “National Trial Register (NTR)” with the number NTR7024. This registry was obtained from the International Clinical Trial Registry Platform (ICTRP) and can be accessed through the ICTRP Search Portal (https://trialsearch.who.int/). © 2023, BioMed Central Ltd., part of Springer Nature.",Adolescents; Complex PTSD; EMDR; Phase-based treatment; PTSD; Stabilization; STAIR; TRAP; Trauma,"Adolescent; Affect; Anxiety; Anxiety Disorders; Child; Humans; Research Design; Stress Disorders, Post-Traumatic; adolescent; adolescent health; age; anxiety assessment; Article; child abuse; Child Behavior Checklist; Child Posttraumatic Cognitions Inventroy; clinical effectiveness; clinical outcome; controlled study; CRIES-13; depression assessment; disease severity; domestic violence; emotion regulation; eye movement desensitization and reprocessing; female; follow up; gender; human; intermethod comparison; male; mental health; parental stress; physical abuse; posttraumatic stress disorder; psychotherapy; quality of life assessment; randomized controlled trial; Revised Anxiety and Depression Scale; Rosenberg Self-Esteem Scale; sexual abuse; single blind procedure; skill training in affective and interpersonal regulation; treatment duration; treatment response; Youth Outcome Questionnaire; affect; anxiety; anxiety disorder; child; methodology; posttraumatic stress disorder",Article,Final,All Open Access; Gold Open Access; Green Open Access,Scopus,2-s2.0-85169698863,Gaming / VR
Ging-Jehli N.R.; Arnold L.E.; Van Zandt T.,"Ging-Jehli, Nadja R. (57215704760); Arnold, L. Eugene (7202014301); Van Zandt, Trish (7004408051)",57215704760; 7202014301; 7004408051,Cognitive-attentional mechanisms of cooperation—with implications for attention-deficit hyperactivity disorder and cognitive neuroscience,2023,"Cognitive, Affective and Behavioral Neuroscience",23,6,,1545,1567,22.0,1,10.3758/s13415-023-01129-w,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85173081503&doi=10.3758%2fs13415-023-01129-w&partnerID=40&md5=9f47b58f059de8d9774d3a29c62af5cf,"People’s cooperativeness depends on many factors, such as their motives, cognition, experiences, and the situation they are in. To date, it is unclear how these factors interact and shape the decision to cooperate. We present a computational account of cooperation that not only provides insights for the design of effective incentive structures but also redefines neglected social-cognitive characteristics associated with attention-deficit hyperactivity disorder (ADHD). Leveraging game theory, we demonstrate that the source and magnitude of conflict between different motives affected the speed and frequency of cooperation. Integrating eye-tracking to measure motivation-based information processing during decision-making shows that participants’ visual fixations on the gains of cooperation rather than its costs and risks predicted their cooperativeness on a trial-by-trial basis. Using Bayesian hierarchical modeling, we find that a situation’s prosociality and participants’ past experience each bias the decision-making process distinctively. ADHD characteristics explain individual differences in responsiveness across contexts, highlighting the clinical importance of experimentally studying reactivity in social interactions. We demonstrate how the use of eye-tracking and computational modeling can be used to experimentally investigate social-cognitive characteristics in clinical populations. We also discuss possible underlying neural mechanisms to be investigated in future studies. © 2023, The Psychonomic Society, Inc.",Attention-deficit hyperactivity disorder; Bayesian hierarchical modeling; Computational modeling; Computational psychiatry; Cooperation; Experimental economics; Eye-tracking; Game theory; Social cognition,Attention Deficit Disorder with Hyperactivity; Bayes Theorem; Cognition; Cognitive Neuroscience; Humans; Motivation; attention deficit hyperactivity disorder; Bayes theorem; cognition; cognitive neuroscience; human; motivation; psychology,Article,Final,,Scopus,2-s2.0-85173081503,Gaming / VR
Shadpour S.; Shafqat A.; Toy S.; Jing Z.; Attwood K.; Moussavi Z.; Shafiei S.B.,"Shadpour, Saeed (57222266800); Shafqat, Ambreen (57195965523); Toy, Serkan (57886862500); Jing, Zhe (57201395479); Attwood, Kristopher (53263257600); Moussavi, Zahra (56274135500); Shafiei, Somayeh B. (56577959100)",57222266800; 57195965523; 57886862500; 57201395479; 53263257600; 56274135500; 56577959100,Developing cognitive workload and performance evaluation models using functional brain network analysis,2023,npj Aging,9,1,22,,,,15,10.1038/s41514-023-00119-z,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85177718939&doi=10.1038%2fs41514-023-00119-z&partnerID=40&md5=e38ab5b2c1755b47ab5d5b486bee3724,"Cognition, defined as the ability to learn, remember, sustain attention, make decisions, and solve problems, is essential in daily activities and in learning new skills. The purpose of this study was to develop cognitive workload and performance evaluation models using features that were extracted from Electroencephalogram (EEG) data through functional brain network and spectral analyses. The EEG data were recorded from 124 brain areas of 26 healthy participants conducting two cognitive tasks on a robot simulator. The functional brain network and Power Spectral Density features were extracted from EEG data using coherence and spectral analyses, respectively. Participants reported their perceived cognitive workload using the SURG-TLX questionnaire after each exercise, and the simulator generated actual performance scores. The extracted features, actual performance scores, and subjectively assessed cognitive workload values were used to develop linear models for evaluating performance and cognitive workload. Furthermore, the Pearson correlation was used to find the correlation between participants’ age, performance, and cognitive workload. The findings demonstrated that combined EEG features retrieved from spectral analysis and functional brain networks can be used to evaluate cognitive workload and performance. The cognitive workload in conducting only Matchboard level 3, which is more challenging than Matchboard level 2, was correlated with age (0.54, p-value = 0.01). This finding may suggest playing more challenging computer games are more helpful in identifying changes in cognitive workload caused by aging. The findings could open the door for a new era of objective evaluation and monitoring of cognitive workload and performance. © 2023, Springer Nature Limited.",,,Article,Final,All Open Access; Green Open Access; Hybrid Gold Open Access,Scopus,2-s2.0-85177718939,Gaming / VR
Jang J.Y.,"Jang, Ju Yeun (57200132687)",57200132687,Analyzing visual behavior of consumers in a virtual reality fashion store using eye tracking,2023,Fashion and Textiles,10,1,24,,,,9,10.1186/s40691-023-00345-9,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85162944635&doi=10.1186%2fs40691-023-00345-9&partnerID=40&md5=d7070fc9e72ec722fb1408c9550f3331,"This study investigated the visual behavior of consumers within an immersive virtual reality (VR) fashion store on the basis of their fashion involvement. Their shopping motivation was considered as a moderator. A total of 23 consumers participated, and the participants’ actual visual behaviors were recorded in real-time during their store experience section using an eye-tracking device attached to a VR head-mounted display. Results revealed that the greater the consumer’s fashion involvement, the greater their attention to the store area, and the greater their ability to observe more diverse areas in the store. Consumers with higher fashion involvement and browsing motivation spent less time focusing on the product area. Meanwhile, consumers with higher fashion involvement and searching motivation spent more time focusing on the product area. Visual attention to the store area positively affected experience satisfaction, and the effect of visual attention to the product area on satisfaction was moderated by consumers’ shopping motivation. © 2023, The Author(s).",Eye-tracking; Immersive virtual reality; Satisfaction; Shopping motivation; Virtual reality store; Visual behavior,,Article,Final,All Open Access; Gold Open Access,Scopus,2-s2.0-85162944635,Gaming / VR
Diemer J.; Mühlberger A.; Yassouridis A.; Zwanzger P.,"Diemer, Julia (24830524400); Mühlberger, Andreas (6507375232); Yassouridis, Alexander (7004195735); Zwanzger, Peter (7004191083)",24830524400; 6507375232; 7004195735; 7004191083,Distraction versus focusing during VR exposure therapy for acrophobia: A randomized controlled trial,2023,Journal of Behavior Therapy and Experimental Psychiatry,81,,101860,,,,9,10.1016/j.jbtep.2023.101860,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85153946062&doi=10.1016%2fj.jbtep.2023.101860&partnerID=40&md5=b43c7463b2f234f8f1719b32f9f506f2,"Background and Objectives: The therapeutic mechanisms of exposure therapy are not well understood. Research suggests that focusing on the most feared aspect might not be necessary, and that distraction with a low cognitive load (e.g., conversation) might enhance exposure. We aimed at systematically testing the efficacy of exposure therapy with focusing vs. conversational distraction, hypothesizing that distracted exposure would yield superior effects. Methods: Thirty-eight patients with acrophobia (specific phobia of heights; clinician-determined) (free from relevant somatic or other mental disorders) were randomly assigned (1:1) to one virtual reality (VR) session of either focused (n = 20) or distracted exposure (n = 18). This monocentric trial took place at a psychiatric university hospital. Results: Both conditions resulted in a significant reduction of acrophobic fear and avoidance, and a significant increase of self-efficacy (primary outcome variables). However, condition did not have a significant effect on any of these variables. Effects were stable at four-week follow-up. Heart rate and skin conductance level indicated significant arousal, but did not differ between conditions. Limitations: Eye-tracking was unavailable, nor did we assess emotions other than fear. Power was limited due to sample size. Conclusions: A balanced exposure protocol combining attention to fear cues with conversational distraction, while not being superior, might be as effective as focused exposure for acrophobia, at least during the initial stages of exposure therapy. These results support previous findings. This study demonstrates how VR can be exploited for therapy process research, as VR supports dismantling designs and the incorporation of online process measures. © 2023 Elsevier Ltd",Anxiety disorders; Distraction; Exposure therapy; Virtual reality,Acrophobia; Fear; Humans; Phobic Disorders; Virtual Reality; Virtual Reality Exposure Therapy; acrophobia; Acrophobia Questionnaire; adult; anxiety assessment; Article; attention; avoidance behavior; clinical article; controlled study; distracted exposure; DSM-IV-TR; fear; female; focused exposure; follow up; heart rate; human; male; outcome assessment; outcome variable; psychotherapy; randomized controlled trial; self concept; skin conductance; software; Structured Clinical Interview for DSM Disorders; therapy effect; virtual reality exposure therapy; acrophobia; phobia; procedures; psychology; virtual reality,Article,Final,,Scopus,2-s2.0-85153946062,Gaming / VR
Villegas E.; Fonts E.; Fernández M.; Fernández-Guinea S.,"Villegas, Eva (35225572000); Fonts, Elisabet (58753480800); Fernández, Marta (58753687400); Fernández-Guinea, Sara (6603639844)",35225572000; 58753480800; 58753687400; 6603639844,Visual Attention and Emotion Analysis Based on Qualitative Assessment and Eyetracking Metrics—The Perception of a Video Game Trailer,2023,Sensors,23,23,9573,,,,4,10.3390/s23239573,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85179131151&doi=10.3390%2fs23239573&partnerID=40&md5=fce01ee58dbb9b44a1a071eac0392dbd,"Video game trailers are very useful tools for attracting potential players. This research focuses on analyzing the emotions that arise while viewing video game trailers and the link between these emotions and storytelling and visual attention. The methodology consisted of a three-step task test with potential users: the first step was to identify the perception of indie games; the second step was to use the eyetracking device (gaze plot, heat map, and fixation points) and link them to fixation points (attention), viewing patterns, and non-visible areas; the third step was to interview users to understand impressions and questionnaires of emotions related to the trailer’s storytelling and expectations. The results show an effective assessment of visual attention together with visualization patterns, non-visible areas that may affect game expectations, fixation points linked to very specific emotions, and perceived narratives based on the gaze plot. The innovation in the mixed methodological approach has made it possible to obtain relevant data regarding the link between the emotions perceived by the user and the areas of attention collected with the device. The proposed methodology enables developers to understand the strengths and weaknesses of the information being conveyed so that they can tailor the trailer to the expectations of potential players. © 2023 by the authors.",emotions analysis; eyetracker analysis; human-centered design; user experience; video game; vision-based sensing; visual attention,Emotions; Perception; Video Games; Visual Perception; Behavioral research; Computer games; Eye tracking; Game design; Human computer interaction; Interactive computer graphics; Emotion analysis; Eye trackers; Eyetracker analyse; Fixation point; Human-centred designs; Qualitative assessments; Users' experiences; Video-games; Vision based sensing; Visual Attention; emotion; perception; psychology; video game; vision; Light trailers,Article,Final,All Open Access; Gold Open Access,Scopus,2-s2.0-85179131151,Gaming / VR
Selaskowski B.; Asché L.M.; Wiebe A.; Kannen K.; Aslan B.; Gerding T.M.; Sanchez D.; Ettinger U.; Kölle M.; Lux S.; Philipsen A.; Braun N.,"Selaskowski, Benjamin (57660098000); Asché, Laura Marie (57203968667); Wiebe, Annika (57659776400); Kannen, Kyra (57449172200); Aslan, Behrem (57216769975); Gerding, Thiago Morano (58081013300); Sanchez, Dario (58080436600); Ettinger, Ulrich (6602766172); Kölle, Markus (36466613100); Lux, Silke (7005576971); Philipsen, Alexandra (6603416864); Braun, Niclas (55972275100)",57660098000; 57203968667; 57659776400; 57449172200; 57216769975; 58081013300; 58080436600; 6602766172; 36466613100; 7005576971; 6603416864; 55972275100,Gaze-based attention refocusing training in virtual reality for adult attention-deficit/hyperactivity disorder,2023,BMC Psychiatry,23,1,74,,,,15,10.1186/s12888-023-04551-z,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85146955961&doi=10.1186%2fs12888-023-04551-z&partnerID=40&md5=a0e8f740a1358c51261e95d12d09c849,"Background: Attention-deficit/hyperactivity disorder (ADHD) is characterized by substantial interindividual heterogeneity that challenges the systematic assessment and treatment. Considering mixed evidence from previous neurofeedback research, we present a novel feedback system that relies on gaze behavior to detect signs of inattention while performing a neuropsychological attention task in a virtual seminar room. More specifically, an audiovisual feedback was given whenever participants averted their gaze from the given task. Methods: Eighteen adults with ADHD and 18 healthy controls performed a continuous performance task (CPT) in virtual reality under three counterbalanced conditions in which either gaze-based feedback, sham feedback, or no feedback was provided. In all conditions, phases of high and low virtual distraction alternated. CPT errors and reaction times, proportions of gaze dwell times (e.g., task focus or distraction focus), saccade characteristics, EEG theta/beta ratios, head movements, and an experience sampling of ADHD symptoms were analyzed. Results: While patients can be discriminated well from healthy controls in that they showed more omission errors, higher reaction times, higher distraction-related dwell times, and more head movements, the feedback did not immediately improve task performance. It was also indicated that sham feedback was rather associated with an aggravation of symptoms in patients. Conclusions: Our findings demonstrate sufficient suitability and specificity for this holistic ADHD symptom assessment. Regarding the feedback, a single-session training was insufficient to achieve learning effects based on the proposed metacognitive strategies. Future longitudinal, multi-session trials should conclusively examine the therapeutic efficacy of gaze-based virtual reality attention training in ADHD. Trial registration: drks.de (identifier: DRKS00022370). © 2023, The Author(s).",ADHD; Adults; Attention training; Continuous performance task; Distractors; EEG; Eye-tracking; Metacognition; Self-regulation; Therapy; Treatment; Virtual reality,Adult; Attention; Attention Deficit Disorder with Hyperactivity; Humans; Neurofeedback; Reaction Time; Virtual Reality; adult; Article; attention deficit hyperactivity disorder; autoregulation; clinical article; cognitive rehabilitation; continuous performance test; controlled study; disease assessment; disease exacerbation; dwell time; electroencephalogram; experimental error; eye tracking; feedback system; female; gaze; holistic care; human; learning; male; metacognitive monitoring; neuropsychological assessment; reaction time; sham procedure; task performance; virtual reality; attention; neurofeedback; psychology,Article,Final,All Open Access; Gold Open Access; Green Open Access,Scopus,2-s2.0-85146955961,Gaming / VR
Bernal-Berdun E.; Martin D.; Malpica S.; Perez P.J.; Gutierrez D.; Masia B.; Serrano A.,"Bernal-Berdun, Edurne (58664086000); Martin, Daniel (57220544695); Malpica, Sandra (57208263238); Perez, Pedro J. (58663703000); Gutierrez, Diego (7005194565); Masia, Belen (57189656516); Serrano, Ana (56121468300)",58664086000; 57220544695; 57208263238; 58663703000; 7005194565; 57189656516; 56121468300,D-SAV360: A Dataset of Gaze Scanpaths on 360° Ambisonic Videos,2023,IEEE Transactions on Visualization and Computer Graphics,29,11,,4350,4360,10.0,3,10.1109/TVCG.2023.3320237,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85174846846&doi=10.1109%2fTVCG.2023.3320237&partnerID=40&md5=868ec6fdf6b61a7e2c310a56f1a26591,"Understanding human visual behavior within virtual reality environments is crucial to fully leverage their potential. While previous research has provided rich visual data from human observers, existing gaze datasets often suffer from the absence of multimodal stimuli. Moreover, no dataset has yet gathered eye gaze trajectories (i.e., scanpaths) for dynamic content with directional ambisonic sound, which is a critical aspect of sound perception by humans. To address this gap, we introduce D-SAV360, a dataset of 4,609 head and eye scanpaths for 360° videos with first-order ambisonics. This dataset enables a more comprehensive study of multimodal interaction on visual behavior in virtual reality environments. We analyze our collected scanpaths from a total of 87 participants viewing 85 different videos and show that various factors such as viewing mode, content type, and gender significantly impact eye movement statistics. We demonstrate the potential of D-SAV360 as a benchmarking resource for state-of-the-art attention prediction models and discuss its possible applications in further research. By providing a comprehensive dataset of eye movement data for dynamic, multimodal virtual environments, our work can facilitate future investigations of visual behavior and attention in virtual reality.  © 2023 IEEE.",360Â° Videos; Ambisonics; Dataset; Fixations; Gaze; Saliency,"Attention; Computer Graphics; Eye Movements; Fixation, Ocular; Humans; Virtual Reality; Audio recordings; Behavioral research; Benchmarking; Stereo image processing; Virtual reality; 360° video; Ambisonics; Behavioral science; Dataset; Fixation; Gaze; Predictive models; Saliency; Solid modelling; Video; attention; computer graphics; eye fixation; eye movement; human; virtual reality; Eye movements",Article,Final,All Open Access; Green Open Access,Scopus,2-s2.0-85174846846,Gaming / VR
Popa L.L.; Chira D.; Strilciuc Ș.; Mureșanu D.F.,"Popa, Livia Livinț (53985180400); Chira, Diana (57219529052); Strilciuc, Ștefan (56731101200); Mureșanu, Dafin F. (6603418219)",53985180400; 57219529052; 56731101200; 6603418219,Non-Invasive Systems Application in Traumatic Brain Injury Rehabilitation,2023,Brain Sciences,13,11,1594,,,,12,10.3390/brainsci13111594,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85178278961&doi=10.3390%2fbrainsci13111594&partnerID=40&md5=c1872fe7475b057a6c5184215ccccc81,"Traumatic brain injury (TBI) is a significant public health concern, often leading to long-lasting impairments in cognitive, motor and sensory functions. The rapid development of non-invasive systems has revolutionized the field of TBI rehabilitation by offering modern and effective interventions. This narrative review explores the application of non-invasive technologies, including electroencephalography (EEG), quantitative electroencephalography (qEEG), brain–computer interface (BCI), eye tracking, near-infrared spectroscopy (NIRS), functional near-infrared spectroscopy (fNIRS), magnetic resonance imaging (MRI), functional magnetic resonance imaging (fMRI), magnetoencephalography (MEG), and transcranial magnetic stimulation (TMS) in assessing TBI consequences, and repetitive transcranial magnetic stimulation (rTMS), low-level laser therapy (LLLT), neurofeedback, transcranial direct current stimulation (tDCS), transcranial alternative current stimulation (tACS) and virtual reality (VR) as therapeutic approaches for TBI rehabilitation. In pursuit of advancing TBI rehabilitation, this narrative review highlights the promising potential of non-invasive technologies. We emphasize the need for future research and clinical trials to elucidate their mechanisms of action, refine treatment protocols, and ensure their widespread adoption in TBI rehabilitation settings. © 2023 by the authors.",non-invasive technologies; rehabilitation; traumatic brain injury,attention; blood oxygenation; blood vessel reactivity; BOLD signal; cognition; electroencephalography; executive function; eye tracking; functional magnetic resonance imaging; functional near-infrared spectroscopy; human; low level laser therapy; magnetoencephalography; motor performance; near infrared spectroscopy; nerve cell plasticity; neurofeedback; neuromodulation; neuroprotection; neurorehabilitation; non invasive measurement; nuclear magnetic resonance imaging; oxygen saturation; processing speed; quantitative electroencephalography; repetitive transcranial magnetic stimulation; Review; saccadic eye movement; synaptogenesis; transcranial alternating current stimulation; transcranial direct current stimulation; transcranial magnetic stimulation; traumatic brain injury; virtual reality,Review,Final,All Open Access; Gold Open Access; Green Open Access,Scopus,2-s2.0-85178278961,Gaming / VR
Schroeder P.A.; Gehrer N.A.; Reents M.; Reimer N.; Vagedes J.; Svaldi J.,"Schroeder, Philipp A. (55927033500); Gehrer, Nina A. (57194728839); Reents, Mareike (58691549000); Reimer, Nele (58691267400); Vagedes, Jan (55312227500); Svaldi, Jennifer (6507259442)",55927033500; 57194728839; 58691549000; 58691267400; 55312227500; 6507259442,Body Dissatisfaction Directs Avatar Perception: Embodiment and Selective Visual Attention in Body Mass-Modified Self-Avatars,2023,"Cyberpsychology, Behavior, and Social Networking",26,11,,850,860,10.0,3,10.1089/cyber.2022.0385,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85176440835&doi=10.1089%2fcyber.2022.0385&partnerID=40&md5=c60f173001ffb4dff81cd9d7cd51cf71,"Human agents immersed in metaverse technologies such as virtual reality (VR) are routinely disconnected from their actual physical appearance and embodied in another virtual body, referred to as self-avatar. Such body transformations can have implications for patients with eating disorders, or persons with extreme body dissatisfaction (BD). Changes in BD, weight anxiety, or body image are theoretically linked to visual selective attention, which can be measured with eye tracking. In the present study, 43 women with high or low BD were immersed in animated body weight-manipulated self-avatars in VR. Before a brief mirror exposure with their self-avatars, they experienced synchronous visuomotor and visuo-tactile contingencies in VR to increase embodiment, delivered through small movement exercises with real-time animation from first-person perspective and passive haptics. In a crossover study design, self-avatar weight was manipulated (normal weight vs. overweight) in both groups (low BD vs. high BD), and subjective experience was assessed before and after exposure. In contrast to our hypotheses, BD was not affected by the self-avatar condition. Embodiment decreased during mirror exposure, possibly due to the avatars wearing head-mounted displays. Interestingly, disembodiment was stronger in women with low BD. Furthermore, eye tracking showed that participants with high BD looked longer at weight-related body parts when immersed in the overweight self-avatar, whereas participants with low BD looked longer at weight-related body parts when immersed in the normal weight self-avatar. Overall, the results support body-specific visual attention and suggest that particularly participants with low BD show stronger disembodiment during self-avatar mirror exposure, possibly alleviating momentary body experience. Preregistration: https://doi.org/10.23668/psycharchives.4949 © Mary Ann Liebert, Inc.",body dissatisfaction; eye tracking; selective attention; self-avatar; virtual reality,Body Dissatisfaction; Body Image; Cross-Over Studies; Female; Humans; Overweight; Virtual Reality; body dissatisfaction; body image; crossover procedure; female; human; obesity; virtual reality,Article,Final,All Open Access; Green Open Access,Scopus,2-s2.0-85176440835,Gaming / VR
Monteiro P.; Coelho H.; Gonçalves G.; Melo M.; Bessa M.,"Monteiro, Pedro (57201131572); Coelho, Hugo (57201129186); Gonçalves, Guilherme (57212862503); Melo, Miguel (7102354924); Bessa, Maximino (14031038800)",57201131572; 57201129186; 57212862503; 7102354924; 14031038800,Exploring the user experience of hands-free VR interaction methods during a Fitts’ task,2023,Computers and Graphics (Pergamon),117,,,1,12,11.0,5,10.1016/j.cag.2023.10.005,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85174237516&doi=10.1016%2fj.cag.2023.10.005&partnerID=40&md5=ea9ea694ed33b94c04c0fcf709aa339f,"Despite advancements in interaction with immersive Virtual Reality (VR) systems, using hand gestures for all interactions still imposes some challenges, especially in interactions with graphical user interfaces that are usually performed with point-and-click interfaces. Therefore, exploring the use of alternative hands-free methods for selection is essential to overcome usability problems and provide natural interaction for users. The results and insights gained from this exploration can lead to enhanced user experiences in VR applications. This study aims to contribute to the literature with the evaluation of the usability of the most commonly used hands-free methods for selection and system control tasks in immersive VR and their impact on standard and validated experience and usability metrics, namely the sense of presence, cybersickness, system usability, workload, and user satisfaction. A Fitts’ selection task was performed using a within-subjects design by nine participants experienced in VR. The methods evaluated were the handheld controllers, the head gaze, eye gaze, and voice commands for pointing at the targets, and dwell time and voice commands to confirm the selections. Results show that the methods provide similar levels of sense of presence and low cybersickness while showing low workload values and high user satisfaction, matching the experience of traditional handheld controllers for non-multimodal approaches. The assisted eye gaze with dwell was the preferred hands-free method and the one with the highest values of usability. Still, developers should minimize the number of gaze movements to reduce fatigue. The evaluation also showed that using a multimodal approach for selections, especially using the voice, decreases user satisfaction and increases users’ frustration. © 2023 The Author(s)",Hands-free; HCI; Immersive Virtual Reality; Interaction; Usability,Graphical user interfaces; Usability engineering; Cybersickness; Eye-gaze; Handheld controllers; Hands-free; Immersive virtual reality; Interaction; Sense of presences; Usability; Users' experiences; Users' satisfactions; Virtual reality,Article,Final,All Open Access; Hybrid Gold Open Access,Scopus,2-s2.0-85174237516,Gaming / VR
Ichino J.; Ide M.; Yoshiki T.; Yokoyama H.; Asano H.; Miyachi H.; Okabe D.,"Ichino, Junko (6507470781); Ide, Masahiro (57222163566); Yoshiki, Takehito (58774889600); Yokoyama, Hitomi (55921295400); Asano, Hirotoshi (28567521600); Miyachi, Hideo (36799203100); Okabe, Daisuke (56224860300)",6507470781; 57222163566; 58774889600; 55921295400; 28567521600; 36799203100; 56224860300,How Gaze Visualization Facilitates Initiation of Informal Communication in 3D Virtual Spaces,2023,ACM Transactions on Computer-Human Interaction,31,1,5,,,,8,10.1145/3617368,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85180365588&doi=10.1145%2f3617368&partnerID=40&md5=3de97f85659f066d5c2433bb5829ec67,"This study explores how gaze visualization in virtual spaces facilitates the initiation of informal communication. Three styles of gaze cue visualization (arrow, bubbles, and miniature avatar) with two types of gaze behavior (one-sided gaze and joint gaze) were evaluated. 96 participants used either a non-visualized gaze cue or one of the three visualized gaze cues. The results showed that all visualized gaze cues facilitated the initiation of informal communication more effectively than the non-visualized gaze cue. For one-sided gaze, overall, bubbles had more positive effects on the gaze receiver’s behaviors and experiences than the other two visualized gaze cues, although the only statistically significant difference was in the verbal reaction rates. For joint gaze, all three visualized gaze cues had positive effects on the receiver’s behaviors and experiences. The design implications of the gaze visualization and the confederate-based evaluation method contribute to research on informal communication and social virtual reality. © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Eye gaze; Gaze cue; Informal communication; Joint attention; Joint gaze; Multiuser VR; One-sided gaze; Shared Gaze Visualization; Social cue; Social VR,Reaction rates; Three dimensional computer graphics; Virtual reality; Eye-gaze; Gaze cue; Informal communication; Joint attention; Joint gaze; Multiuser VR; Multiusers; One-sided gaze; Shared gaze visualization; Shared gazes; Social cues; Social VR; Visualization,Article,Final,All Open Access; Bronze Open Access,Scopus,2-s2.0-85180365588,Gaming / VR
Fu M.; Liu R.; Liu Q.,"Fu, Meiqing (57197871462); Liu, Rui (36045152400); Liu, Qipeng (58636695900)",57197871462; 36045152400; 58636695900,How individuals sense environments during indoor emergency wayfinding: An eye-tracking investigation,2023,Journal of Building Engineering,79,,107854,,,,13,10.1016/j.jobe.2023.107854,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85173588257&doi=10.1016%2fj.jobe.2023.107854&partnerID=40&md5=c6e5780885ea5a1ccf8d56be622d500e,"Emergency wayfinding is a critical aspect of indoor evacuations. People typically rely on environmental cues to determine their location and navigate their way. However, there is limited exploration into how individuals perceive their surroundings during emergency wayfinding. Through a virtual reality (VR) controlled experiment integrated with eye-tracking, this study explores how individuals sense the environment during evacuations. Participants were immersed in a virtual scenario simulating a building fire and had to choose between Routes 1 and 2 at an intersection for evacuations. The experimental results show individuals' visual attention is positively related to their wayfinding choices. Specifically, familiarity has a significant influence on individuals' visual attention and route choices. Participants who were familiar with Route 2 before evacuations showed increased fixation frequency but smaller saccade velocity during evacuations, reducing their likelihood of detecting the exit sign at the intersection and using Route 1. Neighbor behavior influenced participants' visual information searching and route choices. When virtual neighbors evacuated via Route 1 at the intersection, participants gazed at Route 1 more and were also more likely to use it. The visibility of the exit door on Route 1 also affected both participants' visual attention and route choices. However, the change in emergency lighting did not significantly influence participants’ route choices or their visual attention to Route 1 and the exit sign. These findings enrich our knowledge of human emergency wayfinding and hold significant implications for enhancing building design and evacuation planning. © 2023 Elsevier Ltd",Eye-tracking; Indoor evacuation; Visual attention; VR experiment; Wayfinding,Architectural design; Behavioral research; Emergency lighting; Eye movements; Virtual reality; Building fires; Controlled experiment; Environmental cues; Eye-tracking; Indoor evacuation; Route choice; Virtual reality experiment; Virtual scenario; Visual Attention; Way finding; Eye tracking,Article,Final,,Scopus,2-s2.0-85173588257,Gaming / VR
Thammineni C.; Manjunatha H.; Esfahani E.T.,"Thammineni, Chaitanya (57221693757); Manjunatha, Hemanth (57192710332); Esfahani, Ehsan T. (16424554200)",57221693757; 57192710332; 16424554200,Selective eye-gaze augmentation to enhance imitation learning in Atari games,2023,Neural Computing and Applications,35,32,,23401,23410,9.0,4,10.1007/s00521-021-06367-y,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85112378475&doi=10.1007%2fs00521-021-06367-y&partnerID=40&md5=56c77fd8ed566b0b83cd1d6dfc30b632,"This paper presents the selective use of eye-gaze information in learning human actions in Atari games. Extensive evidence suggests that our eye movements convey a wealth of information about the direction of our attention and mental states and encode the information necessary to complete a task. Based on this evidence, we hypothesize that selective use of eye-gaze, as a clue for attention direction, will enhance the learning from demonstration. For this purpose, we propose a selective eye-gaze augmentation (SEA) network that learns when to use the eye-gaze information. The proposed network architecture consists of three sub-networks: gaze prediction, gating, and action prediction network. Using the prior 4 game frames, a gaze map is predicted by the gaze prediction network, which is used for augmenting the input frame. The gating network will determine whether the predicted gaze map should be used in learning and is fed to the final network to predict the action at the current frame. To validate this approach, we use publicly available Atari Human Eye-Tracking And Demonstration (Atari-HEAD) dataset consists of 20 Atari games with 28 million human demonstrations and 328 million eye-gazes (over game frames) collected from four subjects. We demonstrate the efficacy of selective eye-gaze augmentation compared to the state-of-the-art Attention Guided Imitation Learning (AGIL) and Behavior Cloning (BC). The results indicate that the selective augmentation approach (the SEA network) performs significantly better than the AGIL and BC. Moreover, to demonstrate the significance of selective use of gaze through the gating network, we compare our approach with the random selection of the gaze. Even in this case, the SEA network performs significantly better, validating the advantage of selectively using the gaze in demonstration learning. © 2021, The Author(s), under exclusive licence to Springer-Verlag London Ltd., part of Springer Nature.",Human-in-the-loop learning; Imitation learning; Learning by demonstration,Demonstrations; Eye movements; Eye tracking; Information use; Network architecture; Eye-gaze; Human actions; Human-in-the-loop; Human-in-the-loop learning; Imitation learning; Learn+; Learning by demonstration; Learning from demonstration; Mental state; Wealth of information; Forecasting,Article,Final,All Open Access; Green Open Access,Scopus,2-s2.0-85112378475,Gaming / VR
Oberfrank L.; Kruse L.; Steinicke F.,"Oberfrank, Lukas (58686113500); Kruse, Lucie (57203976551); Steinicke, Frank (8883314100)",58686113500; 57203976551; 8883314100,Sphere Saber: A Virtual Reality Exergame to Study Age-Related Differences in Selective Visual Attention and Information Processing,2023,Proceedings - SUI 2023: ACM Symposium on Spatial User Interaction,,,22,,,,1,10.1145/3607822.3614534,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85176085807&doi=10.1145%2f3607822.3614534&partnerID=40&md5=17153ee4baf0e26d57177f93171be7db,"Virtual reality (VR) applications enable users to perform physical exercises from home. Due to the absence of a real trainer, visual cues are typically used to provide important information about game mechanics or the current tasks. In this paper, we investigate age-related differences regarding visual perception and processing of these game cues during a VR exergame. Therefore, we conducted a comparative user study with nine older and nine younger adults, in which participants had to hit balls of different colors, which approached them. Our results show that younger participants performed better in the game than older participants, especially if a dual-task with additional information was presented. This implies an effect of age on both, task performance, as well as information processing and memory. Eye-tracking data suggests that these differences are not caused by not seeing the visual information signs that were presented, but rather differences in the processing. Furthermore, older adults reported a higher feeling of presence and lower simulator sickness scores. These insights provide important implications for the development of VR exergames for different age groups, especially if additional game information needs to be displayed.  © 2023 ACM.",ageing; Exergames; memory; older adults; virtual reality; visual attention,Behavioral research; Eye tracking; 'current; Age-related differences; Exergames; Older adults; Physical exercise; Visual Attention; Visual cues; Visual information; Visual perception; Visual-processing; Virtual reality,Conference paper,Final,,Scopus,2-s2.0-85176085807,Gaming / VR
Kruse L.; Mostajeran F.; Steinicke F.,"Kruse, Lucie (57203976551); Mostajeran, Fariba (57197835374); Steinicke, Frank (8883314100)",57203976551; 57197835374; 8883314100,The Influence of Virtual Agent Visibility in Virtual Reality Cognitive Training,2023,Proceedings - SUI 2023: ACM Symposium on Spatial User Interaction,,,14,,,,3,10.1145/3607822.3614526,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85176115786&doi=10.1145%2f3607822.3614526&partnerID=40&md5=030edcafdfef077039736993637572cd,"For avatars - virtual bodys controlled by a human - an established visualization is to display incomplete visualizations, e.g. only head and torso. However, the preference for the body visibility of intelligent virtual agents (IVAs) - fully computer generated virtual humans - may differ. Additionally, the presence of IVAs can have a psychological effect on users that are similar to that of a real human, e.g. facilitation or inhibition of cognitive performance. To investigate the connection between these two topics, we examine the effects of an IVA's level of body visibility on the users' sense of social presence and task performance in virtual reality. In a within-subject user study, 30 participants solved anagram tasks in the presence of five different levels of visibility of our IVA: voice-only, mouth-only, head-only, upper body and full body. While we could not find any differences in the task performance of the users, lower levels of visibility led to a decreased feeling of social presence. Furthermore, by using eye tracking, we found that visually rich representations were looked at for a longer amount of time, but only during the explanation of the task. Afterwards, the users did not pay much visual attention to the agent anymore. Finally, preferences of the users show that the chosen representation is dependent on some factors; most importantly, it should support the users, but not distract them from their task.  © 2023 ACM.",cognitive training; exergames; virtual agents; virtual reality; visual representation,Behavioral research; E-learning; Eye tracking; Intelligent virtual agents; Visualization; Cognitive performance; Cognitive training; Computer generated; Exergames; Psychological effects; Social presence; Task performance; Virtual agent; Virtual humans; Visual representations; Visibility,Conference paper,Final,,Scopus,2-s2.0-85176115786,Gaming / VR
Qiu M.; Guo Y.; Zhang M.; Zhang J.; Lan T.; Liu Z.,"Qiu, Mengyu (58685928100); Guo, Yi (57372183600); Zhang, Mingguang (57418071400); Zhang, Jingwei (58023356200); Lan, Tian (58711492000); Liu, Zhilin (58686305200)",58685928100; 57372183600; 57418071400; 58023356200; 58711492000; 58686305200,Simulating Human Visual System Based on Vision Transformer,2023,Proceedings - SUI 2023: ACM Symposium on Spatial User Interaction,,,35,,,,3,10.1145/3607822.3616408,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85176099660&doi=10.1145%2f3607822.3616408&partnerID=40&md5=0775c9073defee37ad0f9af1235dd441,"The human visual system (HVS) is capable of responding in real-time to complex visual environments. During the process of freely observing visual scenes, predicting eye movements and visual fixations is a task known as scanpath prediction, which aims to simulate the HVS. In this paper, we propose a visual transformer-based model to study the attentional processes of the human visual system in analyzing visual scenes, thereby achieving scanpath prediction. This technology has important applications in human-computer interaction, virtual reality, augmented reality, and other fields. We have significantly simplified the workflow of scanpath prediction and the overall model architecture, achieving performance superior to existing methods.  © 2023 ACM.",fixation duration prediction; saccade Sequences; scene analysis; visual attention; Visual scanpath prediction,Augmented reality; Behavioral research; Eye movements; Human computer interaction; Virtual reality; Duration predictions; Fixation duration; Fixation duration prediction; Human Visual System; Saccade sequence; Scan path; Scene analysis; Visual Attention; Visual scanpath prediction; Visual scene; Forecasting,Conference paper,Final,,Scopus,2-s2.0-85176099660,Gaming / VR
Woodworth J.W.; Borst C.W.; Rahman Y.; Kulshreshth A.,"Woodworth, Jason Wolfgang (57192676048); Borst, Christoph W (9736479200); Rahman, Yitoshee (57215358246); Kulshreshth, Arun (55315352400)",57192676048; 9736479200; 57215358246; 55315352400,Study of Visual Guidance Cues in VR Field Trips at High Schools,2023,"Proceedings of the ACM Symposium on Virtual Reality Software and Technology, VRST",,,80,,,,1,10.1145/3611659.3617213,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85175253025&doi=10.1145%2f3611659.3617213&partnerID=40&md5=836a9d5be9bde59a0390547a0dde164c,"We assess the effectiveness of attention guidance cues in an educational platform in local high schools with real students. Three eye-tracked visual cues, previously assessed for their ability to guide and restore attention, are compared against a baseline absence of cue in a VR field trip of a virtual solar energy field. Students experienced four presentations on solar energy production including in-world animations and teacher imagery, in three of which the visual cues guided attention to the relevant object or teacher in the scene. Attention guidance using visual cues is commonly studied using ""search and selection""style tasks, but has not been studied in the context of maintaining attention in real-world environments. © 2023 Owner/Author.",,Eye tracking; Students; Educational platforms; Energy fields; Energy productions; Field trips; Guidance cues; Higher School; Local higher schools; Teachers'; Visual cues; Visual guidance; Solar energy,Conference paper,Final,,Scopus,2-s2.0-85175253025,Gaming / VR
Jeong D.; Paik S.; Noh Y.T.; Han K.,"Jeong, Dayoung (57998453100); Paik, Seungwon (57218931280); Noh, YoungTae (36105411600); Han, Kyungsik (58643233700)",57998453100; 57218931280; 36105411600; 58643233700,"MAC: multimodal, attention-based cybersickness prediction modeling in virtual reality",2023,Virtual Reality,27,3,,2315,2330,15.0,15,10.1007/s10055-023-00804-0,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85160249328&doi=10.1007%2fs10055-023-00804-0&partnerID=40&md5=098451cb753a2ba323db2b1fd858d55e,"Cybersickness is one of the greatest barriers to the adoption of virtual reality. A growing body of research has focused on identifying the characteristics of cybersickness and finding ways to mitigate it through the utilization of data from VR content, physiological signals, and body movement, along with artificial intelligence techniques. In this work, we extend prior research on cybersickness prediction by considering the role of different data modalities. We propose a novel deep learning model named multimodal, attention-based cybersickness (MAC), which learns temporal sequences and characteristics of video flows, eye movement, head movement, and electrodermal activity. Based on data collected from 27 participants, we demonstrate the effectiveness of MAC, showing an F1-score of 0.87. Our experimental results further show not only the influences of gender and prior VR experience but also the effectiveness of the attention mechanism on model performance, emphasizing the importance of considering the characteristics of data types and users in cybersickness modeling. © 2023, The Author(s), under exclusive licence to Springer-Verlag London Ltd., part of Springer Nature.",Cybersickness; Deep learning; User characteristics; Virtual reality,Deep learning; E-learning; Eye movements; Artificial intelligence techniques; Body movements; Cybersickness; Deep learning; Growing bodies; Learning models; Multi-modal; Physiological signals; Prediction modelling; User characteristics; Virtual reality,Article,Final,,Scopus,2-s2.0-85160249328,Gaming / VR
Mack N.A.; Heun M.; Rose M.,"Mack, Nils Adrian (57210914973); Heun, Markus (58575355800); Rose, Marion (58575733400)",57210914973; 58575355800; 58575733400,Head-anchored text placements and cognitive load in information-rich virtual environments,2023,ACM International Conference Proceeding Series,,,,27,36,9.0,3,10.1145/3603555.3603575,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85171160842&doi=10.1145%2f3603555.3603575&partnerID=40&md5=b5707b56baee5e9dac2732eee35f9b40,"In recent years, there has been a growing trend toward using information-rich virtual environments that combine virtual environments (VE) with text as educational tools. This has been made viable by advances in head-mounted displays (HMDs). However, the use of virtual reality (VR) can create a problem of high extraneous cognitive load (ECL) caused by the VR technology itself. This can hinder learning if it exceeds a user's limited working memory. To reduce this load, designers of VEs can only address design elements like the text placement method. To give insights into the placement with the lowest ECL, we conducted a study with 30 participants, evaluating different head-anchored text placements in two abstracted, non-stationary learning tasks to assess their cognitive load, usability, and task load. Our results showed that head-anchored text should be placed above eye level for tasks at normal working height. Furthermore, the horizontal movement of text had little to no influence on cognitive load.  © 2023 Owner/Author.",cognitive load; text placement; usability; virtual reality,Eye movements; Helmet mounted displays; Cognitive loads; Design elements; Educational tools; Head-mounted-displays; Non-stationary learning; Placement methods; Text placement; Usability; Virtual reality technology; Working memory; Virtual reality,Conference paper,Final,All Open Access; Hybrid Gold Open Access,Scopus,2-s2.0-85171160842,Gaming / VR
Kakaria S.; Saffari F.; Z. Ramsøy T.; Bigné E.,"Kakaria, Shobhit (57211822428); Saffari, Farzad (57349815100); Z. Ramsøy, Thomas (58293713500); Bigné, Enrique (55132662600)",57211822428; 57349815100; 58293713500; 55132662600,Cognitive load during planned and unplanned virtual shopping: Evidence from a neurophysiological perspective,2023,International Journal of Information Management,72,,102667,,,,38,10.1016/j.ijinfomgt.2023.102667,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85160564166&doi=10.1016%2fj.ijinfomgt.2023.102667&partnerID=40&md5=a671e63d2445f27d1fa28d7b66adb322,"Rapid adoption of virtual-reality-assisted retail applications is inadvertently reshaping consumer buying patterns, making it crucial for businesses to enhance their shopping experience. This new scenario challenges marketers with unique hurdles in both the commercialization of products and in managing information cues derived via VR retailing. Therefore, this study examined consumers’ impulsive behavior and unplanned purchases in a virtual retail store, using self-reports and electroencephalography. Borrowing assorted perspectives from retailing, virtual reality, and neuromarketing literature, we extended the stimulus-organism-response framework to evaluate how unplanned behavior evolves through conscious and unconscious measures. We found that consumers’ impulsiveness was significantly associated with their unplanned expenditure and the number of unplanned purchases. Using mediation analysis, we observed that flow experience during shopping partially mediated the relationship between the sense of presence and the desire to stay longer in a virtual shopping store. Desire to stay in the virtual store positively influenced store satisfaction, basket-size deviation, and budget deviation. Additionally, cognitive workload obtained via electroencephalogram revealed significant differences during both planned and unplanned purchases. These findings provide fresh opportunities for retailers to leverage the disruptive potential of immersive and interactive virtual technology to transform consumer shopping experiences. © 2023 The Authors",Cognitive load; Electroencephalogram; Impulse buying; Unplanned purchase; Virtual reality,Budget control; Consumer behavior; Electrophysiology; Purchasing; Retail stores; Sales; Virtual reality; Buying patterns; Cognitive loads; Commercialisation; Consumer buying; Impulse buying; Impulsive behavior; Information cues; Pattern making; Unplanned purchase; Virtual shopping; Electroencephalography,Article,Final,All Open Access; Green Open Access; Hybrid Gold Open Access,Scopus,2-s2.0-85160564166,Gaming / VR
Karnath H.-O.; Schenk T.,"Karnath, Hans-Otto (7006721562); Schenk, Thomas (34572979200)",7006721562; 34572979200,"Abridged Version of the S2k Guideline ""Diagnostics and Therapy of Spatial Neglect and Further Disorders of Spatial Cognition""; [Kurzfassung der S2k-Leitlinie ""diagnostik und Therapie von Neglect und anderen Störungen der Raumkognition"" (AWMF-030/126)]",2023,Zeitschrift fur Neuropsychologie,34,3,,119,128,9.0,0,10.1024/1016-264X/a000377,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85171185086&doi=10.1024%2f1016-264X%2fa000377&partnerID=40&md5=4444a97dde0983b414845819aa1b2276,"In the newly revised guideline, the proven exploration training in treating spatial neglect has been supplemented by suggestions for the therapeutic procedure for different degrees of severity of spatial neglect, resulting in a respective reduction in time. Furthermore, the exploration training currently undergoes interesting enhancements by applying ""augmented reality""and ""virtual reality""procedures. Further recommendations for treating spatial neglect include training using slow smooth pursuit eye movements to the contralateral side and neck muscle vibration. Among the noninvasive transcranial brain stimulation procedures, continuous theta burst stimulation (cTBS) protocol has proved effective - if combined with at least one other neglect training procedure (e. g., exploration training). For treatment of the Pusher syndrome, different groups have successfully used visual feedback training and robot-assisted gait training. The situation is more problematic for procedures used for treating the other disorders of spatial cognition discussed in the guideline (Bálint syndrome, simultanagnosia, optic ataxia, disorders of visuospatial perception, visuoconstructive disorders, topographic disorders); single-case or small-group studies dominate here. The evaluation of individual, methodologically high-quality, and well-documented therapy studies currently provides the only basis for deriving recommendations for treating this latter group of disorders of spatial cognition. © 2023 The Author(s).",Bálint syndrome; lateropulsion; optic ataxia; Pusher syndrome; simultanagnosia; spatial neglect; topographical disorders; visuo-spatial perception disorders; visuoconstructive disorders,,Article,Final,All Open Access; Green Open Access; Hybrid Gold Open Access,Scopus,2-s2.0-85171185086,Gaming / VR
Choi H.; Kwon J.; Nam S.,"Choi, Haram (57987991100); Kwon, Joungheum (39861642200); Nam, Sanghun (55421451100)",57987991100; 39861642200; 55421451100,Research on the application of gaze visualization interface on virtual reality training systems,2023,Journal on Multimodal User Interfaces,17,3,,203,211,8.0,1,10.1007/s12193-023-00409-6,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85168350833&doi=10.1007%2fs12193-023-00409-6&partnerID=40&md5=daafe8d0ca130b8b791f9874309462d4,"Although virtual reality (VR) training provides a realistic environment that offers an immersive experience for learners, it can affect the training progress by triggering a cognitive load or distracting learners from focusing on the main content of the training. In this study, the application of a gaze visualization interface to a medical safety training system was proposed to prevent learner distraction, which is an issue usually experienced in traditional VR training systems. An experiment was conducted to analyze the effects of applying the gaze visualization interface to a medical training system on the learning performance. Based on related studies, the following three metrics were selected to evaluate the learning performance: learning flow, achievement, and concentration. After the experiment, flow and achievement were evaluated through surveys and quizzes. These evaluation methods can also distort learners’ memories, making it difficult to analyze objective learning outcomes. Eye-tracking technology is useful for measuring learning performance in multimedia learning environments and can represent learner’s eye movements in objective metrics. Therefore, in this study, eye-tracking technology was used to evaluate concentration. The experimental results suggest that the gaze visualization interface had positive effects on learners’ concentration, learning flow, and achievement. © 2023, The Author(s), under exclusive licence to Springer Nature Switzerland AG.",Eye-tracking; Gaze visualization; Learning performance; Training system; Virtual reality,Computer aided instruction; E-learning; Eye movements; Learning systems; Virtual reality; Visualization; Cognitive loads; Eye tracking technologies; Eye-tracking; Gaze visualization; Immersive; Learning flows; Learning performance; Realistic environments; Training Systems; Virtual reality training; Eye tracking,Article,Final,,Scopus,2-s2.0-85168350833,Gaming / VR
Asish S.M.; Kulshreshth A.K.; Borst C.,"Asish, Sarker Monojit (57215357842); Kulshreshth, Arun K. (55315352400); Borst, Christoph (9736479200)",57215357842; 55315352400; 9736479200,Internal Distraction Detection Utilizing EEG Data in an Educational VR Environment,2023,Proceedings - SAP 2023: ACM Symposium on Applied Perception,,,8,,,,5,10.1145/3605495.3605790,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85171184123&doi=10.1145%2f3605495.3605790&partnerID=40&md5=9198e71e1afb11e1f52daf5a4963cc66,"Virtual reality (VR) makes learning more interesting for students and could help them remember what they have learned better than traditional methods. However, a student could get distracted in a VR environment because of stress, wandering thoughts, unwanted noise, outside sounds, etc. Distractions could be classified as either external (due to the environment) or internal (due to internal thoughts). To identify external distractions, previous researchers have used eye-gaze data. Eye-gaze data cannot, however, detect internal distractions because a user may be looking at the educational material in VR while also thinking about something else. We explored the usage of electroencephalogram (EEG) data to detect internal distractions. We designed an educational VR environment and trained three machine learning models: Random Forest (RF), Support Vector Machine (SVM), and k-nearest-neighbors (kNN), to detect internal distractions of students. For data labeling, we considered two window lengths (20 and 30 seconds) starting at 5 seconds after the distraction task started. We did cross-subject and cross-session tests, and our results show that kNN provides a better accuracy (64%) compared to RF and SVM. We also found that the shorter window length of 20 seconds provided a slightly better accuracy then the 30 second window. Our results are not far from such random guessing. Therefore, our contribution lies more in the fostering of ideas for future work that must employ more advanced and sophisticated techniques.  © 2023 ACM.",Education; EEG; Human-computer interaction (HCI); Machine learning; Virtual reality,E-learning; Education computing; Forestry; Human computer interaction; Learning systems; Nearest neighbor search; Students; Support vector machines; Virtual reality; Classifieds; Data labelling; Educational materials; Eye-gaze; Human-computer interaction; Machine learning models; Machine-learning; Random forests; Support vectors machine; Virtual-reality environment; Electroencephalography,Conference paper,Final,,Scopus,2-s2.0-85171184123,Gaming / VR
Ascione M.; Carulla-Roig M.; Miquel-Nabau H.; Porras-Garcia B.; Meschberger-Annweiler F.-A.; Serrano-Troncoso E.; Ferrer-Garcia M.; Moreno-Sánchez M.; Gutierrez-Maldonado J.,"Ascione, Mariarca (57772657700); Carulla-Roig, Marta (57195225883); Miquel-Nabau, Helena (57242136500); Porras-Garcia, Bruno (57201200642); Meschberger-Annweiler, Franck-Alexandre (57773528000); Serrano-Troncoso, Eduardo (40262408400); Ferrer-Garcia, Marta (13405944200); Moreno-Sánchez, Manuel (56433228600); Gutierrez-Maldonado, Jose (9334173600)",57772657700; 57195225883; 57242136500; 57201200642; 57773528000; 40262408400; 13405944200; 56433228600; 9334173600,Attentional Bias Modification Training Based on Virtual Reality and Eye Tracking in Anorexia Nervosa Patients,2023,Journal of Clinical Medicine,12,18,5932,,,,12,10.3390/jcm12185932,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85172776769&doi=10.3390%2fjcm12185932&partnerID=40&md5=25e0666189443f207cee5d06e7a988ce,"Anorexia nervosa (AN) patients exhibit attentional bias (AB) related to the body, which is the tendency to pay greater attention to weight-related body areas compared to non-weight-related ones. This phenomenon has been linked to elevated levels of body dissatisfaction (BD) and may potentially reduce the effectiveness of body exposure therapy. The purpose of this pilot study is to assess the efficacy of a single session of a new body-related AB modification task (ABMT) that combines virtual reality with eye tracking in patients with AN. The goals of the ABMT are to reduce body-related AB by balancing attention between weight and non-weight-related body areas and to reduce BD levels. Twenty-three adolescent patients with AN were embodied in a virtual avatar and immersed in a virtual environment where they completed the ABMT. Body-related AB measures and BD levels were assessed before and after the training. A paired samples t-test showed statistically significant differences between pre-assessment and post-assessment; the complete fixation time on weight-related body parts was reduced and BD levels decreased. The initial evidence of the efficacy of this ABMT has important clinical implications, since AB and BD are considered risk factors for developing and maintaining eating disorder symptomatology among patients with AN. © 2023 by the authors.",anorexia nervosa; attentional bias modification; body dissatisfaction,antidepressant agent; anxiolytic agent; neuroleptic agent; adolescent; age; anorexia nervosa; Article; attentional bias; body dissatisfaction; body regions; body weight; child; clinical article; controlled study; eating disorder; eye tracking; female; human; male; risk factor; virtual reality,Article,Final,All Open Access; Gold Open Access; Green Open Access,Scopus,2-s2.0-85172776769,Gaming / VR
Schirm J.; Gómez-Vargas A.R.; Perusquía-Hernández M.; Skarbez R.T.; Isoyama N.; Uchiyama H.; Kiyokawa K.,"Schirm, Johannes (57210919413); Gómez-Vargas, Andrés Roberto (58535123900); Perusquía-Hernández, Monica (56149284500); Skarbez, Richard T. (15023405500); Isoyama, Naoya (54972028000); Uchiyama, Hideaki (35318642200); Kiyokawa, Kiyoshi (7005065755)",57210919413; 58535123900; 56149284500; 15023405500; 54972028000; 35318642200; 7005065755,Identification of Language-Induced Mental Load from Eye Behaviors in Virtual Reality,2023,Sensors,23,15,6667,,,,7,10.3390/s23156667,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85167829336&doi=10.3390%2fs23156667&partnerID=40&md5=63c89005cdf23b37fd072a3916736e0f,"Experiences of virtual reality (VR) can easily break if the method of evaluating subjective user states is intrusive. Behavioral measures are increasingly used to avoid this problem. One such measure is eye tracking, which recently became more standard in VR and is often used for content-dependent analyses. This research is an endeavor to utilize content-independent eye metrics, such as pupil size and blinks, for identifying mental load in VR users. We generated mental load independently from visuals through auditory stimuli. We also defined and measured a new eye metric, focus offset, which seeks to measure the phenomenon of “staring into the distance” without focusing on a specific surface. In the experiment, VR-experienced participants listened to two native and two foreign language stimuli inside a virtual phone booth. The results show that with increasing mental load, relative pupil size on average increased 0.512 SDs (0.118 mm), with 57% reduced variance. To a lesser extent, mental load led to fewer fixations, less voluntary gazing at distracting content, and a larger focus offset as if looking through surfaces (about 0.343 SDs, 5.10 cm). These results are in agreement with previous studies. Overall, we encourage further research on content-independent eye metrics, and we hope that hardware and algorithms will be developed in the future to further increase tracking stability. © 2023 by the authors.",cognitive load; eye tracking; flow state; language task; listening comprehension; mental load; virtual reality,Auditory Perception; Humans; Language; Surveys and Questionnaires; User-Computer Interface; Virtual Reality; Virtual reality; Behavioral measures; Cognitive loads; Content dependent; Eye-tracking; Flow state; Focus offset; Language task; Listening comprehensions; Mental loads; Pupil size; hearing; human; language; questionnaire; virtual reality; Eye tracking,Article,Final,All Open Access; Gold Open Access,Scopus,2-s2.0-85167829336,Gaming / VR
Wang Y.; Wang Y.; Li X.; Zhao C.; Ma N.; Guo Z.,"Wang, Yueyang (57222080205); Wang, Yahui (57194112027); Li, Xiaoqiong (27169828800); Zhao, Chengyi (58533363100); Ma, Ning (57203021754); Guo, Zixuan (59041559900)",57222080205; 57194112027; 27169828800; 58533363100; 57203021754; 59041559900,A Comparative Study of the Typing Performance of Two Mid-Air Text Input Methods in Virtual Environments,2023,Sensors,23,15,6988,,,,3,10.3390/s23156988,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85167762959&doi=10.3390%2fs23156988&partnerID=40&md5=6e62ec6b2f8443965b570bfe186ea9cc,"Inputting text is a prevalent requirement among various virtual reality (VR) applications, including VR-based remote collaboration. In order to eliminate the need for complex rules and handheld devices for typing within virtual environments, researchers have proposed two mid-air input methods—the trace and tap methods. However, the specific impact of these input methods on performance in VR remains unknown. In this study, typing tasks were used to compare the performance, subjective report, and cognitive load of two mid-air input methods in VR. While the trace input method was more efficient and novel, it also entailed greater frustration and cognitive workload. Fortunately, the levels of frustration and cognitive load associated with the trace input method could be reduced to the same level as those of the tap input method via familiarity with VR. These findings could aid the design of virtual input methods, particularly for VR applications with varying text input demands. © 2023 by the authors.",gestural input; keyboards; pointing devices; text input; usability testing; virtual reality,Virtual Reality; Workload; Cognitive loads; Comparatives studies; Gestural input; Input methods; Keyboard; Performance; Pointing devices; Text input; Text input methods; Usability testing; comparative study; virtual reality; workload; Virtual reality,Article,Final,All Open Access; Gold Open Access; Green Open Access,Scopus,2-s2.0-85167762959,Gaming / VR
Penedo T.; Rodrigues S.T.; Gotardi G.C.; Simieli L.; Barela J.A.; Polastri P.F.; Barbieri F.A.,"Penedo, Tiago (57194388296); Rodrigues, Sérgio T. (37001390900); Gotardi, Gisele C. (56426570600); Simieli, Lucas (55371999800); Barela, José A. (6602459997); Polastri, Paula F. (8062214200); Barbieri, Fabio A. (35798078800)",57194388296; 37001390900; 56426570600; 55371999800; 6602459997; 8062214200; 35798078800,Gaze behavior data in the vitrine of human movement science: considerations on eye-tracking technique,2023,Brazilian Journal of Motor Behavior,17,4,,75,88,13.0,1,10.20338/bjmb.v17i4.352,https://www.scopus.com/inward/record.uri?eid=2-s2.0-105010088557&doi=10.20338%2fbjmb.v17i4.352&partnerID=40&md5=34647a351db33efbdf45405e12e6f0f1,"BACKGROUND: Eyes are the main gateway of visual information input. Moving the eyes is essential to extract visual information from scenes while performing motor actions. This helps to explain motor behavior, especially related to visual attention mechanisms, gaze training and learning, and the relevance of visual information in controlling actions. Thus, collecting data on gaze behavior is important for explaining motor behavior. AIM: We present the main video-based eye-tracking techniques, briefly describe the anatomy of the eyes, explain the operation of the eye-tracker (eye capture techniques, calibration, and data analysis), and propose interpretations of the main variables that were extracted by the technique. This way we develop considerations (limitations and advantages) on the eye-tracking technique that placed gaze behavior data under the view of human movement science. INTERPRETATION: Eye-tracking has become an excellent tool to assist in the analysis of human movement through gaze behavior. It is possible to make inferences, mainly from the combination of sensory information, such as visual information, with performance during motor tasks, about perception, cognition, and human behavior during the most diverse day-to-day activities. Eye-tracker systems have been employed in different majors related to motor behavior, such as medicine, commerce, and game development. © 2023 Penedo, Rodrigues, Gotardi, Simieli, Barela, Polastri and Barbieri and BJMB.",Eye movement; Motor behavior; Vision,,Article,Final,,Scopus,2-s2.0-105010088557,Gaming / VR
Ennadifi E.; Ravet T.; Mancas M.; El Amine Mokhtari M.; Gosselin B.,"Ennadifi, Elias (57219547600); Ravet, Thierry (29068044400); Mancas, Matei (8569088000); El Amine Mokhtari, Mohammed (58651442500); Gosselin, Bernard (57162610800)",57219547600; 29068044400; 8569088000; 58651442500; 57162610800,Enhancing VR Gaming Experience using Computational Attention Models and Eye-Tracking,2023,IMX 2023 - Proceedings of the 2023 ACM International Conference on Interactive Media Experiences,,,,194,198,4.0,3,10.1145/3573381.3597218,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85174231404&doi=10.1145%2f3573381.3597218&partnerID=40&md5=0125b9a7776af7862b58aa94e30831d2,"This study explores the potential of enhancing interaction experiences, such as virtual reality (VR) games, through the use of computational attention models. Our proposed approach utilizes a saliency map generated by attention models to dynamically adjust game difficulty levels and to help in the game level design, resulting in a more immersive and engaging experience for users. To inform the development of this approach, we present an experimental setup that is able tp collect data in a VR environment and intends to be able to validate the adaptation of attention models to this domain. Through this work, we aim to create a framework for VR game design that leverages attention models to offer a new level of immersion and engagement for users. We believe our contributions have significant potential to enhance VR experiences and advance the field of game design.  © 2023 Owner/Author.",Computational attention models; Difficulty adjustment; Eye-tracking; Salience map; Virtual reality,Game design; Virtual reality; Attention model; Computational attention model; Difficulty adjustment; Eye-tracking; Game design; Gaming experiences; Interaction experiences; Model tracking; Salience map; Saliency map; Eye tracking,Conference paper,Final,All Open Access; Bronze Open Access,Scopus,2-s2.0-85174231404,Gaming / VR
Díaz-García J.; García-Calvo T.; Manzano-Rodríguez D.; López-Gajardo M.Á.; Parraca J.A.; Ring C.,"Díaz-García, Jesús (57210716066); García-Calvo, Tomás (58262725600); Manzano-Rodríguez, David (58498823000); López-Gajardo, Miguel Ángel (57216882168); Parraca, José Alberto (36239261500); Ring, Christopher (7006494007)",57210716066; 58262725600; 58498823000; 57216882168; 36239261500; 7006494007,Brain endurance training improves shot speed and accuracy in grassroots padel players,2023,Journal of Science and Medicine in Sport,26,7,,386,393,7.0,15,10.1016/j.jsams.2023.06.002,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85165028451&doi=10.1016%2fj.jsams.2023.06.002&partnerID=40&md5=c9a40c17d239fad8a8d1481995bc1c83,"Objectives: Evidence that mental fatigue impairs sport performance has created a demand for countermeasures. We examined the effects of brain endurance training, a form of fatigue-inoculation, on shot performance in grassroots padel players. Design: A pre-, mid- and post-test design, with participants randomized to brain endurance training (n = 30) or control (n = 31) groups. Methods: During testing, participants completed a Padel Stroke Performance Test, before and after a demanding 30-min cognitive task (Stroop). Training comprised 3 sessions/week for 6 weeks. In each training session, participants completed 10-min warm-up, 15-min technical drills, 15-min tactical drills, and 20-min simulated games. These physical activities were intermixed with short 4-min periods of Stroop (brain endurance training group) or rest (control group) totaling 20-min. Performance was measured by shot speed and accuracy of padel strokes. Mental fatigue indices were measured before and after the Stroop task using a visual analog scale rating, a psychomotor vigilance task, and a go/no-go task. Results: During testing, the 30-min Stroop task elicited a state of mental fatigue, confirmed by higher subjective ratings, slower responses during the psychomotor vigilance task, and slower saccade latencies during the go/no go task. Compared to pre-testing, in mid- and post-testing, the brain endurance training group hit progressively faster and more accurate padel shots after the Stroop task compared to controls. Conclusions: Brain endurance training enhanced skill-based psychomotor performance when fatigued compared to standard padel training. Brain endurance training is a countermeasure that promotes mental fatigue durability. © 2023 The Author(s)",Cognitive load; Fatigue inoculation; Psychomotor performance; Racket sport,Athletic Performance; Brain; Endurance Training; Humans; Mental Fatigue; Psychomotor Performance; accuracy; adult; alertness; Article; clinical article; cognitive load; controlled study; endurance training; female; Go No Go task; human; male; mental fatigue; physical activity; psychomotor performance; psychomotor vigilance task; pupil diameter; racquet sport; reaction time; saccadic eye movement; Stroop test; visual stimulation; warm up; athletic performance; brain; mental fatigue; physiology; psychomotor performance; randomized controlled trial,Article,Final,,Scopus,2-s2.0-85165028451,Gaming / VR
Lomos C.; Seineke U.; Kesting F.; Luyten J.W.,"Lomos, Catalina (36834110900); Seineke, Undine (58668484500); Kesting, Frauke (58112315300); Luyten, J.W. (58065950000)",36834110900; 58668484500; 58112315300; 58065950000,The Design of Incentive Systems in Digital Game-Based Learning: How Primary School Children Interact with It,2023,Education Sciences,13,7,668,,,,10,10.3390/educsci13070668,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85175099950&doi=10.3390%2feducsci13070668&partnerID=40&md5=e0c79f3a4449229925a0ca63e8b6e5aa,"Digital game-based learning builds on the general characteristics of games. The incentive system (points, scores, stars, levels, and performance feedback) integrates design elements to keep a learner engaged. In the work described here, we investigated which elements of the incentive system design—rewards, penalties, or feedback—have the potential to trigger students’ motivation to play the game. We used eye tracking of eight primary school children, aged 8–11 years, as they interacted with the incentive system of a mathematics game-based item and its specific design, followed by a semi-structured interview. Eye-tracking results show that students paid minimal visual attention to the incentive system during the game, regardless of their level of performance in the game or their age group. The feedback at the end of the game attracted more of their visual attention and provided a good opportunity to inform them about their performance. The semi-structured interviews revealed a high level of self-reported excitement about playing the game, mainly related to the design of the incentive system. Elements of the incentive system triggered students’ wish for student-to-student competition, which has been shown in the literature on traditional tangible rewards to stifle intrinsic motivation under certain conditions. The results of this study show that the design of the incentive system has the potential to promote extrinsic motivation with the game through rewards and penalties, and open the reflection on its possible spillover effect on intrinsic motivation in digital game-based learning. © 2023 by the authors.",digital game-based learning; elementary mathematics; eye tracking; incentive system design; multi-case study,,Article,Final,All Open Access; Gold Open Access,Scopus,2-s2.0-85175099950,Gaming / VR
,,,Proceedings - ETRA 2023: ACM Symposium on Eye Tracking Research and Applications,2023,Eye Tracking Research and Applications Symposium (ETRA),,,,,,431.0,0,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85161101240&partnerID=40&md5=cd4f78060e22144c2afe4e955134e04d,The proceedings contain 92 papers. The topics discussed include: a deep learning architecture for egocentric time-to-saccade prediction using Weibull mixture-models and historic priors; area of interest adaption using feature importance; bridging the gap: gaze events as interpretable concepts to explain deep neural sequence models; comparing visual search patterns in chest x-ray diagnostics; eye tracking to evaluate the effectiveness of electronic medical record training; gaze pattern recognition in dyadic communication; gaze-based mode-switching to enhance interaction with menus on tablets; getting the most from eye-tracking: user-interaction based reading region estimation dataset and models; introducing explicit gaze constraints to face swapping; multi-rate sensor fusion for unconstrained near-eye gaze estimation; on the visibility of fiducial markers for mobile eye tracking; predicting the allocation of attention: using contextual guidance of eye movements to examine the distribution of attention; prediction procedure for dementia levels based on waveform features of binocular pupil light reflex; and pupil diameter during counting tasks as potential baseline for virtual reality experiments.,,,Conference review,Final,,Scopus,2-s2.0-85161101240,Gaming / VR
Toki E.I.; Tatsis G.; Tatsis V.A.; Plachouras K.; Pange J.; Tsoulos I.G.,"Toki, Eugenia I. (36544342700); Tatsis, Giorgos (35488955700); Tatsis, Vasileios A. (14625686000); Plachouras, Konstantinos (57200880847); Pange, Jenny (6507161641); Tsoulos, Ioannis G. (10042633500)",36544342700; 35488955700; 14625686000; 57200880847; 6507161641; 10042633500,Employing Classification Techniques on SmartSpeech Biometric Data towards Identification of Neurodevelopmental Disorders,2023,Signals,4,2,,401,420,19.0,11,10.3390/signals4020021,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85178371286&doi=10.3390%2fsignals4020021&partnerID=40&md5=2d8ed62202d16b0f23d248aec34dcd2c,"Early detection and evaluation of children at risk of neurodevelopmental disorders and/or communication deficits is critical. While the current literature indicates a high prevalence of neurodevelopmental disorders, many children remain undiagnosed, resulting in missed opportunities for effective interventions that could have had a greater impact if administered earlier. Clinicians face a variety of complications during neurodevelopmental disorders’ evaluation procedures and must elevate their use of digital tools to aid in early detection efficiently. Artificial intelligence enables novelty in taking decisions, classification, and diagnosis. The current research investigates the efficacy of various machine learning approaches on the biometric SmartSpeech datasets. These datasets come from a new innovative system that includes a serious game which gathers children’s responses to specifically designed speech and language activities and their manifestations, intending to assist during the clinical evaluation of neurodevelopmental disorders. The machine learning approaches were used by utilizing the algorithms Radial Basis Function, Neural Network, Deep Learning Neural Networks, and a variation of Grammatical Evolution (GenClass). The most significant results show improved accuracy (%) when using the eye tracking dataset; more specifically: (i) for the class Disorder with GenClass (92.83%), (ii) for the class Autism Spectrum Disorders with Deep Learning Neural Networks layer 4 (86.33%), (iii) for the class Attention Deficit Hyperactivity Disorder with Deep Learning Neural Networks layer 4 (87.44%), (iv) for the class Intellectual Disability with GenClass (86.93%), (v) for the class Specific Learning Disorder with GenClass (88.88%), and (vi) for the class Communication Disorders with GenClass (88.70%). Overall, the results indicated GenClass to be nearly the top competitor, opening up additional probes for future studies toward automatically classifying and assisting clinical assessments for children with neurodevelopmental disorders. © 2023 by the authors.",classification; deep learning neural networks; grammatical evolution; machine learning optimizers; neurodevelopmental disorders; radial basis function neural network; SmartSpeech,,Article,Final,All Open Access; Gold Open Access,Scopus,2-s2.0-85178371286,Gaming / VR
Meschberger-Annweiler F.-A.; Ascione M.; Prieto-Perpiña J.; Verdesca C.; Ferrer-Garcia M.; Gutierrez-Maldonado J.,"Meschberger-Annweiler, Franck-Alexandre (57773528000); Ascione, Mariarca (57772657700); Prieto-Perpiña, Julia (58814317900); Verdesca, Chiara (58814517100); Ferrer-Garcia, Marta (13405944200); Gutierrez-Maldonado, José (9334173600)",57773528000; 57772657700; 58814317900; 58814517100; 13405944200; 9334173600,Body Dissatisfaction and Self-Disgust as Significant Predictors of Body-Related Attentional Bias. A Virtual Reality and Eye-Tracking Study,2023,Annual Review of CyberTherapy and Telemedicine,21,,,76,82,6.0,1,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85182467420&partnerID=40&md5=a11e43afde66e5da18e2a1ffeecf6953,"Body dissatisfaction, fear of gaining weight (FGW) and body anxiety have been extensively studied as some of the strongest risk and maintenance factors of anorexia nervosa (AN) symptomatology. Recently, a new theoretical model introduced self-disgust as a factor that can lead to avoidance behaviors when patients with AN face their body. This can make them vulnerable to relapse. In addition, body-related attentional bias (AB) (e.g., selective attention to weight-related body areas) can limit the efficacy of body exposure therapies. This study aims to investigate the possible predictors of AB, to better understand the underlying mechanisms that contribute to the maintenance of AN symptomatology. A total of 116 college students from the University of Barcelona participated in the study, using a combination of virtual reality and eye-tracking techniques to provide an objective and reliable assessment of AB in a highly realistic environment. Stepwise multiple linear regression analyses were performed to identify possible predictors of AB among body mass index, FGW, body anxiety, body dissatisfaction and self-disgust. The results shows that both body dissatisfaction and self-disgust are significant predictors of AB. While an increase in body dissatisfaction predicted a greater AB towards weight-related body areas (positive regression coefficients: BBody_dissatisfaction→AB > 0, p <.001), the opposite occurred with self-disgust (negative regression coefficients: BSelf_disgust→AB < 0, p <.02). Such results provide initial evidence that self-disgust, which is a more intense negative feeling than body dissatisfaction, leads to gaze avoidance towards weight-related body areas, which are considered disgust elicitors. © 2023, Interactive Media Institute. All rights reserved.",anorexia nervosa; attentional bias; body dissatisfaction; eye-tracking; self-disgust; virtual reality,adult; anorexia nervosa; Article; avoidance behavior; body dissatisfaction; body mass; eating disorder; eye tracking; eye-tracking technology; female; human; human experiment; male; mental disease; multiple linear regression analysis; normal human; psychosis; questionnaire; selective attention; self concept; symptomatology; tactile stimulation; theoretical model; virtual reality,Article,Final,,Scopus,2-s2.0-85182467420,Gaming / VR
Walter J.L.; Schmidt V.; König S.U.; König P.,"Walter, Jasmin L. (57326651500); Schmidt, Vincent (58030175700); König, Sabine U. (56274699800); König, Peter (7102563952)",57326651500; 58030175700; 56274699800; 7102563952,Navigating Virtual Worlds: Examining Spatial Navigation Using a Graph Theoretical Analysis of Eye Tracking Data Recorded in Virtual Reality,2023,Eye Tracking Research and Applications Symposium (ETRA),,,48,,,,0,10.1145/3588015.3590124,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85161211641&doi=10.1145%2f3588015.3590124&partnerID=40&md5=6f8d74b947bac542c6f7da3875b74d16,"In this work we apply a graph-theoretical analysis approach to eye tracking data recorded in virtual reality to investigate the underlying patterns of visual attention during spatial navigation. Based on the eye tracking data recorded in one virtual city, our graph-theoretical analysis identifies a subset of houses outstanding in their graph-theoretical properties which we define as gaze-graph-defined landmarks. Moreover, we are able to replicate these results with a different eye tracking data set recorded in a different virtual city. Finally, the initial model selection process of the participant's performance in a point-to-building task in the second city suggests a stronger influence of graph-theoretical predictors on the performance compared to the non-graph related measures, however more research will be necessary to determine their relationship.  © 2023 Owner/Author.",Eye tracking; graph theory; spatial navigation; virtual reality,Air navigation; Behavioral research; Eye tracking; Graph theory; Analysis approach; Eye-tracking; Graph theoretical analysis; Performance; Property; Spatial navigation; Tracking data; Virtual cities; Virtual worlds; Visual Attention; Virtual reality,Conference paper,Final,,Scopus,2-s2.0-85161211641,Gaming / VR
Kang D.; Kwon J.; Nam S.,"Kang, Donghyun (35766768600); Kwon, Joungheum (39861642200); Nam, Sanghun (55421451100)",35766768600; 39861642200; 55421451100,Research on Effective Advertising Types in Virtual Environment,2023,Applied Sciences (Switzerland),13,12,7063,,,,5,10.3390/app13127063,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85163985312&doi=10.3390%2fapp13127063&partnerID=40&md5=6e499829d67d733a3f26978410ec58de,"Virtual reality (VR) applies various types of advertisements (ads) to promote brands while collaborating with companies. This study aims to present effective advertisement types by verifying user responses in a VR environment. First, by analyzing the cases of advertisements with immersive content, the types of advertisements in VR were defined as avatar costumes, products, and wall posters. The user response was measured in two categories: gaze response measured by the eye-tracking VR advertisement monitoring system (EVAMS) and the advertisement effect analyzed through surveys. As a result of analyzing the user responses, the avatar costumes among the advertisement types caused the highest visual attention and advertisement effect. In addition, by analyzing the correlation between visual attention and the advertisement effect, it was observed that there was a positive relationship between the number of fixations and advertisement attention, fixation time, and advertisement recall. Thus, it was confirmed that the higher the number of fixations and the longer the fixation time, the more positively an advertisement was affected. In addition, it is expected that the results of this study can be used as a reference for effective advertisement directing in VR content development and advertisement directing and processing. © 2023 by the authors.",advertising effect; eye-tracking; virtual reality; VR advertisement,,Article,Final,All Open Access; Gold Open Access,Scopus,2-s2.0-85163985312,Gaming / VR
Aatrai S.; Jha S.K.; Guha R.,"Aatrai, Sonali (58305271200); Jha, Sparsh Kumar (58305271300); Guha, Rajlakshmi (57189351098)",58305271200; 58305271300; 57189351098,Visual Perception and Performance: An Eye Tracking Study,2023,Eye Tracking Research and Applications Symposium (ETRA),,,23,,,,3,10.1145/3588015.3588424,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85161121812&doi=10.1145%2f3588015.3588424&partnerID=40&md5=389a6e711226e42d7bc116032e5a1147,"This study explores the relationship between visual perception and performance. We investigate whether eye-metrics are consistent across various visual problem-solving tasks and if task complexity affects eye-metrics. Experiments were conducted on 102 participants using Tower of Hanoi (TOH), Image Sliding Puzzle (ISP) and 4 visual reasoning tasks with increasing complexity from the CLEVR dataset. Total Scanning Duration, Fixation count, Total Fixation Duration, and Total Saccadic Duration were found significant for distinguishing good and bad performers across tasks. Peak Velocity and Mean Pupil Diameter were found significant for varying task complexity. This was also reflected in time-matched samples of good and bad performers in TOH and ISP, though the content complexity of both these tasks remained constant. We propose that Peak Velocity and Mean Pupil Diameter are markers of 'perceived task complexity'. Poor performers perceive tasks to be more complex even when content complexity is constant, and this affects their performance.  © 2023 ACM.",Eye movements and cognition; Scanning strategies; Visual search behavior,Eye tracking; Eye movement and cognition; Peak velocities; Scanning strategies; Search behavior; Task complexity; Towers of Hanoi; Visual perception; Visual performance; Visual search; Visual search behavior; Eye movements,Conference paper,Final,,Scopus,2-s2.0-85161121812,Gaming / VR
Miquel-Nabau H.; Briseño-Oloriz N.; Porras-Garcia B.; Ascione M.; Meschberger-Annweiler F.-A.; Ferrer-Garcia M.; Moreno-Sanchez M.; Serrano-Troncoso E.; Carulla-Roig M.; Gutiérrez Maldonado J.,"Miquel-Nabau, Helena (57242136500); Briseño-Oloriz, Natalia (57863612500); Porras-Garcia, Bruno (57201200642); Ascione, Mariarca (57772657700); Meschberger-Annweiler, Franck-Alexandre (57773528000); Ferrer-Garcia, Marta (13405944200); Moreno-Sanchez, Manuel (56433228600); Serrano-Troncoso, Eduardo (40262408400); Carulla-Roig, Marta (57195225883); Gutiérrez Maldonado, José (9334173600)",57242136500; 57863612500; 57201200642; 57772657700; 57773528000; 13405944200; 56433228600; 40262408400; 57195225883; 9334173600,Modification of Body-Related Attentional Bias through Virtual Reality and Eye-Tracking in Healthy Participants: Implications for Anorexia Nervosa Treatments,2023,Brain Sciences,13,5,764,,,,8,10.3390/brainsci13050764,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85160350368&doi=10.3390%2fbrainsci13050764&partnerID=40&md5=7f273dbe026ff42525459a4311f23a08,"Cognitive biases have a significant impact on the etiology and treatment of eating disorders (EDs). These biases, including selective attentional bias (AB) to disliked body parts, may reinforce concerns about body shape, fear of gaining weight and body image disturbances and may contribute to dietary restriction and restraint. Decreasing AB could reduce core symptoms in anorexia nervosa (AN). This study represents a preliminary exploration aiming to assess whether AB towards weight-related (WR) and non-weight-related (NW) body parts could be reduced through an AB modification task in a virtual reality (VR) environment in healthy participants. A total of 54 female participants, aged 22.98 ± 1.89, were recruited. The task consisted of directing the participants’ attention towards all body parts equally in a VR setting. Eye-tracking (ET) measurements (complete fixation time [CFT] and number of fixations [NF]) were made before and after the task. The results showed a significant reduction of the AB in the two groups with an initial AB towards WR body parts or towards NW body parts. Participants showed a tendency to more balanced (non-biased) attention after the intervention. This study provides evidence of the usefulness of AB modification tasks in a non-clinical sample. © 2023 by the authors.",anorexia nervosa; attentional bias; body image; eye-tracking; virtual reality,adult; anorexia nervosa; Article; attention; attentional bias; bipolar disorder; body dissatisfaction; body image; body mass; body regions; body weight; body weight gain; cognition; complete fixation time; eating disorder; eye tracking; female; human; human experiment; mental disease; mental disease assessment; normal human; obesity; perception; physical activity; physical appearance; prevalence; questionnaire; schizophrenia; scoring system; self report; State Trait Anxiety Inventory; stimulus; task performance; virtual reality; visual attention; visual stimulation,Article,Final,All Open Access; Gold Open Access; Green Open Access,Scopus,2-s2.0-85160350368,Gaming / VR
Fernandes A.S.; Murdison T.S.; Proulx M.J.,"Fernandes, Ajoy S. (57077719300); Murdison, T. Scott (55810288000); Proulx, Michael J. (15078243900)",57077719300; 55810288000; 15078243900,Leveling the Playing Field: A Comparative Reevaluation of Unmodified Eye Tracking as an Input and Interaction Modality for VR,2023,IEEE Transactions on Visualization and Computer Graphics,29,5,,2269,2279,10.0,43,10.1109/TVCG.2023.3247058,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85149420539&doi=10.1109%2fTVCG.2023.3247058&partnerID=40&md5=d306128736723f9ebb2a85634e2c7682,"In this study, we establish a much-needed baseline for evaluating eye tracking interactions using an eye tracking enabled Meta Quest 2 VR headset with 30 participants. Each participant went through 1098 targets using multiple conditions representative of AR/VR targeting and selecting tasks, including both traditional standards and those more aligned with AR/VR interactions today. We use circular white world-locked targets, and an eye tracking system with sub-1-degree mean accuracy errors running at approximately 90Hz. In a targeting and button press selection task, we, by design, compare completely unadjusted, cursor-less, eye tracking with controller and head tracking, which both had cursors. Across all inputs, we presented targets in a configuration similar to the ISO 9241-9 reciprocal selection task and another format with targets more evenly distributed near the center. Targets were laid out either flat on a plane or tangent to a sphere and rotated toward the user. Even though we intended this to be a baseline study, we see unmodified eye tracking, without any form of a cursor, or feedback, outperformed the head by 27.9% and performed comparably to the controller (5.63% decrease) in throughput. Eye tracking had improved subjective ratings relative to head in Ease of Use, Adoption, and Fatigue (66.4%, 89.8%, and 116.1 % improvements, respectively) and had similar ratings relative to the controller (reduction by 4.2%, 8.9%, and 5.2% respectively). Eye tracking had a higher miss percentage than controller and head (17.3% vs 4.7% vs 7.2% respectively). Collectively, the results of this baseline study serve as a strong indicator that eye tracking, with even minor sensible interaction design modifications, has tremendous potential in reshaping interactions in next-generation AR/VR head mounted displays.  © 1995-2012 IEEE.",3D user interaction; Eye tracking; Gaze targeting; Human factors and ergonomics; Input devices; User experience,Controllers; Ergonomics; Helmet mounted displays; Job analysis; Target tracking; Three dimensional displays; 3d user interaction; Eye-tracking; Gaze targeting; Gaze-tracking; Human factors and ergonomics; Input devices; Performances evaluation; Targets tracking; Task analysis; Three-dimensional display; User interaction; Users' experiences; Eye tracking,Article,Final,All Open Access; Hybrid Gold Open Access,Scopus,2-s2.0-85149420539,Gaming / VR
Keshava A.; Gottschewsky N.; Balle S.; Nezami F.N.; Schüler T.; König P.,"Keshava, Ashima (57202213703); Gottschewsky, Nina (57247187800); Balle, Stefan (57248488200); Nezami, Farbod Nosrat (56208678400); Schüler, Thomas (26429994300); König, Peter (7102563952)",57202213703; 57247187800; 57248488200; 56208678400; 26429994300; 7102563952,Action affordance affects proximal and distal goal-oriented planning,2023,European Journal of Neuroscience,57,9,,1546,1560,14.0,6,10.1111/ejn.15963,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85150990430&doi=10.1111%2fejn.15963&partnerID=40&md5=50f82797b92d90014f649ed04e237c7d,"Visual attention is mainly goal directed and allocated based on the upcoming action. However, it is unclear how far this feature of gaze behaviour generalizes in more naturalistic settings. The present study investigates the influence of action affordances on active inference processes revealed by eye movements during interaction with familiar and novel tools. In a between-subject design, a cohort of participants interacted with a virtual reality controller in a low-realism environment; another performed the task with an interaction setup that allowed differentiated hand and finger movements in a high-realism environment. We investigated the differences in odds of fixations and their eccentricity towards the tool parts before action initiation. The results show that participants fixate more on the tool's effector part before action initiation when asked to produce tool-specific movements, especially with unfamiliar tools. These findings suggest that fixations are made in a task-oriented way to plan the distal goals of producing the task- and tool-specific actions well before action initiation. Moreover, with more realistic action affordance, fixations were biased towards the tool handle when it was oriented incongruent with the subjects' handedness. We hypothesize that these fixations are made towards the proximal goal of planning the grasp even though the perceived action on the tools is identical for both experimental setups. Taken together, proximal and distal goal-oriented planning is contextualized to the realism of action/interaction afforded by an environment. © 2023 The Authors. European Journal of Neuroscience published by Federation of European Neuroscience Societies and John Wiley & Sons Ltd.",action-oriented attention; active inference; anticipatory behaviour; eye-tracking; natural cognition; virtual reality,Cognition; Eye Movements; Goals; Humans; Movement; Psychomotor Performance; adult; article; attention; cognition; cohort analysis; controlled study; eye movement; eye tracking; female; finger; hand movement; handedness; human; human experiment; male; virtual reality; cognition; motivation; movement (physiology); psychomotor performance,Article,Final,All Open Access; Green Open Access; Hybrid Gold Open Access,Scopus,2-s2.0-85150990430,Gaming / VR
Kourtesis P.; Amir R.; Linnell J.; Argelaguet F.; MacPherson S.E.,"Kourtesis, Panagiotis (57210959726); Amir, Rayaan (58095750900); Linnell, Josie (58095902900); Argelaguet, Ferran (15021985300); MacPherson, Sarah E. (57212061035)",57210959726; 58095750900; 58095902900; 15021985300; 57212061035,"Cybersickness, Cognition, & Motor Skills: The Effects of Music, Gender, and Gaming Experience",2023,IEEE Transactions on Visualization and Computer Graphics,29,5,,2326,2336,10.0,23,10.1109/TVCG.2023.3247062,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85149392351&doi=10.1109%2fTVCG.2023.3247062&partnerID=40&md5=4ee023a237e02dca30c95aa62a5cf586,"Recent research has attempted to identify methods to mitigate cybersickness and examine its aftereffects. In this direction, this paper examines the effects of cybersickness on cognitive, motor, and reading performance in VR. Also, this paper evaluates the mitigating effects of music on cybersickness, as well as the role of gender, and the computing, VR, and gaming experience of the user. This paper reports two studies. In the 1st study, 92 participants selected the music tracks considered most calming (low valence) or joyful (high valence) to be used in the 2nd study. In the 2nd study, 39 participants performed an assessment four times, once before the rides (baseline), and then once after each ride (3 rides). In each ride either Calming, or Joyful, or No Music was played. During each ride, linear and angular accelerations took place to induce cybersickness in the participants. In each assessment, while immersed in VR, the participants evaluated their cybersickness symptomatology and performed a verbal working memory task, a visuospatial working memory task, and a psychomotor task. While responding to the cybersickness questionnaire (3D UI), eye-tracking was conducted to measure reading time and pupillometry. The results showed that Joyful and Calming music substantially decreased the intensity of nausea-related symptoms. However, only Joyful music significantly decreased the overall cybersickness intensity. Importantly, cybersickness was found to decrease verbal working memory performance and pupil size. Also, it significantly decelerated psychomotor (reaction time) and reading abilities. Higher gaming experience was associated with lower cybersickness. When controlling for gaming experience, there were no significant differences between female and male participants in terms of cybersickness. The outcomes indicated the efficiency of music in mitigating cybersickness, the important role of gaming experience in cybersickness, and the significant effects of cybersickness on pupil size, cognition, psychomotor skills, and reading ability.  © 1995-2012 IEEE.",cognition; Cybersickness; eye-tracking; gaming experience; gender; mitigation; reaction time; reading; virtual reality,Acceleration; Diagnosis; Job analysis; Virtual reality; Cognition; Cybersickness; Eye-tracking; Gaming experiences; Gender; Mitigation; Motion sickness; Pupil; Reading; Task analysis; Tracking; Eye tracking,Article,Final,All Open Access; Green Open Access,Scopus,2-s2.0-85149392351,Gaming / VR
Hu Z.; Bulling A.; Li S.; Wang G.,"Hu, Zhiming (57208101391); Bulling, Andreas (6505807414); Li, Sheng (56002421500); Wang, Guoping (7407150270)",57208101391; 6505807414; 56002421500; 7407150270,EHTask: Recognizing User Tasks From Eye and Head Movements in Immersive Virtual Reality,2023,IEEE Transactions on Visualization and Computer Graphics,29,4,,1992,2004,12.0,32,10.1109/TVCG.2021.3138902,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122280420&doi=10.1109%2fTVCG.2021.3138902&partnerID=40&md5=d37afaee2fc859b5767d591345912cae,"Understanding human visual attention in immersive virtual reality (VR) is crucial for many important applications, including gaze prediction, gaze guidance, and gaze-contingent rendering. However, previous works on visual attention analysis typically only explored one specific VR task and paid less attention to the differences between different tasks. Moreover, existing task recognition methods typically focused on 2D viewing conditions and only explored the effectiveness of human eye movements. We first collect eye and head movements of 30 participants performing four tasks, i.e., Free viewing, Visual search, Saliency, and Track, in 15 360-degree VR videos. Using this dataset, we analyze the patterns of human eye and head movements and reveal significant differences across different tasks in terms of fixation duration, saccade amplitude, head rotation velocity, and eye-head coordination. We then propose EHTask - a novel learning-based method that employs eye and head movements to recognize user tasks in VR. We show that our method significantly outperforms the state-of-the-art methods derived from 2D viewing conditions both on our dataset (accuracy of 84.4% versus 62.8%) and on a real-world dataset (61.9% versus 44.1%). As such, our work provides meaningful insights into human visual attention under different VR tasks and guides future work on recognizing user tasks in VR.  © 1995-2012 IEEE.",deep learning; eye movements; head movements; task recognition; virtual reality; Visual attention,Computer Graphics; Eye Movements; Head Movements; Humans; Virtual Reality; Behavioral research; Deep learning; Eye movements; Job analysis; Magnetic heads; Deep learning; Head; Head movements; Human visual attention; Immersive virtual reality; Solid modelling; Task analysis; Task recognition; Video; Visual Attention; computer graphics; eye movement; head movement; human; virtual reality; Virtual reality,Article,Final,,Scopus,2-s2.0-85122280420,Gaming / VR
Gupta A.; Sendhilnathan N.; Hartcher-O'brien J.; Pezent E.; Benko H.; Jonker T.R.,"Gupta, Aakar (26654850500); Sendhilnathan, Naveen (57194543267); Hartcher-O'brien, Jess (35725312900); Pezent, Evan (57197801010); Benko, Hrvoje (9737287100); Jonker, Tanya R. (55510841000)",26654850500; 57194543267; 35725312900; 57197801010; 9737287100; 55510841000,Investigating Eyes-away Mid-air Typing in Virtual Reality using Squeeze haptics-based Postural Reinforcement,2023,Conference on Human Factors in Computing Systems - Proceedings,,,230,,,,9,10.1145/3544548.3581467,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85160002592&doi=10.1145%2f3544548.3581467&partnerID=40&md5=ba9fb649a0780563b74c602012add981,"In this paper, we investigate postural reinforcement haptics for mid-air typing using squeeze actuation on the wrist. We propose and validate eye-tracking based objective metrics that capture the impact of haptics on the user's experience, which traditional performance metrics like speed and accuracy are not able to capture. To this end, we design four wrist-based haptic feedback conditions: no haptics, vibrations on keypress, squeeze+vibrations on keypress, and squeeze posture reinforcement + vibrations on keypress. We conduct a text input study with 48 participants to compare the four conditions on typing and gaze metrics. Our results show that for expert qwerty users, posture reinforcement haptics significantly benefit typing by reducing the visual attention on the keyboard by up to 44% relative to no haptics, thus enabling eyes-away behaviors. © 2023 ACM.",,Behavioral research; Eye tracking; Virtual reality; Condition; Eye-tracking; Haptic feedbacks; Haptics; Objective metrics; Performance metrices; Text input; Users' experiences; Visual Attention; Reinforcement,Conference paper,Final,,Scopus,2-s2.0-85160002592,Gaming / VR
Bovo R.; Giunchi D.; Sidenmark L.; Newn J.; Gellersen H.; Costanza E.; Heinis T.,"Bovo, Riccardo (57204780702); Giunchi, Daniele (57205711925); Sidenmark, Ludwig (57210111157); Newn, Joshua (57188820341); Gellersen, Hans (6701531333); Costanza, Enrico (22834126300); Heinis, Thomas (36874211600)",57204780702; 57205711925; 57210111157; 57188820341; 6701531333; 22834126300; 36874211600,Speech-Augmented Cone-of-Vision for Exploratory Data Analysis,2023,Conference on Human Factors in Computing Systems - Proceedings,,,162,,,,7,10.1145/3544548.3581283,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85160009761&doi=10.1145%2f3544548.3581283&partnerID=40&md5=28bb8acbc1957a53d3b89e8b5df0d120,"Mutual awareness of visual attention is crucial for successful collaboration. Previous research has explored various ways to represent visual attention, such as field-of-view visualizations and cursor visualizations based on eye-tracking, but these methods have limitations. Verbal communication is often utilized as a complementary strategy to overcome such disadvantages. This paper proposes a novel method that combines verbal communication with the Cone of Vision to improve gaze inference and mutual awareness in VR. We conducted a within-group study with pairs of participants who performed a collaborative analysis of data visualizations in VR. We found that our proposed method provides a better approximation of eye gaze than the approximation provided by head direction. Furthermore, we release the first collaborative head, eyes, and verbal behaviour dataset. The results of this study provide a foundation for investigating the potential of verbal communication as a tool for enhancing visual cues for joint attention. © 2023 ACM.",eye-tracking; Field of View; multi-modal visual attention cues; VR collaborative analytics,Behavioral research; Information analysis; Visualization; Exploratory data analysis; Eye-tracking; Field of views; Multi-modal visual attention cue; Multi-modal visual attentions; Mutual awareness; Novel methods; Verbal communications; Visual Attention; VR collaborative analytic; Eye tracking,Conference paper,Final,All Open Access; Bronze Open Access; Green Open Access,Scopus,2-s2.0-85160009761,Gaming / VR
Appel T.; Gerjets P.; Hoffmann S.; Moeller K.; Ninaus M.; Scharinger C.; Sevcenko N.; Wortha F.; Kasneci E.,"Appel, Tobias (57191500368); Gerjets, Peter (6507437265); Hoffmann, Stefan (57203514423); Moeller, Korbinian (23019055400); Ninaus, Manuel (55797096600); Scharinger, Christian (57210630060); Sevcenko, Natalia (57211756979); Wortha, Franz (57201334648); Kasneci, Enkelejda (56059892600)",57191500368; 6507437265; 57203514423; 23019055400; 55797096600; 57210630060; 57211756979; 57201334648; 56059892600,Cross-Task and Cross-Participant Classification of Cognitive Load in an Emergency Simulation Game,2023,IEEE Transactions on Affective Computing,14,2,,1558,1571,13.0,20,10.1109/TAFFC.2021.3098237,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85111032068&doi=10.1109%2fTAFFC.2021.3098237&partnerID=40&md5=bbed7e1a7d4c4da92a50a9af7b23021d,"Assessment of cognitive load is a major step towards adaptive interfaces. However, non-invasive assessment is rather subjective as well as task specific and generalizes poorly, mainly due to methodological limitations. Additionally, it heavily relies on performance data like game scores or test results. In this study, we present an eye-tracking approach that circumvents these shortcomings and allows for effective generalizing across participants and tasks. First, we established classifiers for predicting cognitive load individually for a typical working memory task (n-back), which we then applied to an emergency simulation game by considering the similar ones and weighting their predictions. Standardization steps helped achieve high levels of cross-task and cross-participant classification accuracy between 63.78 and 67.25 percent for the distinction between easy and hard levels of the emergency simulation game. These very promising results could pave the way for novel adaptive computer-human interaction across domains and particularly for gaming and learning environments.  © 2010-2012 IEEE.",adaptive and intelligent educational systems; cognitive model; Eye tracking; intelligent systems; physiological measures; physiology; psychology,Cognitive systems; Computer aided instruction; Eye tracking; Human computer interaction; Interactive computer systems; Noninvasive medical procedures; Physiological models; Psychophysiology; Real time systems; Adaptation models; Adaptive and intelligent educational system; Atmospheric measurement; Cognitive model; Eye-tracking; Game; Intelligent educational systems; Load modeling; Particle measurement; Physiological measures; Psychology; Real - Time system; Task analysis; Intelligent systems,Article,Final,All Open Access; Green Open Access,Scopus,2-s2.0-85111032068,Gaming / VR
Zhang J.; Huang M.; Yang R.; Wang Y.; Tang X.; Han J.; Liang H.-N.,"Zhang, Jingjing (57221743999); Huang, Mengjie (57220907643); Yang, Rui (57213114979); Wang, Yiqi (59852149800); Tang, Xiaohang (59876472200); Han, Ji (57190442322); Liang, Hai-Ning (8636386200)",57221743999; 57220907643; 57213114979; 59852149800; 59876472200; 57190442322; 8636386200,Understanding the effects of hand design on embodiment in virtual reality,2023,"Artificial Intelligence for Engineering Design, Analysis and Manufacturing: AIEDAM",37,,e10,,,,20,10.1017/S0890060423000045,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85149387587&doi=10.1017%2fS0890060423000045&partnerID=40&md5=d602706cfc2030cda10972476ad09757,"Understanding user perceptions of interacting with the virtual world is one of the research focuses in recent years, given the rapid proliferation of virtual reality (VR) and driven to establish the metaverse. Users can generate a familiar connection between their bodies and the virtual world by being embodied in virtual hands, and hand representations can induce users' embodiment in VR. The sense of embodiment represents the cognitive awareness of one's manifestation and includes three subcomponents: the sense of body ownership, agency and self-location. There is insufficient evidence in the literature about the effects of hand designs on the embodiment, especially based on studying its three subcomponents. This study investigates how virtual hand designs with five realism levels influence the three subcomponents of embodiment in VR. This research employs a self-report questionnaire commonly used in the literature to assess embodiment and evaluates agency and self-location by introducing implicit methods (intentional binding and proprioceptive measurement) derived from psychology. Besides, the objective data of eye tracking is used to explore the connection between embodiment and hand designs, and classifying participants' eye tracking data to help analyze the link between embodiment and user attention. Overall, this research makes a major contribution through a systematic exploration of users' embodied experience in VR and offers important evidence of the effects of virtual hand designs on body ownership, agency, and self-location, respectively. In addition, this study provides a valuable reference for further investigation of embodiment through implicit and objective methods, and practical design recommendations for virtual hand design in VR applications.  © 2023 The Author(s). Published by Cambridge University Press.",Embodiment; eye tracking; virtual hand design; virtual reality,Eye tracking; Location; Palmprint recognition; Embodiment; Eye-tracking; Hand-design; Implicit methods; Metaverses; Research focus; User perceptions; Virtual hand; Virtual hand design; Virtual worlds; Virtual reality,Article,Final,,Scopus,2-s2.0-85149387587,Gaming / VR
Zhao M.; Pierce A.M.; Tan R.; Zhang T.; Wang T.; Jonker T.R.; Benko H.; Gupta A.,"Zhao, Maozheng (57218348572); Pierce, Alec M. (58176988600); Tan, Ran (58177408300); Zhang, Ting (56739503800); Wang, Tianyi (57203515045); Jonker, Tanya R. (55510841000); Benko, Hrvoje (9737287100); Gupta, Aakar (26654850500)",57218348572; 58176988600; 58177408300; 56739503800; 57203515045; 55510841000; 9737287100; 26654850500,Gaze Speedup: Eye Gaze Assisted Gesture Typing in Virtual Reality,2023,"International Conference on Intelligent User Interfaces, Proceedings IUI",,,,595,606,11.0,25,10.1145/3581641.3584072,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85152122977&doi=10.1145%2f3581641.3584072&partnerID=40&md5=280182ed7fcae5e30fb44830d4008e69,"Mid-air text input in augmented or virtual reality (AR/VR) is an open problem. One proposed solution is gesture typing where the user performs a gesture trace over the keyboard. However, this requires the user to move their hands precisely and continuously, potentially causing arm fatigue. With eye tracking available on AR/VR devices, multiple works have proposed gaze-driven gesture typing techniques. However, such techniques require the explicit use of gaze which are prone to Midas touch problems, conflicting with other gaze activities in the same moment. In this work, the user is not made aware that their gaze is being used to improve the interaction, making the use of gaze completely implicit. We observed that a user's implicit gaze fixation location during gesture typing is usually the gesture cursor's target location if the gesture cursor is moving toward it. Based on this observation, we propose the Speedup method in which we speed up the gesture cursor toward the user's gaze fixation location, the speedup rate depends on how well the gesture cursor's moving direction aligns with the gaze fixation. To reduce the overshooting near the target in the Speedup method, we further proposed the Gaussian Speedup method in which the speedup rate is dynamically reduced with a Gaussian function when the gesture cursor gets nearer to the gaze fixation. Using a wrist IMU as input, a 12-person study demonstrated that the Speedup method and Gaussian Speedup method reduced users' hand movement by and respectively without any loss of typing speed or accuracy.  © 2023 Owner/Author.",gesture input; implicit eye gaze; virtual reality,Gaussian distribution; Location; User interfaces; Virtual reality; Arm fatigue; Eye-gaze; Eye-tracking; Gaussians; Gesture input; Implicit eye gaze; Speed-up method; Target location; Text input; Virtual reality devices; Eye tracking,Conference paper,Final,All Open Access; Bronze Open Access,Scopus,2-s2.0-85152122977,Gaming / VR
Shadiev R.; Li D.,"Shadiev, Rustam (24825659700); Li, Dandan (58065188000)",24825659700; 58065188000,A review study on eye-tracking technology usage in immersive virtual reality learning environments,2023,Computers and Education,196,,104681,,,,55,10.1016/j.compedu.2022.104681,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85146278506&doi=10.1016%2fj.compedu.2022.104681&partnerID=40&md5=7d21e7317c98e65ae7e5484e552d5506,"This systematic review study synthesizes research findings pertaining to the use of eye-tracking technology in immersive virtual reality (IVR) learning environments created by using head mounted displays. Research published between January 2012 and May 2022 has been explored and specific inclusion and exclusion criteria have been applied to select articles. Fifty articles were selected and then reviewed with respect to (1) devices employed in research; (2) learning domains; (3) the number and academic level of participants, data collection methods, and duration of IVR activities; and (4) indicators and themes of visual attention. The main findings from this review demonstrate Tobii and HTC Vive are the most popular tools in research on eye-tracking technology usage in IVR learning environments. What's more, cognitive science and educational technology were found to be the most frequent domains in research on eye-tracking technology usage in IVR. The number of participants varied across reviewed articles but tertiary education was the most common academic level of participating students. Scholars used questionnaires and tests frequently to collect their research data. Fixation duration was the most frequently used indicator. The most frequent themes in reviewed research were task performance, teaching and learning strategies, and learning tools. The study concludes with suggestions for future research. © 2022 Elsevier Ltd",Applications in subject areas; Augmented and virtual reality; Data science applications in education; Human-computer interface; Improving classroom teaching,Behavioral research; Computer aided instruction; E-learning; Eye tracking; Helmet mounted displays; Learning systems; Virtual reality; Application in education; Applications in subject areas; Augmented and virtual realities; Data science application in education; Eye tracking technologies; Human computer interfaces; Immersive virtual reality; Improving classroom teaching; Science applications; Technology usages; Data acquisition,Article,Final,,Scopus,2-s2.0-85146278506,Gaming / VR
Sevcenko N.; Appel T.; Ninaus M.; Moeller K.; Gerjets P.,"Sevcenko, Natalia (57211756979); Appel, Tobias (57191500368); Ninaus, Manuel (55797096600); Moeller, Korbinian (23019055400); Gerjets, Peter (6507437265)",57211756979; 57191500368; 55797096600; 23019055400; 6507437265,Theory-based approach for assessing cognitive load during time-critical resource-managing human–computer interactions: an eye-tracking study,2023,Journal on Multimodal User Interfaces,17,1,,1,19,18.0,19,10.1007/s12193-022-00398-y,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85142903104&doi=10.1007%2fs12193-022-00398-y&partnerID=40&md5=f200ca670f01c4fa46560b60a0f21a21,"Computerized systems are taking on increasingly complex tasks. Consequently, monitoring automated computerized systems is becoming increasingly demanding for human operators, which is particularly relevant in time-critical situations. A possible solution might be adapting human–computer interfaces (HCI) to the operators’ cognitive load. Here, we present a novel approach for theory-based measurement of cognitive load based on tracking eye movements of 42 participants while playing a serious game simulating time-critical situations that required resource management at different levels of difficulty. Gaze data was collected within narrow time periods, calculated based on log data interpreted in the light of the time-based resource-sharing model. Our results indicated that eye fixation frequency, saccadic rate, and pupil diameter significantly predicted task difficulty, while performance was best predicted by eye fixation frequency. Subjectively perceived cognitive load was significantly associated with the rate of microsaccades. Moreover our results indicated that more successful players tended to use breaks in gameplay to actively monitor the scene, while players who use these times to rest are more likely to fail the level. The presented approach seems promising for measuring cognitive load in realistic situations, considering adaptation of HCI. © 2022, The Author(s).",Cognitive ergonomics; Cognitive load; Eye-tracking; HCI; Human–machine interaction; Serious game; Time-critical,Ergonomics; Eye movements; Eye tracking; Human computer interaction; Cognitive ergonomics; Cognitive loads; Computerized systems; Critical resources; Eye fixations; Eye-tracking; Eye-tracking studies; Human computer interfaces; Human machine interaction; Time-critical; Serious games,Article,Final,All Open Access; Hybrid Gold Open Access,Scopus,2-s2.0-85142903104,Gaming / VR
Yabutani M.; Uchida H.; Mizutani K.; Wakatuki N.; Zempo K.,"Yabutani, Mizuki (57955910400); Uchida, Hiroki (57349020700); Mizutani, Koichi (7202243508); Wakatuki, Naoto (58147583500); Zempo, Keiichi (55329072700)",57955910400; 57349020700; 7202243508; 58147583500; 55329072700,Melting into shadow: Toward less cognitively loaded communication in the dark,2023,ACM International Conference Proceeding Series,,,,338,341,3.0,1,10.1145/3582700.3583712,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85150395407&doi=10.1145%2f3582700.3583712&partnerID=40&md5=8a5e725aae4d7978fddb4bdffe4b1c92,"As digital devices become an integral part of daily life, people are increasingly experiencing fatigue from constant exposure to information. For example, videoconferencing has been found to cause fatigue, attributed to the cognitive load caused by constant exposure to the gaze of multiple persons and the effort to acquire limited non-verbal information from visual sources. However, current videoconferencing and VR technologies are being developed to increase the amount of visual information to enhance telepresence, resulting in a higher cognitive load.Therefore, in this study, we focused on low-light environments where the amount of visual information is low, and by using shadows instead of camera images as a communication interface, we developed a system that enables communication with people in remote locations with low cognitive load for long periods of time without disturbing their immersion in other content. We conducted an experiment in which participants were asked to read a manga for 5 minutes while engaging in a brief conversation about the manga by using this system every minute. The results showed that the system significantly reduced vertical eye movement compared to videoconferencing using a display (p < 0.05) and that the system did not interfere with concentration on reading manga more than videoconferencing. These results suggest that this system is more effective than videoconferencing in terms of sharing the same time with person located remotely with minimal cognitive load, even while they are connected for a long time and doing in different activities.  © 2023 Owner/Author.",dark; low cognitive load; shadow,Eye movements; Visual communication; 'current; Cognitive loads; Daily lives; Dark; Integral part; Low cognitive load; Non-verbal information; Shadow; Visual information; VR technology; Digital devices,Conference paper,Final,,Scopus,2-s2.0-85150395407,Gaming / VR
Gou Q.; Li S.,"Gou, Qifeng (57954419200); Li, Sunnan (57835779300)",57954419200; 57835779300,Study on the correlation between basketball players' multiple-object tracking ability and sports decision-making,2023,PLoS ONE,18,4-Apr,e0283965,,,,11,10.1371/journal.pone.0283965,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85151798101&doi=10.1371%2fjournal.pone.0283965&partnerID=40&md5=c839ff069b8c3442d31cf1d86a3f8af4,"Background: Players' multiple-object tracking (MOT) ability is very important in basketball because it may affect players' sports decision-making (SDM), thus affecting the results of the game. The purpose of this study was to investigate the differences between expert and novice basketball players in MOT ability and SDM and to explore the correlation between basketball players' visual attention and SDM. Methods: A total of 48 female basketball players (24 categorized in the expert group and 24 in the novice group) participated in the MOT task in Experiment 1 and the basketball 3 vs. 3 games in Experiment 2. Experiment 1 examined the difference in dynamic visual attention characteristics between expert players and novice players by changing the tracking number. Experiment 2 examined the differences between expert players and novice players through the SDM of basketball 3 vs. 3 games. Sports decisions were evaluated by basketball experts. MOT ability and SDM ability were analyzed through Pearson correlation. Results: The overall MOT accuracy of expert players (64.6%) and novice players (55.7%) was significantly different (χ2 = 59.693, P = 0.000). There was no significant difference in accuracy when tracking 2-3 targets (P > 0.05), but there was a significant difference in accuracy when tracking 4-6 targets (P < 0.05). The overall SDM accuracy of expert players (91.6%) and novice players (84.5%) was significantly different (χ2 = 31.975, P = 0.000). There was no significant difference between expert players and novice players in the accuracy of dribbling decision-making (P > 0.05), but there was a significant difference in the accuracy of passing decision-making and shooting decision-making (P < 0.01). When tracking 4-5 targets, the tracking score was positively correlated with the passing decision score and dribbling decision score of expert players, and the tracking score of novice players was positively correlated with the passing decision score (r > 0.6, P < 0.01). Conclusions: First, the tracking accuracy of expert players was significantly higher than that of novice players, especially when tracking 4-6 targets. As the number of targets increased, accuracy decreased. Second, the accuracy of expert players' SDM was significantly higher than that of novice players, especially in passing decision-making and shooting decision-making. Expert players exhibited fast and accurate SDM. Third, there was a correlation between MOT ability and SDM performance. The MOT ability of 4-5 targets was positively correlated with passing decision-making, which was statistically significant. The correlation between the MOT ability and SDM performance of expert players was greater and more significant. Having too many targets to track (more than 6) interfered with players' decisions.  © 2023 Gou, Li. This is an open access article distributed under the terms of the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.",,Athletic Performance; Basketball; Female; Humans; adult; article; basketball; basketball player; decision making; eye tracking; female; human; visual attention; athletic performance,Article,Final,All Open Access; Gold Open Access; Green Open Access,Scopus,2-s2.0-85151798101,Gaming / VR
Zhu Y.; Min X.; Zhu D.; Zhai G.; Yang X.; Zhang W.; Gu K.; Zhou J.,"Zhu, Yucheng (57189598758); Min, Xiongkuo (56030205300); Zhu, Dandan (57197848549); Zhai, Guangtao (15847120000); Yang, Xiaokang (7406503333); Zhang, Wenjun (57206987547); Gu, Ke (55265130000); Zhou, Jiantao (55938080200)",57189598758; 56030205300; 57197848549; 15847120000; 7406503333; 57206987547; 55265130000; 55938080200,Toward Visual Behavior and Attention Understanding for Augmented 360 Degree Videos,2023,"ACM Transactions on Multimedia Computing, Communications and Applications",19,,99,,,,23,10.1145/3565024,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85162874868&doi=10.1145%2f3565024&partnerID=40&md5=8c3cd22c08d5edd4fb756f0dff607066,"Augmented reality (AR) overlays digital content onto reality. In an AR system, correct and precise estimations of user visual fixations and head movements can enhance the quality of experience by allocating more computational resources for analyzing, rendering, and 3D registration on the areas of interest. However, there is inadequate research to help in understanding the visual explorations of the users when using an AR system or modeling AR visual attention. To bridge the gap between the saliency prediction on real-world scenes and on scenes augmented by virtual information, we construct the ARVR saliency dataset. The virtual reality (VR) technique is employed to simulate the real-world. Annotations of object recognition and tracking as augmented contents are blended into omnidirectional videos. The saliency annotations of head and eye movements for both original and augmented videos are collected and together constitute the ARVR dataset. We also design a model that is capable of solving the saliency prediction problem in AR. Local block images are extracted to simulate the viewport and offset the projection distortion. Conspicuous visual cues in the local block images are extracted to constitute the spatial features. The optical flow information is estimated as an important temporal feature. We also consider the interplay between virtual information and reality. The composition of the augmentation information is distinguished, and the joint effects of adversarial augmentation and complementary augmentation are estimated. The Markov chain is constructed with block images as graph nodes. In the determination of the edge weights, both the characteristics of the viewing behaviors and the visual saliency mechanisms are considered. The order of importance for block images is estimated through the state of equilibrium of the Markov chain. Extensive experiments are conducted to demonstrate the effectiveness of the proposed method.  © 2023 Association for Computing Machinery.",Augmented reality; saliency prediction; virtual reality; visual attention; visual behavior,Augmented reality; Eye movements; Image annotation; Optical flows; Video analysis; Virtual environments; Virtual reality; Augmented reality systems; Computational resources; Digital contents; Head movements; Real-world; Saliency prediction; Virtual information; Visual Attention; Visual behavior; Visual fixations; Markov chains,Article,Final,,Scopus,2-s2.0-85162874868,Gaming / VR
Vasiljevas M.; Damaševičius R.; Maskeliūnas R.,"Vasiljevas, Mindaugas (55366519000); Damaševičius, Robertas (6603451290); Maskeliūnas, Rytis (27467587600)",55366519000; 6603451290; 27467587600,A Human-Adaptive Model for User Performance and Fatigue Evaluation during Gaze-Tracking Tasks,2023,Electronics (Switzerland),12,5,1130,,,,7,10.3390/electronics12051130,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85149981687&doi=10.3390%2felectronics12051130&partnerID=40&md5=e5063c61014ce60d0ce1e001cd3d3903,"Eye gaze interfaces are an emerging technology that allows users to control graphical user interfaces (GUIs) simply by looking at them. However, using gaze-controlled GUIs can be a demanding task, resulting in high cognitive and physical load and fatigue. To address these challenges, we propose the concept and model of an adaptive human-assistive human–computer interface (HA-HCI) based on biofeedback. This model enables effective and sustainable use of computer GUIs controlled by physiological signals such as gaze data. The proposed model allows for analytical human performance monitoring and evaluation during human–computer interaction processes based on the damped harmonic oscillator (DHO) model. To test the validity of this model, the authors acquired gaze-tracking data from 12 healthy volunteers playing a gaze-controlled computer game and analyzed it using odd–even statistical analysis. The experimental findings show that the proposed model effectively describes and explains gaze-tracking performance dynamics, including subject variability in performance of GUI control tasks, long-term fatigue, and training effects, as well as short-term recovery of user performance during gaze-tracking-based control tasks. We also analyze the existing HCI and human performance models and develop an extension to the existing physiological models that allows for the development of adaptive user-performance-aware interfaces. The proposed HA-HCI model describes the interaction between a human and a physiological computing system (PCS) from the user performance perspective, incorporating a performance evaluation procedure that interacts with the standard UI components of the PCS and describes how the system should react to loss of productivity (performance). We further demonstrate the applicability of the HA-HCI model by designing an eye-controlled game. We also develop an analytical user performance model based on damped harmonic oscillation that is suitable for describing variability in performance of a PC game based on gaze tracking. The model’s validity is tested using odd–even analysis, which demonstrates strong positive correlation. Individual characteristics of users established by the damped oscillation model can be used for categorization of players under their playing skills and abilities. The experimental findings suggest that players can be categorized as learners, whose damping factor is negative, and fatiguers, whose damping factor is positive. We find a strong positive correlation between amplitude and damping factor, indicating that good starters usually have higher fatigue rates, but slow starters have less fatigue and may even improve their performance during play. The proposed HA-HCI model and analytical user performance models provide a framework for developing an adaptive human-oriented HCI that enables monitoring, analysis, and increased performance of users working with physiological-computing-based user interfaces. The proposed models have potential applications in improving the usability of future human-assistive gaze-controlled interface systems. © 2023 by the authors.",central moments; circular statistics; eye tracking; gaze tracking; human–computer interaction; human–computer interface; mental fatigue; usability,,Article,Final,All Open Access; Gold Open Access,Scopus,2-s2.0-85149981687,Gaming / VR
Bernard F.; Clavreul A.; Casanova M.; Besnard J.; Lemée J.-M.; Soulard G.; Séguier R.; Menei P.,"Bernard, Florian (55794323100); Clavreul, Anne (6602967441); Casanova, Morgane (57222634199); Besnard, Jérémy (36494043800); Lemée, Jean-Michel (56784165300); Soulard, Gwénaëlle (57208444774); Séguier, Renaud (6505976299); Menei, Philippe (56277676100)",55794323100; 6602967441; 57222634199; 36494043800; 56784165300; 57208444774; 6505976299; 56277676100,Virtual Reality-Assisted Awake Craniotomy: A Retrospective Study,2023,Cancers,15,3,949,,,,2,10.3390/cancers15030949,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85147861446&doi=10.3390%2fcancers15030949&partnerID=40&md5=1fafa422c51ef8643df7553de80d8ec1,"Background: Awake craniotomy (AC) with brain mapping for language and motor functions is often performed for tumors within or adjacent to eloquent brain regions. However, other important functions, such as vision and visuospatial and social cognition, are less frequently mapped, at least partly due to the difficulty of defining tasks suitable for the constrained AC environment. Objective: The aim of this retrospective study was to demonstrate, through illustrative cases, how a virtual reality headset (VRH) equipped with eye tracking can open up new possibilities for the mapping of language, the visual field and complex cognitive functions in the operating room. Methods: Virtual reality (VR) tasks performed during 69 ACs were evaluated retrospectively. Three types of VR tasks were used: VR-DO80 for language evaluation, VR-Esterman for visual field assessment and VR-TANGO for the evaluation of visuospatial and social functions. Results: Surgery was performed on the right hemisphere for 29 of the 69 ACs performed (42.0%). One AC (1.5%) was performed with all three VR tasks, 14 ACs (20.3%) were performed with two VR tasks and 54 ACs (78.3%) were performed with one VR task. The median duration of VRH use per patient was 15.5 min. None of the patients had “VR sickness”. Only transitory focal seizures of no consequence and unrelated to VRH use were observed during AC. Patients were able to perform all VR tasks. Eye tracking was functional, enabling the medical team to analyze the patients’ attention and exploration of the visual field of the VRH directly. Conclusions: This preliminary experiment shows that VR approaches can provide neurosurgeons with a way of investigating various functions, including social cognition during AC. Given the rapid advances in VR technology and the unbelievable sense of immersion provided by the most recent devices, there is a need for ongoing reflection and discussions of the ethical and methodological considerations associated with the use of these advanced technologies in AC and brain mapping procedures. © 2023 by the authors.",awake craniotomy; social cognition; unilateral spatial neglect; virtual reality; visual field; visuospatial cognition,adult; aged; Article; attention; awake craniotomy; cognition; controlled study; depth perception; eye tracking; female; focal seizures; human; language; major clinical study; male; neurosurgeon; operation duration; preliminary data; retrospective study; right hemisphere; social cognition; social status; task performance; virtual reality; visual field,Article,Final,All Open Access; Gold Open Access; Green Open Access,Scopus,2-s2.0-85147861446,Gaming / VR
Masullo M.; Cioffi F.; Li J.; Maffei L.; Ciampi G.; Sibilio S.; Scorpio M.,"Masullo, Massimiliano (35737296400); Cioffi, Federico (57203484916); Li, Jian (57222100755); Maffei, Luigi (7103160021); Ciampi, Giovanni (55767442200); Sibilio, Sergio (55892665600); Scorpio, Michelangelo (55893596900)",35737296400; 57203484916; 57222100755; 7103160021; 55767442200; 55892665600; 55893596900,Urban Park Lighting Quality Perception: An Immersive Virtual Reality Experiment,2023,Sustainability (Switzerland),15,3,2069,,,,24,10.3390/su15032069,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85147914086&doi=10.3390%2fsu15032069&partnerID=40&md5=e7a4a79869ffe32c318880e54531dce6,"Green areas and parks are increasingly important in improving citizens’ physical and mental recovery. Lighting systems play a considerable role in affecting city park life and activities along with people’s moods and behavior in the evening and at night. Immersive virtual reality laboratory experiments may support urban and lighting research by providing information on the combination of lighting setup and visual context of existing or new urban parks. Gaze behaviors obtained from eye-tracking recordings and self-reported measurements using the perceived outdoor lighting quality questionnaire were used to determine the factors affecting human perception, comfort, and cognitive load, as the overall illuminance levels of the scene and correlated color temperature changes. Results pointed out that overall illuminance level and CCT significantly affect the perceived strength and comfort qualities of lighting with a dominance of the first compared with the latter when subjects were free to explore the lit environment. Low CCT and intermediate or high overall illuminance levels can improve the sense of accessibility as well as minimize the cognitive load. © 2023 by the authors.",correlated color temperature; outdoor illuminance level; urban park; virtual reality; visual perception,greenspace; perception; questionnaire survey; urban area; virtual reality,Article,Final,All Open Access; Gold Open Access,Scopus,2-s2.0-85147914086,Gaming / VR
Matsangidou M.; Solomou T.; Frangoudes F.; Ioannou K.; Theofanous P.; Papayianni E.; Pattichis C.S.,"Matsangidou, Maria (57196007400); Solomou, Theodoros (57674397800); Frangoudes, Fotos (55825605500); Ioannou, Konstantinos (58118790300); Theofanous, Panagiotis (58118959400); Papayianni, Ersi (57218311417); Pattichis, Constantinos S. (35495139000)",57196007400; 57674397800; 55825605500; 58118790300; 58118959400; 57218311417; 35495139000,Affective Out-World Experience via Virtual Reality for Older Adults Living with Mild Cognitive Impairments or Mild Dementia,2023,International Journal of Environmental Research and Public Health,20,4,2919,,,,19,10.3390/ijerph20042919,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85148965600&doi=10.3390%2fijerph20042919&partnerID=40&md5=a609887f13b7bb0bd0c76aaabe45a7a3,"Older adults with cognitive impairments may face barriers to accessing experiences beyond their physical premises. Previous research has suggested that missing out on emotional experiences may affect mental health and impact cognitive abilities. In recent years, there has been growing research interest in designing non-pharmacological interventions to improve the health-related quality of life of older adults. With virtual reality offering endless opportunities for health support, we must consider how virtual reality can be sensitively designed to provide comfortable, enriching out-world experiences to older adults to enhance their emotional regulation. Thirty older adults living with mild cognitive impairment or mild dementia participated in the study. Affect and emotional behavior were measured. The usability and the sense of presence were also assessed. Finally, we assessed the virtual reality experiences based on physiological responses and eye-tracking data. The results indicated that virtual reality can positively enhance the mental health of this population by eliciting a positive affective state and enhancing their emotional regulation. Overall, this paper raises awareness of the role of virtual reality in emotion elicitation, regulation, and expression and enhances our understanding of the use of virtual reality by older adults living with mild cognitive impairments or mild dementia. © 2023 by the authors.",affective computing; dementia; emotional experiences; human-centered technologies; mild cognitive impairment; older adults; user experience; virtual reality,Aged; Cognition; Cognitive Dysfunction; Dementia; Humans; Quality of Life; Virtual Reality; elderly care; elderly population; mental health; quality of life; virtual reality; adult; affect; affective computing; aged; Article; clinical article; controlled study; dementia; disease severity; emotion; emotion regulation; emotional disorder; emotional experience; experience; eye-tracking technology; female; geriatric patient; human; human computer interaction; male; mental health; mild cognitive impairment; out world experience; personal experience; population research; user experience; very elderly; virtual reality; virtual reality exposure therapy; cognition; cognitive defect; psychology; quality of life,Article,Final,All Open Access; Gold Open Access; Green Open Access,Scopus,2-s2.0-85148965600,Gaming / VR
Fan L.; Wang J.; Li Q.; Song Z.; Dong J.; Bao F.; Wang X.,"Fan, Lei (58193253500); Wang, Junjie (56587084600); Li, Qi (55703508000); Song, Zhenhao (58193992000); Dong, Jinhui (57849284900); Bao, Fangjun (24343123500); Wang, Xiaofei (36769701900)",58193253500; 56587084600; 55703508000; 58193992000; 57849284900; 24343123500; 36769701900,Eye movement characteristics and visual fatigue assessment of virtual reality games with different interaction modes,2023,Frontiers in Neuroscience,17,,1173127,,,,16,10.3389/fnins.2023.1173127,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85153355190&doi=10.3389%2ffnins.2023.1173127&partnerID=40&md5=c4c0acf3a0dd46d2d437d8517a81c1b6,"This study aimed to investigate the eye movement characteristics and visual fatigue of virtual reality games with different interaction modes. Eye movement data were recorded using the built-in eye tracker of the VR device and eye movement parameters were calculated from the recorded raw data. The Visual Fatigue Scales and Simulator Sickness Questionnaire were used to subjectively assess visual fatigue and overall discomfort of the VR experience. Sixteen male and 17 female students were recruited for this study. Results showed that both the primary and 360 mode of VR could cause visual fatigue after 30 min of gameplay, with significant differences observed in eye movement behavior between the two modes. The primary mode was more likely to cause visual fatigue, as shown by objective measurements of blinking and pupil diameter. Fixation and saccade parameters also showed significant differences between the two modes, possibly due to the different interaction modes employed in the 360 mode. Further research is required to examine the effects of different content and interactive modes of VR on visual fatigue, as well as to develop more objective measures for assessing it. Copyright © 2023 Fan, Wang, Li, Song, Dong, Bao and Wang.",eye movements; interaction mode; video games; virtual reality; visual fatigue,adult; Article; best corrected visual acuity; blinking; blurred vision; clinical article; disorientation; dizziness; eye fixation; eye movement; eye movement disorder; fatigue; fatigue scales and simulator sickness questionnaire; female; Functional Assessment of Chronic Illness Therapy Fatigue Scale; hiccup; human; hypersalivation; luminance; male; nausea; pupil diameter; reliability; rod-cone interaction; saccadic eye movement; Simulator Sickness Questionnaire; sweating; video game; virtual reality; young adult,Article,Final,All Open Access; Gold Open Access; Green Open Access,Scopus,2-s2.0-85153355190,Gaming / VR
Alhasan A.; Caruana N.,"Alhasan, Ayeh (57219895969); Caruana, Nathan (56251828400)",57219895969; 56251828400,Evidence for the adaptive parsing of non-communicative eye movements during joint attention interactions,2023,PeerJ,11,,e16363,,,,1,10.7717/peerj.16363,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85180990745&doi=10.7717%2fpeerj.16363&partnerID=40&md5=9c10071344b149142821bb661a2d50fe,"During social interactions, the ability to detect and respond to gaze-based joint attention bids often involves the evaluation of non-communicative eye movements. However, very little is known about how much humans are able to track and parse spatial information from these non-communicative eye movements over time, and the extent to which this influences joint attention outcomes. This was investigated in the current study using an interactive computer-based joint attention game. Using a fully within-subjects design, we specifically examined whether participants were quicker to respond to communicative joint attention bids that followed predictive, as opposed to random or no, non-communicative gaze behaviour. Our results suggest that in complex, dynamic tasks, people adaptively use and dismiss non-communicative gaze information depending on whether it informs the locus of an upcoming joint attention bid. We also went further to examine the extent to which this ability to track dynamic spatial information was specific to processing gaze information. This was achieved by comparing performance to a closely matched non-social task where eye gaze cues were replaced with dynamic arrow stimuli. Whilst we found that people are also able to track and use dynamic non-social information from arrows, there was clear evidence for a relative advantage for tracking gaze cues during social interactions. The implications of these findings for social neuroscience and autism research are discussed. © 2023 PeerJ Inc.. All rights reserved.",Attention; Eye-tracking; Joint attention; Non-verbal communication; Ostensive signals; Relevance Theory; Social communication signal,accuracy; adaptation; adult; algorithm; Article; attention; autism; behavior; cognition; dynamics; eye movement; eye tracking; eye-tracking technology; female; follow up; gaze; human; human experiment; information; interpersonal communication; joint; male; model fit analysis; nerve injury; neuroscience; nonverbal communication; psychology; rating scale; reaction time; social interaction; spatial analysis; stimulus; task performance; visual attention; young adult,Article,Final,All Open Access; Gold Open Access; Green Open Access,Scopus,2-s2.0-85180990745,Gaming / VR
Gao H.; Niu J.; Wang C.; Xie Y.; Niu Y.,"Gao, Haowen (58080842400); Niu, Jiaxin (57222010816); Wang, Chang (57207369475); Xie, Yijia (58160887500); Niu, Yifeng (56222112100)",58080842400; 57222010816; 57207369475; 58160887500; 56222112100,Evaluating Multi-UAV Operator’s Cognitive Workload Using Eye Tracking Data,2023,Lecture Notes in Electrical Engineering,1010 LNEE,,,2167,2177,10.0,1,10.1007/978-981-99-0479-2_202,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85151051343&doi=10.1007%2f978-981-99-0479-2_202&partnerID=40&md5=8c5eb3357bdf4fd3e70931a2d60b24fd,"In order to solve the problem of crowded task flow output in the process of multi-UAV task monitoring, this paper evaluates the cognitive load by analyzing the operator's eye movement data to feedback the task flow status. Based on our previous work, we expand the feature dimension of the eye tracking data, improve the eye tracking classification, and optimize the evaluation of cognitive busy states. Then, we gradually increase the number of UAVs, analyze the characteristics of human operation and their attention distribution in different numbers and operation modes. A cognitive workload assessment method is proposed that takes the task participation time as the main evaluation feature. Finally, we validate the two proposed methods in a word search game. © 2023, Beijing HIWING Sci. and Tech. Info Inst.",Cognitive workload; Eye tracking; Global monitoring; Supervisory control,Aircraft detection; Eye movements; Unmanned aerial vehicles (UAV); Cognitive loads; Cognitive workloads; Eye movement datum; Eye-tracking; Global monitoring; Multi UAV; Supervisory control; Task flows; Task monitoring; Tracking data; Eye tracking,Conference paper,Final,,Scopus,2-s2.0-85151051343,Gaming / VR
Raza M.A.; Kiran R.; Ghazal S.; Kang Z.; Salehi S.; Cokely E.; Jeon J.,"Raza, Muhammad A. (57204695917); Kiran, Raj (58694362200); Ghazal, Saima (54956261000); Kang, Ziho (56486906100); Salehi, Saeed (59266592100); Cokely, Edward (16303029600); Jeon, Jiwon (57191577718)",57204695917; 58694362200; 54956261000; 56486906100; 59266592100; 16303029600; 57191577718,An Eye Tracking Based Framework for Safety Improvement of Offshore Operations,2023,Journal of Eye Movement Research,16,3,2,,,,8,10.16910/jemr.16.3.2,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85176559790&doi=10.16910%2fjemr.16.3.2&partnerID=40&md5=97f35067cd4138c0d426eacd87eba749,"Offshore drilling operations consist of complex and high-risk processes. Lack of situational awareness in drilling operations has become an important human factor issue that causes safety accidents. Prolonged work shifts and fatigue are some of the crucial issues that impact performance. Eye tracking technology can be used to distinguish the degree of awareness or alertness of participants that might be related to fatigue or onsite distractions. Oculomotor activity can be used to obtain visual cues that can quantify the drilling operators’ situational awareness that might enable us to develop warning alarms to alert the driller. Such systems can help reduce accidents and save non-productive time. In this paper, eye movement char-acteristics were investigated to differentiate the situational awareness between a representa-tive expert and a group of novices using a scenario-based Virtual Reality Drilling Simulator. Significant visual oculomotor activity differences were identified between the expert and the novices that indicate an eye-tracking based system can detect the distraction and alert-ness exhibited by the workers. Results show promise on developing a framework which implements a real-time eye tracking technology in various drilling operations at drilling rigs and Real Time Operation Centers to improve process safety. © This article is licensed under a Creative Commons Attribution 4.0 International license",Eye tracking; gaze; individual differences; offshore operations; saccades; safety; situational awareness,,Article,Final,All Open Access; Gold Open Access; Green Open Access,Scopus,2-s2.0-85176559790,Gaming / VR
Brescia-Zapata M.; Krejtz K.; Duchowski A.T.; Hughes C.J.; Orero P.,"Brescia-Zapata, Marta (57221691373); Krejtz, Krzysztof (55258716700); Duchowski, Andrew T. (6701824388); Hughes, Christopher J. (57040431000); Orero, Pilar (24921771100)",57221691373; 55258716700; 6701824388; 57040431000; 24921771100,Eye-tracked Evaluation of Subtitles in Immersive VR 360° Video,2023,"Proceedings - 2023 IEEE Conference on Virtual Reality and 3D User Interfaces Abstracts and Workshops, VRW 2023",,,,769,770,1.0,0,10.1109/VRW58643.2023.00227,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85159676504&doi=10.1109%2fVRW58643.2023.00227&partnerID=40&md5=cb7840ed55da05e61857d69e2a452716,The paper presents an analysis of visual attention to subtitles within immersive media. We implement a 360° video rendering with eye movements recorded in Virtual Reality. Position and color of im-mersive subtitles are compared in terms of perceived task load and cognitive processing of the content. Results show that head-locked subtitles afford more focal visual inspection of the scene and pre-sumably better comprehension. This type of in-depth analysis would not be possible without the eye movement analyses.  © 2023 IEEE.,Applied computing; Arts and humanities; Media arts; Psychology,Arts computing; Behavioral research; Virtual reality; Applied computing; Art and humanity; Cognitive processing; Immersive media; Immersive VR; Media arts; Psychology; Video rendering; Visual Attention; Visual inspection; Eye movements,Conference paper,Final,,Scopus,2-s2.0-85159676504,Gaming / VR
Chien Y.-L.; Lee C.-H.; Chiu Y.-N.; Tsai W.-C.; Min Y.-C.; Lin Y.-M.; Wong J.-S.; Tseng Y.-L.,"Chien, Yi-Ling (26535653800); Lee, Chia-Hsin (58064620400); Chiu, Yen-Nan (7202775750); Tsai, Wen-Che (12791356900); Min, Yuan-Che (58064620500); Lin, Yang-Min (58064384800); Wong, Jui-Shen (58064852000); Tseng, Yi-Li (25653143200)",26535653800; 58064620400; 7202775750; 12791356900; 58064620500; 58064384800; 58064852000; 25653143200,Game-Based Social Interaction Platform for Cognitive Assessment of Autism Using Eye Tracking,2023,IEEE Transactions on Neural Systems and Rehabilitation Engineering,31,,,749,758,9.0,21,10.1109/TNSRE.2022.3232369,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85146227267&doi=10.1109%2fTNSRE.2022.3232369&partnerID=40&md5=7f302594dd814dd66a5cc7c53f2c12c6,"The design goals of recently developed serious games are to improve attention, affective recognition, and social interactions among individuals with autism. However, most previous studies on serious games used behavioral questionnaires to evaluate their effectiveness. The cognitive assessment of individuals with autism after behavioral intervention or drug treatment has become important because it provides promising biomarkers to assess improvement after cognitive intervention. In this study, we developed a game-based social interaction platform incorporating an eye-tracking system for children and preadolescents with autism. Three modules (focusing on gaze following, facial emotion recognition, and social interaction skills) are included in the platform; participants with autism learn these according to their cognitive abilities. The eye-tracking results showed decreased fixation durations when autistic children looked at positive emotional expressions and focused on multiple targets. Prolonged saccade durations and shorter fixation times for social-related facial emotion expressions were also found in preadolescents and teenagers with autism. Our findings suggest that these atypical gaze patterns are reliable biomarkers for evaluating the social and cognitive functions of autistic individuals while playing serious games. The proposed platform's game-based modules and the findings regarding aberrant gaze patterns in autistic individuals demonstrate the possibility of evaluating cognitive functions and intervention effectiveness by using eye-tracking signals in a serious game or real-life environment.  © 2001-2011 IEEE.",autism spectrum disorder; eye tracking; serious games; Social interaction game,Biomarkers; Brain; Diseases; Emotion Recognition; Eye tracking; Function evaluation; Serious games; Speech recognition; Autism; Autism spectrum disorders; Emotion recognition; Eye-tracking; Game; Game-Based; Gaze-tracking; Social interaction game; Social interaction platform; Social interactions; accuracy; adolescent; Article; autism; autism assessment; child; clinical article; cognition; controlled study; DSM-5; emotion; eye tracking; facial expression; facial recognition; female; game; gaze; human; ICD-10; male; preschool child; saccadic eye movement; school child; skill; social cognition; social interaction; social status; Face recognition,Article,Final,All Open Access; Gold Open Access,Scopus,2-s2.0-85146227267,Gaming / VR
Mattioli P.; Orso B.; Liguori C.; Famà F.; Giorgetti L.; Donniaquio A.; Massa F.; Giberti A.; Vállez García D.; Meles S.K.; Leenders K.L.; Placidi F.; Spanetta M.; Chiaravalloti A.; Camedda R.; Schillaci O.; Izzi F.; Mercuri N.B.; Pardini M.; Bauckneht M.; Morbelli S.; Nobili F.; Arnaldi D.,"Mattioli, Pietro (57216360491); Orso, Beatrice (57213192590); Liguori, Claudio (55066893500); Famà, Francesco (57076226000); Giorgetti, Laura (57279367200); Donniaquio, Andrea (57216364713); Massa, Federico (57196120473); Giberti, Andrea (57915211900); Vállez García, David (57195461242); Meles, Sanne K. (56677750100); Leenders, Klaus L. (16676330100); Placidi, Fabio (7004897465); Spanetta, Matteo (57211600742); Chiaravalloti, Agostino (55087901500); Camedda, Riccardo (57367386600); Schillaci, Orazio (7006797595); Izzi, Francesca (7801415604); Mercuri, Nicola B. (7004633884); Pardini, Matteo (23493507300); Bauckneht, Matteo (55617887700); Morbelli, Silvia (24471961700); Nobili, Flavio (57206948479); Arnaldi, Dario (35809752900)",57216360491; 57213192590; 55066893500; 57076226000; 57279367200; 57216364713; 57196120473; 57915211900; 57195461242; 56677750100; 16676330100; 7004897465; 57211600742; 55087901500; 57367386600; 7006797595; 7801415604; 7004633884; 23493507300; 55617887700; 24471961700; 57206948479; 35809752900,Derivation and Validation of a Phenoconversion-Related Pattern in Idiopathic Rapid Eye Movement Behavior Disorder,2023,Movement Disorders,38,1,,57,67,10.0,6,10.1002/mds.29236,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85139208333&doi=10.1002%2fmds.29236&partnerID=40&md5=fa73bc94f2685eabccc41c1950d67076,"Background: Idiopathic rapid eye movement sleep behavior disorder (iRBD) represents the prodromal stage of α-synucleinopathies. Reliable biomarkers are needed to predict phenoconversion. Objective: The aim was to derive and validate a brain glucose metabolism pattern related to phenoconversion in iRBD (iRBDconvRP) using spatial covariance analysis (Scaled Subprofile Model and Principal Component Analysis [SSM-PCA]). Methods: Seventy-six consecutive iRBD patients (70 ± 6 years, 15 women) were enrolled in two centers and prospectively evaluated to assess phenoconversion (30 converters, 73 ± 6 years, 14 Parkinson's disease and 16 dementia with Lewy bodies, follow-up time: 21 ± 14 months; 46 nonconverters, 69 ± 6 years, follow-up time: 33 ± 19 months). All patients underwent [18F]FDG-PET (18F-fluorodeoxyglucose positron emitting tomography) to investigate brain glucose metabolism at baseline. SSM-PCA was applied to obtain the iRBDconvRP; nonconverter patients were considered as the reference group. Survival analysis and Cox regression were applied to explore prediction power. Results: First, we derived and validated two distinct center-specific iRBDconvRP that were comparable and significantly able to predict phenoconversion. Then, SSM-PCA was applied to the whole set, identifying the iRBDconvRP. The iRBDconvRP included positive voxel weights in cerebellum; brainstem; anterior cingulate cortex; lentiform nucleus; and middle, mesial temporal, and postcentral areas. Negative voxel weights were found in posterior cingulate, precuneus, middle frontal gyrus, and parietal areas. Receiver operating characteristic analysis showed an area under the curve of 0.85 (sensitivity: 87%, specificity: 72%), discriminating converters from nonconverters. The iRBDconvRP significantly predicted phenoconversion (hazard ratio: 7.42, 95% confidence interval: 2.6–21.4). Conclusions: We derived and validated an iRBDconvRP to efficiently discriminate converter from nonconverter iRBD patients. [18F]FDG-PET pattern analysis has potential as a phenoconversion biomarker in iRBD patients. © 2022 The Authors. Movement Disorders published by Wiley Periodicals LLC on behalf of International Parkinson and Movement Disorder Society. © 2022 The Authors. Movement Disorders published by Wiley Periodicals LLC on behalf of International Parkinson and Movement Disorder Society.",disease-related pattern; fluorodeoxyglucose positron emitting tomography; phenoconversion; rapid eye movement sleep behavior disorder; α-synucleinopathy,"Biomarkers; Female; Fluorodeoxyglucose F18; Glucose; Humans; Parkinson Disease; REM Sleep Behavior Disorder; Sleep, REM; fluorodeoxyglucose f 18; glucose; biological marker; glucose; activity of daily living assessment; aged; anterior cingulate; area under the curve; Article; brain stem; cerebellum; cognition assessment; cohort analysis; controlled study; diffuse Lewy body disease; female; follow up; glucose brain level; glucose metabolism; human; hypermetabolism; leave one out cross validation; major clinical study; male; MDS-Unified Parkinson Disease Rating Scale; middle frontal gyrus; mild cognitive impairment; Mini Mental State Examination; neuropsychological assessment; parasomnia; Parkinson disease; polysomnography; positron emission tomography; posterior cingulate; precuneus; prospective study; sensitivity and specificity; signal noise ratio; survival time; synucleinopathy; T1 weighted imaging; diagnostic imaging; metabolism; Parkinson disease; REM sleep",Article,Final,All Open Access; Green Open Access; Hybrid Gold Open Access,Scopus,2-s2.0-85139208333,Gaming / VR
Zhao S.; Cheng S.; Zhu C.,"Zhao, Song (58161437500); Cheng, Shiwei (25630245300); Zhu, Chenshuang (58161312200)",58161437500; 25630245300; 58161312200,3D Gaze Vis: Sharing Eye Tracking Data Visualization for Collaborative Work in VR Environment,2023,Communications in Computer and Information Science,1682 CCIS,,,610,621,11.0,3,10.1007/978-981-99-2385-4_46,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85161205464&doi=10.1007%2f978-981-99-2385-4_46&partnerID=40&md5=61b409fad7381817e7896504767f2310,"Conducting collaborative tasks, e.g., multi-user game, in virtual reality (VR) could enable us to explore more immersive and effective experience. However, for current VR systems, users cannot communicate properly with each other via their gaze points, and this would interfere with users’ mutual understanding of the intention. In this study, we aimed to find the optimal eye tracking data visualization, which minimized the cognitive interference and improved the understanding of the visual attention and intention between users. We designed three different eye tracking data visualizations: gaze cursor, gaze spotlight and gaze trajectory in VR scene for a course of human heart, and found that gaze cursor from doctors could help students learn complex 3D heart models more effectively. To further explore, two students as a pair were asked to finish a quiz in VR environment, with sharing gaze cursors with each other, and obtained more efficiency and scores. It indicated that sharing eye tracking data visualization could improve the quality and efficiency of collaborative work in the VR environment. © 2023, The Author(s), under exclusive license to Springer Nature Singapore Pte Ltd.",computer supported collaborative learning; Gaze fixation; information visualization; medical visualization,Behavioral research; Data visualization; E-learning; Efficiency; Eye movements; Information systems; Three dimensional computer graphics; Virtual reality; Visualization; Collaborative tasks; Collaborative Work; Computer Supported Collaborative Learning; Eye-tracking; Gaze fixation; Information visualization; Medical visualization; Multi-user games; Tracking data; Virtual-reality environment; Eye tracking,Conference paper,Final,,Scopus,2-s2.0-85161205464,Gaming / VR
Deng X.; Wu S.X.; Ding H.; Zhu Y.,"Deng, Xiaomei (58930004300); Wu, Sissi Xiaoxiao (35957466500); Ding, Huijun (25932070800); Zhu, Yingying (57191698811)",58930004300; 35957466500; 25932070800; 57191698811,Metaverse User Portrait Construction Based on Human Gaze-tracking Data,2023,"Proceedings - 2023 IEEE SmartWorld, Ubiquitous Intelligence and Computing, Autonomous and Trusted Vehicles, Scalable Computing and Communications, Digital Twin, Privacy Computing and Data Security, Metaverse, SmartWorld/UIC/ATC/ScalCom/DigitalTwin/PCDS/Metaverse 2023",,,,,,,0,10.1109/SWC57546.2023.10448591,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85187331588&doi=10.1109%2fSWC57546.2023.10448591&partnerID=40&md5=5c5c4044ec18beab0ee61c78e5aba75c,"With the rapid growth of virtual reality technology in the Metaverse, where a digital virtual world is integrated with the real world, users are exposed to a variety of content in different scenarios. Similar to the Web2 scenario, accurately profiling users in Metaverse applications is critical for improving user satisfaction. This involves collecting and inferring information such as users' basic attributes, interests, and consumption habits. In this study, we propose a new approach for profiling users in virtual worlds based on gaze-tracking. We developed a Metaverse shopping experiment platform and recruited volunteers to record their attention time on different commodities in 3-D scenes by wearing eye-tracking devices. We treat this gaze-tracking data as implicit feedback to capture users' interests, thereby predicting users' attributes. We tried different ways of extracting features from collected gaze-tracking data. To our knowledge, we are the first to mine user portraits using gaze-tracking sequences. Our experimental results demonstrate that the fusion of sequence and image features significantly improves the accuracy of user gender prediction, reaching 88.89%, which is better than using sequence or image features only. These findings demonstrated the practical and effective advantages of using gaze-tracking data to build user portraits in the Metaverse. © 2023 IEEE.",feature fusion; gaze-tracking; Metaverse; time sequence,Image enhancement; User profile; Virtual reality; Features fusions; Gaze-tracking; Image features; Metaverses; Rapid growth; Sequence features; Time sequences; Tracking data; Virtual reality technology; Virtual worlds; Eye tracking,Conference paper,Final,,Scopus,2-s2.0-85187331588,Gaming / VR
Guo R.; Lin Y.; Luo X.; Gao X.; Zhang S.,"Guo, Rongxiao (57918522200); Lin, Yanfei (55265036200); Luo, Xi (57918720000); Gao, Xiaorong (7403873062); Zhang, Shangen (57193686680)",57918522200; 55265036200; 57918720000; 7403873062; 57193686680,A robotic arm control system with simultaneous and sequential modes combining eye-tracking with steady-state visual evoked potential in virtual reality environment,2023,Frontiers in Neurorobotics,17,,1146415,,,,5,10.3389/fnbot.2023.1146415,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85152514204&doi=10.3389%2ffnbot.2023.1146415&partnerID=40&md5=16b5056ccfe08ca6b7e5674595eb49ab,"At present, single-modal brain-computer interface (BCI) still has limitations in practical application, such as low flexibility, poor autonomy, and easy fatigue for subjects. This study developed an asynchronous robotic arm control system based on steady-state visual evoked potentials (SSVEP) and eye-tracking in virtual reality (VR) environment, including simultaneous and sequential modes. For simultaneous mode, target classification was realized by decision-level fusion of electroencephalography (EEG) and eye-gaze. The stimulus duration for each subject was non-fixed, which was determined by an adjustable window method. Subjects could autonomously control the start and stop of the system using triple blink and eye closure, respectively. For sequential mode, no calibration was conducted before operation. First, subjects’ gaze area was obtained through eye-gaze, and then only few stimulus blocks began to flicker. Next, target classification was determined using EEG. Additionally, subjects could reject false triggering commands using eye closure. In this study, the system effectiveness was verified through offline experiment and online robotic-arm grasping experiment. Twenty subjects participated in offline experiment. For simultaneous mode, average ACC and ITR at the stimulus duration of 0.9 s were 90.50% and 60.02 bits/min, respectively. For sequential mode, average ACC and ITR at the stimulus duration of 1.4 s were 90.47% and 45.38 bits/min, respectively. Fifteen subjects successfully completed the online tasks of grabbing balls in both modes, and most subjects preferred the sequential mode. The proposed hybrid brain-computer interface (h-BCI) system could increase autonomy, reduce visual fatigue, meet individual needs, and improve the efficiency of the system. Copyright © 2023 Guo, Lin, Luo, Gao and Zhang.",eye-tracking; hybrid brain-computer interface (h-BCI); robotic arm; steady-state visual evoked potentials (SSVEP); virtual reality (VR),Control theory; Electroencephalography; Electrophysiology; Eye tracking; Interface states; Robotic arms; Virtual reality; Arms control; Eye-gaze; Eye-tracking; Hybrid brain-computer interface; Sequential mode; Steady-state visual evoked potential; Steady-state visual evoked potentials; Target Classification; Virtual reality; Virtual-reality environment; adult; Article; electroencephalography; electroencephalography eye fusion method; experimental test; eye gaze; eye movement; eye tracking; eyelid closure; eyelid reflex; false triggering; fatigue; female; gaze; gaze area; grabbing balls; human; human activities; human experiment; information processing; male; normal human; online control mode; online robotic arm grasping experiment; physical parameters; simultaneous and sequential modes; steady state visual evoked potential; stimulus duration; triple blink; virtual reality; visual evoked potential; Brain computer interface,Article,Final,All Open Access; Gold Open Access; Green Open Access,Scopus,2-s2.0-85152514204,Gaming / VR
Merino V.; Garrido A.,"Merino, Victoria (58614664300); Garrido, Agustín (58613914800)",58614664300; 58613914800,Digital Biomarkers for Early Detection of Cognitive Decline in Alzheimer's Disease,2023,Revista de Psiquiatria Clinica,50,6,,182,188,6.0,1,10.15761/0101-60830000000725,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85172024235&doi=10.15761%2f0101-60830000000725&partnerID=40&md5=a1b8139c786f2dacd6bf5ebbdb8d9740,"Alzheimer's disease (AD), which is characterized by gradual cognitive deterioration, is a serious worldwide health issue. Early AD identification is essential for prompt treatment and better patient outcomes. Digital biomarkers, which are products of many digital technologies, present a potential way to detect cognitive deterioration in AD early on. This article examines a variety of digital biomarkers and how they can be used to detect Alzheimer's disease, including cognitive assessment apps, eye tracking technology, voice analysis, keyboard interaction, wearables, neuroimaging, sensor-equipped homes, machine learning, biometric data, virtual reality, and social media analysis. Digital biomarkers have a lot of potential, but they also come with difficulties including validation, data protection, ethics, and regulatory issues. However, adopting these cutting-edge techniques offers the potential of earlier diagnosis, individualized therapy, and a better comprehension of Alzheimer's disease, marking a critical step towards improved AD management and, in the end, a cure. © 2023, Universidade de Sao Paulo. Museu de Zoologia. All rights reserved.",Alzheimer Disease (AD); Digital Biomarkers (DB); SPSS software,amyloid protein; biological marker; algorithm; Alzheimer disease; anxiety; Article; artificial intelligence; attention; cognition; cognitive defect; cognitive impairment assessment; daily life activity; data privacy; data protection; depression; digital technology; eating; eye movement; functional magnetic resonance imaging; human; language; machine learning; memory; mental deterioration; mental health; nerve cell network; neurofibrillary tangle; neuroimaging; Parkinson disease; personalized medicine; physiological stress; positron emission tomography; sleep; social change; social media; social media analysis; treatment outcome; virtual reality; voice analysis; walking,Article,Final,,Scopus,2-s2.0-85172024235,Gaming / VR
Zhao H.; Bian S.; Liu X.; Jing S.,"Zhao, Hongxia (56438159100); Bian, Siyu (58164174500); Liu, Xiwei (55193285400); Jing, Sifeng (55555982500)",56438159100; 58164174500; 55193285400; 55555982500,Learning State Detection with Multimodal Information in Virtual Reality Learning,2023,"2023 IEEE 3rd International Conference on Digital Twins and Parallel Intelligence, DTPI 2023",,,,,,,0,10.1109/DTPI59677.2023.10365428,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85182729568&doi=10.1109%2fDTPI59677.2023.10365428&partnerID=40&md5=1789af4d112f6666c3a5c6f499e8c426,"The learning state is a crucial factor that significantly impacts students' learning efficiency and overall learning outcomes. It also serves as a vital indicator for evaluating teachers' effectiveness in their instructional practices. To assess the learning state, various aspects of students' behavior, including their expressions, actions, and language, can effectively reveal valuable insights. Traditionally, studies have focused on investigating the learning state in conventional classroom and online education settings. However, limited research has been conducted in the context of virtual reality (VR) immersive learning environments. Considering the unique challenges posed by VR, such as the occlusion of the upper part of the face by VR devices and the distinct characteristics of VR interaction, we propose a novel method for detecting the learning state while wearing VR during the learning process. This method integrates multimodal information, including video captured by a camera, eye movement sensor data from the VR device, and electroencephalography (EEG) signals from an EEG headband. By comprehensively analyzing these inputs, we can determine the learning state effectively. An experimental evaluation was conducted to validate the effectiveness of the proposed multimodal information infusion learning state detection method in the context of VR immersive learning. The results demonstrate that this approach significantly enhances the accuracy of determining the learning state. Ultimately, this advancement has the potential to improve the overall learning quality and facilitate the widespread adoption of VR in educational settings. © 2023 IEEE.",Action Recognition; EGG; Fatigue Detection; Learning State; Virtual Reality,Computer aided instruction; E-learning; Electroencephalography; Electrophysiology; Eye movements; Learning systems; Students; Action recognition; EGG; Fatigue detection; Immersive learning; Learning state; Multi-modal information; Reality learning; State Detection; Student learning; Virtual reality devices; Virtual reality,Conference paper,Final,,Scopus,2-s2.0-85182729568,Gaming / VR
Chen X.; Wu J.C.; Zi Y.H.; Xue C.Q.; Yin H.F.,"Chen, X. (58865357200); Wu, J.C. (57222358934); Zi, Y.H. (58907987300); Xue, C.Q. (14053333400); Yin, H.F. (58865007200)",58865357200; 57222358934; 58907987300; 14053333400; 58865007200,"Evaluating Pedestrian Wayfinding Behaviour in Day and Night Environments Across Different Urban Zoning via VR, Eye Tracking, and EEG",2023,"2023 IEEE International Conference on Industrial Engineering and Engineering Management, IEEM 2023",,,,163,167,4.0,2,10.1109/IEEM58616.2023.10406571,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85186088854&doi=10.1109%2fIEEM58616.2023.10406571&partnerID=40&md5=59440da21dc12986eff63b1751fed178,"Previous studies have shown significant differences in pedestrian wayfinding behavior in day and night environments, but it is still unclear whether there are differences among different urban zoning. The purpose of this study is to investigate the differences in pedestrian wayfinding behavior among different urban districts in day and night environments. We developed an immersive wayfinding environment by virtual reality (VR), using eye tracking and electroencephalography (EEG) to collect and analyze participants' behavioral and physiological data.. The research results indicate that pedestrians in commercial and residential areas maintain consistency in their route selection strategy. In terms of visual attention, pedestrians in residential and commercial areas exhibit different patterns. At the same time, we found that there were significant differences in the power spectral density of theta in the occipital parietal lobe region related to spatial navigation ability in different urban spaces, while there was no significant difference in the day and night environment. The research results will help to comprehensively understand the differences in wayfinding behavior in different spatial environments. © 2023 IEEE.",EEG; eye-tracking; Urban zoning; Virtual reality; Wayfinding,Behavioral research; Brain; Electrophysiology; Eye tracking; Housing; Spectral density; Virtual reality; Zoning; Behavioral data; Eye-tracking; Immersive; Pedestrian wayfinding; Physiological data; Research results; Residential areas; Route Selection; Urban zoning; Way finding; Electroencephalography,Conference paper,Final,,Scopus,2-s2.0-85186088854,Gaming / VR
Chawoush B.; Draschkow D.; van Ede F.,"Chawoush, Babak (58522942900); Draschkow, Dejan (56270096800); van Ede, Freek (35759550000)",58522942900; 56270096800; 35759550000,Capacity and selection in immersive visual working memory following naturalistic object disappearance,2023,Journal of Vision,23,8,9,,,,4,10.1167/jov.23.8.9,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85166760882&doi=10.1167%2fjov.23.8.9&partnerID=40&md5=0b67115d5425d9f7fa30402295963bbd,"Visual working memory—holding past visual information in mind for upcoming behavior—is commonly studied following the abrupt removal of visual objects from static two-dimensional (2D) displays. In everyday life, visual objects do not typically vanish from the environment in front of us. Rather, visual objects tend to enter working memory following self or object motion: disappearing from view gradually and changing the spatial relation between memoranda and observer. Here, we used virtual reality (VR) to investigate whether two classic findings from visual working memory research—a capacity of around three objects and the reliance on space for object selection—generalize to more naturalistic modes of object disappearance. Our static reference condition mimicked traditional laboratory tasks whereby visual objects were held static in front of the participant and removed from view abruptly. In our critical flow condition, the same visual objects flowed by participants, disappearing from view gradually and behind the observer.We considered visual working memory performance and capacity, as well as space-based mnemonic selection, indexed by directional biases in gaze. Despite vastly distinct modes of object disappearance and altered spatial relations between memoranda and observer, we found comparable capacity and comparable gaze signatures of space-based mnemonic selection. This finding reveals how classic findings from visual working memory research generalize to immersive situations with more naturalistic modes of object disappearance and with dynamic spatial relations between memoranda and observer. © 2023 The Authors",capacity; eye movements; selective attention; virtual reality; visual working memory,"Cognition; Humans; Memory, Short-Term; Virtual Reality; Visual Perception; cognition; human; short term memory; virtual reality; vision",Article,Final,All Open Access; Gold Open Access; Green Open Access,Scopus,2-s2.0-85166760882,Gaming / VR
Trifonova K.; Yoztyurk T.; Slaveykov K.; Stoyanov V.,"Trifonova, K. (57222102758); Yoztyurk, T. (58565419700); Slaveykov, K. (55674561600); Stoyanov, V. (56765780900)",57222102758; 58565419700; 55674561600; 56765780900,THE IMPACT OF VIDEOGAMING ON VISION,2023,General Medicine,25,3,,55,60,5.0,0,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85170048577&partnerID=40&md5=cd47bc1d63559411e60d0501379de52b,"Video games are one of the most common reasons for using various digital devices for entertainment purposes. The aim of our study was to examine the effects of video games on vision through an analysis of the available literature on the subject. The majority of studies have focused on the consequences of digital visual strain from the use of video displays such as burning, irritation, tearing, dryness, discomfort, eye pain and headaches. In childhood, video games can lead to increased frequency of tics, double vision and dizziness associated with tracking fast-moving objects and the need to play without rest to advance to the next level. However, many studies show that playing video games also has positive effects such as improved contrast sensitivity, shorter saccade reaction time, higher spatial resolution of vision, faster visualmotor brain activity, improving visual attention and even expanding the field of vision. Results are promising in treating lazy eye in both children and adults. Limited use of video games as a means of entertainment is required due to the risk of addiction and visual strain. Of course, we should not forget their potential positive effects, for which additional studies are needed. © 2023, Central Medical Library Medical University – Sofia. All rights reserved.",addiction; amblyopia; digital strain; video displays,Article; childhood; contrast sensitivity; diplopia; dizziness; dry eye; epiphora; eye burning; eye discomfort; eye irritation; eye pain; game addiction; headache; human; reaction time; saccadic eye movement; tic; video game; vision; visual attention; visual field,Article,Final,,Scopus,2-s2.0-85170048577,Gaming / VR
Van V.; Okamoto I.,"Van, Volkert (58539875300); Okamoto, Ichiro (58539822500)",58539875300; 58539822500,Assessing the efficacy of virtual reality therapy in the treatment of post-traumatic stress disorder (PTSD) in veterans,2023,Revista de Psiquiatria Clinica,50,3,,26,32,6.0,1,10.15761/0101-60830000000585,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85172364492&doi=10.15761%2f0101-60830000000585&partnerID=40&md5=48f27e7058f6b6f76ecc1aa2246dbea0,"The aim of research study is determine also assessing the efficacy related to virtual reality therapy in the treatment of post traumatic stress disorder in the veterans. This research study based on theory and theoretical analysis related to them. Particularly among veterans who have gone through horrific events while serving in the military, post-horrific stress disorder (PTSD) continues to be a major mental health problem. Although beneficial to varied degrees, traditional therapy techniques frequently fall short in addressing the complex symptomatology of PTSD. Virtual Reality Therapy (VRT) has become a unique and promising method for the treatment of PTSD in recent years. The effectiveness of VRT in the treatment of PTSD in veterans is thoroughly discussed in this research. Controlled exposure treatment is made possible by VRT, which uses realistic virtual worlds to reproduce traumatic situations. The intensity of PTSD symptoms is intended to be lessened by this exposure in conjunction with cognitive restructuring. This research explores the evidence supporting VRT's efficacy, its possible advantages, and the underlying neurological mechanisms using a review of recent literature. © 2023, Universidade de Sao Paulo. Museu de Zoologia. All rights reserved.",Efficacy €; Treatment of post traumatic stress disorder (PTSD); Virtual Therapy (VT),prazosin; serotonin uptake inhibitor; analgesia; anxiety; Article; attention; autism; chronic pain; cognitive behavioral therapy; cognitive rehabilitation; cognitive restructuring; depression; drug dependence; eating disorder; exercise; eye movement desensitization and reprocessing; family therapy; fear; gamification; group therapy; human; memory; mental health; mindfulness meditation; nociception; phobia; posttraumatic stress disorder; psychophysiology; self help; social phobia; support group; trauma-focused cognitive behavioral therapy; veteran; virtual reality exposure therapy; wellbeing,Article,Final,,Scopus,2-s2.0-85172364492,Gaming / VR
Zarour M.; Ben Abdessalem H.; Frasson C.,"Zarour, Mahdi (58399160000); Ben Abdessalem, Hamdi (57202437771); Frasson, Claude (7003506234)",58399160000; 57202437771; 7003506234,Distraction Detection and Monitoring Using Eye Tracking in Virtual Reality,2023,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),13891 LNCS,,,491,503,12.0,3,10.1007/978-3-031-32883-1_44,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85163413952&doi=10.1007%2f978-3-031-32883-1_44&partnerID=40&md5=5565d67c7c6d38c5c2d190b3cce48ade,"Effective learning is highly affected by attention levels. Hence, Intelligent Tutoring Systems and other technologies for learning should be able to monitor the attention levels of learners and detect distractions in real-time to improve the learning process. We study the feasibility of detecting and monitoring visual distraction of participants, while they complete cognitive tasks, using Eye Tracking in a Virtual Reality environment. We also investigate the possibility of improving the attention of participants using relaxation in Virtual Reality. The Eye Tracking distraction model we developed correctly predicts the distraction state of participants with an F1-score of 86%. We also found that the most appropriate window size to detect distraction ranges from three to six seconds. Furthermore, results suggest that our relaxation method significantly decreased the visual distraction of the participants. © 2023, The Author(s), under exclusive license to Springer Nature Switzerland AG.",Attention; Distraction; Eye Tracking; Human Interaction; Virtual Reality,Computer aided instruction; Learning systems; Real time systems; Virtual reality; Attention; Attention level; Distraction; Effective learning; Eye-tracking; Humaninteraction; Intelligent tutoring; Real- time; Tutoring system; Visual distractions; Eye tracking,Conference paper,Final,,Scopus,2-s2.0-85163413952,Gaming / VR
Chahine W.; Hachem N.; Moacdieh N.M.,"Chahine, Wafic (58995976900); Hachem, Nour (58996359500); Moacdieh, Nadine Marie (55583410100)",58995976900; 58996359500; 55583410100,Eye Tracking-Based Adaptive Displays: A Review of the Recent Literature,2023,Proceedings of the Human Factors and Ergonomics Society,67,1,,927,932,5.0,0,10.1177/21695067231192631,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85190967163&doi=10.1177%2f21695067231192631&partnerID=40&md5=517bd7d1497efb8935e302aff26e4aec,"Adaptive displays have long been touted as a means of improving the usability of different types of interfaces. However, purely eye tracking-based adaptive displays have not yet lived up to the initial promise. In many cases, adaptive displays are tailored to users with special needs, developed to supplement virtual reality, or combine eye tracking with other physiological measures. This mapping review focuses instead on recent adaptive displays that rely solely on eye tracking input to understand a user’s needs while interacting with a regular computer display. We aimed to answer three main research questions related to 1) the application domains of such adaptive displays, 2) the eye tracking metrics that have been adopted to track attention allocation in real time, and 3) the adaptation triggering mechanisms. We provide a summary of the current state of eye tracking-based adaptive displays, identify gaps in the literature, and suggest topics for future work. © 2023 Human Factors and Ergonomics Society.",Adaptive display; Display design; Eye tracking,Virtual reality; Adaptive display; Applications domains; Computer display; Display designs; Eye-tracking; Physiological measures; Real- time; Research questions; Special needs; Triggering mechanism; Eye tracking,Conference paper,Final,,Scopus,2-s2.0-85190967163,Gaming / VR
Wu Y.; Lukosch S.; Lukosch H.; Lindeman R.W.; McKee R.D.; Fukuden S.; Ross C.; Collins D.,"Wu, Yuanjie (57204820987); Lukosch, Stephan (12140786200); Lukosch, Heide (34977131800); Lindeman, Robert W. (7006360047); McKee, Ryan Douglas (57222574548); Fukuden, Shunsuke (57670619900); Ross, Cameron (57671833300); Collins, Dave (7403254199)",57204820987; 12140786200; 34977131800; 7006360047; 57222574548; 57670619900; 57671833300; 7403254199,Training mental imagery skills of elite athletes in virtual reality,2023,Frontiers in Virtual Reality,4,,1189717,,,,8,10.3389/frvir.2023.1189717,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85169312084&doi=10.3389%2ffrvir.2023.1189717&partnerID=40&md5=7975c277fd15be9798c986645a7922e7,"Mental imagery practice is widely used to help athletes prepare for competitions, as it can produce motor actions that enhance performance. The goal of imagery training for athletes is to create realistic images in their minds and to familiarize them with certain procedures, environments, and other aspects related to competition. Traditional imagery training methods use still images or videos, and athletes study the pictures or watch the videos in order to mentally rehearse. However, factors such as distractions and low realism can affect the training quality. In this paper, we present a Virtual Reality (VR) solution and a study that explores our hypotheses that H1: high-fidelity VR systems improve mental imagery skills, that H2: the presence of elements such as virtual onlookers or photographers in the VR environment arouse stronger emotional reactions and affect, and that H3: the presence of elements such as onlookers or photographers in the VR environment results in better mental imagery skill improvement. For that purpose, seven elite snow sports athletes were exposed to three training methods, Video, VR-Empty, and VR-Crowded. Our results show that a VR simulation with virtual onlookers (VR-Crowded) can significantly increase heart rate, which can induce increased emotional arousal. The results from validated questionnaires show no significant difference for the three training methods in terms of mental imagery and affect, but the results show an ascending trend for the athlete’s arousal from Video to the VR-Crowded condition. Gaze detection heat maps of interest areas for the two VR conditions support hypothesis H2 that environmental factors such as the presence of photographers, staff, and onlookers can increase head and eye movement, possibly indicating an increase in emotional arousal during imagery training. According to verbal feedback and interviews, athletes are more likely to use innovative training methods (e.g., the high-fidelity VR method) than traditional video-training methods. Copyright © 2023 Wu, Lukosch, Lukosch, Lindeman, McKee, Fukuden, Ross and Collins.",elite athletes; mental imagery; positive and negative affect; sports training; Virtual Reality (VR),,Article,Final,All Open Access; Gold Open Access,Scopus,2-s2.0-85169312084,Gaming / VR
Asish S.M.; Kulshreshth A.K.; Borst C.W.,"Asish, Sarker M. (57215357842); Kulshreshth, Arun K (55315352400); Borst, Christoph W (9736479200)",57215357842; 55315352400; 9736479200,Detecting Distracted Students in an Educational VR Environment Utilizing Machine Learning on EEG and Eye-Gaze Data,2023,"Proceedings - 2023 IEEE Conference on Virtual Reality and 3D User Interfaces Abstracts and Workshops, VRW 2023",,,,703,704,1.0,3,10.1109/VRW58643.2023.00194,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85159663115&doi=10.1109%2fVRW58643.2023.00194&partnerID=40&md5=e9d1b70e3b72d71a69e3665695aeff50,"Virtual Reality (VR) is frequently used in various educational contexts since it could improve knowledge retention compared to traditional learning methods. However, distraction is an unavoidable problem in the educational VR environment due to stress, mind-wandering, unwanted noise/sounds, irrelevant stimuli, etc. We explored the combination of EEG and eye gaze data to detect student distractions in an educational VR environment. We designed an educational VR environment and trained three machine learning models (CNN-LSTM, Random Forest and SVM) to detect distracted students. Our preliminary study results show that Random Forest and CNN-LSTM provide better accuracy (98%) compared to SVM.  © 2023 IEEE.",Computer graphics; Computing methodologies; Computing methodologies; Distraction; Education; EEG; Eye-tracking; Graphics systems and interfaces; Machine Learning; Machine learning; Virtual Reality; Virtual reality,E-learning; Education computing; Eye tracking; Learning systems; Long short-term memory; Students; Computing methodologies; Distraction; Eye-gaze; Eye-tracking; Graphic system; Graphics interface; Machine-learning; Virtual-reality environment; Virtual reality,Conference paper,Final,,Scopus,2-s2.0-85159663115,Gaming / VR
Kitala D.; Łabuś W.; Szatan M.; Hepa A.; Ludyga M.; Ambroziak-Łabuś J.; Ochała-Gierek G.; Bergler-Czop B.; Gierek M.,"Kitala, Diana (57096073000); Łabuś, Wojciech (46761244700); Szatan, Magdalena (57202076565); Hepa, Anna (57958580000); Ludyga, Marcin (58398340900); Ambroziak-Łabuś, Joanna (58393654900); Ochała-Gierek, Gabriela (57687116400); Bergler-Czop, Beata (59157672800); Gierek, Marcin (55520412800)",57096073000; 46761244700; 57202076565; 57958580000; 58398340900; 58393654900; 57687116400; 59157672800; 55520412800,Eye-tracked computer games as a method for pain perception alleviation in chronic wound management,2023,Postepy Dermatologii i Alergologii,40,2,,283,290,7.0,2,10.5114/ada.2022.119970,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85163354470&doi=10.5114%2fada.2022.119970&partnerID=40&md5=0848b151705996a3e41e4d229f7a90dc,"Introduction: Chronic pain frequently accompanies the daily lives of many chronic wound patients. The degree of pain experienced significantly increases when performing medical procedures related to wound management. The use of eye-tracked games in order to distract the patient’s attention from the painful activities performed can be an effective procedure. Aim: Assesment of eye-trackers as a distractors during wound management. Material and methods: Forty patients suffering from chronic wounds were qualified for the study. Patients performed eye tracking games during dressing changes and wound cleaning. Pain sensations were surveyed. The survey concerned the pain experienced on a daily basis, when changing the dressing without use and with the use of eye trackers. Results: It was found that eye trackers significantly reduced the pain experienced during dressing changes compared to the pain caused by these procedures, but without the use of eye trackers. Conclusions: On the basis of the obtained results, it was proposed to introduce eye trackers into routine clinical practice during chronic wound management. © 2023 Termedia Publishing House Ltd.. All rights reserved.",chronic wounds; dressing change; eye trackers; pain management,analgesic agent; adult; aged; analgesia; Article; chronic wound; clinical article; controlled study; eye tracking; female; human; male; middle aged; nociception; outpatient department; procedural pain; video game; visual analog scale; vocational education; wound care,Article,Final,All Open Access; Gold Open Access; Green Open Access,Scopus,2-s2.0-85163354470,Gaming / VR
Alarifi H.; Aldhalaan H.; Nomicos L.B.; Hayes L.,"Alarifi, Hana (57209656749); Aldhalaan, Hesham (24170976000); Nomicos, Laura Barcelos (57889312000); Hayes, Linda (7103222024)",57209656749; 24170976000; 57889312000; 7103222024,Evaluation of Using Virtual Reality for Applied Behavioral Intervention,2023,"Proceedings - 2023 Congress in Computer Science, Computer Engineering, and Applied Computing, CSCE 2023",,,,2044,2051,7.0,0,10.1109/CSCE60160.2023.00336,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85191157307&doi=10.1109%2fCSCE60160.2023.00336&partnerID=40&md5=615435fc7a06d2b42144bcb30f008db4,"In recent research, Virtual Reality has become well recognized method of delivering therapeutic and educational services. Current technology also provides bimetric objective evidence of attention such as eye-tracking and Electrdermal Activity that can inform therapist and educators.A number of relevant papers of VR usability were analyzed to investigate the effectiveness of VR interventions. The study aims to explore Applied Behavior Analysis ABA specific VR interventions potency. It is concluded that VR is an efficient, accessible and successful method of providing ABA interventions. © 2023 IEEE.",Applied Behavior Analysis ABA; EDA; Electrodermal Activity; Eye-tracking; Galvanic Skin Reaction; GSR; Virtual Reality; VR,Virtual reality; Applied behavior analyse ABA; Behavior analysis; Behavioral interventions; EDA; Electrodermal activity; Eye-tracking; Galvanic skin reaction; GSR; Skin reaction; VR; Eye tracking,Conference paper,Final,,Scopus,2-s2.0-85191157307,Gaming / VR
Pastel S.; Marlok J.; Bandow N.; Witte K.,"Pastel, Stefan (57196456795); Marlok, Josua (57815235000); Bandow, Nicole (55769748385); Witte, Kerstin (23487565200)",57196456795; 57815235000; 55769748385; 23487565200,Application of eye-tracking systems integrated into immersive virtual reality and possible transfer to the sports sector - A systematic review,2023,Multimedia Tools and Applications,82,3,,4181,4208,27.0,29,10.1007/s11042-022-13474-y,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85134685518&doi=10.1007%2fs11042-022-13474-y&partnerID=40&md5=9e275c304543e57fde34e951352a2381,"In recent years, Virtual Reality (VR) has become a valuable tool in rehabilitation and sports training applications. New technologies offer opportunities to combine various systems and use them for sports-related scientific purposes. For instance, examining the visual perception of athletes within a standardized environment could be helpful to understand the differences between novices and experts in their visual behavior and could further reveal possible training applications for enhancing athletes’ visual attention. The current systematic literature review thematizes the importance of eye-tracking (ET) systems’ usage integrated into head-mounted displays (HMDs) in virtual environments for further inclusion in sports-related usage. An overview of possible implementations is given, and additional recommendations for using the combined technic regarding sports are made. Although only one study examined gaze behavior during sports activity within a standardized virtual environment, 38 relevant papers were identified using the ET systems integrated into the HMDs, which ideas can be transferred to the sports sector. The increased usability and fidelity in the virtual environment enabled through the combined technology were illustrated, and different approaches were listed in using and calculating gaze parameters. This literature review examines the possibility of integrating ET in VR, which can be further used to improve usability, interaction methods, image presentation, and visual perception analyses within future physical training scenarios. The compiled studies have shown that the existing methods are feasible due to the performance of the integrated ET systems but still need to be improved for practical use. © 2022, The Author(s).",Eye-tracking; Gaze behavior; Head-mounted display; Virtual reality; Visual perception,Behavioral research; Helmet mounted displays; Image enhancement; Sports; Virtual reality; Eye tracking systems; Eye-tracking; Gaze behaviours; Head-mounted-displays; Immersive virtual reality; Rehabilitation training; Sports trainings; Systematic Review; Training applications; Visual perception; Eye tracking,Article,Final,,Scopus,2-s2.0-85134685518,Gaming / VR
Roßkopf S.; Kroczek L.O.H.; Stärz F.; Blau M.; Van De Par S.; Mühlberger A.,"Roßkopf, S. (58198818300); Kroczek, L.O.H. (57199653714); Stärz, F. (58199721500); Blau, M. (7007017382); Van De Par, S. (6701627290); Mühlberger, A. (6507375232)",58198818300; 57199653714; 58199721500; 7007017382; 6701627290; 6507375232,COMPARABLE SOUND SOURCE LOCALIZATION OF PLAUSIBLE AURALIZATIONS AND REAL SOUND SOURCES EVALUATED IN A NATURALISTIC EYE-TRACKING TASK IN VIRTUAL REALITY,2023,Proceedings of Forum Acusticum,,,,,,,3,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85191260948&partnerID=40&md5=da06a0b68ff790360355827e0b7dfb79,"Highly plausible audiovisual virtual scenes can be created using head-tracked binaural audio renderings presented via headphones combined with a visually-realistic scene presented via virtual reality (VR) glasses. Open questions are whether these plausible auralizations enhance social presence in VR and whether they allow sound source localization comparable to real sound sources. To address these questions, we implemented an eye-tracking paradigm in VR as naturalistic tool to measure spatial attention and sound source localization. In this study, 25 participants completed localization tasks and rated social presence and spatial audio quality. We compared three highly plausible auralizations to loudspeakers and to an anchor (gaming audio engine). Participants reported higher (almost 100%) externalization rates for all plausible auralizations compared to the anchor. Sound distance perception of plausible auralizations and loudspeakers do not differ. For azimuthal error, only for audio renderings based on individual HRTFs lower accuracy was found in comparison to the loudspeaker condition. Social presence was significantly higher in loudspeaker and plausible auralizations compared to the anchor condition. Furthermore, social presence and audio quality are strongly correlated. The implementation of audio renderings is therefore suggested for VR settings in which high levels of (social) presence are relevant (for example, VR exposure therapy). © 2023 S. Roßkopf et al. This is an open-access article distributed under the terms of the Creative Commons Attribution 3.0 Unported License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.",binaural headtracked auralizations; eye-tracking; presence; sound source localization; virtual reality,Audio acoustics; Depth perception; Eye tracking; Virtual reality; Audio quality; Audio rendering; Auralizations; Binaural headtracked auralization; Eye-tracking; Head-tracked; Presence; Social presence; Sound source; Sound source localization; Loudspeakers,Conference paper,Final,,Scopus,2-s2.0-85191260948,Gaming / VR
Ahmed H.S.; Abdelkrim B.; Miguel M.M.J.,"Ahmed, Hadri Sid (58055603500); Abdelkrim, Bouramoul (36056231200); Miguel, Mota MacIas Jose (56537911900)",58055603500; 36056231200; 56537911900,A Lossless Virtual Reality Based Alternative to Eye-Tracking for Attention Quantification,2023,"2023 International Conference on Decision Aid Sciences and Applications, DASA 2023",,,,93,96,3.0,0,10.1109/DASA59624.2023.10286755,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85177472642&doi=10.1109%2fDASA59624.2023.10286755&partnerID=40&md5=2707917e4999d7a6289f49376a593ee2,"Precise measurement of visual attention is crucial for various fields such as psychology, neuroscience, and human-computer interaction. Recent scientific literature has focused on eye-tracking techniques to quantify visual allocation or to predict it. Eye tracking is certainly an efficient and accurate technique, but it comes with a high cost and a limited context of use. In this paper, we propose a novel computational model that accurately quantifies attention allocation in a virtual reality setting. The experimental results demonstrate that our model performs seamlessly with minimum hardware requirements. Our findings suggest that this model has the potential to measure spatio-visual attention allocation accurately and reliably over extended periods, and could be a serious alternative to eye-tracking techniques in a variety of usage, such as therapy interventions.  © 2023 IEEE.",Attention Quantification; Computational Model; Psychotherapy; Virtual Reality; Visual Attention,Behavioral research; Computation theory; Computational methods; Eye tracking; Human computer interaction; Attention quantification; Computational modelling; Eye-tracking; Lossless; Measurements of; Precise measurements; Psychotherapy; Scientific literature; Tracking techniques; Visual Attention; Virtual reality,Conference paper,Final,,Scopus,2-s2.0-85177472642,Gaming / VR
Zheleva A.; De Bruyne J.; Durnez W.; Van De Walle S.; Verreyken S.; Demanet J.; Bombeke K.,"Zheleva, Aleksandra (57217857323); De Bruyne, Jonas (57226611325); Durnez, Wouter (55743705400); Van De Walle, Sam (58527801000); Verreyken, Siemon (58527801100); Demanet, Jelle (56213126100); Bombeke, Klaas (55567481800)",57217857323; 57226611325; 55743705400; 58527801000; 58527801100; 56213126100; 55567481800,CaliBrainVR: Using Psycho-physiological Measures to Calibrate Virtual Reality Training,2023,"2023 15th International Conference on Quality of Multimedia Experience, QoMEX 2023",,,,123,126,3.0,2,10.1109/QoMEX58391.2023.10178522,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85167346177&doi=10.1109%2fQoMEX58391.2023.10178522&partnerID=40&md5=fa2c3a935a2fe99dd71730537982fc59,"The use of extended reality (XR) in training has surged with technological advancements. To meet the demand for adaptive and effective XR training, biofeedback measures like heart rate, eye gaze, and pupil dilation have become crucial. Even though, XR providers have developed headsets equipped with physiological sensors, the accuracy of interpreting the signals has yet to consider inter-individual differences in physiology. Nevertheless, it is important to measure an individual's respon-sivity in terms of biometrics (i.e., the degree to which a person's physiological signal changes in response to changes in their cognitive state) to build accurate XR assessment and adaptive XR training. Currently, we present CaliBrainVR, a virtual reality 'calibration' application that aims to create tailored user profiles using eye-tracking and heart rate data outlining the individual's responsivity. The present text describes the virtual environment of the application, the data logging and preprocessing and the results of the initial validation stage. Finally, we outline the future development of the application and its current impact on training and assessment.  © 2023 IEEE.",calibration; cognitive load; eye-tracking; heart rate; spatial awareness; virtual reality,Biofeedback; E-learning; Eye tracking; Heart; Physiological models; Psychophysiology; Virtual reality; Cognitive loads; Eye-gaze; Eye-tracking; Heart-rate; Physiological measures; Psycho-physiological; Pupil dilation; Spatial awareness; Technological advancement; Virtual reality training; Calibration,Conference paper,Final,All Open Access; Green Open Access,Scopus,2-s2.0-85167346177,Gaming / VR
Rengarajan S.; Cannon J.; Baron B.; Mohan N.; Chukoskie L.,"Rengarajan, Sundararaman (57220025225); Cannon, Jonathan (56083049800); Baron, Brendan (58620544000); Mohan, Naren (58620605900); Chukoskie, Leanne (6507740817)",57220025225; 56083049800; 58620544000; 58620605900; 6507740817,Measuring the Interaction of Conflict-Minimizing and Goal-Seeking Motor Imperatives in Autism Spectrum Disorder,2023,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),13622 LNCS,,,185,198,13.0,0,10.1007/978-3-031-37171-4_13,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85172423288&doi=10.1007%2f978-3-031-37171-4_13&partnerID=40&md5=51f7bb9cae85265a62ea112921feb437,"When subjected to sensory conflict, our bodies respond spontaneously and unconsciously to reduce or resolve it. We developed a Virtual Reality (VR) phase-matching task to establish a framework for explaining behaviors under conflict using hand-tracking, eye-tracking and pupillometry. We also describe a computational model of this behavior that was designed to help us understand differences in observed behavior from autistic individuals and non-autistic peers. We expect that known differences in prediction and attention mechanisms in autism will be important for accounting for behavioral differences observed in the task, highlighting these mechanisms as targets for future intervention. © 2023, The Author(s), under exclusive license to Springer Nature Switzerland AG.",Active Inference; Autism; Eye-tracking; Modeling; Virtual Reality,Diseases; Phase matching; Virtual reality; Active inference; Autism; Autism spectrum disorders; Computational modelling; Eye-tracking; Goal-seeking; Hand-tracking; Modeling; Phase-matching; Pupillometry; Eye tracking,Conference paper,Final,,Scopus,2-s2.0-85172423288,Gaming / VR
Karazor A.; Bayar A.E.; Topal C.; Çevikalp H.,"Karazor, Ahmet (58634306300); Bayar, Alperen Enes (57904161900); Topal, Cihan (16231520700); Çevikalp, Hakan (15130518100)",58634306300; 57904161900; 16231520700; 15130518100,Gaze Estimation by Attention Using a Two-Stream Regression Network; [Dikkat Tabanli Bakiş Noktasi Tahmini Için Iki Akişli Regresyon Aǧi],2023,"31st IEEE Conference on Signal Processing and Communications Applications, SIU 2023",,,,,,,0,10.1109/SIU59756.2023.10223906,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85173501888&doi=10.1109%2fSIU59756.2023.10223906&partnerID=40&md5=523b75ceba25a44ae9f9cd421f7af3bd,"Determining the point of view of people is an important human-computer interaction problem that has been studied for a long time. This subject, which has many applications, is used in different fields such as marketing, automotive, medical, games and entertainment. In this study, we propose a remote eye tracking method that makes gaze estimation using convolutional neural network based on regression. The proposed method uses a two-stream deep learning architecture that utilizes eye images and iris segmentation masks obtained through segmentation neural network. The architecture employed selective attention-based mechanisms to enhance its performance. Experimental results demonstrate that the attention-based two-stream architecture outperforms both single-stream deep learning architectures and architectures without attention mechanisms. © 2023 IEEE.",attention mechanism; gaze estimation; humancomputer interaction,Convolutional neural networks; Deep learning; Eye tracking; Human computer interaction; Network architecture; Attention mechanisms; Automotives; Convolutional neural network; Eye tracking methods; Gaze estimation; Humancomputer interaction; Interaction problems; Learning architectures; Network-based; Two-stream; Image segmentation,Conference paper,Final,,Scopus,2-s2.0-85173501888,Gaming / VR
Wang S.; Kulyk D.; Rikhtehgar D.J.; Heylen D.; Rieffe C.,"Wang, Shenghui (57191711697); Kulyk, Daria (58869819000); Rikhtehgar, Delaram Javdani (57810567500); Heylen, Dirk (6602878289); Rieffe, Carolien (6603039339)",57191711697; 58869819000; 57810567500; 6602878289; 6603039339,Correlating Eye Gaze with Object to Enrich Cultural Heritage Knowledge Graph,2023,CEUR Workshop Proceedings,3632,,,,,,1,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85184379635&partnerID=40&md5=c6dd819353d2af21446bd244513f0091,"Virtual Reality (VR) technology has the potential to enhance cultural heritage experiences by providing immersive environments. This study proposes a novel approach that combines VR environments with eye-tracking data to identify users’ points of interest in exhibition paintings. By leveraging gaze patterns, valuable insights into user preferences, behavior, and attention can be extracted and integrated into the cultural heritage knowledge graph. To achieve this, an object detection model is fine-tuned on historical artwork datasets, and statistical tests are conducted to analyze gaze-object correlations. The results demonstrate the feasibility of using an object detection algorithm to detect points of interest and reveal correlations between eye gaze patterns and meaningful objects in paintings. This approach has the potential to enrich the knowledge graph, enabling more personalized and immersive experiences for art enthusiasts and visitors. © 2023 Copyright for this paper by its authors.",Cultural Heritage; Eye gaze; Image object detection; Knowledge Graph; Virtual reality,Behavioral research; Eye tracking; Knowledge graph; Knowledge management; Object detection; Object recognition; Cultural heritages; Eye-gaze; Eye-tracking; Image object detection; Immersive environment; Knowledge graphs; Tracking data; User behaviors; Virtual reality technology; Virtual-reality environment; Virtual reality,Conference paper,Final,,Scopus,2-s2.0-85184379635,Gaming / VR
Pei W.; Guo X.; Lo T.,"Pei, Wanyu (57219209543); Guo, Xiangmin (36571734100); Lo, Tiantian (56039940300)",57219209543; 36571734100; 56039940300,Pre-Evaluation method of the experiential architecture based on multidimensional physiological perception,2023,Journal of Asian Architecture and Building Engineering,22,3,,1170,1194,24.0,16,10.1080/13467581.2022.2074019,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85130943556&doi=10.1080%2f13467581.2022.2074019&partnerID=40&md5=850152567f1261285044abb5e0798d93,"With a people-oriented principle, contemporary architectural design regards the users’ spiritual and emotional satisfaction as fundamental needs. However, it was tough for designers to collect and quantify the users’ accurate perception of a design scheme in the past. This research aims to propose a method incorporating virtual reality (VR) and sensors, and wearable technology to pre-evaluate the impact of spatial schemes on user experience. Specifically, it collected users’ physiological signals to detect emotions and eye-tracking data to understand visual attention during their roaming in a VR environment. An empirical design scheme for the International School of Design’s outdoor spaces in Harbin Institute of Technology was adopted as a study case. For this purpose, multiple groups of contrast schemes were established in VR for users to experience. Participants’ physiological signals were collected and analyzed using the paired-samples T-test method. Subjective questionnaire data were also collected to assist in the verification of objective physiological data. The results showed that designers could evaluate the experience of the designs by comparing user reactions through analysis of physiological signals. Simultaneously, this evaluation method explores a feasible way for designers to promote architecture designs to be more humanized and scientific. © 2022 The Author(s). Published by Informa UK Limited, trading as Taylor & Francis Group on behalf of the Architectural Institute of Japan, Architectural Institute of Korea and Architectural Society of China.",Experiential architecture; Eye-tracking; multidimensional physiological signals; Pre-evaluation; virtual reality,Architecture; Behavioral research; Eye tracking; Physiological models; Physiology; Signal analysis; Wearable technology; Architecture-based; Design scheme; Evaluation methods; Experiential architecture; Eye-tracking; Multidimensional physiological signal; Physiological signals; Pre-evaluation; Spatial scheme; Users' experiences; Virtual reality,Article,Final,All Open Access; Gold Open Access,Scopus,2-s2.0-85130943556,Gaming / VR
El Iskandarani M.; Riggs S.L.,"El Iskandarani, Mohamad (58995655900); Riggs, Sara Lu (57004526300)",58995655900; 57004526300,The Effect of Time Pressure on Visual Search Eye Tracking Metrics in Virtual Reality,2023,Proceedings of the Human Factors and Ergonomics Society,67,1,,1525,1528,3.0,1,10.1177/21695067231192429,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85190996308&doi=10.1177%2f21695067231192429&partnerID=40&md5=1c3b3a0ff9e397ae1897587aa175d38e,"The study of visual search has focused on various guiding factors, but less attention has been given to how environmental factors affect visual search in virtual reality (VR). The visual search literature has primarily been based on 2D laboratory tasks, which lack the complexity of real-life search tasks. Thus, this study studies the effect of time pressure on visual search in a naturalistic environment. To do that, participants were immersed in a virtual living room using VR and tasked with finding objects under a time constraint. Eye gaze data was collected, and convex hull volumes and scanning rates were calculated and analyzed. The results show that time pressure reduced convex hull volume and increased scanning rate, indicating faster search speed and a gaze tunneling effect. Understanding how time pressure affects visual search can help improve training strategies and design better user interfaces for visual search critical domains. © 2023 Human Factors and Ergonomics Society.",3D environment; Eye tracking; Human performance; Naturalistic environment; Time stress; Virtual reality; Visual search,Computational geometry; Eye tracking; Pressure effects; User interfaces; 3-D environments; Effect of time; Environmental factors; Eye-tracking; Human performance; Naturalistic environment; Scanning rate; Time pressures; Time stress; Visual search; Virtual reality,Conference paper,Final,,Scopus,2-s2.0-85190996308,Gaming / VR
Schröder B.; Mühlberger A.,"Schröder, Benedikt (57763203400); Mühlberger, Andreas (6507375232)",57763203400; 6507375232,Measuring attentional bias in smokers during and after psychosocial stress induction with a Trier Social Stress Test in virtual reality via eye tracking,2023,Frontiers in Psychology,14,,1129422,,,,2,10.3389/fpsyg.2023.1129422,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85153358168&doi=10.3389%2ffpsyg.2023.1129422&partnerID=40&md5=91ae3488d5c3667085256ca4e564851e,"Introduction: Attentional bias (AB) is considered an important factor not only in the etiology of addiction, but also with respect to relapse. However, evidence for the predictive ability of AB for relapse is not robust. One reason for this might be fluctuations of AB due to stress. Therefore, the current study investigated whether AB was present during and after stress induction and whether AB was enhanced by stress induction. Methods: A Virtual Reality (VR) adaptation of the Trier Social Stress Test (VR-TSST) was used to induce psychosocial stress in smokers (n = 34) and non-smokers (n = 37) followed by a novel free-viewing task in VR. Eye tracking data was recorded to examine gaze behavior to smoking-related and neutral stimuli presented in the VR-TSST and the free-viewing task. Results: Stress ratings increased significantly from baseline to post VR-TSST in smokers and non-smokers. During the VR-TSST we observed, more frequent, longer, and earlier fixations on smoke-related compared with neutral stimuli without significant group differences. However, in the free-viewing task following the stress induction, a specific AB of smokers in terms of earlier and longer fixations on smoke stimuli was found. Conclusion: Results indicate that AB is not a persistent trait in smokers, but is context dependent. It is suggested that emotional learning processes such as smoking in the context of relief after stress may contribute to changes of AB both in terms of increased initial attention and deeper stimulus processing. Additionally, the potential of the VR-TSST to induce psychosocial stress could be replicated. Copyright © 2023 Schröder and Mühlberger.",addiction; attention bias; eye tracking; smoking; stress; Trier Social Stress Test; virtual reality,,Article,Final,All Open Access; Gold Open Access; Green Open Access,Scopus,2-s2.0-85153358168,Gaming / VR
Thomay C.; Fermitsch A.; Fessler J.; Garatva P.; Gollan B.; Katharina Lietz A.; Matscheko M.; Wagner M.,"Thomay, Christian (55505910700); Fermitsch, Andreas (58883759000); Fessler, Johannes (58883730400); Garatva, Patricia (58086695400); Gollan, Benedikt (48361077700); Katharina Lietz, Andrea (58883759100); Matscheko, Michael (24605414700); Wagner, Michael (57190175055)",55505910700; 58883759000; 58883730400; 58086695400; 48361077700; 58883759100; 24605414700; 57190175055,Towards Cognitive Load-Based Decision Making in VR Training,2023,"2023 IEEE 2nd International Conference on Cognitive Aspects of Virtual Reality, CVR 2023",,,,,,,2,10.1109/CVR58941.2023.10395602,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85184851357&doi=10.1109%2fCVR58941.2023.10395602&partnerID=40&md5=8dee3aa0fa9e311b651285ae8afb9f86,"We present a methodology to achieve adaptive, cognitive load-based decision making in the context of virtual reality training and assessed in the scope of medical training. The aim is to create dynamic training scenarios that adapt according to the cognitive load of the trainee, adjusting training parameters to keep the trainee at an optimal learning level, where they are appropriately challenged and engaged, but not overexerted. This is achieved by monitoring the trainee's cognitive load via eye tracking and pupillometry, employing an adaptive algorithm compensating for the effect of ambient light on the human eye. The cognitive load measurement has been validated in a real-life training study using a sim lab with haptic dolls, and the same training study is currently being evaluated in VR. In this paper, we report the findings of the real-life study, detail preliminary results from the VR study, and describe the methodology used to achieve adaptive cognitive load-based decision making in the VR scenario. © 2023 IEEE.",adaptive learning; computer aided instruction; human computer interaction; Virtual reality,Adaptive algorithms; Computer aided instruction; E-learning; Eye tracking; Human computer interaction; Virtual reality; Adaptive learning; Cognitive loads; Decisions makings; Eye-tracking; Learning levels; Medical training; Pupillometry; Training parameters; Training scenario; Virtual reality training; Decision making,Conference paper,Final,,Scopus,2-s2.0-85184851357,Gaming / VR
Kwon J.; Linihan S.; Iedema A.; Schmidt A.; Luo C.; Marrufo K.,"Kwon, Jain (56157489500); Linihan, Suzie (58483378700); Iedema, Alyssa (57610887800); Schmidt, Alea (58482518600); Luo, Chenyi (58483203800); Marrufo, Karime (58482518700)",56157489500; 58483378700; 57610887800; 58482518600; 58483203800; 58482518700,How interior design responds to neurodiversity: implementing wearable technologies in neurodesign processes,2023,Frontiers in Built Environment,9,,1211519,,,,6,10.3389/fbuil.2023.1211519,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85164503089&doi=10.3389%2ffbuil.2023.1211519&partnerID=40&md5=6ea5f2b1ab20e178e7c484265e9c5c93,"This perspective article, looking through the lens of neurodiversity, discusses the benefits and challenges of implementing virtual environments and wearable technologies in interior design and related fields. While the relationship between human perception and built environments has long been studied in the environmental design disciplines, the direct impact on occupant performance related to neurodiversity has been underexplored in research, with a shortage of knowledge supporting how it can be applied in design practice concerning the end users. Individuals’ perceptual, cognitive, and affective responses to their surroundings vary, as neurodiversity plays a key role in the invisible, human-environment interaction. Thus, measuring, analyzing, and understanding affective, perceptual, and cognitive experiences is a challenging process in which various factors come into play, and no single method or measurement can adequately work for all. Due to such challenges, research has also utilized various biometric measurements and tools for immersive experiments in physical and virtual environments, e.g., eye tracking used in studies on gaze behaviors and immersive virtual reality (IVR) used in studies on the spatial perception of dementia patients. Along with empirical methods, studies have stressed the contribution of phenomenology to looking into the hidden dimension, the ‘why factors’ of perception, cognition, and affectivity. Concerning the methodological approach, this perspective article shares insights into a novel process model, Participatory Neurodesign (PND) framework, used in wayfinding research and design processes utilizing eye tracking and IVR. Opportunities for neurodesign research and design practice are also discussed, focusing on the health, safety, and wellbeing of end-users. Copyright © 2023 Kwon, Linihan, Iedema, Schmidt, Luo and Marrufo.",eye tracking; immersive experience; interior design; neurodesign; neurodiversity; participatory neurodesign; spatial perception; virtual reality,,Article,Final,All Open Access; Gold Open Access; Green Open Access,Scopus,2-s2.0-85164503089,Gaming / VR
Obade C.; Kim H.W.; Cook-Chennault K.,"Obade, Chelsea (58838087200); Kim, Hyeon Woo (57435175500); Cook-Chennault, Kimberly (25651701700)",58838087200; 57435175500; 25651701700,WIP: Using Multimodal Approaches to Understand the Attention and Focus of Students Engaging in Intuition-Based Online Engineering Learning Games,2023,"Proceedings - Frontiers in Education Conference, FIE",,,,,,,1,10.1109/FIE58773.2023.10343057,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85183055980&doi=10.1109%2fFIE58773.2023.10343057&partnerID=40&md5=e34251583cb9030cf0ab1870a0a7e3e1,"With the emergence of advanced computing and simulation capabilities, interest in online learning tools has steadily increased over the last several decades. The use of physiological responses to explore human learning and behavior has recently become an area of investigation. It is an effective method of assessing students' attention, memory, engagement, and cognition. Previous studies have focused on developing methods for capturing and interpreting people's physiological responses as they engage in learning games. However, few have validated their reactions with other quantitative and qualitative data forms. This work-in-progress research category study leverages two theoretical frameworks, i.e., the Technology Acceptance Model and Intersectionality. Intersectional women of color within the engineering population reside at the intersection of marginalized racial and gender groups. This study also builds on our previous studies of educational games, where it was found that women students expressed a higher distaste for an engineering digital game than their male counterparts. However, an understanding of what aspects of the game lead to these sentiments and lower performance in the game was inconclusive. Hence, a multimodal research approach is used in this study that combines physiological, quantitative, and qualitative data to understand engineering undergraduate students' perceptions of an engineering mechanics intuition-based online learning game. We hypothesize that a multimodal approach for this study will further our understanding of all students' perceptions of engineering educational games and provide insight into how students learn and interact with online intuition-based games designed to facilitate learning via discovery of possible physical scenarios, leveraging pre-existing knowledge in physics and use of mobile and computer gaming technology. This study utilizes eye tracking, interviews, and an assessment instrument to understand which aspects of the learning design and educational scaffolding foster attention and focus. The findings from this study will advance the engineering education communities' understanding of how to interpret and conduct multimodal studies and how 21st-century engineering undergraduate students engage in intuition-based engineering gaming technology. © 2023 IEEE.",engineering serious games; eye tracking; mixed methods; multimodal analysis,Behavioral research; E-learning; Engineering education; Eye tracking; Knowledge management; Learning systems; Modal analysis; Physiological models; Physiology; Scaffolds; Serious games; Teaching; Educational game; Engineering serious game; Eye-tracking; Learning game; Mixed method; Multi-modal approach; Multimodal analysis; Physiological response; Qualitative data; Quantitative data; Students,Conference paper,Final,,Scopus,2-s2.0-85183055980,Gaming / VR
Broussard D.; Borst C.W.,"Broussard, David (57215364589); Borst, Christoph W. (9736479200)",57215364589; 9736479200,Heat Metaphor for Attention Estimation for Educational VR,2023,"Proceedings - 2023 IEEE Conference on Virtual Reality and 3D User Interfaces Abstracts and Workshops, VRW 2023",,,,683,684,1.0,0,10.1109/VRW58643.2023.00184,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85159643310&doi=10.1109%2fVRW58643.2023.00184&partnerID=40&md5=90d7caa53cd0f032fb53d0dace29c290,"We prototype a technique, for educational VR applications, to estimate each student's level of attention in real time. Our system attaches scores to both students and objects, which change in response to eye-tracked gaze intersections. Compared to a simple angle-based approach, our system provides a dynamic and granular representation of object importance and frees the lesson designer from having to fully define objects of interest and timings. Our system takes into account simultaneous behaviors of multiple students and filters out brief behavioral deviations of attentive students. The results may help a teacher or a virtual agent better guide students.  © 2023 IEEE.",Applied computing-Education; Human-centered computing-Virtual reality,E-learning; Eye tracking; Software prototyping; Virtual reality; Applied computing; Applied computing-education; Attention estimations; Computing education; Human-centered computing; Human-centered computing-virtual reality; Real- time; Simple++; Student levels; VR applications; Students,Conference paper,Final,,Scopus,2-s2.0-85159643310,Gaming / VR
Liu W.; Andrade G.; Schulze J.; Courtney K.E.,"Liu, Weichen (57670315400); Andrade, Gianna (57459683600); Schulze, Jürgen (24460807500); Courtney, Kelly E. (25959492400)",57670315400; 57459683600; 24460807500; 25959492400,Evaluation of Gaze-to-Object Mapping Algorithms for Use in “Real-World” Translatable Neuropsychological Paradigms,2023,Psychology and Neuroscience,16,4,,339,348,9.0,2,10.1037/pne0000324,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85184893843&doi=10.1037%2fpne0000324&partnerID=40&md5=9d6c2da271fd90455a4f99b89d88ea38,"Objective: Eye-tracking technology is commonly used for identifying objects of visual attention. However, applying this technology to virtual reality (VR) applications is challenging. This report analyzes the performance of two gaze-to-object mapping (GTOM) algorithms applied to eye-gaze data acquired during a “real-world” VR cue-reactivity paradigm. Method: Two groups of participants completed a VR paradigm using a High Tech Computer Corporation (HTC) Vive Pro Eye. The gazed objects were determined by the reported gaze rays and one of two GTOM algorithms—naïve ray-casting (n = 18) or a combination of ray-casting and Tobii’s G2OM algorithm (n = 18). Percent gaze duration was calculated from 1-s intervals before each object interaction to estimate gaze accuracy. The object volume of maximal divergence between algorithms was determined by maximizing the difference in Hedge’s g effect sizes between small and large percent gaze duration distributions. Differences in percent gaze duration based on algorithm and target object size were tested with a mixed analysis of variance (ANOVA). Results: The maximumHedge’s g effect sizes differentiating large and small target objectswere observed at an 800 cm3 threshold. The combination algorithm performed better than the naïve raycasting algorithm (p =.003, η2p =.23), and large objects (>800 cm3) were associated with a higher gaze duration percentage than small objects (≤800 cm3; p <.001, η2 p =.76). No significant interaction between algorithm and size was observed. Conclusions: Results demonstrated that Tobii’s G2OM method outperformed naïve ray-casting in this “realworld ” paradigm. As both algorithms show a clear decrease in performance for detecting objects with volumes <800 cm3, we recommend using gaze-interactable objects?>800 cm3 for future HTC Vive Pro Eye applications. © 2023 American Psychological Association",eye tracking; gaze accuracy; gaze-to-object mapping; virtual reality; visual attention,nicotine; accuracy; adult; algorithm; analysis of variance; Article; blindness; consciousness; effect size; eye position; eye tracking; eye-tracking technology; gaze; head injury; human; machine learning; major clinical study; male; mental disease; motion sickness; neurologic disease; neuropsychology; software; virtual reality; visual acuity; visual attention; visual field; visual information,Article,Final,All Open Access; Green Open Access; Hybrid Gold Open Access,Scopus,2-s2.0-85184893843,Gaming / VR
Mizukami K.; Taguchi M.; Kouketsu T.; Sato N.; Tanaka Y.; Iwakiri M.; Nishina Y.; Chernyak I.; Karaki S.,"Mizukami, Katsuyoshi (7005239526); Taguchi, Masatomo (58134971900); Kouketsu, Takashi (58134972000); Sato, Naoki (58135172600); Tanaka, Yoshiro (58134972100); Iwakiri, Masahiko (58133967800); Nishina, Yoichiro (58134374200); Chernyak, Iakov (57193348711); Karaki, Shintaro (58134776400)",7005239526; 58134971900; 58134972000; 58135172600; 58134972100; 58133967800; 58134374200; 57193348711; 58134776400,A cognitive function test utilizing eye tracking technology in virtual reality,2023,Japanese Journal of Geriatrics,60,1,,43,50,7.0,1,10.3143/geriatrics.60.43,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85149672967&doi=10.3143%2fgeriatrics.60.43&partnerID=40&md5=918de86f4a2099123dcbcef653a0a073,"Aim: There is a need for a cognitive function test that is less burdensome to perform cognitive function tests used to date and can detect mild changes in the cognitive function and mild cognitive impairment (MCI). We developed a cognitive function examination using a virtual reality device (VR-E). The purpose of this study was to verify its usability. Methods: Seventy-seven participants (29 males and 48 females, average age 75.1 years old) were classified according to their Clinical Dementia Rating (CDR). To estimate the validity of VR-E in measuring cognitive function, we used the Mini Mental State Examination (MMSE) and Montreal Cognitive Assessment-Japanese version (MoCA-J) scores as benchmarks. The MMSE was performed for all subjects, while the MoCA-J was performed for subjects with an MMSE score ≥20. Results: VR-E scores were highest in the CDR 0 group (0.77±0.15, mean±SD), decreasing for subsequent groups (CDR 0.5: 0.65 ±0.19, CDR 1-3: 0.22±0.21). The receiver operating characteristic analysis showed that all three methods were able to distinguish CDR groups. For CDR 0 vs. 0.5, the areas under the curve for MMSE/MoCA-J/VR-E were 0.85/0.80/0.70, respectively, and those for CDR 0.5 vs. 1-3 were 0.89/0.92/0.90, respectively. The time required to complete VR-E was approximately 5 minutes. Of the 77 subjects, 12 were difficult to assess using the VR-E due to poor understanding or eye diseases or Meniere’s syndrome. Conclusions: The present findings suggested that the VR-E can be used as a cognitive function test that correlates with existing standard assessments for dementia and MCI. © 2023, Japan Geriatrics Society. All rights reserved.",Cognitive function test; Dementia; Mild cognitive impairment; Virtual reality,Aged; Cognition; Cognitive Dysfunction; Dementia; Eye-Tracking Technology; Female; Humans; Male; Neuropsychological Tests; Virtual Reality; aged; cognition; cognitive defect; dementia; female; human; male; neuropsychological assessment; virtual reality,Article,Final,All Open Access; Gold Open Access,Scopus,2-s2.0-85149672967,Gaming / VR
,,,"2023 9th International Conference on Virtual Reality, ICVR 2023",2023,"2023 9th International Conference on Virtual Reality, ICVR 2023",,,,,,628.0,0,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85175116813&partnerID=40&md5=85f89bf244d9c50b66a11a70a2ad8c6f,The proceedings contain 88 papers. The topics discussed include: analysis of sampling strategies for implicit 3D reconstruction; dense voxel 3D reconstruction using a monocular event camera; application of virtual reality in dance/movement therapy; dense point cloud reconstruction based on a single image; multiple human tracking using deep learning with shadow clues; research on crop fruit segmentation method based on point cloud; a scene understanding and positioning system from RGB images for tele-meeting application in augmented reality; camera field calibration method using collinear point target joint constraints; a method for classifying cognitive load of visual tasks based on eye tracking features; a task estimation method based on image recognition and its application to EMG prosthetic hand control; detecting zero-shot human-object interaction with visual-text modeling; design of an automatic 3D reconstruction scanning system in virtual reality; and study of different interaction methods on the healing effect of natural environment in virtual reality.,,,Conference review,Final,,Scopus,2-s2.0-85175116813,Gaming / VR
Amat A.Z.; Adiani D.; Tauseef M.; Breen M.; Hunt S.; Swanson A.R.; Weitlauf A.S.; Warren Z.E.; Sarkar N.,"Amat, Ashwaq Z. (57203133051); Adiani, Deeksha (57210154706); Tauseef, Mahrukh (57215310702); Breen, Michael (57226641558); Hunt, Spencer (57226654943); Swanson, Amy R (53265026200); Weitlauf, Amy S. (57191707622); Warren, Zachary E. (25032282500); Sarkar, Nilanjan (7201361624)",57203133051; 57210154706; 57215310702; 57226641558; 57226654943; 53265026200; 57191707622; 25032282500; 7201361624,Design of a Desktop Virtual Reality-Based Collaborative Activities Simulator (ViRCAS) to Support Teamwork in Workplace Settings for Autistic Adults,2023,IEEE Transactions on Neural Systems and Rehabilitation Engineering,31,,,2184,2194,10.0,10,10.1109/TNSRE.2023.3271139,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85159555700&doi=10.1109%2fTNSRE.2023.3271139&partnerID=40&md5=81ccf51c4363e52c9991487bb013efec,"Autistic adults possess many skills sought by employers, but may be at a disadvantage in the workplace if social-communication differences negatively impact teamwork. We present a novel collaborative virtual reality (VR)-based activities simulator, called ViRCAS, that allows autistic and neurotypical adults to work together in a shared virtual space, offering the chance to practice teamwork and assess progress. ViRCAS has three main contributions: 1) a new collaborative teamwork skills practice platform; 2) a stakeholder-driven collaborative task set with embedded collaboration strategies; and 3) a framework for multimodal data analysis to assess skills. Our feasibility study with 12 participant pairs showed preliminary acceptance of ViRCAS, a positive impact of the collaborative tasks on supported teamwork skills practice for autistic and neurotypical individuals, and promising potential to quantitatively assess collaboration through multimodal data analysis. The current work paves the way for longitudinal studies that will assess whether the collaborative teamwork skill practice that ViRCAS provides also contributes towards improved task performance.  © 2001-2011 IEEE.",autism; collaborative virtual environment; human computer interaction; Intelligent system; teamwork; virtual reality,Adult; Autistic Disorder; Communication; Humans; Virtual Reality; Workplace; Behavioral research; Data handling; Information analysis; Intelligent systems; Job analysis; Virtual reality; Autism; Behavioral science; Collaborative activities; Collaborative tasks; Collaborative virtual environment; Stakeholder; Task analysis; Teamwork; Teamwork skills; adult; Article; autism; behavioral science; cognition; data analysis; employment; eye tracking; facial expression; facial recognition; feasibility study; female; functional magnetic resonance imaging; hospitalization; human; longitudinal study; male; motivation; perception; quantitative analysis; simulation training; social interaction; task performance; teamwork; training; verbal communication; virtual reality; visual acuity; workplace; interpersonal communication; workplace; Human computer interaction,Article,Final,All Open Access; Gold Open Access,Scopus,2-s2.0-85159555700,Gaming / VR
Nenna F.; Zanardi D.; Gamberini L.,"Nenna, Federica (57218995768); Zanardi, Davide (57817751700); Gamberini, Luciano (6603650210)",57218995768; 57817751700; 6603650210,Emphasizing Humans In Industry 5.0: A Cross-Age Analysis Of Behavioral Entropy And Cognitive Workload In Vr-Based Telerobotics,2023,"Proceedings of the International Conferences on Interfaces and Human Computer Interaction 2023, IHCI 2023; Computer Graphics, Visualization, Computer Vision and Image Processing 2023, CGVCVIP 2023; and Game and Entertainment Technologies 2023, GET 2023",,,,80,88,8.0,0,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85181799398&partnerID=40&md5=f6dd52d59022cc8cbc1ca92d914af26b,"Human motion patterns have been shown to reflect cognitive processes in psychology and applied Human-Computer Interaction (HCI) research. Specifically, behavioral entropy of users' movement trajectories has been proposed to consistently reflect workload and fatigue in various scenarios, including Virtual Reality (VR). In VR, this measure is typically calculated based on the movements of the VR controllers, known as Entropy of Controller Movements (ECM). While having great potential as a highly sensitive and unobtrusive measure of human workload, the effectiveness of ECM in more applied and realistic instances of VR-based applications (e.g., teleoperation platforms) has not been proven yet. Furthermore, the existing literature is based on young experimental samples, living open the question of whether age-related changes in motor performance might prevent using ECM as a measure of workload. We here covered these aspects by assessing relations between workload and ECM in 15 young and 15 senior participants who physically maneuvered an industrial robot in VR. Specifically, they were asked to guide the robot through a pick-and-place task by using their own physical movements in VR. Our findings evidence some criticalities of ECM as a measure of workload in our VR-based industrial contexts, opening new questions on its applicability and effectiveness. © 2023 Proceedings of the International Conferences on Interfaces and Human Computer Interaction 2023, IHCI 2023; Computer Graphics, Visualization, Computer Vision and Image Processing 2023, CGVCVIP 2023; and Game and Entertainment Technologies 2023, GET 2023. All rights reserved.",Behavioral Entropy; Entropy of Controller Movements; Industrial Robotics; Virtual Reality; Workload,Computer games; Computer vision; Entropy; Human computer interaction; Industrial robots; Industry 4.0; Virtual reality; Behavioral entropy; Cognitive process; Cognitive workloads; Entropy of controller movement; Human motions; Human-computer interaction researches; Industrial robotics; Motion pattern; Tele-robotics; Workload; Controllers,Conference paper,Final,,Scopus,2-s2.0-85181799398,Gaming / VR
Gao Y.; Sun X.; Zhang Z.; Zhang W.; Meng H.; Zhang T.,"Gao, Yu (57221270994); Sun, Xiaomei (57190879817); Zhang, Zhi (57204478666); Zhang, Weikang (56549310900); Meng, Huan (57204479393); Zhang, Tong (57204481533)",57221270994; 57190879817; 57204478666; 56549310900; 57204479393; 57204481533,How does the landscape uniformity in different forest landscapes affect the visual behavior and preference evaluation intention of participants—a case study of forest landscape in northern China (Liaoning),2023,Frontiers in Forests and Global Change,6,,1243649,,,,1,10.3389/ffgc.2023.1243649,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85189438272&doi=10.3389%2fffgc.2023.1243649&partnerID=40&md5=d378d5575557afcfed2a67095222754e,"Background: High-quality in-forest landscapes are very important when landscape designers and foresters are planning and managing forest landscape resources. Understanding people’s visual behavior toward in-forest landscapes plays an important role in creating high-quality in-forest landscapes. Methods: People’s visual information about in-forest landscapes with different landscape uniformity (IFLDLU) was visualized and digitized by eye-tracking technology and virtual reality method of picture, and the data were statistically analyzed by MANOVA, one-way ANOVA, Spearman’s rho correlation analysis, and linear regression in SPSS. Purpose: This study aimed to discuss the influence of landscape uniformity on people’s visual behavior, satisfaction preference, and re-viewing intention, and to reveal the main reasons for the spatial cognitive characteristics of in-forest scenes that arouse their visual attention and re-viewing intention. Main results: (1) The landscape uniformity of the in-forest landscape (IFL) affects people’s observation mode; (2) People’s visual attention differs across in-forest landscape uniformity, as mainly reflected in visual span and pupil diameter; (3) Overall, people prefer a cluster distribution of in-forest landscapes, for which they have higher re-viewing intentions; (4) Spatial cognitive characteristics that arouse participants’ visual behavior, satisfaction preference, and re-viewing intention vary with the landscape uniformity of in-forest landscapes. Among them, many spatial cognitive characteristics should be optimized in random distribution for IFL, arousing higher satisfaction preference and re-viewing intention. Therefore, when planning, designing, and managing in-forest landscapes, we suggest that spatial cognitive characteristics should be improved and optimized based on the landscape uniformity of the IFL to arouse participants’ positive visual attention, enhance their place identity, make them “reluctant to bid farewell,” and further arouse their place attachment. Copyright © 2024 Gao, Sun, Zhang, Zhang, Meng and Zhang.",forest landscape; landscape uniformity; re-viewing intention; spatial cognitive characteristic; visual behavior,,Article,Final,All Open Access; Gold Open Access,Scopus,2-s2.0-85189438272,Gaming / VR
Messeri C.,"Messeri, Costanza (57207943011)",57207943011,Enhancing the Quality of Human-Robot Cooperation Through the Optimization of Human Well-Being and Productivity,2023,SpringerBriefs in Applied Sciences and Technology,,,,15,25,10.0,4,10.1007/978-3-031-15374-7_2,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85141895518&doi=10.1007%2f978-3-031-15374-7_2&partnerID=40&md5=89165db7126a3d0926473699de03fc3c,"In human-robot interaction frameworks maximizing the team efficiency is crucial. However, it is also essential to mitigate the physical and cognitive workload experienced by the shop-floor worker during the collaborative task. In this chapter we first investigate the impact of the robot interaction role (whether being leader or follower during cooperation) on both the human physiological stress and production rate. Based on that, a game-theoretic approach is proposed to model the trade-off between the maximization of the human performance and the minimization of the human cognitive stress. Then, we describe a closed-loop robot control strategy that, based on the proposed game-theoretic model, enables the robot to simultaneously minimize the human cognitive stress and maximize his/her performance during cooperation, by adjusting its role. Eventually, a real-time task allocation strategy is proposed to both ensure the minimization of the human physical fatigue and the effectiveness of the production process. This method relies on a new sophisticated musculoskeletal model of the human upper-body. All these methodologies have been experimentally tested in realistic human-robot collaborative scenarios involving several volunteers and the ABB IRB 14000 dual-arm “YuMi"" collaborative robot. © 2023, The Author(s).",Human-robot collaboration; Stress and fatigue estimation; Task allocation and optimization,Collaborative robots; Game theory; Man machine systems; Fatigue estimation; Human-robot collaboration; Human-robot-cooperation; Minimisation; Optimisations; Stress estimation; Task allocation; Task allocation and optimization; Well being; Well productivity; Economic and social effects,Book chapter,Final,,Scopus,2-s2.0-85141895518,Gaming / VR
Hao J.; Weng D.; Luo L.; Li M.; Guo J.; Liang B.,"Hao, Jie (57273470700); Weng, Dongdong (24386043400); Luo, Le (57212510914); Li, Ming (58742205200); Guo, Jie (58593602800); Liang, Bin (56473223400)",57273470700; 24386043400; 57212510914; 58742205200; 58593602800; 56473223400,Does Cognitive Workload Impact the Effect on Pain Distraction in Virtual Reality? A Study Design,2023,"Proceedings - 2023 IEEE Conference on Virtual Reality and 3D User Interfaces Abstracts and Workshops, VRW 2023",,,,847,848,1.0,0,10.1109/VRW58643.2023.00266,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85159682752&doi=10.1109%2fVRW58643.2023.00266&partnerID=40&md5=253593fd373b528fe93b74a6930ad115,"Pain is an important issue for post-surgery patients. Virtual reality treatment (VRT) has been used to reduce pain perception, while there are few studies have investigated the effect of cognitive workload (CWL). The present study design hypothesizes that higher CWL will lead to stronger distraction in a virtual environment. To this end, a soothing psychological relief environment with a cognitive task, in which participants' CWL can be controlled, is constructed. In the future, this study will be applied to patients with pain to explore whether CWL is one of the key factors in reducing pain in VRT.  © 2023 IEEE.",Human computer interaction (HCI); Human-centered computing; Interaction paradigms; Virtual reality,Human computer interaction; User interfaces; Cognitive task; Cognitive workloads; Human computer interaction; Human-centered computing; Interaction paradigm; Key factors; Pain perception; Study design; Virtual reality,Conference paper,Final,,Scopus,2-s2.0-85159682752,Gaming / VR
Polat M.D.; Izzetoglu K.; Aksoy M.E.; Kitapcioglu D.; Usseli T.; Yoner S.I.,"Polat, Mert Deniz (58590046600); Izzetoglu, Kurtulus (6603128234); Aksoy, Mehmet Emin (23970353200); Kitapcioglu, Dilek (55078240400); Usseli, Tuba (58590939700); Yoner, Serhat Ilgaz (57193696035)",58590046600; 6603128234; 23970353200; 55078240400; 58590939700; 57193696035,Cognitive Load Quantified via Functional Near Infrared Spectroscopy During Immersive Training with VR Based Basic Life Support Learning Modules in Hostile Environment,2023,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),14019 LNAI,,,359,372,13.0,3,10.1007/978-3-031-35017-7_23,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85171472892&doi=10.1007%2f978-3-031-35017-7_23&partnerID=40&md5=c1aa143c6dfc160cad3b124499a3526c,"This study investigates the use of functional near infrared spectroscopy (fNIRS) as a tool for assessing cognitive workload in virtual reality (VR) based medical education. Specifically, the study explores the effect of simulator immersion and distraction on cognitive workload during a Basic Life Support (BLS) training course delivered by a VR-based serious gaming module. Nineteen participants with no prior knowledge of BLS guidelines completed a VR-based serious gaming BLS training module and were randomly assigned to two groups for the BLS examination; the first group had experienced medium distraction while the second group had a high level of distraction. All participants then took a hands-on BLS exam at the end of the study protocol. The results show significant decrease in cognitive workload measured by fNIRS during the training sessions. The oxygenation levels at the prefrontal cortex were significantly higher for the participants who had taken the high distraction VR-exam, suggesting a higher neural involvement compared to the group who had taken the medium distraction VR-exam. However, unlike hands-on exam, no significant difference between two groups was determined for the VR-based exam. The results demonstrate that fNIRS can be used to measure cognitive workload in VR-based medical training and provide further insights for optimizing serious game-based learning tools. © 2023, The Author(s), under exclusive license to Springer Nature Switzerland AG.",cognitive workload; fNIRS; functional brain imaging; Human performance; learning; medical training; virtual reality,Brain mapping; E-learning; Infrared devices; Medical education; Near infrared spectroscopy; Serious games; Basic life supports; Cognitive loads; Cognitive workloads; Functional brain imaging; Functional near infrared spectroscopy; Human performance; Immersive; Learning; Medical training; Serious gaming; Virtual reality,Conference paper,Final,,Scopus,2-s2.0-85171472892,Gaming / VR
Hu B.; Wang W.; Chan K.; Chen Z.; Tang C.; Li P.,"Hu, Bin (57212305362); Wang, Wenxiao (57221427246); Chan, Kai (58087566700); Chen, Zhian (58087484900); Tang, Chaolan (56337460500); Li, Ping (55268425500)",57212305362; 57221427246; 58087566700; 58087484900; 56337460500; 55268425500,Graphic Design and Evaluation of an Augmented Reality for Advergame,2022,Proceedings - VRCAI 2022: 18th ACM SIGGRAPH International Conference on Virtual-Reality Continuum and its Applications in Industry,,,36,,,,0,10.1145/3574131.3574462,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85147246903&doi=10.1145%2f3574131.3574462&partnerID=40&md5=185df2e778a069d4ba362a25674b6196,"This letter attempts to introduce an Augmented Reality Advergame (ARA), which is designed to eliminate the pain-points present of current advergames. i.e., cumbersome interface design, excessive entertainment elements lead to user distraction, and low participation among middle-aged and elderly users. For this purpose, our ARA improves color scheme, optimizes graphic proportions, and reduces typographical complexity of interface design. Furthermore, we introduce an augmented reality way in interaction. Eye-tracking studies demonstrate that ARA is better at directing user attention to key information than the current advergames. Perceptive studies confirm that ARA user engagement is 25% higher than the control advergame.  © 2022 ACM.",Augmented Reality Advergame (ARA).; eye-tracking; graphic design; interactive design,Augmented reality; Design; User interfaces; 'current; Advergames; Augmented reality advergame .; Color schemes; Design and evaluations; Elderly users; Eye-tracking; Graphic design; Interactive design; Interface designs; Eye tracking,Conference paper,Final,,Scopus,2-s2.0-85147246903,Gaming / VR
Merzon L.; Pettersson K.; Aronen E.T.; Huhdanpää H.; Seesjärvi E.; Henriksson L.; MacInnes W.J.; Mannerkoski M.; Macaluso E.; Salmi J.,"Merzon, Liya (57214114460); Pettersson, Kati (55931088700); Aronen, Eeva T. (7003841876); Huhdanpää, Hanna (57205433597); Seesjärvi, Erik (56094658200); Henriksson, Linda (8576345700); MacInnes, W. Joseph (6603524373); Mannerkoski, Minna (6504150469); Macaluso, Emiliano (6701716382); Salmi, Juha (8977740700)",57214114460; 55931088700; 7003841876; 57205433597; 56094658200; 8576345700; 6603524373; 6504150469; 6701716382; 8977740700,Eye movement behavior in a real-world virtual reality task reveals ADHD in children,2022,Scientific Reports,12,1,20308,,,,33,10.1038/s41598-022-24552-4,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85142493742&doi=10.1038%2fs41598-022-24552-4&partnerID=40&md5=dfbf202f73b5c9fef0cf79fab5394b22,"Eye movements and other rich data obtained in virtual reality (VR) environments resembling situations where symptoms are manifested could help in the objective detection of various symptoms in clinical conditions. In the present study, 37 children with attention deficit hyperactivity disorder and 36 typically developing controls (9–13 y.o) played a lifelike prospective memory game using head-mounted display with inbuilt 90 Hz eye tracker. Eye movement patterns had prominent group differences, but they were dispersed across the full performance time rather than associated with specific events or stimulus features. A support vector machine classifier trained on eye movement data showed excellent discrimination ability with 0.92 area under curve, which was significantly higher than for task performance measures or for eye movements obtained in a visual search task. We demonstrated that a naturalistic VR task combined with eye tracking allows accurate prediction of attention deficits, paving the way for precision diagnostics. © 2022, The Author(s).",,Attention Deficit Disorder with Hyperactivity; Child; Eye Movements; Humans; Task Performance and Analysis; Virtual Reality; attention deficit hyperactivity disorder; child; eye movement; human; task performance; virtual reality,Article,Final,All Open Access; Gold Open Access; Green Open Access,Scopus,2-s2.0-85142493742,Gaming / VR
Khokhar A.; Borst C.,"Khokhar, Adil (57210910170); Borst, Christoph (9736479200)",57210910170; 9736479200,Modifying Pedagogical Agent Spatial Guidance Sequences to Respond to Eye-Tracked Student Gaze in VR,2022,Proceedings - SUI 2022: ACM Conference on Spatial User Interaction,,,15,,,,3,10.1145/3565970.3567697,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85144015690&doi=10.1145%2f3565970.3567697&partnerID=40&md5=7adfde73e5b0350826be66034993b536,"In a guided virtual field trip, students often need to pay attention to the correct objects in a 3D scene. Distractions or misunderstandings of a virtual agent's spatial guidance may cause students to miss critical information. We present a generalizable virtual reality (VR) avatar animation architecture that is responsive to a viewer's eye gaze and we evaluate the rated effectiveness (e.g., naturalness) of enabled agent responses. Our novel annotation-driven sequencing system modifies the playing, seeking, rewinding, and pausing of teacher recordings to create appropriate teacher avatar behavior based on a viewer's eye-tracked visual attention. Annotations are contextual metadata that modify sequencing behavior during critical time points and can be adjusted in a timeline editor. We demonstrate the success of our architecture with a study that compares 3 different teacher agent behavioral responses when pointing to and explaining objects on a virtual oil rig while an in-game mobile device provides an experiment control mechanism for 2 levels of distractions. Results suggest that users consider teacher agent behaviors with increased interactivity to be more appropriate, more natural, and less strange than default agent behaviors, implying that more elaborate agent behaviors can improve a student's educational VR experience. Results also provide insights into how or why a minimal response (Pause) and a more dynamic response (Respond) are perceived differently.  © 2022 Owner/Author.",3D hotspots; gaze movements; pedagogical agents; virtual reality,Animation; Behavioral research; Eye movements; Eye tracking; Virtual reality; 3d hotspot; 3D scenes; Agent behavior; Eye-gaze; Gaze movements; Hotspots; Pedagogical agents; Teachers'; Virtual agent; Virtual field trips; Students,Conference paper,Final,All Open Access; Bronze Open Access,Scopus,2-s2.0-85144015690,Gaming / VR
Li J.; Zhou Y.; Gao X.,"Li, Jiayu (57193280514); Zhou, Yuhong (57196955423); Gao, Xuemei (56804079000)",57193280514; 57196955423; 56804079000,The advantage for action video game players in eye movement behavior during visual search tasks,2022,Current Psychology,41,12,,8374,8383,9.0,4,10.1007/s12144-022-03017-x,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85127594846&doi=10.1007%2fs12144-022-03017-x&partnerID=40&md5=127748bfc1cb7037f1abf783c21ecd35,"Research has shown that playing action games is effective in promoting abilities for visual attention tasks. However, it is unclear whether the advantages of playing these games are wider peripheral vision (PV), a greater ability to process information, or a greater PV ability. This study aims to investigate the characteristics and advantages regarding the eye movements of action video game players (AVGPs) during a visual search comprising 154 participants in Experiment 1 and 166 participants in Experiment 2. The results show that compared with non-video game players (NVGPs), AVGPs have a significant time advantage in visual search tasks, with a shorter response time and fixation duration. Further, this advantage is not present unconditionally in central vision (CV) and PV but is only apparent with cues; that is, AVGPs show a greater ability to use cues. Especially in CV with cues, the saccade velocity of AVGPs is significantly faster than that of NVGPs. The results also show that AVGPs have a significant advantage in visual searching, which is mainly reflected in their use of cues and their saccade velocity of eye movement behavior in CV. © 2022, The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature.",Action video game; Central vision; Eye movements; Peripheral vision; Visual search,,Article,Final,,Scopus,2-s2.0-85127594846,Gaming / VR
Haskins A.J.; Mentch J.; Botch T.L.; Garcia B.D.; Burrows A.L.; Robertson C.E.,"Haskins, Amanda J. (57201212494); Mentch, Jeff (57210840182); Botch, Thomas L. (57217186010); Garcia, Brenda D. (57351596100); Burrows, Alexandra L. (57918033400); Robertson, Caroline E. (57196949630)",57201212494; 57210840182; 57217186010; 57351596100; 57918033400; 57196949630,Reduced social attention in autism is magnified by perceptual load in naturalistic environments,2022,Autism Research,15,12,,2310,2323,13.0,8,10.1002/aur.2829,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85139389199&doi=10.1002%2faur.2829&partnerID=40&md5=5fd0d59ffdae238a9f0b499298aa3b38,"Individuals with autism spectrum conditions (ASC) describe differences in both social cognition and sensory processing, but little is known about the causal relationship between these disparate functional domains. In the present study, we sought to understand how a core characteristic of autism—reduced social attention—is impacted by the complex multisensory signals present in real-world environments. We tested the hypothesis that reductions in social attention associated with autism would be magnified by increasing perceptual load (e.g., motion, multisensory cues). Adult participants (N = 40; 19 ASC) explored a diverse set of 360° real-world scenes in a naturalistic, active viewing paradigm (immersive virtual reality + eyetracking). Across three conditions, we systematically varied perceptual load while holding the social and semantic information present in each scene constant. We demonstrate that reduced social attention is not a static signature of the autistic phenotype. Rather, group differences in social attention emerged with increasing perceptual load in naturalistic environments, and the susceptibility of social attention to perceptual load predicted continuous measures of autistic traits across groups. Crucially, this pattern was specific to the social domain: we did not observe differential impacts of perceptual load on attention directed toward nonsocial semantic (i.e., object, place) information or low-level fixation behavior (i.e., overall fixation frequency or duration). This study provides a direct link between social and sensory processing in autism. Moreover, reduced social attention may be an inaccurate characterization of autism. Instead, our results suggest that social attention in autism is better explained by “social vulnerability,” particularly to the perceptual load of real-world environments. © 2022 The Authors. Autism Research published by International Society for Autism Research and Wiley Periodicals LLC.",adults; attention; eye movement; sensory; social cognition,Attention; Autism Spectrum Disorder; Autistic Disorder; Cues; Humans; adolescent; adult; Article; association; attention; autism; clinical article; eye-tracking technology; female; human; image analysis; image processing; male; perception; psychometry; semantics; sensorimotor function; social behavior; social environment; social vulnerability; task performance; attention,Article,Final,All Open Access; Green Open Access; Hybrid Gold Open Access,Scopus,2-s2.0-85139389199,Gaming / VR
Foerster F.R.; Chidharom M.; Bonnefond A.; Giersch A.,"Foerster, Francois R. (57188675737); Chidharom, Matthieu (57221439722); Bonnefond, Anne (6603780474); Giersch, Anne (6701709610)",57188675737; 57221439722; 6603780474; 6701709610,Neurocognitive analyses reveal that video game players exhibit enhanced implicit temporal processing,2022,Communications Biology,5,1,1082,,,,8,10.1038/s42003-022-04033-0,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85139706078&doi=10.1038%2fs42003-022-04033-0&partnerID=40&md5=759df2582999f05f11759644da83f06e,"Winning in action video games requires to predict timed events in order to react fast enough. In these games, repeated waiting for enemies may help to develop implicit (incidental) preparation mechanisms. We compared action video game players and non-video game players in a reaction time task involving both implicit time preparations and explicit (conscious) temporal attention cues. Participants were immersed in virtual reality and instructed to respond to a visual target appearing at variable delays after a warning signal. In half of the trials, an explicit cue indicated when the target would occur after the warning signal. Behavioral, oculomotor and EEG data consistently indicate that, compared with non-video game players, video game players better prepare in time using implicit mechanisms. This sheds light on the neglected role of implicit timing and related electrophysiological mechanisms in gaming research. The results further suggest that game-based interventions may help remediate implicit timing disorders found in psychiatric populations. © 2022, The Author(s).",,Cues; Eye Movements; Humans; Reaction Time; Time Perception; Video Games; association; eye movement; human; psychology; reaction time; time perception; video game,Article,Final,All Open Access; Gold Open Access; Green Open Access,Scopus,2-s2.0-85139706078,Gaming / VR
Knoppe K.; Schlichting N.; Schmidt-Wilcke T.; Zimmermann E.,"Knoppe, Kira (57953318900); Schlichting, Nadine (57224138694); Schmidt-Wilcke, Tobias (9039797400); Zimmermann, Eckart (7101967705)",57953318900; 57224138694; 9039797400; 7101967705,Increased scene complexity during free visual exploration reveals residual unilateral neglect in recovered stroke patients,2022,Neuropsychologia,177,,108400,,,,7,10.1016/j.neuropsychologia.2022.108400,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85141293080&doi=10.1016%2fj.neuropsychologia.2022.108400&partnerID=40&md5=63650d15bfe137f0344596c5629f36af,"Unilateral neglect is a common cognitive syndrome after stroke, which is defined as a spatially specific unawareness of the contralesional space. The syndrome is caused by disruptions of attentional networks in the brain, which impair the patients’ ability to direct attention towards the contralesional space. During recovery, patients often learn to compensate by voluntarily directing their attention to the neglected side at the expense of cognitive resources. In this study, we examined the impact of the complexity of visual input on free visual exploration behavior of unilateral neglect and apparently recovered patients. We asked whether increasing scene complexity would allow the detection of residual unilateral neglect in recovered patients by increasing the amount of cognitive resources needed for visual processing and limiting capacities for compensation. Using virtual reality, we analyzed the spatial distribution of gaze of unilateral neglect patients, patients who had, according to conventional diagnostics, recovered from the syndrome, stroke patients with no history of unilateral neglect, and age-matched healthy controls. We manipulated the complexity of an immersive virtual scene presented on head mounted displays. We identified the orientation bias towards the ipsilesional side as a sensitive and specific marker of unilateral neglect, which was present in unilateral neglect and recovered patients but absent in stroke patients with no history of unilateral neglect and controls. Increasing scene complexity exacerbated the orientation shift in unilateral neglect patients and revealed that three out of nine (33%) recovered patients had a high probability of suffering from residual unilateral neglect as estimated by a generalized linear model using the median horizontal gaze position as a predictor. © 2022 The Authors",Eye tracking; Perceptual load; Subacute/chronic stroke; Unilateral neglect; Virtual reality; Visual attention,Brain; Cognition; Functional Laterality; Humans; Perceptual Disorders; Space Perception; Stroke; Visual Perception; adult; aged; Article; brain hemorrhage; cerebrovascular accident; cognition; controlled study; diagnostic test accuracy study; entropy; exploratory behavior; eye fixation; eye tracking; functional magnetic resonance imaging; gaze; hemianopia; human; major clinical study; prediction; pupil; receiver operating characteristic; stroke patient; virtual reality; visual attention; visual field defect; visual stimulation; brain; complication; depth perception; hemispheric dominance; perception disorder; psychology; vision,Article,Final,All Open Access; Green Open Access; Hybrid Gold Open Access,Scopus,2-s2.0-85141293080,Gaming / VR
Lo Buono V.; Torrisi M.; Leonardi S.; Pidalà A.; Corallo F.,"Lo Buono, Viviana (57208514917); Torrisi, Michele (56911998900); Leonardi, Simona (56921579300); Pidalà, Alessandra (57201092420); Corallo, Francesco (54785814000)",57208514917; 56911998900; 56921579300; 57201092420; 54785814000,Multisensory stimulation and rehabilitation for disability improvement: Lessons from a case report,2022,Medicine (United States),101,46,,E31404,,,4,10.1097/MD.0000000000031404,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85142402307&doi=10.1097%2fMD.0000000000031404&partnerID=40&md5=78bdcd32f0309f3b1716cd0a17270bf4,"Rationale: Spastic quadriplegia is most severe form of Infantile Cerebral Palsy. Patients are unable to use their legs, arms and body and show language disorder and profound intellectual disability. The treatment of patients diagnosed with spastic quadriplegia is complex and multidisciplinary. In this case report we described the positive effect of multisensory environment (MSEs) rehabilitation, and the strategies and technologies used to provide child who have to severe spastic quadriplegia and intellectual disability, palsy with playful and fun activities designed according to his abilities. Patient concern: A 7-years-old boy diagnosed with spastic quadriplegia and severe intellectual disability began rehabilitation by MSEs. Diagnoses: Spastic quadriplegia is most severe form of Infantile Cerebral Palsy. Patients are unable to use their legs, arms and body and show language disorder and profound intellectual disability. Interventions: Multisensory room is a large environment containing various elements where child can interact spontaneously and independently. Outcomes: The comparison scores between T0-T1 showed a reduction in self-harm and motor stereotypies (hand flapping). Sustained attention was improved and we observed a better therapeutic compliance by means of greater involvement in gaming activities. Conclusion: The stimuli within the MSEs provided the child opportunities to express himself with facilities more suited to his potential. Future research should project designed randomized controlled trials to examine the efficacy of multisensory on reduction disability. © 2022 Lippincott Williams and Wilkins. All rights reserved.",multisensory environment; rehabilitation; spastic quadriplegia,Cerebral Palsy; Child; Humans; Intellectual Disability; Language Disorders; Male; Quadriplegia; risperidone; adaptive behavior; Article; attention; automutilation; body weight; case report; cerebral palsy; child; clinical article; clinical assessment; cognitive rehabilitation; color; compulsion; disease classification; disease severity; environment; environmental factor; eye movement disorder; Functional Independence Measure; gestational age; Gross Motor Function Classification System; growth retardation; hearing; hospital admission; human; image display; intellectual impairment; joint mobility; leisure; light; limb disease; lower limb; male; motor dysfunction; muscle hypertonia; muscle rigidity; music; neurologic examination; patient compliance; physiotherapist; play; proprioception; psychologist; psychomotor disorder; quadriplegia; rehabilitation center; respiratory failure; restlessness; school child; sensory stimulation; snoezelen; spasticity; sphincter; stereotypy; stretching exercise; technology; touch; treatment duration; Vineland Adaptive Behavior Scale; vision; cerebral palsy; intellectual impairment; language disability; quadriplegia,Article,Final,All Open Access; Gold Open Access; Green Open Access,Scopus,2-s2.0-85142402307,Gaming / VR
Asish S.M.; Kulshreshth A.K.; Borst C.W.,"Asish, Sarker Monojit (57215357842); Kulshreshth, Arun K. (55315352400); Borst, Christoph W. (9736479200)",57215357842; 55315352400; 9736479200,Detecting distracted students in educational VR environments using machine learning on eye gaze data,2022,Computers and Graphics (Pergamon),109,,,75,87,12.0,27,10.1016/j.cag.2022.10.007,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85141891541&doi=10.1016%2fj.cag.2022.10.007&partnerID=40&md5=569c5046e6299898a29b9b01c8affda4,"Virtual Reality (VR) has been found useful to improve engagement and retention level of students, for some topics, compared to traditional learning tools such as books, and videos. However, a student could still get distracted and disengaged due to a variety of factors including stress, mind-wandering, unwanted noise, and external alerts. Student eye gaze data could be useful for detecting these distracted students. Gaze data-based visualizations have been proposed in the past to help a teacher monitor distracted students. However, it is not practical for a teacher to monitor a large number of student indicators while teaching. To help filter students based on distraction level, we propose an automated system based on machine learning to classify students based on their distraction level. The key aspects are: (1) we created a labeled eye gaze dataset from an educational VR environment, (2) we propose an automatic system to gauge a student's distraction level from gaze data, and (3) we apply and compare several classifiers for this purpose. Each classifier classifies distraction, per educational activity section, into one of three levels (low, mid or high). Our results show that Random Forest (RF) classifier had the best accuracy (98.88%) compared to the other models we tested. Additionally, a personalized machine learning model using either RF, kNN, or Extreme Gradient Boosting (XGBoost) model was found to improve the classification accuracy significantly. © 2022 The Author(s)",Deep Learning; Distraction Detection; Education; Eye Tracking; Machine Learning; Virtual Reality,Adaptive boosting; Automation; Decision trees; Deep learning; E-learning; Education computing; Eye tracking; Learning systems; Virtual reality; Deep learning; Distraction detection; Engagement levels; Eye-gaze; Eye-tracking; Machine-learning; Retention levels; Student-based; Teachers'; Virtual-reality environment; Students,Article,Final,All Open Access; Hybrid Gold Open Access,Scopus,2-s2.0-85141891541,Gaming / VR
Gao H.; Hasenbein L.; Bozkir E.; Göllner R.; Kasneci E.,"Gao, Hong (57217014766); Hasenbein, Lisa (57211289472); Bozkir, Efe (57210111357); Göllner, Richard (56661056800); Kasneci, Enkelejda (56059892600)",57217014766; 57211289472; 57210111357; 56661056800; 56059892600,Evaluating the Effects of Virtual Human Animation on Students in an Immersive VR Classroom Using Eye Movements,2022,"Proceedings of the ACM Symposium on Virtual Reality Software and Technology, VRST",,,11,,,,6,10.1145/3562939.3565623,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85143620098&doi=10.1145%2f3562939.3565623&partnerID=40&md5=ce2d65b565dce29e181afb0fa3bc9dd9,"Virtual humans presented in VR learning environments have been suggested in previous research to increase immersion and further positively influence learning outcomes. However, how virtual human animations affect students' real-Time behavior during VR learning has not yet been investigated. This work examines the effects of social animations (i.e., hand raising of virtual peer learners) on students' cognitive response and visual attention behavior during immersion in a VR classroom based on eye movement analysis. Our results show that animated peers that are designed to enhance immersion and provide companionship and social information elicit different responses in students (i.e., cognitive, visual attention, and visual search responses), as reflected in various eye movement metrics such as pupil diameter, fixations, saccades, and dwell times. Furthermore, our results show that the effects of animations on students differ significantly between conditions (20%, 35%, 65%, and 80% of virtual peer learners raising their hands). Our research provides a methodological foundation for investigating the effects of avatar animations on users, further suggesting that such effects should be considered by developers when implementing animated virtual humans in VR. Our findings have important implications for future works on the design of more effective, immersive, and authentic VR environments.  © 2022 ACM.",education; eye-Tracking; immersive virtual reality; virtual human animation; visual attention,Animation; Behavioral research; Computer aided instruction; E-learning; Economic and social effects; Eye movements; Eye tracking; Virtual reality; Eye-tracking; Human animation; Immersive virtual reality; Immersive VR; Learning environments; Learning outcome; Real time behavior; Virtual human animation; Virtual humans; Visual Attention; Students,Conference paper,Final,,Scopus,2-s2.0-85143620098,Gaming / VR
Takamido R.; Kurihara S.; Umeda Y.; Asama H.; Kasahara S.; Tanaka Y.; Fukumoto S.; Kato T.; Korenaga M.; Hoshi M.; Ota J.,"Takamido, Ryota (57206890890); Kurihara, Satoya (58010207000); Umeda, Yasushi (7101998421); Asama, Hajime (35448146500); Kasahara, Seiji (58010392700); Tanaka, Yuichi (58010021500); Fukumoto, Seigo (58010490900); Kato, Toshiya (58010299000); Korenaga, Masahiro (58010588800); Hoshi, Misaki (58010392800); Ota, Jun (7006719514)",57206890890; 58010207000; 7101998421; 35448146500; 58010392700; 58010021500; 58010490900; 58010299000; 58010588800; 58010392800; 7006719514,Evaluation of expert skills in refinery patrol inspection: visual attention and head positioning behavior,2022,Heliyon,8,12,e12117,,,,2,10.1016/j.heliyon.2022.e12117,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85143980317&doi=10.1016%2fj.heliyon.2022.e12117&partnerID=40&md5=ea1b4d2f07fc7e294c64d1ca0188fd22,"We aimed to clarify expert skills in refinery patrol inspection using data collected through a virtual reality experimental system. As body positioning and postural changes are relevant factors during refinery patrol inspection tasks, we measured and analyzed both visual attention and head positioning behavior among experts and “knowledgeable novices” who were engaged in the engineering of the refinery but had less inspection experience. The participants performed a simulated inspection task, and the results showed that 1) expert inspectors could find more defects compared to knowledgeable novices, 2) visual attention behavior was similar between knowledgeable novices and experts, and 3) experts tended to position their heads at various heights and further from the inspection target to obtain visual information more effectively from the target compared to knowledgeable novices. This study presented the differences in head positioning behavior between expert and novice inspectors for the first time. These results suggest that to evaluate the skills used in inspecting relatively larger targets, both visual attention and head positioning behavior of the inspectors must be measured. © 2022 The Author(s)",Eye tracking; Virtual reality; Visual inspection,,Article,Final,All Open Access; Gold Open Access; Green Open Access,Scopus,2-s2.0-85143980317,Gaming / VR
Jeong D.; Jeong M.; Yang U.; Han K.,"Jeong, Dayoung (57998453100); Jeong, Mingon (57703894700); Yang, Ungyeon (56239985100); Han, Kyungsik (58643233700)",57998453100; 57703894700; 56239985100; 58643233700,Eyes on me: Investigating the role and influence of eye-tracking data on user modeling in virtual reality,2022,PLoS ONE,17,12-Dec,e0278970,,,,9,10.1371/journal.pone.0278970,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85145242103&doi=10.1371%2fjournal.pone.0278970&partnerID=40&md5=3321e56fc48f87a4821d0b3ef240accf,"Research has shown that sensor data generated by a user during a VR experience is closely related to the user’s behavior or state, meaning that the VR user can be quantitatively understood and modeled. Eye-tracking as a sensor signal has been studied in prior research, but its usefulness in a VR context has been less examined, and most extant studies have dealt with eye-tracking within a single environment. Our goal is to expand the understanding of the relationship between eye-tracking data and user modeling in VR. In this paper, we examined the role and influence of eye-tracking data in predicting a level of cybersickness and types of locomotion. We developed and applied the same structure of a deep learning model to the multi-sensory data collected from two different studies (cybersickness and locomotion) with a total of 50 participants. The experiment results highlight not only a high applicability of our model to sensor data in a VR context, but also a significant relevance of eye-tracking data as a potential supplement to improving the model’s performance and the importance of eye-tracking data in learning processes overall. We conclude by discussing the relevance of these results to potential future studies on this topic. © 2022 Jeong et al. This is an open access article distributed under the terms of the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.",,Eye-Tracking Technology; Humans; Virtual Reality; adult; Article; attention; behavior; controlled study; cybersickness; deep learning; eye tracking; female; human; information processing; learning; locomotion; male; prediction; virtual reality,Article,Final,All Open Access; Gold Open Access; Green Open Access,Scopus,2-s2.0-85145242103,Gaming / VR
Sanuki F.; Nakphu N.; Tahara A.; Iramina K.,"Sanuki, Fumiya (57224940395); Nakphu, Nonthaporn (56572850100); Tahara, Ayumi (57576682700); Iramina, Keiji (7003477377)",57224940395; 56572850100; 57576682700; 7003477377,The comparison of electroencephalography power and event related potential in success and failure during multitask game,2022,Frontiers in Neurorobotics,16,,1044071,,,,1,10.3389/fnbot.2022.1044071,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85142923809&doi=10.3389%2ffnbot.2022.1044071&partnerID=40&md5=6a40aa3f8c05f17db82384f7164fc1c1,"The first game-based treatment for children with attention-deficit hyperactivity disorder (ADHD) was approved by the United States Food and Drug Administration (FDA) in 2020. This game was developed for use at home as part of everyday training and can be used along with one’s usual training plan. In this game, two tasks are performed in parallel: (1) a perceptual discrimination targeting task (response and not response and avoiding responding to sudden pop-up targets) and (2) a sensory-motor navigation task (players continuously adjust their location to interact with or avoid positional targets). However, the brain activity of people playing this game was not examined, and the immersive environment (3D virtual world) was not considered. Therefore, we aimed to develop a system to investigate brain activity using electroencephalography (EEG) during multitask gameplay in virtual reality (VR). In this experiment, we focused on the difference between the success and failure of the Go/No-Go task in a multitask game. We created a color discrimination task and a target tracking task in VR. The content of this game task was designed using previous multitask training. EEG and event data were recorded. Using event data, we can analyze the data in detail. We divided the trial types (Go and No-Go) and results (success and failure). We then compared the success and failure of each task. In the Go trial, the relative theta power in success at Fz was significantly higher than that of failure. However, no difference in power was observed in the No-Go trial. On the other hand, theta power was no different between success and failure in the other task. These results of the Go trial suggest that the participants were attentive to processing both tasks. Thus, it is possible that theta power in the frontal area 1 s before stimulation could predict the success or failure of the Go trial. On the other hand, the results of the No-Go trial may be due to the low number of No-Go failure trials and the fact that stimulus oversight is one of the factors for success. Copyright © 2022 Sanuki, Nakphu, Tahara and Iramina.",EEG; ERP; game; Go/No-Go task; multitask; VR,Brain; Electroencephalography; Electrophysiology; Neurophysiology; Target tracking; Attention deficit hyperactivity disorder; Brain activity; Event related potentials; Game; Game-Based; Go/no-go tasks; Multitask; Power; Training plan; United states food and drug administrations; adult; Article; attention; behavioral science; brain function; color discrimination; comparative study; controlled study; electroencephalography; event related potential; eye tracking; female; game; Go No Go task; human; human experiment; male; task performance; virtual reality; young adult; Virtual reality,Article,Final,All Open Access; Gold Open Access; Green Open Access,Scopus,2-s2.0-85142923809,Gaming / VR
Asish S.M.; Kulshreshth A.; Borst C.,"Asish, Sarker Monojit (57215357842); Kulshreshth, Arun (55315352400); Borst, Christoph (9736479200)",57215357842; 55315352400; 9736479200,Detecting Internal Distraction in an Educational VR Environment Using EEG Data,2022,Proceedings - SUI 2022: ACM Conference on Spatial User Interaction,,,26,,,,2,10.1145/3565970.3568188,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85144084376&doi=10.1145%2f3565970.3568188&partnerID=40&md5=ec9e4b3d573f38cdac920e301089403b,"Virtual Reality (VR) provides a more engaging learning experience to students and could improve knowledge retention compared to traditional learning methods. However, a student could get distracted in the VR environment due to stress, mind-wandering, unwanted noise, external sounds, etc. Distractions could be classified as either external (due to environment) or internal (due to internal thoughts). Past researchers have used eye-gaze data to detect external distractions. However, eye-gaze data can not measure internal distractions since a user could look at the educational content and may be thinking about something else. We explored the usage of electroencephalogram (EEG) data to detect internal distraction. We designed an educational VR environment and trained three machine learning models: Random Forest (RF), Support Vector Machine (SVM), and Linear Discriminant Analysis (LDA), to detect internal distractions of students. Our preliminary study results show that RF provides a better accuracy (98%) compared to SVM and LDA.  © 2022 Owner/Author.",Distraction; Education; EEG; Machine learning; Virtual Reality,Discriminant analysis; E-learning; Learning systems; Students; Support vector machines; Virtual reality; Distraction; Eye-gaze; Knowledge retention; Learning experiences; Linear discriminant analyze; Machine-learning; Random forests; Support vectors machine; Traditional learning; Virtual-reality environment; Electroencephalography,Conference paper,Final,,Scopus,2-s2.0-85144084376,Gaming / VR
Slevitch L.; Chandrasekera T.; Mejia-Puig L.; Korneva K.; Akosa J.S.,"Slevitch, Lisa (30567820600); Chandrasekera, Tilanka (56784199900); Mejia-Puig, Luis (55164759100); Korneva, Kate (57992187900); Akosa, Josephine S. (57838183400)",30567820600; 56784199900; 55164759100; 57992187900; 57838183400,Virtual Reality images’ impact on cognition and affect in hotel promotions: Application of self-reported and psycho-physiological measures,2022,Journal of Hospitality and Tourism Management,53,,,176,187,11.0,19,10.1016/j.jhtm.2022.10.005,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85143286596&doi=10.1016%2fj.jhtm.2022.10.005&partnerID=40&md5=fe312d581b380ebfa5367f2e55d45dbb,"The current study examines how two types of Virtual Reality (VR) hotel images impact cognition and affect when used for promotional purposes. The study employs psycho-physiological measurement tools along with a self-reported survey method to evaluate: (1) affective responses, (2) Cognitive Load, and (3) and attitudinal and behavioral intention responses that two types of VR hotel images, static and 360°, produce in an experimental lab setting. A boutique hotel lobby and a guest room were captured as static and 360° VR images. Participants were randomly assigned to one of the two types of VR images. The sample was comprised of 60 university students from the South-Central United States. Parametric and non-parametric tests were used to compare survey, fNIR, Biopac MP-160 skin conductance and heart rate, and eye tracking responses. No statistically significant differences between two experimental conditions were found by means of self-reported measures, except temporal dimension of Cognitive Load. However, psycho-physiological measures detected statistically significant differences between the two experimental groups in arousal (one of the main affective dimensions), thus, supporting usefulness of adding psycho-physiological tools in consumer behavior studies and usage of more immersive and engaging 360° VR images. The findings also help to explain inconsistencies in previous VR imaging studies that relied on self-reported measures only. © 2022 The Authors",Cognitive load; Hotels; PAD Model; Promotion visuals; Psycho-physiological measurements; Self-reported measurements; Virtual reality,,Article,Final,,Scopus,2-s2.0-85143286596,Gaming / VR
Kim S.Y.; Park H.; Kim H.; Kim J.; Seo K.,"Kim, Se Young (57881434700); Park, Hahyeon (57881281800); Kim, Hongbum (55928148300); Kim, Joon (57881281900); Seo, Kyoungwon (57195072325)",57881434700; 57881281800; 55928148300; 57881281900; 57195072325,Technostress causes cognitive overload in high-stress people: Eye tracking analysis in a virtual kiosk test,2022,Information Processing and Management,59,6,103093,,,,36,10.1016/j.ipm.2022.103093,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85137617849&doi=10.1016%2fj.ipm.2022.103093&partnerID=40&md5=4e94fa900545246bdd8805461e90fc80,"In the midst of the COVID-19 pandemic, the use of non-face-to-face information and communication technology (ICT) such as kiosks has increased. While kiosks are useful overall, those who do not adapt well to these technologies experience technostress. The two most serious technostressors are inclusion and overload issues, which indicate a sense of inferiority due to a perceived inability to use ICT well and a sense of being overwhelmed by too much information, respectively. This study investigated the different effects of hybrid technostress—induced by both inclusion and overload issues—on the cognitive load among low-stress and high-stress people when using kiosks to complete daily life tasks. We developed a ‘virtual kiosk test’ to evaluate participants’ cognitive load with eye tracking features and performance features when ordering burgers, sides, and drinks using the kiosk. Twelve low-stress participants and 13 high-stress participants performed the virtual kiosk test. As a result, regarding eye tracking features, high-stress participants generated a larger number of blinks, a longer scanpath length, a more distracted heatmap, and a more complex gaze plot than low-stress participants. Regarding performance features, high-stress participants took significantly longer to order and made more errors than low-stress participants. A support-vector machine (SVM) using both eye tracking features (i.e., number of blinks, scanpath length) and a performance feature (i.e., time to completion) best differentiated between low-stress and high-stress participants (89% accuracy, 100% sensitivity, 83.3% specificity, 75% precision, 85.7% F1 score). Overall, under technostress, high-stress participants experienced cognitive overload and consequently decreased performance; whereas, low-stress participants felt moderate arousal and improved performance. These varying effects of technostress can be interpreted through the Yerkes-Dodson law. Based on our findings, we proposed an adaptive interface, multimodal interaction, and virtual reality training as three implications for technostress relief in non-face-to-face ICT. © 2022",Cognitive load; Eye tracking; Kiosk; Technostress; Virtual reality,Beverages; Eye tracking; Support vector machines; Cognitive loads; Cognitive overload; Eye-tracking; High stress; Information and Communication Technologies; Kiosk; Lower stress; Performance; Technostress; Tracking feature; Virtual reality,Article,Final,All Open Access; Green Open Access; Hybrid Gold Open Access,Scopus,2-s2.0-85137617849,Gaming / VR
Großekathöfer J.D.; Seis C.; Gamer M.,"Großekathöfer, Jonas D. (57216175046); Seis, Christian (58853573800); Gamer, Matthias (8979946100)",57216175046; 58853573800; 8979946100,Reality in a sphere: A direct comparison of social attention in the laboratory and the real world,2022,Behavior Research Methods,54,5,,2286,2301,15.0,5,10.3758/s13428-021-01724-0,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85121369553&doi=10.3758%2fs13428-021-01724-0&partnerID=40&md5=0b3d4aa0640514660c93e1598aa6ed58,"Humans often show reduced social attention in real situations, a finding rarely replicated in controlled laboratory studies. Virtual reality is supposed to allow for ecologically valid and at the same time highly controlled experiments. This study aimed to provide initial insights into the reliability and validity of using spherical videos viewed via a head-mounted display (HMD) to assess social attention. We chose five public places in the city of Würzburg and measured eye movements of 44 participants for 30 s at each location twice: Once in a real environment with mobile eye-tracking glasses and once in a virtual environment playing a spherical video of the location in an HMD with an integrated eye tracker. As hypothesized, participants demonstrated reduced social attention with less exploration of passengers in the real environment as compared to the virtual one. This is in line with earlier studies showing social avoidance in interactive situations. Furthermore, we only observed consistent gaze proportions on passengers across locations in virtual environments. These findings highlight that the potential for social interactions and an adherence to social norms are essential modulators of viewing behavior in social situations and cannot be easily simulated in laboratory contexts. However, spherical videos might be helpful for supplementing the range of methods in social cognition research and other fields. Data and analysis scripts are available at https://osf.io/hktdu/. © 2021, The Author(s).",Ecological validity; Eye tracking; Social attention; Spherical videos; Virtual reality,Attention; Eye Movements; Humans; Reproducibility of Results; Social Behavior; Virtual Reality; attention; eye movement; human; reproducibility; social behavior; virtual reality,Article,Final,All Open Access; Green Open Access; Hybrid Gold Open Access,Scopus,2-s2.0-85121369553,Gaming / VR
Han J.; Lee S.,"Han, J. (57219173152); Lee, S. (56331529300)",57219173152; 56331529300,RESIDENT'S SATISFACTION IN STREET LANDSCAPE USING THE IMMERSIVE VIRTUAL ENVIRONMENT-BASED EYE-TRACKING TECHNIQUE AND DEEP LEARNING MODEL,2022,"International Archives of the Photogrammetry, Remote Sensing and Spatial Information Sciences - ISPRS Archives",48,4/W4-2022,,45,52,7.0,6,10.5194/isprs-archives-XLVIII-4-W4-2022-45-2022,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85140317836&doi=10.5194%2fisprs-archives-XLVIII-4-W4-2022-45-2022&partnerID=40&md5=486d2b23f015163dc03f4e3dbc1b35dd,"Virtual reality technology provides a significant clue to understanding the human visual perception process by enabling the interaction between humans and computers. In addition, deep learning techniques in the visual field provide analysis methods for image classification, processing, and segmentation. This study reviewed the applicability of gaze movement and deep learning-based satisfaction evaluation on the landscape using an immersive virtual reality-based eye-tracking device. To this end, the following research procedures were established and analysed. First, the gaze movement of the test taker is measured using an immersive virtual environment-based eye tracker. The relationship between the gaze movement pattern of the test taker and the satisfaction evaluation result for the landscape image is analysed. Second, using the Convolutional Neural Networks (CNN)-based Class Activation Map (CAM) technique, a model for estimating the satisfaction evaluation result is constructed, and the gaze pattern of the test taker is derived. Third, we compare and analyse the similarity between the gaze heat map derived through the immersive virtual environment-based gaze tracker and the heat map generated by CAM. This study suggests the applicability of urban environment technology and deep learning methods to understand landscape planning factors that affect urban landscape satisfaction, resulting from the three-dimensional and immediate visual cognitive activity. Copyright © 2022 J. Han.",Deep-learning; Eye-tracking; Immersive Virtual Reality; Street Landscape; Visual Attention,Behavioral research; Deep learning; E-learning; Eye movements; Eye tracking; Image analysis; Learning systems; Virtual reality; Activation maps; Deep-learning; Evaluation results; Eye-tracking; Gaze movements; Heat maps; Immersive virtual environments; Immersive virtual reality; Street landscape; Visual Attention; Image segmentation,Conference paper,Final,All Open Access; Gold Open Access,Scopus,2-s2.0-85140317836,Gaming / VR
Zhang Q.; Barbareschi G.; Huang Y.; Li J.; Pai Y.S.; Ward J.; Kunze K.,"Zhang, Qing (57278328300); Barbareschi, Giulia (57038765200); Huang, Yifei (57201856134); Li, Juling (57565352200); Pai, Yun Suen (56267209600); Ward, Jamie (57196494666); Kunze, Kai (21743317500)",57278328300; 57038765200; 57201856134; 57565352200; 56267209600; 57196494666; 21743317500,Seeing our Blind Spots: Smart Glasses-based Simulation to Increase Design Students' Awareness of Visual Impairment,2022,UIST 2022 - Proceedings of the 35th Annual ACM Symposium on User Interface Software and Technology,,,42,,,,8,10.1145/3526113.3545687,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85141702802&doi=10.1145%2f3526113.3545687&partnerID=40&md5=725cfc4b249ba86cf389d474cb87c7a3,"As the population ages, many will acquire visual impairments. To improve design for these users, it is essential to build awareness of their perspective during everyday routines, especially for design students. Although several visual impairment simulation toolkits exist in both academia and as commercial products, analog, and static visual impairment simulation tools do not simulate effects concerning the user's eye movements. Meanwhile, VR and video see-through-based AR simulation methods are constrained by smaller fields of view when compared with the natural human visual field and also suffer from vergence-accommodation conflict (VAC) which correlates with visual fatigue, headache, and dizziness. In this paper, we enable an on-the-go, VAC-free, visually impaired experience by leveraging our optical see-through glasses. The FOV of our glasses is approximately 160 degrees for horizontal and 140 degrees for vertical, and participants can experience both losses of central vision and loss of peripheral vision at different severities. Our evaluation (n =14) indicates that the glasses can significantly and effectively reduce visual acuity and visual field without causing typical motion sickness symptoms such as headaches and or visual fatigue. Questionnaires and qualitative feedback also showed how the glasses helped to increase participants' awareness of visual impairment.  © 2022 ACM.",aging vision; eye-tracking; smart eyewear; visual impairment,Eye movements; Glass; Ophthalmology; Surveys; Aging vision; Blind spots; Eye-tracking; Eyewear; Smart eyewear; Smart glass; Vergences; Visual fatigue; Visual fields; Visual impairment; Eye tracking,Conference paper,Final,All Open Access; Green Open Access,Scopus,2-s2.0-85141702802,Gaming / VR
Bian M.; Shen Y.; Huang Y.; Wu L.; Wang Y.; He S.; Huang D.; Mao Y.,"Bian, Minjie (56285723000); Shen, Yuxian (57331362700); Huang, Yijie (57210445684); Wu, Lishan (57260357800); Wang, Yueyan (57946997500); He, Suyue (57947798700); Huang, Dongfeng (7403891468); Mao, Yurong (55237992600)",56285723000; 57331362700; 57210445684; 57260357800; 57946997500; 57947798700; 7403891468; 55237992600,A non-immersive virtual reality-based intervention to enhance lower-extremity motor function and gait in patients with subacute cerebral infarction: A pilot randomized controlled trial with 1-year follow-up,2022,Frontiers in Neurology,13,,985700,,,,10,10.3389/fneur.2022.985700,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85140900227&doi=10.3389%2ffneur.2022.985700&partnerID=40&md5=052935c8f0cea6f1213e677847982142,"Introduction: This study was conducted to evaluate whether a non-immersive virtual reality (VR)-based intervention can enhance lower extremity movement in patients with cerebral infarction and whether it has greater short-term and long-term effectiveness than conventional therapies (CTs). Materials and methods: This was a single-blinded, randomized clinical controlled trial. Forty-four patients with subacute cerebral infarction were randomly allocated to the VR or CT group. All intervention sessions were delivered in the inpatient unit for 3 weeks. Outcomes were measured before (baseline) and after the interventions and at 3-month, 6-month and 1-year follow-ups. The outcomes included clinical assessments of movement and balance function using the Fugl–Meyer Assessment of Lower Extremity (FMA-LE) and Berg Balance Scale (BBS), and gait parameters in the sagittal plane. Results: In the VR group, the walking speed after intervention, at 3-month, 6-month, and 1-year follow-ups were significantly greater than baseline (p = 0.01, <0.001, 0.007, and <0.001, respectively). Compared with baseline, BBS scores after intervention, at 3-month, 6-month, and 1-year follow-ups were significantly greater in both the VR group (p = 0.006, 0.002, <0.001, and <0.001, respectively) and CT group (p = <0.001, 0.002, 0.001, and <0.001, respectively), while FMA-LE scores after intervention, at 3-month, 6-month, and 1-year follow-ups were significant increased in the VR group (p = 0.03, <0.001, 0.003, and <0.001, respectively), and at 3-month, 6-month, and 1-year follow-ups in the CT group (p = 0.02, 0.004 and <0.001, respectively). In the VR group, the maximum knee joint angle in the sagittal plane enhanced significantly at 6-month follow-up from that at baseline (p = 0.04). Conclusion: The effectiveness of the non-immersive VR-based intervention in our study was observed after the intervention and at the follow-ups, but it was not significantly different from that of CTs. In sum, our results suggest that non-immersive VR-based interventions may thus be a valuable addition to conventional physical therapies to enhance treatment efficacy. Clinical trial registration: http://www.chictr.org.cn/showproj.aspx?proj=10541, ChiCTR-IOC-15006064. Copyright © 2022 Bian, Shen, Huang, Wu, Wang, He, Huang and Mao.",gait analysis; ischemic stroke; motor activity; rehabilitation; virtual reality,adult; aged; Article; brain infarction; clinical article; cognition; cognitive defect; controlled study; daily life activity; eye movement; female; follow up; hemiparesis; human; hypertension; ischemic stroke; knee function; lower limb; male; middle aged; motor activity; motor performance; nuclear magnetic resonance imaging; randomized controlled trial; range of motion; respiratory failure; simulation training; single blind procedure; Unified Parkinson Disease Rating Scale; virtual reality,Article,Final,All Open Access; Gold Open Access; Green Open Access,Scopus,2-s2.0-85140900227,Gaming / VR
Wu J.; Zhang D.; Liu T.; Yang H.H.; Wang Y.; Yao H.; Zhao S.,"Wu, Jun (58027388700); Zhang, Di (58027428800); Liu, Tao (56508630700); Yang, Helen Hong (59284111500); Wang, Yi (58027429000); Yao, Huili (58027429100); Zhao, Shinan (57200758167)",58027388700; 58027428800; 56508630700; 59284111500; 58027429000; 58027429100; 57200758167,Usability Evaluation of Augmented Reality: A Neuro-Information-Systems Study,2022,Journal of Visualized Experiments,2022,189,e64667,,,,2,10.3791/64667,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85144587555&doi=10.3791%2f64667&partnerID=40&md5=fb66a7438899e3e2fd208df21803b6f4,"This study introduces an experimental paradigm for a usability test of emerging technologies in a management information system (MIS). The usability test included both subjective and objective evaluations. For the subjective evaluation, a usability questionnaire and a NASA-TLX scale were adopted. For the objective evaluation, methods of Neuro-Information-Systems (NeuroIS) were used. From a NeuroIS perspective, this study used mobile fNIRS and eye tracking glasses for multimodal measurements, which solved the problem of ecological validity of cognitive neuroscience tools used in real-world behavior experiments. This study used Augmented Reality (AR) integrated into the Internet of Things (IoT) as an experimental object. Comparing the differences in the neuroimaging data, the physiological data, the usability questionnaire, and the NASA-TLX scale data between the two information-search modes (AR versus a website), information search with AR had a higher efficiency and a lower cognitive load compared with information search with a website during the process of consumption decision-making. The usability experiment results demonstrate that AR, as an emerging technology in retail, can effectively enhance consumer experiences and increase their purchase intention. The experimental paradigm, combining both subjective and objective evaluations in this study, could be applied to a usability test for emerging technologies, such as augmented reality, virtual reality, artificial intelligence, wearable technology, robotics, and big data. It provides a practical experimental solution for the user experience in human-computer-interactions with the adoption of emerging technologies. © 2022 JoVE Journal of Visualized Experiments.",,Artificial Intelligence; Augmented Reality; Humans; Robotics; Virtual Reality; Wearable Electronic Devices; artificial intelligence; electronic device; human; robotics; virtual reality,Article,Final,,Scopus,2-s2.0-85144587555,Gaming / VR
Yan W.; Li J.; Mi C.; Wang W.; Xu Z.; Xiong W.; Tang L.; Wang S.; Li Y.; Wang S.,"Yan, Wanling (57934433700); Li, Jialing (57561691100); Mi, Can (57561691200); Wang, Wei (57562264400); Xu, Zhengjia (57562455300); Xiong, Wenjing (57562644300); Tang, Longxing (57562644400); Wang, Siyu (57561494300); Li, Yanzhang (56849637000); Wang, Shuai (59870291100)",57934433700; 57561691100; 57561691200; 57562264400; 57562455300; 57562644300; 57562644400; 57561494300; 56849637000; 59870291100,Does global positioning system-based navigation dependency make your sense of direction poor? A psychological assessment and eye-tracking study,2022,Frontiers in Psychology,13,,983019,,,,4,10.3389/fpsyg.2022.983019,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85140263211&doi=10.3389%2ffpsyg.2022.983019&partnerID=40&md5=2eb2972bcc23d129f865c946425b0d8a,"Background: Global positioning system (GPS)-based navigation apps are very useful in our lives. However, whether and how the usage of these apps affects spatial cognition and the sense of direction is still unclear. Methods: A total of 108 individuals were recruited and completed the GPS dependence, internet gaming behavior, and impulsivity test using scales. The eye-tracking-based general mental rotation (MR) task and target finding (TF; require individuals to find a target specified in a 3D street map in a rotated version of top 2D view map) task were used to assess their spatial cognition and the sense of direction. The correlation was used to relate GPS navigation usage, spatial cognition ability, and impulsivity. Subgroup analyses stratifying by gaming hours of individuals (< 2 h or ≥ 2 h) or maps (countryside or city) in TF task were performed. The moderating and mediating effect analyses were conducted to verify these relationships. Results: The GPS dependency score was nominal positively correlated with fixations in the TF task in the entire cohort (r = 0.202, unadjusted p = 0.036); it was significant in city (r = 0.254, p = 0.008) and gaming time of < 2 h (r = 0.459, p = 0.001) subgroups. The high-score (upper 30%) group of GPS dependency had more fixations on the original target building in the training area and indicative building in the test area than the low-score (lower 30%) group. GPS dependency was not associated with the correct rate and reaction time in the TF task or any of the indicators in the MR task (p > 0.05). The GPS dependency mediated the indirect effect of impulsivity on the fixations on TF. The internet gaming time moderated the association between GPS dependency and fixations on TF. Conclusion: The dependency on GPS-based navigation apps was associated with impaired spatial cognition but may not significantly affect the sense of direction. Copyright © 2022 Yan, Li, Mi, Wang, Xu, Xiong, Tang, Wang, Li and Wang.",correlation; dependency; eye-tracking tasks; global positioning system; impulsive; internet gaming; sense of direction; spatial cognition,,Article,Final,All Open Access; Gold Open Access; Green Open Access,Scopus,2-s2.0-85140263211,Gaming / VR
Wang S.; Li J.; Wang S.; Wang W.; Mi C.; Xiong W.; Xu Z.; Tang L.; Li Y.,"Wang, Shuai (59870291100); Li, Jialing (57561691100); Wang, Siyu (57561494300); Wang, Wei (57562264400); Mi, Can (57561691200); Xiong, Wenjing (57562644300); Xu, Zhengjia (57562455300); Tang, Longxing (57562644400); Li, Yanzhang (56849637000)",59870291100; 57561691100; 57561494300; 57562264400; 57561691200; 57562644300; 57562455300; 57562644400; 56849637000,Abnormal psychological performance as potential marker for high risk of internet gaming disorder: An eye-tracking study and support vector machine analysis,2022,Frontiers in Psychology,13,,995918,,,,3,10.3389/fpsyg.2022.995918,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85139186832&doi=10.3389%2ffpsyg.2022.995918&partnerID=40&md5=392f9bad29cb776b2e8712a7d9242b74,"Individuals with high risk of internet gaming disorder (HIGD) showed abnormal psychological performances in response inhibition, impulse control, and emotion regulation, and are considered the high-risk stage of internet gaming disorder (IGD). The identification of this population mainly relies on clinical scales, which are less accurate. This study aimed to explore whether these performances have highly accurate for discriminating HIGD from low-risk ones. Eye tracking based anti-saccade task, Barratt impulsiveness scale (BIS), and Wong and Law emotional intelligence scale (WLEIS) were used to evaluate psychological performances in 57 individuals with HIGD and 52 matched low risk of internet gaming disorder (LIGD). HIGD group showed significantly increased BIS total (t = −2.875, p = 0.005), attention (t = −2.139, p = 0.035), motor (t = −2.017, p = 0.046), and non-planning (t = −2.171, p = 0.032) scores, but significantly decreased WLEIS emotion regulation score (t = 2.636, p = 0.010) and correct rate of eye tracking anti-saccade task (t = 2.294, p = 0.024) compared with LIGD group. BIS total score was negatively correlated with the WLEIS total (r = −0.473, p < 0.001) and WLEIS emotion regulation (r = −0.366, p < 0.001) scores. A combination of the WLEIS emotion regulation score and the correct rate of anti-saccade task could discriminate HIGD from LIGD with 91.23% sensitivity, 82.69% specificity, and 87.16% accuracy. Participants with higher gaming hours daily were 40 times more likely to be high risk than their counterparts (p < 0.001). Hence, psychological performances were worse in HIGD. A combination of abnormal emotion regulation and response inhibition might be a potential marker to identify HIGD individuals. Copyright © 2022 Wang, Li, Wang, Wang, Mi, Xiong, Xu, Tang and Li.",emotion regulation; high risk of internet gaming disorder; impulse control; marker; response inhibition,,Article,Final,All Open Access; Gold Open Access; Green Open Access,Scopus,2-s2.0-85139186832,Gaming / VR
Bhojwani T.M.; Lynch S.D.; Bühler M.A.; Lamontagne A.,"Bhojwani, Trineta M. (57744937600); Lynch, Sean D. (57225318851); Bühler, Marco A. (57197733306); Lamontagne, Anouk (7005939483)",57744937600; 57225318851; 57197733306; 7005939483,Impact of dual tasking on gaze behaviour and locomotor strategies adopted while circumventing virtual pedestrians during a collision avoidance task,2022,Experimental Brain Research,240,10,,2633,2645,12.0,11,10.1007/s00221-022-06427-2,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85136242205&doi=10.1007%2fs00221-022-06427-2&partnerID=40&md5=05c22659b09fa784b348475076bc5737,"We investigated gaze behaviour and collision avoidance strategies in 16 healthy young individuals walking towards a goal while exposed to virtual pedestrians (VRPs) approaching from different directions (left, middle, right). This locomotor task and an auditory-based cognitive task were performed under single and dual-task conditions. Longer gaze fixation durations were observed on the approaching vs. other VRPs, with longer fixations devoted to the upper trunk and head compared to other body segments. Compared to other pedestrian approaches, the middle pedestrian received longer fixations and elicited faster walking speeds, larger onset distances of trajectory devitation and smaller obstacle clearances. Gaze and locomotor behaviours were similar between single and dual-task conditions but dual-task costs were observed for the cognitive task. The longer gaze fixations on approaching vs. other pedestrians suggest that enhanced visual attention is devoted to pedestrians posing a greater risk of collision. Likewise, longer gaze fixations for the middle pedestrians may be due to the greater collision risk entailed by this condition, and/or to the fact that this pedestrian was positioned in front of the end goal. Longer fixations on approaching VRPs’ trunk and head may serve the purpose of anticipating their walking trajectory. Finally, the dual-task effects that were limited to the cognitive task suggest that healthy young adults prioritize the locomotor task and associated acquisition of visual information. The healthy patterns of visuomotor behaviour characterized in this study will serve as a basis for comparison to further understand defective collision avoidance strategies in patient populations. © 2022, The Author(s), under exclusive licence to Springer-Verlag GmbH Germany, part of Springer Nature.",Cognitive load; Collision avoidance; Eye movements; Locomotion; Pedestrian interactions; Virtual reality,"Fixation, Ocular; Humans; Pedestrians; Walking; Young Adult; adult; article; avoidance behavior; clinical article; cognition; cognitive load; controlled study; eye movement; female; gaze; human; locomotion; male; pedestrian; trunk; virtual reality; visual attention; visual information; walking; walking speed; young adult; eye fixation; psychology",Article,Final,,Scopus,2-s2.0-85136242205,Gaming / VR
Chi H.-Y.; Juan Y.-K.; Lu S.,"Chi, Hao-Yun (57202457141); Juan, Yi-Kai (12767232600); Lu, Shiliang (55119225800)",57202457141; 12767232600; 55119225800,"Comparing BIM-Based XR and Traditional Design Process from Three Perspectives: Aesthetics, Gaze Tracking, and Perceived Usefulness",2022,Buildings,12,10,1728,,,,11,10.3390/buildings12101728,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85140774345&doi=10.3390%2fbuildings12101728&partnerID=40&md5=2bafe32be86bbb633d0b3e723cfac708,"With technological development and industrial transformation, the architecture, engineering, and construction (AEC) industry, comprising architecture, engineering, and construction, has shifted from a traditional drawing-based design mode to a digital and computer-based mode. In recent years, the application of extended reality (XR) technology, including virtual reality (VR), augmented reality (AR), and mixed reality (MR) technology, emphasizes the immersive and interactive experiences between reality and virtuality, bringing breakthrough developments to architectural projects. This study proposes a new design process mode—the BIM-based XR system—and compares it with the traditional design process mode through an actual stadium design project. Three evaluation perspectives including aesthetics, gaze tracking, and perceived usefulness assessment are used to compare the differences between the two modes. The result showed that the use of the BIM-based XR system could bring users more immersive experience and aesthetic assessment preference, and perceived usefulness in design decision-making, communication, and spatial cognition. The gaze tracking result also revealed that the BIM-based XR system can implement the design process more efficient. It is expected that XR and BIM technologies can be effectively integrated to enhance the integrity of industrial applications and establish a new design collaboration mode for the AEC industry. © 2022 by the authors.",augmented reality (AR); Building Information Modeling (BIM); extended reality (XR); gaze tracking; mixed reality (MR); sports architecture; virtual reality (VR),,Article,Final,All Open Access; Gold Open Access,Scopus,2-s2.0-85140774345,Gaming / VR
Limballe A.; Kulpa R.; Vu A.; Mavromatis M.; Bennett S.J.,"Limballe, Annabelle (57210916853); Kulpa, Richard (6507692858); Vu, Alexandre (57646346500); Mavromatis, Maé (57670620200); Bennett, Simon J. (7403105607)",57210916853; 6507692858; 57646346500; 57670620200; 7403105607,Virtual reality boxing: Gaze-contingent manipulation of stimulus properties using blur,2022,Frontiers in Psychology,13,,902043,,,,7,10.3389/fpsyg.2022.902043,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85140014945&doi=10.3389%2ffpsyg.2022.902043&partnerID=40&md5=5f02e58997e68c7c2e3eab68a6381e91,"It has been reported that behavior of experts and novices in various sporting tasks is impervious to the introduction of blur. However, studies have used diverse methods of blurring the visual stimulus (i.e., dioptric blur and Gaussian blur), and tasks that did not always preserve the normal perception-action coupling. In the current study, we developed a novel experimental protocol to examine the effect of different levels of Gaussian blur on interception performance and eye gaze data using an immersive VR task. Importantly, this provided a realistic simulation of a real-world boxing scenario (e.g., the presence of a feint prior to the onset of different combinations of punches) in which expert combat athletes (n = 18) experienced a first-person, adaptive viewpoint of the visual environment, which could be blurred according to their gaze location (central blur, peripheral blur, no blur). We found that participants exhibited similar interception performance in the presence of central blur or peripheral blur compared to a control condition with no blur. However, interception performance was significantly better with a central blur compared to peripheral blur. Eye gaze data indicated that although participants fixated at similar areas of interest irrespective of the presence of blur, fixation duration was significantly longer with a strong level of blur in the peripheral viewing condition than all levels of central blur and the control condition. These findings can be explained by relocating attention to different areas of the environment, which thereby influenced the perception of salient information. Participants also performed better on the first punch of a sequence preceded by a foot feint compared to arm feint or no feint. Still, irrespective of feint type, performance was significantly better on the second and third punch compared to the first punch. These findings are consistent with participants using additional information from the opponent's body movements and situational probabilities to increase performance as the sequence of punches developed. Overall, these are the first evidence for the use of VR as a means to examine gaze-contingent manipulations of the environment, and hence highlight the potential for facilitating learning and transfer to a real sporting situations. Copyright © 2022 Limballe, Kulpa, Vu, Mavromatis and Bennett.",anticipation; blur; boxing; sport; virtual reality,,Article,Final,All Open Access; Gold Open Access; Green Open Access,Scopus,2-s2.0-85140014945,Gaming / VR
Wei Z.-H.; Li Q.-Y.; Liang C.-J.; Liu H.-Z.,"Wei, Zi-Han (57217022553); Li, Qiu-Yue (57644941300); Liang, Ci-Juan (57921152200); Liu, Hong-Zhi (57202104797)",57217022553; 57644941300; 57921152200; 57202104797,Cognitive process underlying ultimatum game: An eye-tracking study from a dual-system perspective,2022,Frontiers in Psychology,13,,937366,,,,3,10.3389/fpsyg.2022.937366,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85139546704&doi=10.3389%2ffpsyg.2022.937366&partnerID=40&md5=02ad532cd3e6ad6f8a41faeb9cbc0c2f,"According to the dual-system theories, the decisions in an ultimatum game (UG) are governed by the automatic System 1 and the controlled System 2. The former drives the preference for fairness, whereas the latter drives the self-interest motive. However, the association between the contributions of the two systems in UG and the cognitive process needs more direct evidence. In the present study, we used the process dissociation procedure to estimate the contributions of the two systems and recorded participants eye movements to examine the cognitive processes underlying UG decisions. Results showed that the estimated contributions of the two systems are uncorrelated and that they demonstrate a dissociated pattern of associations with third variables, such as reaction time (RT) and mean fixation duration (MFD). Furthermore, the relative time advantage (RTA) and the transitions between the two payoffs can predict the final UG decisions. Our findings provide evidence for the independent contributions of preference for fairness (System 1) and self-interest maximizing (System 2) inclinations to UG and shed light on the underlying processes. Copyright © 2022 Wei, Li, Liang and Liu.",attention allocation; cognitive effort; dual-system; eye-tracking; ultimatum game,,Article,Final,All Open Access; Gold Open Access; Green Open Access,Scopus,2-s2.0-85139546704,Gaming / VR
Duan H.; Ren X.; Wang L.; Shi F.; Fan L.; Zhai G.,"Duan, Huiyu (57195265438); Ren, Xiaoyu (58102623600); Wang, Linlin (58851948300); Shi, Fangyu (57208839344); Fan, Lei (58466260700); Zhai, Guangtao (15847120000)",57195265438; 58102623600; 58851948300; 57208839344; 58466260700; 15847120000,Processing and Analysis of Eye Movement Signals for Autistic Children in Omnidirectional Space; [全方位空间自闭症儿童眼动信号处理与分析],2022,Journal of Signal Processing,38,9,,1797,1808,11.0,1,10.16798/j.issn.1003-0530.2022.09.003,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85203607148&doi=10.16798%2fj.issn.1003-0530.2022.09.003&partnerID=40&md5=cea4c3d05650189ffaedb2d14c297212,"To address the problem that previous eye-movement studies for autism spectrum disorder(ASD)cannot show the visual attention of autistic patients in the real omnidirectional world, in this paper, we propose to utilize virtual reality (VR)technology and omnidirectional 360 degree images to collect eye movement data and head movement data in VR environment, and then further analyze and simulate the visual attention of autistic children in the real omnidirectional world. Specifically, we first establish a large-scale panoramic image eye-movement dataset for autistic children. Then, based on the constructed dataset, we improve the extracted features and methods in the three-layer saliency calculation model. We quantify the features of all panoramic images of the dataset, and calculate the influence of different features on visual attention via support vector machine(SVM). Finally, based on the collected data and computational results, we qualitatively and quantitatively analyze the correlation and difference between the visual attention of autistic children and typically developing controls including the visual preference for different features and head-eye movement correlations. This study can contribute to analyzing the visual characteristics of autism and can further assist the procedure of classification, diagnosis and rehabilitation of autism. © 2022 Editorial Board of Journal of Signal Processing. All rights reserved.",autism; eye movement; panoramic images; saliency model; visual attention,,Article,Final,,Scopus,2-s2.0-85203607148,Gaming / VR
Zhang H.; Pan J.S.,"Zhang, Huiyuan (57887818600); Pan, Jing Samantha (55884945800)",57887818600; 55884945800,Visual search as an embodied process: The effects of perspective change and external reference on search performance,2022,Journal of Vision,22,10,22(10),,,,6,10.1167/jov.22.10.13,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85137936543&doi=10.1167%2fjov.22.10.13&partnerID=40&md5=2814a0face74a8ec0bf346e75c4287d1,"typically involve looking for targets in 2D displays with exemplar views of objects. In real life, visual search commonly entails 3D objects in 3D spaces with nonperpendicular viewing and relative motions between observers and search array items, both of which lead to transformations of objects’ projected images in lawful but unpredicted ways. Furthermore, observers often do not have to memorize a target before searching, but may refer to it while searching, for example, holding a picture of someone while looking for them from a crowd. Extending the traditional visual search task, in this study, we investigated the effects of image transformation as a result of perspective change yielded by discrete viewing angle change (Experiment 1) or continuous rotation of the search array (Experiment 2) and of having external references on visual search performance. Results showed that when searching from 3D objects with a non-zero viewing angle, performance was similar to searching from 2D exemplar views of objects; when searching for 3D targets from rotating arrays in virtual reality, performance was similar to searching from stationary arrays. In general, discrete or continuous perspective change did not affect the search outcomes in terms of accuracy, response time, and self-rated confidence, or the search process in terms of eye movement patterns. Therefore, visual search does not require the exact match of retinal images. Additionally, being able to see the target during the search improved search accuracy and observers’ confidence. It increased search time because, as revealed by the eye movements, observers actively checked back on the reference target. Thus, visual search is an embodied process that involves real-time information exchange between the observers and the environment. © Downloaded from jov.arvojournals.org on 09/16/2022 Commons Attribution-NonCommercial-NoDerivatives 4.0 International License.",Just-in-time information; Metacognitive sensitivity; Perspective change; Virtual reality; Visual search,Attention; Eye Movements; Humans; Reaction Time; Visual Perception; attention; eye movement; human; physiology; reaction time; vision,Article,Final,All Open Access; Gold Open Access; Green Open Access,Scopus,2-s2.0-85137936543,Gaming / VR
Jangard S.; Lindström B.; Khemiri L.; Pärnamets P.; Jayaram-Lindström N.; Olsson A.,"Jangard, Simon (57195595285); Lindström, Björn (54891341800); Khemiri, Lotfi (54684161700); Pärnamets, Philip (55918105000); Jayaram-Lindström, Nitya (8623577400); Olsson, Andreas (8690139600)",57195595285; 54891341800; 54684161700; 55918105000; 8623577400; 8690139600,Alcohol Use Disorder Displays Trait-Related Reductions in Prosocial Decision Making,2022,Biological Psychiatry: Cognitive Neuroscience and Neuroimaging,7,9,,925,934,9.0,8,10.1016/j.bpsc.2022.05.002,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85136204280&doi=10.1016%2fj.bpsc.2022.05.002&partnerID=40&md5=158f0adc7939461651e55a2902b3016d,"Background: Alcohol use disorder (AUD) is associated with deficits in social cognition, but the relationship between harmful alcohol use and the processes underlying interactive social behavior is still unknown. We hypothesized that prosocial decision making is reduced in AUD and that individual differences in the underlying processes are key to better understanding these reductions. Methods: In one laboratory study (Swedish participants, n = 240) and one confirmatory online study (American participants, n = 260), we compared young adults with AUD with age-, gender-, and education-matched healthy control subjects on 6 facets of prosocial decision making. We used standardized behavioral economic tasks, namely the dictator game, ultimatum game, trust game, and third-party game. To better understand the expected differences in prosociality, we evaluated attention by tracking eye gaze, decision response time, clinical symptoms, and social cognition. Results: Altruism (lab study: p = .007; online study: p < .001), fairness (lab study: p = .003; online study: p = .007), and reciprocal trust (lab study: p = .007; online study: p = .039) were reduced in individuals with AUD compared with healthy control subjects, whereas trust and third-party punishment and compensation were comparable in both studies. Reduced prosociality was associated with attending to the selfish response option, faster response time, and moral attitudes, while being dissociated from both psychiatric symptoms and drinking history in AUD. Conclusions: Individuals with AUD have trait-related reductions in prosocial decision making that do not vary with drinking history or psychiatric symptom load. These reductions were confined to one-to-one interactions accompanied by differences in attention, decision time, and moral attitudes. © 2022 Society of Biological Psychiatry",Alcohol use disorder; Behavioral economics; Prosocial behavior; Social cognition; Social decision making,Alcoholism; Altruism; Decision Making; Humans; Reaction Time; Social Behavior; Young Adult; adult; Adult ADHD Self-Report Scale; alcoholism; altruism; Article; autism-spectrum quotient; controlled study; decision making; decision making task; DSM-5; eye tracking; female; game; gaze; human; major clinical study; male; mental disease; mini international neuropsychiatric interview; prosocial behavior; reaction time; self report; social behavior; social cognition; trust game; Wechsler adult intelligence scale; young adult; decision making; physiology; social behavior,Article,Final,All Open Access; Hybrid Gold Open Access,Scopus,2-s2.0-85136204280,Gaming / VR
Guglielmo G.; Mavromoustakos Blom P.; Klincewicz M.; Huis In 'T Veld E.; Spronck P.,"Guglielmo, Gianluca (57317500400); Mavromoustakos Blom, Paris (56443691300); Klincewicz, Michal (37077547400); Huis In 'T Veld, Elisabeth (54890701400); Spronck, Pieter (57195385803)",57317500400; 56443691300; 37077547400; 54890701400; 57195385803,Blink To Win Blink Patterns of Video Game Players Are Connected to Expertise,2022,ACM International Conference Proceeding Series,,,12,,,,3,10.1145/3555858.3555864,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85142351214&doi=10.1145%2f3555858.3555864&partnerID=40&md5=62547e318b9cb14cdc297e06c2e4fc92,"In this study, we analyzed the blinking behavior of players in a video game tournament. We aimed to test whether spontaneous blink patterns differ across levels of expertise. We used blink rate (blinks/m), blink duration, and general eyelid movements represented by features extracted from the Eye Aspect Ratio (EAR) to train a machine learning classifier to discriminate between different levels of expertise. Classifier performance was highly influenced by features such as the mean, standard deviation, and median EAR. Moreover, further analysis suggests that the blink rate is likely to increase with the level of expertise. We speculate this may be indicative of a reduction in cognitive load and lowered stress of expert players. In general, our results suggest that EAR and blink patterns can be used to identify different levels of expertise of video game players.  © 2022 Owner/Author.",blink patterns; expertise; pattern stability; video games,Eye movements; Human computer interaction; Interactive computer graphics; Learning systems; Aspect-ratio; Blink duration; Blink pattern; Blink rates; Expertise; Game players; Learning classifiers; Machine-learning; Pattern stability; Video-games; Aspect ratio,Conference paper,Final,All Open Access; Bronze Open Access,Scopus,2-s2.0-85142351214,Gaming / VR
Coburn A.; Harteveld C.; Holmgard C.,"Coburn, Alexander (57973679400); Harteveld, Casper (23392508000); Holmgard, Christoffer (56021987100)",57973679400; 23392508000; 56021987100,Ticket to the Mind: A Mobile Eye-Tracking Exploration of Game Media and Cognitive Functions,2022,ACM International Conference Proceeding Series,,,11,,,,0,10.1145/3555858.3555927,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85142389466&doi=10.1145%2f3555858.3555927&partnerID=40&md5=0230bf3b31a8e5c4fbe8658487e8ee76,"Physical and digital games are increasingly used and acknowledged for their cognitive effects, but there is a lack of understanding of how a game's medium directly impacts cognitive functions. This exploratory case study presents a novel method of using mobile eye-tracking to compare working memory and visual attention between physical and digital versions of the board game Ticket to Ride in natural settings. For visual attention, no significant differences between the versions were measured. However, results indicate that fixation duration is significantly greater while playing the digital version versus the physical version, suggesting the counter-intuitive finding that the digital version increases working memory load. The study design, with evidence for high ecological validity, and its findings provide a foundation for future studies for assessing cognitive processing of game media. This work also provides inspiration for making game media considerations for obtaining specific cognitive experiences and increasing accessibility.  © 2022 ACM.",board games; cognitive functions; digital games; mobile eye-tracking; physical games; visual attention; working memory,Brain; Computer games; Eye tracking; Board games; Cognitive effects; Cognitive functions; Digital games; Digital versions; Exploratory case studies; Mobile eye-tracking; Physical games; Visual Attention; Working memory; Behavioral research,Conference paper,Final,,Scopus,2-s2.0-85142389466,Gaming / VR
Helbing J.; Draschkow D.; L.-H. Võ M.,"Helbing, Jason (57214221531); Draschkow, Dejan (56270096800); L.-H. Võ, Melissa (57838228400)",57214221531; 56270096800; 57838228400,Auxiliary Scene-Context Information Provided by Anchor Objects Guides Attention and Locomotion in Natural Search Behavior,2022,Psychological Science,33,9,,1463,1476,13.0,16,10.1177/09567976221091838,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85135761198&doi=10.1177%2f09567976221091838&partnerID=40&md5=8e2a6b22146ed0f43c21c92948ac7bd9,"Successful adaptive behavior requires efficient attentional and locomotive systems. Previous research has thoroughly investigated how we achieve this efficiency during natural behavior by exploiting prior knowledge related to targets of our actions (e.g., attending to metallic targets when looking for a pot) and to the environmental context (e.g., looking for the pot in the kitchen). Less is known about whether and how individual nontarget components of the environment support natural behavior. In our immersive virtual reality task, 24 adult participants searched for objects in naturalistic scenes in which we manipulated the presence and arrangement of large, static objects that anchor predictions about targets (e.g., the sink provides a prediction for the location of the soap). Our results show that gaze and body movements in this naturalistic setting are strongly guided by these anchors. These findings demonstrate that objects auxiliary to the target are incorporated into the representations guiding attention and locomotion. © The Author(s) 2022.",anchor objects; attention; contextual guidance; locomotion; natural behavior; open data; scene grammar; virtual reality,"Adult; Attention; Eye Movements; Humans; Locomotion; Pattern Recognition, Visual; Soaps; Virtual Reality; soap; adult; attention; eye movement; human; locomotion; pattern recognition; virtual reality",Article,Final,,Scopus,2-s2.0-85135761198,Gaming / VR
Ding X.; Chen Z.,"Ding, Xiaoying (56571839900); Chen, Zhenzhong (55737671700)",56571839900; 55737671700,Towards mesh saliency in 6 degrees of freedom,2022,Neurocomputing,502,,,120,139,19.0,3,10.1016/j.neucom.2022.06.088,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85133422351&doi=10.1016%2fj.neucom.2022.06.088&partnerID=40&md5=c44a99d3a2b311822e632063e4ade546,"In this work, a novel 6DoF mesh saliency database is developed which provides both the subject's 6DoF data and eye-movement data. Different from traditional databases, subjects in the experiment are allowed to move freely to view 3D meshes in the virtual reality environment. Based on the database, we first analyze the inter-observer variation and the influence of viewing direction toward subject's visual attention, then we provide investigations about the subject's visual attention bias and head movement during observation. As traditional 3D mesh saliency detection algorithms do not taking the subject's head movement into consideration, we further propose a 6DoF mesh saliency detection algorithm based on the uniqueness measure and the bias preference. To evaluate the proposed approach, we design an evaluation metric accordingly which takes the 6DoF information into consideration, and extend some state-of-the-art 3D saliency detection methods to make comparisons. The experimental results demonstrate the superior performance of our approach for 6DoF mesh saliency detection, in addition to providing benchmarks for the presented 6DoF mesh saliency database. The database and proposed method will be made publicly available for research purposes. © 2022 Elsevier B.V.",6DoF; 6DoF mesh saliency detection; Visual attention behavior,Behavioral research; Benchmarking; Degrees of freedom (mechanics); Eye movements; Mesh generation; Virtual reality; 3D meshes; 6 degree of freedom; 6dof; 6dof mesh saliency detection; Detection algorithm; Head movements; Mesh saliencies; Saliency detection; Visual Attention; Visual attention behavior; adult; article; attentional bias; degree of freedom; detection algorithm; eye movement; head movement; human; intermethod comparison; observer variation; virtual reality; visual attention; Database systems,Article,Final,,Scopus,2-s2.0-85133422351,Gaming / VR
Hasenbein L.; Stark P.; Trautwein U.; Queiroz A.C.M.; Bailenson J.; Hahn J.-U.; Göllner R.,"Hasenbein, Lisa (57211289472); Stark, Philipp (57223937195); Trautwein, Ulrich (6701777248); Queiroz, Anna Carolina Muller (57202706514); Bailenson, Jeremy (6602840468); Hahn, Jens-Uwe (56115756900); Göllner, Richard (56661056800)",57211289472; 57223937195; 6701777248; 57202706514; 6602840468; 56115756900; 56661056800,Learning with simulated virtual classmates: Effects of social-related configurations on students’ visual attention and learning experiences in an immersive virtual reality classroom,2022,Computers in Human Behavior,133,,107282,,,,60,10.1016/j.chb.2022.107282,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85128813562&doi=10.1016%2fj.chb.2022.107282&partnerID=40&md5=bf38a13449904fe2551cf28e660bce71,"Immersive virtual reality (IVR) provides great potential to experimentally investigate effects of peers on student learning in class and to strategically deploy virtual peer learners to improve learning. The present study examined how three social-related classroom configurations (i.e., students' position in the classroom, visualization style of virtual avatars, and virtual classmates' performance-related behavior) affect students' visual attention toward information presented in the IVR classroom using a large-scale eye-tracking data set of N = 274 sixth graders. ANOVA results showed that the IVR configurations were systematically associated with differences in learners' visual attention on classmates or the instructional content and their overall gaze distribution in the IVR classroom (Cohen's d ranging from 0.28 to 2.04 for different IVR configurations and gaze features). Gaze-based attention on classmates was negatively related to students' interest in the IVR lesson (d = 0.28); specifically, the more boys were among the observed peers, the lower students' situational self-concept (d = 0.24). In turn, gaze-based attention on the instructional content was positively related to students' performance after the IVR lesson (d = 0.26). Implications for the future use of IVR classrooms in educational research and practice are discussed. © 2022 The Authors",Classroom simulation; Eye-tracking; Immersive virtual reality; Network analysis; Peer effects; Visual attention,Behavioral research; Data visualization; E-learning; Students; Virtual reality; Classroom simulation; Eye-tracking; Immersive virtual reality; Investigate effects; Learning experiences; Peer effect; Student learning; Virtual avatar; Visual Attention; Visual learning; analysis of variance; article; attention; child; eye tracking; gaze; human; human experiment; learning; major clinical study; male; network analysis; self concept; simulation; virtual reality; visual attention; Eye tracking,Article,Final,All Open Access; Hybrid Gold Open Access,Scopus,2-s2.0-85128813562,Gaming / VR
Manley C.E.; Bennett C.R.; Merabet L.B.,"Manley, Claire E. (57877184300); Bennett, Christopher R. (55385456500); Merabet, Lotfi B. (6603146795)",57877184300; 55385456500; 6603146795,Assessing Higher-Order Visual Processing in Cerebral Visual Impairment Using Naturalistic Virtual-Reality-Based Visual Search Tasks,2022,Children,9,8,1114,,,,26,10.3390/children9081114,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85137359317&doi=10.3390%2fchildren9081114&partnerID=40&md5=1f7cc2d068235f9442e81714459d11d4,"Cerebral visual impairment (CVI) is a brain-based disorder associated with the maldevelopment of central visual pathways. Individuals with CVI often report difficulties with daily visual search tasks such as finding a favorite toy or familiar person in cluttered and crowded scenes. We developed two novel virtual reality (VR)-based visual search tasks combined with eye tracking to objectively assess higher order processing abilities in CVI. The first (virtual toybox) simulates a static object search, while the second (virtual hallway) represents a dynamic human search task. Participants were instructed to search for a preselected target while task demand was manipulated with respect to the presence of surrounding distractors. We found that CVI participants (when compared to age-matched controls) showed an overall impairment with visual search on both tasks and with respect to all gaze metrics. Furthermore, CVI participants showed a trend of worsening performance with increasing task demand. Finally, search performance was also impaired in CVI participants with normal/near normal visual acuity, suggesting that reduced stimulus visibility alone does not account for these observations. This novel approach may have important clinical utility in helping to assess environmental factors related to functional visual processing difficulties observed in CVI. © 2022 by the authors.",attention; cerebral visual impairment (CVI); eye tracking; higher order visual processing; virtual reality; visual perception; visual search,adolescent; adult; age distribution; Article; attention; brain disease; brain function; cerebral visual impairment; child; clinical article; controlled study; environmental factor; eye tracking; female; higher order visual processing; human; male; task performance; virtual hallway; virtual reality; virtual reality based visual search task; virtual toybox; visual acuity; visual impairment; visual search; visual stimulation; visual system; visual system function; young adult,Article,Final,All Open Access; Gold Open Access; Green Open Access,Scopus,2-s2.0-85137359317,Gaming / VR
Beavan A.; Spielmann J.; Ehmann P.; Mayer J.,"Beavan, Adam (57202055135); Spielmann, Jan (57203717820); Ehmann, Paul (57208865534); Mayer, Jan (57207403185)",57202055135; 57203717820; 57208865534; 57207403185,The Development of Executive Functions in High-Level Female Soccer Players,2022,Perceptual and Motor Skills,129,4,,1036,1052,16.0,12,10.1177/00315125221096989,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85132312834&doi=10.1177%2f00315125221096989&partnerID=40&md5=44e23308e62b445374ae83ae3170d9ed,"Executive functions (EFs) are higher-level cognitive functions that help keep an individual’s goal-oriented thoughts and actions aligned. While many studies have shown the importance of EFs in sport, a limitation in this literature is that female participants have been underrepresented. In this mixed-longitudinal study, we examined the development of EFs in a cohort of high performing female athletes. We collected data over five seasons in a large sample of 175 female soccer players (aged 12–29 years old) from the U14 - senior age groups of a professional German soccer club. Players undertook a large battery of cognitive tasks aimed at measuring higher-level cognitive functioning: a sustained attention task, a stop-signal task, a Go-No-go test, an N-Back Test, and both a 180°- and 360°-multiple-object tracking task. We used linear and non-linear mixed effect regressions to examine the relationship between age and EFs. Second order polynomial curves explained many of these relationships between age and EFs compared to their linear relationships. Negatively accelerated curves reveal that these players’ cognitive abilities mainly developed before players reached early adulthood, with a performance plateau evident at around 21 years of age. Age explained low to moderate proportions of the variance in EFs (<1–50%), while cognitive development across playing positions was not a strong contributor to this variance (M = 2.1, SD = 2.1%). We concluded that age has a negatively accelerated relationship with EFs in female soccer players that does not differ between playing positions. These data support the idea that athletes require only a reasonable level of EF ability to perform at the highest level of their sport. Our research raises new questions regarding the validity of current EF measurement methods for inferring information about in-game use of these cognitive abilities. © The Author(s) 2022.",academy; athletes; cognitive; diagnostics; soccer,Adolescent; Adult; Athletes; Athletic Performance; Child; Executive Function; Female; Humans; Longitudinal Studies; Soccer; Young Adult; adolescent; adult; adulthood; age; article; athlete; attention; child; cognition; cognitive development; cohort analysis; controlled study; executive function; eye tracking; female; Go No Go task; groups by age; human; human experiment; longitudinal study; n-back test; school child; season; soccer; soccer player; validity; young adult; athlete; athletic performance; executive function; psychology,Article,Final,,Scopus,2-s2.0-85132312834,Gaming / VR
Schröder B.; Mühlberger A.,"Schröder, Benedikt (57763203400); Mühlberger, Andreas (6507375232)",57763203400; 6507375232,Assessing the attentional bias of smokers in a virtual reality anti-saccade task using eye tracking,2022,Biological Psychology,172,,108381,,,,9,10.1016/j.biopsycho.2022.108381,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85132719849&doi=10.1016%2fj.biopsycho.2022.108381&partnerID=40&md5=7fe8ced90001d14bf81889ceb8b2c29e,"Introduction: Cognitive biases (among them attentional bias, AB) are considered an important factor in the development, maintenance, and recurrence of addiction. However, traditional paradigms to measure AB have been criticized regarding external validity and methodical issues. Therefore, and because the neurophysiological correlates of anti-saccade tasks are known, we implemented a novel smoking anti-saccade task in virtual reality (VR) to measure AB and inhibitory control in different contexts and with higher ecological validity. Methods: Smokers (n = 20) and non-smokers (n = 20) were tested on a classic pro- and anti-saccade task, a VR anti-saccade task and a VR attention fixation task (all containing smoking-related and neutral stimuli) while eye-tracking data was collected. Two VR contexts (park and office room) were applied. Results: Saccade latencies were significantly higher for the smoking group in the VR anti-saccade task. However, this effect did not differ between smoking-related and neutral stimuli, thus overall no AB was observed. Instead, AB was only present in the park context. Additionally, saccade latencies and error rates were significantly higher in the park context. Conclusions: Results indicate impaired inhibitory control in smokers relative to non-smokers. The lack of evidence for a general AB might be due to the lower severity of smoking dependence in the smoking sample. Instead, results suggest context specificity of AB. Implications for smoking cessation interventions in the field of inhibitory control training and attention bias modification are discussed. © 2022 Elsevier B.V.",Addiction; Anti-saccade task; Attention bias; Eye tracking; Smoking; Virtual reality,Attentional Bias; Cues; Eye-Tracking Technology; Humans; Saccades; Smokers; Virtual Reality; carbon monoxide; adult; anecdotal evidence; anti saccade error rate; anti saccade task; Article; attention fixation task; attentional bias; Bayes theorem; cigarette smoking; clinical article; craving; ecological validity; evidence based practice; executive function; eye tracking; Fagerstrom Test for Nicotine Dependence; female; human; human experiment; inhibitory control; latent period; male; mental function assessment; neurologic disease assessment; neurophysiology; neutral stimulus; non-smoker; normal human; null hypothesis; pro saccade task; saccade latency; saccadic eye movement; smoking; smoking cessation; stimulus response; virtual reality; visual probe task; visual system parameters; association; physiology; psychology; smoking,Article,Final,,Scopus,2-s2.0-85132719849,Gaming / VR
Ye Y.; Shi Y.; Xia P.; Kang J.; Tyagi O.; Mehta R.K.; Du J.,"Ye, Yang (57647823200); Shi, Yangming (57189999728); Xia, Pengxiang (57222470135); Kang, John (57222464159); Tyagi, Oshin (57219992021); Mehta, Ranjana K. (55413685300); Du, Jing (57219889677)",57647823200; 57189999728; 57222470135; 57222464159; 57219992021; 55413685300; 57219889677,Cognitive characteristics in firefighter wayfinding Tasks: An Eye-Tracking analysis,2022,Advanced Engineering Informatics,53,,101668,,,,21,10.1016/j.aei.2022.101668,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85131935778&doi=10.1016%2fj.aei.2022.101668&partnerID=40&md5=07c5ac4ce76078bb9ff7822ee498aff8,"During search and rescue, firefighters need to find paths in an unfamiliar space with minimum time and information available. The effective memorization and retrieval of critical spatial information can help reduce risks and increase mission efficiency. Although evidence has shown that different formats of wayfinding information, including landmarks, routes, and surveys, can impact search and rescue performance in different manners, a deeper understanding of the characteristics of firefighters’ cognitive processes related to the varying wayfinding information formats is less explored. To evaluate firefighters’ performance and cognitive characteristics in search and rescue, a firefighter experiment in Virtual Reality (VR) was conducted. Firefighters (n = 40) were recruited to participate in the simulated rescue task. After reviewing the spatial information in different formats, firefighters were requested to find three victims inside a VR maze as quickly as possible. Task performance was evaluated by the number of victims found and the time spent. Firefighters’ gaze patterns were analyzed to evaluate their cognitive status. The result showed that although the cognitive load under the survey and route conditions was significantly higher than under the landmark condition (p < 0.001), the decision-making involved a more effective cognitive process related to choosing the right path at critical waypoints such as where a turning decision must be made. Thus, the perceived workload and fatigue levels of the two conditions were lower, and the wayfinding performance was better. In contrast, with landmark information, the cognitive load levels were consistently high, along with increased mental fatigue. The findings reveal a series of cognitive features related to a more effective spatial decision-making in search and rescue. In the future, it is expected that these cognitive features can be used to develop real-time monitoring and prediction models for wayfinding performance. © 2022 Elsevier Ltd",Cognitive analysis; Firefighter; Gaze analysis; Virtual Reality; Wayfinding,Cognitive systems; Decision making; Eye tracking; Fire extinguishers; Surveys; Cognitive analysis; Cognitive characteristics; Cognitive loads; Cognitive process; Condition; Gaze analysis; Performance; Search and rescue; Spatial informations; Way finding; Virtual reality,Article,Final,All Open Access; Bronze Open Access,Scopus,2-s2.0-85131935778,Gaming / VR
Chu C.-H.; Chen Y.-A.; Huang Y.-Y.; Lee Y.-J.,"Chu, Chih-Hsing (35247663700); Chen, Yi-An (57458765900); Huang, Ying-Yin (55336625500); Lee, Yun-Ju (36194046300)",35247663700; 57458765900; 55336625500; 36194046300,A Comparative Study of Virtual Footwear Try-On Applications in Virtual and Augmented Reality,2022,Journal of Computing and Information Science in Engineering,22,4,41004,,,,11,10.1115/1.4053328,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124873344&doi=10.1115%2f1.4053328&partnerID=40&md5=ff68b040690b7df2607efc0e51b8da36,"Virtual try-on technology (VTO) in virtual reality (VR) and augmented reality (AR) has been developed for years to create novel shopping experiences for users by allowing them to virtually wear fashion products. Compared to garments or facial accessories, fewer studies have focused on virtual footwear try-on, regardless of user study or technical development. Thus, it is necessary to examine the effectiveness of existing VTO applications on the user's affective responses. In this study, we compared the user experience of three different footwear try-on methods (real, VR, and AR) with both physiological and psychological measures. Subjects conducted a try-on experiment on different pairs of sneakers. Each subject's gaze trajectory was recorded using an eye tracker and analyzed to show his/her visual attention in each method. Afterward, the subjects completed questionnaires to assess the sense of presence, usability, and the user experience score for the try-on processes, and subsequently attended a think-aloud procedure to express their thoughts. The analysis results of the collected data showed that the user experience produced by the VR and AR try-on is not comparable to that of the real environment. The results also revealed factors that negatively affect the quality of the user's interaction with the processes. These findings may provide insights into further improvements in VTO technology. © 2022 American Society of Mechanical Engineers (ASME). All rights reserved.",Augmented reality; Human computer interfaces/interactions; Presence; Usability; User experience; Virtual and augmented reality environments; Virtual prototyping; Virtual reality; Virtual try-on,Behavioral research; Eye tracking; Surveys; Virtual reality; Wear of materials; Comparatives studies; Human computer interface/interaction; Human computer interfaces; Interface interaction; Presence; User study; Users' experiences; Virtual and augmented reality; Virtual and augmented reality environment; Virtual try-on; Augmented reality,Article,Final,,Scopus,2-s2.0-85124873344,Gaming / VR
Zhang B.; Naya Y.,"Zhang, Bo (57226875145); Naya, Yuji (7006813962)",57226875145; 7006813962,A dataset of human fMRI/MEG experiments with eye tracking for spatial memory research using virtual reality,2022,Data in Brief,43,,108380,,,,1,10.1016/j.dib.2022.108380,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85132834346&doi=10.1016%2fj.dib.2022.108380&partnerID=40&md5=296a5e5b8c1c7978d68707a6256104c7,"A dataset consisting of whole-brain fMRI (functional magnetic resonance imaging)/MEG (magnetoencephalography) images, eye tracking files, and behavioral records from healthy adult human participants when they performed a spatial-memory paradigm in a virtual environment was collected to investigate the neural representation of the cognitive map defined by unique spatial relationship of three objects, as well as the neural dynamics of the cognitive map following the task demand from localizing self-location to remembering the target location relative to the self-body. The dataset, including both fMRI and MEG, was also used to investigate the neural networks involved in representing a target within and outside the visual field. The dataset included 19 and 12 university students at Peking University for fMRI and MEG experiments, respectively (fMRI: 12 women, 7 men; MEG: 4 women, 8 men). The average ages of those participants were 24.9 years (MRI: 18–30 years) and 22.5 years (MEG: 19–25 years), respectively. fMRI BOLD and T1-weighted images were acquired using a 3T Siemens Prisma scanner (Siemens, Erlangen, Germany) equipped with a 20-channel receiver head coil. MEG neuromagnetic data were acquired using a 275-channel MEG system (CTF MEG, Canada). The dataset could be further used to investigate a range of neural mechanisms involved in human spatial cognition or to develop a bioinspired deep neural network to enhance machines' abilities in spatial processing. © 2022 The Author(s)",Cognitive map; Egocentric space; Episodic memory; fMRI; Medial temporal lobe; MEG; Navigation; Parietal lobe,,Data paper,Final,All Open Access; Gold Open Access; Green Open Access,Scopus,2-s2.0-85132834346,Gaming / VR
Chen C.-C.; Tsai M.-C.; Wu E.H.-K.; Chung C.-R.; Lee Y.; Chiu P.-R.; Tsai P.-Y.; Sheng S.-R.; Yeh S.-C.,"Chen, Chun-Chuan (8942129700); Tsai, Meng-Chang (34875635300); Wu, Eric Hsiao-Kuang (55366211300); Chung, Chia-Ru (57211181873); Lee, Yuchi (57476634200); Chiu, Po-Ru (57476634300); Tsai, Po-Yi (7202064710); Sheng, Shao-Rong (57476374300); Yeh, Shih-Ching (14619911100)",8942129700; 34875635300; 55366211300; 57211181873; 57476634200; 57476634300; 7202064710; 57476374300; 14619911100,Neuronal Abnormalities Induced by an Intelligent Virtual Reality System for Methamphetamine Use Disorder,2022,IEEE Journal of Biomedical and Health Informatics,26,7,,3458,3465,7.0,10,10.1109/JBHI.2022.3154759,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85125755465&doi=10.1109%2fJBHI.2022.3154759&partnerID=40&md5=93c0c169514536c669315291cbe6fbfd,"Methamphetamine use disorder (MUD) is a brain disease that leads to altered regional neuronal activity. Virtual reality (VR) is used to induce the drug cue reactivity. Previous studies reported significant frequency-specific neuronal abnormalities in patients with MUD during VR induction of drug craving. However, whether those patients exhibit neuronal abnormalities after VR induction that could serve as the treatment target remains unclear. Here, we used an integrated VR system for inducing drug related changes and investigated the neuronal abnormalities after VR exposure in patients. Fifteen patients with MUD and ten healthy subjects were recruited and exposed to drug-related VR environments. Resting-state EEG were recorded for 5 minutes twice-before and after VR and transformed to obtain the frequency-specific data. Three self-reported scales for measurement of the anxiety levels and impulsivity of participants were obtained after VR task. Statistical tests and machine learning methods were employed to reveal the differences between patients and healthy subjects. The result showed that patients with MUD and healthy subjects significantly differed in Θ, α, and γ power changes after VR. These neuronal abnormalities in patients were associated with the self-reported behavioral scales, indicating impaired impulse control. Our findings of resting-state EEG abnormalities in patients with MUD after VR exposure have the translational value and can be used to develop the treatment strategies for methamphetamine use disorder. © 2013 IEEE.",EEG; methamp-hetamine use disorder (MUD); neuronal abnormalities; resting-state; virtual reality (VR),Craving; Cues; Humans; Methamphetamine; User-Computer Interface; Virtual Reality; Brain; Electrophysiology; Learning systems; Multiuser detection; Neurons; Virtual reality; amphetamine; cannabis; diamorphine; ketamine; methamphetamine; midomafetamine; Atmospheric measurement; Drug; Healthy subjects; Index term ---neuronal abnormality; Index terms; Methamphetamine use disorder; Particle measurement; Resting state; Virtual reality; Virtual reality system; adult; aged; anxiety; Article; auditory stimulation; behavior assessment; clinical article; cognition; controlled study; drinking; drug dependence; EEG abnormality; electrocardiography; electrodermal response; eye movement; female; human; impulsiveness; life expectancy; machine learning; male; mental disease; nervousness; physical disease; provocation; questionnaire; self report; smoking; State Trait Anxiety Inventory; association; computer interface; craving; physiology; virtual reality; Electroencephalography,Article,Final,,Scopus,2-s2.0-85125755465,Gaming / VR
Kassem L.; MacMahon C.; Quinn J.; Dogramaci S.; Pang B.; Steel K.A.,"Kassem, Lael (57824093100); MacMahon, Clare (6602849021); Quinn, John (56154475100); Dogramaci, Sera (37080427600); Pang, Bonnie (35790561600); Steel, Kylie A. (16040418900)",57824093100; 6602849021; 56154475100; 37080427600; 35790561600; 16040418900,Examining the Eye Movement Behaviors Associated With Skilled Decision-Making by Elite Australian Rules Football Players,2022,Frontiers in Sports and Active Living,4,,899217,,,,0,10.3389/fspor.2022.899217,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85135197466&doi=10.3389%2ffspor.2022.899217&partnerID=40&md5=ba77e7b3c7969ab989dd20ce75518d21,"Exploration of eye-movement behaviors of humans can provide insight into the processes used to inform and make decisions, with a large body of research revealing general trends, especially in the sporting context. Despite this some questions remain within the sport context particularly for elite groups engaged in diverse sports, and the potential for this information to provide for training, development, and performance. Therefore, the purpose of this study was to examine the critical fixation points and durations associated with superior decision-making within an elite group of Australian Rules football players. To achieve this eye-movement behavior (fixations) and associated decision-making skills of (N = 27; Mage = 25.0 ± 3.7 yrs) elite Australian Rules (AR) football players were measured while they watched game-based video clips. The most skilled players made significantly faster decisions compared to less skilled players (p < 0.001), who also had significantly shorter total fixation duration (p < 0.0001). Further, analysis showed that the most skilled players spent more time fixating on potential options within an area of interest (p = 0.003). Thus, within a group of highly skilled group of athletes, distinctions can be made on perceptual-cognitive skills, for outcome decisions and decision processes. That is, skilled decision-makers appear to have more efficient visual search strategies, which may help them process visual information more effectively. Further, examination of these behaviors may aid sport science and coaching staff identify the process that can be refined to increase player ability between and within various teams. Copyright © 2022 Kassem, MacMahon, Quinn, Dogramaci, Pang and Steel.",decision making; eye-movement; football; sports-cognition; sports-expertise,,Article,Final,All Open Access; Gold Open Access; Green Open Access,Scopus,2-s2.0-85135197466,Gaming / VR
Kasapakis V.; Dzardanova E.; Nikolakopoulou V.; Vosinakis S.; Xenakis I.; Gavalas D.,"Kasapakis, Vlasios (55854154600); Dzardanova, Elena (57195961951); Nikolakopoulou, Vasiliki (57191412563); Vosinakis, Spyros (6506291372); Xenakis, Ioannis (54685469900); Gavalas, Damianos (12808118500)",55854154600; 57195961951; 57191412563; 6506291372; 54685469900; 12808118500,Exploring non-verbal cues and user attention in IVR with eye tracking technologies,2022,"MMVE 2022 - Proceedings of the 2022 International Workshop on Immersive Mixed and Virtual Environment Systems, Part of MMSys 2022",,,,47,50,3.0,3,10.1145/3534086.3534337,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85135411499&doi=10.1145%2f3534086.3534337&partnerID=40&md5=b07017b3e49ec2cd9db6db76b1115f86,"This study is a between-groups preliminary evaluation of how Non-Verbal Cues (NVCs) impact participant attention and degree of social presence with an NPC. Participants are divided in two groups and witness an agent's monologue who features high-fidelity NVCs (motion-captured gaze, blinking, and lower facial expressions) in one group, but who has those ""turned off""for the other. This study aims at establishing appropriate data collection methodology and scenario-related guidelines for follow-up experimentation with a higher volume of interaction variables, particularly real-time interaction between remotely located users. Initial results indicate that real-time tracked NVCs enhance engagement and social presence to variant degrees compared to low-fidelity automated cues.  © 2022 ACM.",Attention; eye-tracking; non-verbal cues; virtual reality,Virtual reality; Attention; Data collection; Eye tracking technologies; Eye-tracking; Facial Expressions; Follow up; High-fidelity; Non-verbal cue; Social presence; User attention; Eye tracking,Conference paper,Final,,Scopus,2-s2.0-85135411499,Gaming / VR
Drai-Zerbib V.; Bernigaud L.; Gaston-Bellegarde A.; Boucheix J.-M.; Baccino T.,"Drai-Zerbib, Véronique (8983377900); Bernigaud, Léa (57890080400); Gaston-Bellegarde, Alexandre (57201195891); Boucheix, Jean-Michel (6506236615); Baccino, Thierry (6506451570)",8983377900; 57890080400; 57201195891; 6506236615; 6506451570,Eye Movements During Comprehension in Virtual Reality: The Influence of a Change in Point of View Between Auditory and Visual Information in the Activation of a Mental Model,2022,Frontiers in Virtual Reality,3,,874054,,,,3,10.3389/frvir.2022.874054,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85138019975&doi=10.3389%2ffrvir.2022.874054&partnerID=40&md5=356c91b29e07e629991aa05d97ebdff4,"This paper provides new research perspectives in the field of multimodal comprehension (auditory crossing visual information) by using immersion and incorporating eye tracking in a virtual reality environment. The objective is to investigate the influence of a change in narrative perspective (point of view) during the activation of a mental model underlying comprehension between visual and auditory modalities. Twenty-eight participants, equipped with a headset SMI HMD HTC eye-tracking 250 Hz watched 16 visual scenes in virtual reality accompanied by their corresponding auditory narration. The change in perspective may occur either in the visual scenes or in listening. Mean fixations durations on typical objects of the visual scenes (Area of Interest) that were related to the perspective shift were analyzed as well as the free recall of narratives. We split each scene into three periods according to different parts of the narration (Before, Target, After), the target was where a shift in perspective could occur. Results shown that when a visual change of perspective occurred, mean fixation duration was shorter (compared to no change) for both Target and After. However, when auditory change of perspective occurred, no difference was found on Target, although during After, mean fixation duration was longer (compared to no change). In the context of 3D video visualization, it seems that auditory processing prevails over visual processing of verbal information: The visual change of perspective induces less visual processing of the Area of Interest (AOIs) included in the visual scene, but the auditory change in perspective leads to increased visual processing of the visual scene. Moreover, the analysis showed higher recall of information (verbatim and paraphrase) when an auditory change in perspective was coupled with no visual change of perspective. Thus, our results indicate a more effective integration of information when there is an inconsistency between the narration heard and viewed. A change in perspective, instead of creating comprehension and integration difficulties, seems to effectively raise the attention and induce a shorter visual inspection. These results are discussed in the context of cross-modal comprehension. Copyright © 2022 Drai-Zerbib, Bernigaud, Gaston-Bellegarde, Boucheix and Baccino.",comprehension; eye tracking; multimodality; shift in narrative perspective; virtual reality,,Article,Final,All Open Access; Gold Open Access,Scopus,2-s2.0-85138019975,Gaming / VR
Choi H.; Nam S.,"Choi, Haram (57987991100); Nam, Sanghun (55421451100)",57987991100; 55421451100,A Study on Attention Attracting Elements of 360-Degree Videos Based on VR Eye-Tracking System,2022,Multimodal Technologies and Interaction,6,7,54,,,,5,10.3390/mti6070054,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85136093780&doi=10.3390%2fmti6070054&partnerID=40&md5=fc1f0115a145b6d6f37b100804e81d0c,"In 360-degree virtual reality (VR) videos, users possess increased freedom in terms of gaze movement. As a result, the users’ attention may not move according to the narrative intended by the director and miss out on important parts of the narrative of the 360-degree video. Therefore, it is necessary to study a directing technique that can attract user attention in 360-degree VR videos. In this study, we analyzed the directing elements that can attract users’ attention in a 360-degree VR video and developed a 360 VR eye-tracking system to investigate the effect of the attention-attracting elements on the user. Elements that can attract user attention were classified into five categories: object movement, hand gesture, GUI insertion, camera movement, and gaze angle variation. We developed a 360 VR eye-tracking system to analyze whether five attention-attracting elements influence the user’s attention. Based on the eye tracking system, an experiment was conducted to analyze whether the user’s attention moves according to the five attention-attracting elements. Based on the experimental results, it can be seen that ‘hand gesture’ attracted the second most attention shift of the subjects, and ‘GUI insertion’ induced the smallest shift of attention of the subjects. © 2022 by the authors.",360-degree VR video; attention-attracting element; directing technique; eye tracking; virtual reality,,Article,Final,All Open Access; Gold Open Access,Scopus,2-s2.0-85136093780,Gaming / VR
Gunawardena N.; Ginige A.; Javadi B.; Lui G.,"Gunawardena, Nishan (57210115715); Ginige, Anupama (57215085995); Javadi, Bahman (6507235083); Lui, Gough (48361538400)",57210115715; 57215085995; 6507235083; 48361538400,Mobile Device Eye Tracking on Dynamic Visual Contents using Edge Computing and Deep Learning,2022,Eye Tracking Research and Applications Symposium (ETRA),,,38,,,,1,10.1145/3517031.3532198,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85132440364&doi=10.1145%2f3517031.3532198&partnerID=40&md5=0aa42d3e9cac619f4fe18762001f5f04,"Eye-Tracking has been used in various domains, including human-computer interaction, psychology, and many others. Compared to commercial eye trackers, eye tracking using off-The-shelf cameras has many advantages, such as lower cost, pervasiveness, and mobility. Quantifying human attention on the mobile device is invaluable in human-computer interaction. Like videos and mobile games, dynamic visual stimuli require higher attention than static visual stimuli such as web pages and images. This research aims to develop an accurate eye-Tracking algorithm using the front-facing camera of mobile devices to identify human attention hotspots when viewing video type contents. The shortage of computational power in mobile devices becomes a challenge to obtain higher user satisfaction. Edge computing moves the processing power closer to the source of the data and reduces the latency introduced by the cloud computing. Therefore, the proposed algorithm will be extended with mobile edge computing to provide a real-Time eye tracking experience for users © 2022 ACM.",deep learning; edge computing; eye tracking; mobile human computer interaction,Cameras; Deep learning; Edge computing; Human computer interaction; Websites; Deep learning; Edge computing; Eye trackers; Eye-tracking; Human attention; Low-costs; Mobile human computer interaction; ON dynamics; Video-games; Visual content; Eye tracking,Conference paper,Final,,Scopus,2-s2.0-85132440364,Gaming / VR
Illahi G.K.; Siekkinen M.; Kämäräinen T.; Ylä-Jääski A.,"Illahi, Gazi Karam (57201131314); Siekkinen, Matti (23393910300); Kämäräinen, Teemu (56681068800); Ylä-Jääski, Antti (23391650600)",57201131314; 23393910300; 56681068800; 23391650600,Real-time gaze prediction in virtual reality,2022,"MMVE 2022 - Proceedings of the 2022 International Workshop on Immersive Mixed and Virtual Environment Systems, Part of MMSys 2022",,,,12,18,6.0,10,10.1145/3534086.3534331,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85135404429&doi=10.1145%2f3534086.3534331&partnerID=40&md5=2ed736c5721437ae195217b836b1abbf,"Gaze is an important indicator of visual attention and knowledge of gaze location can be used to improve and augment Virtual Reality (VR) experiences. This has led to the development of VR Head Mounted Displays (HMD) with inbuilt gaze trackers. Given the latency constraints of VR, foreknowledge of gaze, i.e., before it is reported by the gaze tracker, can similarly be leveraged to preemptively apply gaze-based improvements and augmentations to a VR experience, especially in distributed VR architectures. In this paper, we propose a light weight neural network based method utilizing only past HMD pose and gaze data to predict future gaze locations, forgoing computationally heavy saliency computation. Most work in this domain has focused on either 360°or ego-centric video or synthetic VR content with rather naive interaction dynamics like free viewing or supervised visual search tasks. Our solution considers data from the exhaustive OpenNEEDs dataset which contains 6 Degrees of Freedom (6DoF) data captured in VR experiences with subjects given the freedom to explore the VR scene and/or to engage in tasks. Our solution outperforms the very strict baseline: current gaze to predict gaze in real-time for sub 150ms prediction horizons for VR use-cases.  © 2022 ACM.",Gaze prediction; neural networks; virtual reality,Behavioral research; Degrees of freedom (mechanics); Eye tracking; Helmet mounted displays; User interfaces; Virtual reality; Distributed virtual reality; Gaze prediction; Gaze tracker; Head-mounted-displays; Latency constraints; Neural-networks; Real- time; Virtual reality experiences; Visual Attention; Visual knowledge; Forecasting,Conference paper,Final,All Open Access; Bronze Open Access,Scopus,2-s2.0-85135404429,Gaming / VR
Fang Y.; Huang L.; Yan J.; Liu X.; Liu Y.,"Fang, Yuming (8435698900); Huang, Liping (57218835076); Yan, Jiebin (57192588824); Liu, Xuelin (57205096648); Liu, Yang (57736069400)",8435698900; 57218835076; 57192588824; 57205096648; 57736069400,Perceptual Quality Assessment of Omnidirectional Images,2022,"Proceedings of the 36th AAAI Conference on Artificial Intelligence, AAAI 2022",36,,,580,588,8.0,31,10.1609/aaai.v36i1.19937,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85138132953&doi=10.1609%2faaai.v36i1.19937&partnerID=40&md5=1e58143bed6b269516006c191a17bb01,"Omnidirectional images, also called 360◦ images, have attracted extensive attention in recent years, due to the rapid development of virtual reality (VR) technologies. During omnidirectional image processing including capture, transmission, consumption, and so on, measuring the perceptual quality of omnidirectional images is highly desired, since it plays a great role in guaranteeing the immersive quality of experience (IQoE). In this paper, we conduct a comprehensive study on the perceptual quality of omnidirectional images from both subjective and objective perspectives. Specifically, we construct the largest so far subjective omnidirectional image quality database, where we consider several key influential elements, i.e., realistic non-uniform distortion, viewing condition, and viewing behavior, from the user view. In addition to subjective quality scores, we also record head and eye movement data. Besides, we make the first attempt by using the proposed database to train a convolutional neural network (CNN) for blind omnidirectional image quality assessment. To be consistent with the human viewing behavior in the VR device, we extract viewports from each omnidirectional image and incorporate the user viewing conditions naturally in the proposed model. The proposed model is composed of two parts, including a multi-scale CNN-based feature extraction module and a perceptual quality prediction module. The feature extraction module is used to incorporate the multi-scale features, and the perceptual quality prediction module is designed to regress them to perceived quality scores. The experimental results on our database verify that the proposed model achieves the competing performance compared with the state-of-the-art methods. Copyright © 2022, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.",,Behavioral research; Database systems; Extraction; Eye movements; Image quality; Quality control; Convolutional neural network; Features extraction; Images processing; Immersive; Omnidirectional image; Perceptual quality; Quality assessment; Quality prediction; Viewing conditions; Virtual reality technology; Feature extraction,Conference paper,Final,All Open Access; Gold Open Access; Green Open Access,Scopus,2-s2.0-85138132953,Gaming / VR
Das S.; Pratyush P.; Das D.; Maiti J.; Krishna O.B.,"Das, Souvik (57211601973); Pratyush, Parimal (58190000300); Das, Debjoy (58190013800); Maiti, J. (6603146128); Krishna, O.B. (56196204300)",57211601973; 58190000300; 58190013800; 6603146128; 56196204300,"Eye-tracking data as a way to detect sleep deprivation in an individual, based on attention, mental agility, and problem-solving",2022,Machine Learning Algorithms for Engineering Applications: Future Trends and Research Directions,,,,195,208,13.0,0,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85152978298&partnerID=40&md5=d89bc088c79840b763e2383c77c49613,"In the current study, an experiment is designed to investigate the relationship between the sleep-deprived state of a person and his eye movements metrics. The experiment needs the participant to be in a state of sleep deprivation for a period of 12hrs. to 24hrs. The participant plays a series of games on 'Lumosity' at the end of their sleep deprivation period. An eye-tracking device is used to capture the eye movements of the participants during the experiment. The eye movement metrics used in this experiment are pupil dilation, microsaccades (magnitude and speed) and fixation, and a model is developed to relate these metrics with sleep deprivation. The proposed model helps in predicting the extent of sleep deprivation (in hrs) a participant can handle using eye movement metrics. This study can be extended to as an application in industrial automation, industry 4.0, and many industrial complex operations where cognitive effort prevails over physical effort. © 2022 Nova Science Publishers, Inc.",Eye tracking; Hidden Markov model; Logistic regression; Mental fatigue; Sleep deprivation,,Book chapter,Final,,Scopus,2-s2.0-85152978298,Gaming / VR
Bovo R.; Giunchi D.; Sidenmark L.; Gellersen H.; Costanza E.; Heinis T.,"Bovo, Riccardo (57204780702); Giunchi, Daniele (57205711925); Sidenmark, Ludwig (57210111157); Gellersen, Hans (6701531333); Costanza, Enrico (22834126300); Heinis, Thomas (36874211600)",57204780702; 57205711925; 57210111157; 6701531333; 22834126300; 36874211600,Real-time head-based deep-learning model for gaze probability regions in collaborative VR,2022,Eye Tracking Research and Applications Symposium (ETRA),,,6,,,,5,10.1145/3517031.3529642,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85132424105&doi=10.1145%2f3517031.3529642&partnerID=40&md5=f16dd3927a0d0f3953e4b0173f6295fc,"Eye behavior has gained much interest in the VR research community as an interactive input and support for collaboration. Researchers used head behavior and saliency to implement gaze inference models when eye-tracking is missing. However, these solutions are resource-demanding and thus unfit for untethered devices, and their angle accuracy is around 7°, which can be a problem in high-density informative areas. To address this issue, we propose a lightweight deep learning model that generates the probability density function of the gaze as a percentile contour. This solution allows us to introduce a visual attention representation based on a region rather than a point. In this way, we manage the trade-off between the ambiguity of a region and the error of a point. We tested our model in untethered devices with real-time performances; we evaluated its accuracy, outperforming our identified baselines (average fixation map and head direction). © 2022 ACM.",gaze inference; gaze prediction; neural networks; visual attention,Behavioral research; Deep learning; Economic and social effects; Probability density function; Gaze inference; Gaze prediction; Inference models; Interactive input; Interactive supports; Learning models; Neural-networks; Real- time; Research communities; Visual Attention; Eye tracking,Conference paper,Final,All Open Access; Green Open Access,Scopus,2-s2.0-85132424105,Gaming / VR
Arjun S.; Hebbar A.; Sanjana; Biswas P.,"Arjun, Somnath (57193513088); Hebbar, Archana (56102250400); Sanjana (57221123778); Biswas, Pradipta (14007579800)",57193513088; 56102250400; 57221123778; 14007579800,VR Cognitive Load Dashboard for Flight Simulator,2022,Eye Tracking Research and Applications Symposium (ETRA),,,4,,,,11,10.1145/3517031.3529777,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85132427127&doi=10.1145%2f3517031.3529777&partnerID=40&md5=52441461f419d0bff0b7f6daad7d3b6a,"Estimating the cognitive load of aircraft pilots is essential to monitor them constantly to identify and overcome unfavorable situations. Presently, the cognitive load of pilots is estimated using manual filling up of forms, and there is a lack of a system that can estimate workload automatically. In this paper, we used eye-Tracking technology for cognitive load estimation and developed a Virtual Reality dashboard that visualizes cognitive and ocular data. We undertook a flight simulation study to observe users' workload during primary and secondary task execution while flying the aircraft. We also undertook an eye-Tracking study to identify appropriate 3D graph properties for developing graphs of the cognitive dashboard. We found a significant interaction between users' primary and secondary tasks. We also observed that primitives like size and color were easier to encode numerical and nominal information. Finally, we developed a dashboard leveraging the results of the 3D graph study for estimating the pilot's cognitive load. The VR dashboard enabled visualization of the cognitive load parameters derived from the ocular data in real time. The aim of the 3D graph study was to identify the optimal information to be displayed to the participants/pilots. Apart from estimating cognitive load using ocular data, the dashboard can also visualize ocular data collected in a Virtual Reality environment. © 2022 ACM.",Cognitive Load; Flight Simulator; Virtual Reality; Visualization,Aircraft; Data visualization; Eye tracking; Virtual reality; Visualization; 3d graphs; Aircraft pilots; Cognitive loads; Eye tracking technologies; Flight simulation; Load estimation; Primary task; Secondary tasks; Simulation studies; Task executions; Flight simulators,Conference paper,Final,,Scopus,2-s2.0-85132427127,Gaming / VR
Hsieh Y.-L.; Lee M.-F.; Chen G.-S.; Wang W.-J.,"Hsieh, Yu-Ling (57745571200); Lee, Ming-Feng (57745571300); Chen, Guey-Shya (56169276600); Wang, Wei-Jie (57732784600)",57745571200; 57745571300; 56169276600; 57732784600,Application of Visitor Eye Movement Information to Museum Exhibit Analysis,2022,Sustainability (Switzerland),14,11,6932,,,,6,10.3390/su14116932,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85132126419&doi=10.3390%2fsu14116932&partnerID=40&md5=3928c5ac95281c88d1e123f29738819d,"The motivation of this study is that after the COVID-19 epidemic, museum exhibition visits have also been significantly affected. The purpose of this research is to better understand the visual cognition of visitors, so as to improve the application of physical field or online exhibitions. Currently, no research is available on the differences in the visitor’s viewing and cognitive process with eye movements sequence analysis that stem from the exhibition planning and design of different museums. This study tracks and analyzes the eye movement trajectories of visitors and studies its relation to learning and cognition and finds the key to influencing cognition through behavioral sequence analysis of displayed content. The results show that those interested in the displayed content have better cognitive performance, are immersed in reading text, and have a substantial shift in eye movement. Contrarily, those not interested in the displayed content are distracted and often turn their attention back to the title of the content. In this study, eye movement and fixation are indicators that can be used as a reference for the future design of displays to improve the effectiveness of presenting information to a visitor. Furthermore, this research can also provide future applications in integrating the virtual world and cognitive information, in the application of AR, VR, or metaverse environment, to provide people’s cognition of rapid information in the virtual environment. © 2022 by the authors. Licensee MDPI, Basel, Switzerland.",eye movement; eye tracking; fixation; sequence analysis,cognition; COVID-19; design method; information; learning; museum,Article,Final,All Open Access; Gold Open Access,Scopus,2-s2.0-85132126419,Gaming / VR
Ehmann P.; Beavan A.; Spielmann J.; Mayer J.; Ruf L.; Altmann S.; Forcher L.; Klever N.; Rohrmann S.; Nuß C.; Englert C.,"Ehmann, Paul (57208865534); Beavan, Adam (57202055135); Spielmann, Jan (57203717820); Mayer, Jan (57207403185); Ruf, Ludwig (57203132959); Altmann, Stefan (56754814700); Forcher, Leon (57255514300); Klever, Niklas (57732697600); Rohrmann, Sonja (6603807799); Nuß, Christian (55747259200); Englert, Chris (57189178989)",57208865534; 57202055135; 57203717820; 57207403185; 57203132959; 56754814700; 57255514300; 57732697600; 6603807799; 55747259200; 57189178989,Perceptual-cognitive performance of youth soccer players in a 360°-environment – An investigation of the relationship with soccer-specific performance and the effects of systematic training,2022,Psychology of Sport and Exercise,61,,102220,,,,6,10.1016/j.psychsport.2022.102220,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85131546140&doi=10.1016%2fj.psychsport.2022.102220&partnerID=40&md5=76eb12cb772319ff3bdd6466b1c303f0,"Introduction: Soccer is a complex game in which athletes perform in a dynamic 360°-environment. The results of numerous studies highlight the importance of perceptual-cognitive functions for soccer performance. Moreover, in recent years, the idea of improving sports performance through systematic perceptual-cognitive training has been increasingly investigated. Contradictory results and limitations in previous research call for further investigation. The current study aims to investigate both the relationship between perceptual-cognitive performance in a dynamic 360°-environment and soccer performance as well as the effects of perceptual-cognitive training in such an environment on soccer performance. Methods: 42 youth soccer players aged 11–13 years were tested at a first time of measurement (T1) on their perceptual-cognitive functions using a 360°-multiple object tracking task (360-MOT) and a visuospatial attention task. Soccer performance was assessed using an isolated, validated 360°-passing task and a small-sided game. Subsequently, participants were randomly divided into a perceptual-cognitive training group, an active control group, or a passive control group. Participants in the training group received 360-MOT training twice per week during a 5-week intervention phase, while participants in the active control group received a pseudo video training. Perceptual-cognitive and soccer-specific performance was assessed after the intervention phase at a second time of measurement (T2). Results: At T1, there was a significant positive relationship between 360-MOT performance and both the accuracy score in the 360°-passing task and the defensive performance score in the small-sided game. Regarding the perceptual-cognitive training intervention, the analysis at T2 revealed significant task-specific training effects but no transfer effects on perceptual-cognitive or soccer-specific performance. Conclusions: The results highlight the relevance of perceptual-cognitive performance in a 360°-environment for soccer-specific performance but question the effects of short isolated perceptual-cognitive training interventions on soccer-specific performance. © 2022 Elsevier Ltd",Athletes; Cognition; Object tracking; Perceptual-cognitive training; Soccer,article; athlete; athletic performance; attention; child; cognition; controlled study; eye tracking; female; human; human experiment; juvenile; male; randomized controlled trial; school child; soccer; soccer player; task performance; videorecording,Article,Final,,Scopus,2-s2.0-85131546140,Gaming / VR
,,,Proceedings - ETRA 2022: ACM Symposium on Eye Tracking Research and Applications,2022,Eye Tracking Research and Applications Symposium (ETRA),,,,,,411.0,0,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85132379095&partnerID=40&md5=b6e44cf9293aa8d46fbbbe4f0ac76446,"The proceedings contain 70 papers. The topics discussed include: faster, better blink detection through curriculum learning by augmentation; consider the head movements! saccade computation in mobile eye-tracking; estimating perceptual depth changes with eye vergence and interpupillary distance using an eye tracker in virtual reality; VR cognitive load dashboard for flight simulator; an assessment of the eye tracking signal quality captured in the HoloLens 2; regressive saccadic eye movements on fake news; measuring cognitive effort with pupillary activity and fixational eye movements when reading: longitudinal comparison of children with and without primary music education; for your eyes only: privacy-preserving eye-tracking datasets; eye movements in extended tasks: analyses of ambient/focal attention with coefficient K; and gaze estimation with imperceptible marker displayed dynamically using polarization.",,,Conference review,Final,,Scopus,2-s2.0-85132379095,Gaming / VR
Wender C.L.A.; Tomporowski P.D.; Ahn S.J.G.; O'Connor P.J.,"Wender, Carly L.A. (57191091164); Tomporowski, Phillip D. (57189255992); Ahn, Sun Joo (Grace) (55602294100); O'Connor, Patrick J. (35240603700)",57191091164; 57189255992; 55602294100; 35240603700,"Virtual reality-based distraction on pain, performance, and anxiety during and after moderate-vigorous intensity cycling",2022,Physiology and Behavior,250,,113779,,,,7,10.1016/j.physbeh.2022.113779,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85126703745&doi=10.1016%2fj.physbeh.2022.113779&partnerID=40&md5=b10dfeee573d1a3201482d9c73c3a197,"Purpose: To determine whether increased visual perceptual load (PL) within an immersive virtual environment may help explain previously shown pain-relieving effects of virtual reality (VR) during high intensity cycling. Methods: Using a within-subjects design, participants cycled at a perceptually “hard” intensity for 10 min on three separate occasions. The first session did not use VR (i.e., no perceptual load - NPL). Subsequent sessions employed VR during cycling with either a low or high perceptual load (LPL or HPL). Quadriceps pain intensity (PI) was reported by participants throughout cycling. Results: Data were analyzed for 43 healthy participants (20 females, mean age 21 [SD 1.4]). For PI, ANOVA showed there were significant main effects of condition (F = 13.458, df =1.579, 66.334, p<0.001) and time (F = 113.045, df =1.618, 227.683, p<0.001). At every time point, t-tests revealed mean PI was significantly lower in the NPL than in the LPL condition (t(42)=4.737, p<0.001, d = 0.472) and HPL condition (t(42)=3.380, p = 0.002, d = 0.391). Dependent t-tests showed that more work (kilojoules) was performed during the LPL condition than the NPL (t(42)=2.992, p = 0.005) and HPL (t(42)=5.810, p<0.001) conditions. Conclusions: Compared to a traditional 10-minute bout of cycle ergometry (NPL), individuals who cycled in the LPL condition chose to exercise at a higher intensity despite greater PI. Those who cycled in the HPL condition did not change their exercise intensity, but did report higher PI, possibly, because of the greater mental effort/energy requirement. © 2022","Anxiety; Cycling, Pain intensity; Perceptual load, Mental effort; Virtual reality",Adult; Anxiety; Anxiety Disorders; Bicycling; Female; Humans; Male; Pain; Virtual Reality; Young Adult; accuracy; adult; analgesia; anxiety; Article; bicycle ergometry; body mass; chronic pain; cognitive flexibility; cycling; decision making; dizziness; ergometry; exercise intensity; eye tracking; fatigue; female; human; human experiment; Likert scale; male; mental capacity; mental effort; mental fatigue; myalgia; nausea; normal human; pain; pain intensity; peak oxygen uptake; perceptual load; performance; physical activity; Physical Activity Readiness Questionnaire; physical performance; pilot study; quadriceps femoris muscle; questionnaire; State Trait Anxiety Inventory; virtual reality; warm up; anxiety; anxiety disorder; cycling; young adult,Article,Final,,Scopus,2-s2.0-85126703745,Gaming / VR
Walter J.L.; Essmann L.; König S.U.; König P.,"Walter, Jasmin L. (57326651500); Essmann, Lucas (57326467200); König, Sabine U. (56274699800); König, Peter (7102563952)",57326651500; 57326467200; 56274699800; 7102563952,Finding landmarks–An investigation of viewing behavior during spatial navigation in VR using a graph-theoretical analysis approach,2022,PLoS Computational Biology,18,6,e1009485,,,,11,10.1371/journal.pcbi.1009485,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85131810869&doi=10.1371%2fjournal.pcbi.1009485&partnerID=40&md5=e9e47a556ddf469a272059886e54d3dd,"Vision provides the most important sensory information for spatial navigation. Recent technical advances allow new options to conduct more naturalistic experiments in virtual reality (VR) while additionally gathering data of the viewing behavior with eye tracking investigations. Here, we propose a method that allows one to quantify characteristics of visual behavior by using graph-theoretical measures to abstract eye tracking data recorded in a 3D virtual urban environment. The analysis is based on eye tracking data of 20 participants, who freely explored the virtual city Seahaven for 90 minutes with an immersive VR headset with an inbuild eye tracker. To extract what participants looked at, we defined “gaze” events, from which we created gaze graphs. On these, we applied graph-theoretical measures to reveal the underlying structure of visual attention. Applying graph partitioning, we found that our virtual environment could be treated as one coherent city. To investigate the importance of houses in the city, we applied the node degree centrality measure. Our results revealed that 10 houses had a node degree that exceeded consistently two-sigma distance from the mean node degree of all other houses. The importance of these houses was supported by the hierarchy index, which showed a clear hierarchical structure of the gaze graphs. As these high node degree houses fulfilled several characteristics of landmarks, we named them “gaze-graph-defined landmarks”. Applying the rich club coefficient, we found that these gaze-graph-defined landmarks were preferentially connected to each other and that participants spend the majority of their experiment time in areas where at least two of those houses were visible. Our findings do not only provide new experimental evidence for the development of spatial knowledge, but also establish a new methodology to identify and assess the function of landmarks in spatial navigation based on eye tracking data. © 2022 Walter et al. This is an open access article distributed under the terms of the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.",,"Humans; Knowledge; Spatial Navigation; Virtual Reality; Vision, Ocular; Abstracting; Behavioral research; Eye tracking; Graph theory; Houses; Navigation; Analysis approach; Eye-tracking; Graph theoretical analysis; Graph theoretical measures; New options; Node degree; Sensory information; Spatial navigation; Technical advances; Tracking data; adult; algorithm; Article; degree of centrality; eye tracking; female; gaze; human; human experiment; male; quantitative analysis; spatial orientation; theoretical study; urban area; virtual reality; visual attention; knowledge; vision; Virtual reality",Article,Final,All Open Access; Gold Open Access; Green Open Access,Scopus,2-s2.0-85131810869,Gaming / VR
Vu A.; Sorel A.; Limballe A.; Bideau B.; Kulpa R.,"Vu, Alexandre (57646346500); Sorel, Anthony (35732607700); Limballe, Annabelle (57210916853); Bideau, Benoit (6507606435); Kulpa, Richard (6507692858)",57646346500; 35732607700; 57210916853; 6507606435; 6507692858,Multiple Players Tracking in Virtual Reality: Influence of Soccer Specific Trajectories and Relationship With Gaze Activity,2022,Frontiers in Psychology,13,,901438,,,,12,10.3389/fpsyg.2022.901438,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85131716286&doi=10.3389%2ffpsyg.2022.901438&partnerID=40&md5=91f1bfbd609b3b051a56f3d604301b91,"The perceptual-cognitive ability to track multiple moving objects and its contribution to team sports performance has traditionally been studied in the laboratory under non-sports specific conditions. It is thus questionable whether the measured visual tracking performance and the underlying gaze activity reflected the actual ability of team sports players to track teammates and opponents on a real field. Using a Virtual Reality-based visual tracking task, the ability of participants to track multiple moving virtual players as they would do on a soccer field was observed to pursue two objectives. (i) See the influence of different scenario types (soccer-specific trajectories versus pseudo-random trajectories) on the visual tracking performance of soccer (n = 15) compared to non-soccer players (n = 16). (ii) Observe the influence of spatial features of the simulated situations on gaze activity between soccer players and non-soccer players. (i) The linear mixed model regression revealed a significant main effect of the group but no interaction effect between group and the type of trajectories, suggesting that the visual tracking ability of soccer players did not benefit from their specific knowledge when they faced scenarios with real game trajectories. (ii) Virtual players’ spatial dispersion and crowding affected the participants’ gaze activity and their visual tracking performance. Furthermore, the gaze activity of soccer players differed in some aspects from the gaze activity of non-soccer players. Assumptions are formulated as to the implication of these results in the difference in visual tracking performance between soccer players and non-soccer players. Overall, using soccer-specific trajectories might not be enough to replicate the representativeness of the field conditions in the study of visual tracking performance. Multitasking constraints should be considered along with motor-cognitive dual-tasks in future research to develop the representativeness of visual exploration conditions. Copyright © 2022 Vu, Sorel, Limballe, Bideau and Kulpa.",attention; eye-tracking; soccer (football); virtual reality; visual exploration; visual tracking task,,Article,Final,All Open Access; Gold Open Access; Green Open Access,Scopus,2-s2.0-85131716286,Gaming / VR
Li G.; McGill M.; Brewster S.; Chen C.P.; Anguera J.A.; Gazzaley A.; Pollick F.,"Li, Gang (55949077500); McGill, Mark (35213728600); Brewster, Stephen (7006514160); Chen, Chao Ping (56372110600); Anguera, Joaquin A. (23017788300); Gazzaley, Adam (6602657531); Pollick, Frank (7004147308)",55949077500; 35213728600; 7006514160; 56372110600; 23017788300; 6602657531; 7004147308,Multimodal Biosensing for Vestibular Network-Based Cybersickness Detection,2022,IEEE Journal of Biomedical and Health Informatics,26,6,,2469,2480,11.0,24,10.1109/JBHI.2021.3134024,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85121380921&doi=10.1109%2fJBHI.2021.3134024&partnerID=40&md5=3a59bc8648f3c41f1a7a4e6919723772,"Virtual reality (VR) has the potential to induce cybersickness (CS), which impedes CS-susceptible VR users from the benefit of emerging VR applications. To better detect CS, the current study investigated whether/how the newly proposed human vestibular network (HVN) is involved in flagship consumer VR-induced CS by simultaneously recording autonomic physiological signals as well as neural signals generated in sensorimotor and cognitive domains. The VR stimuli were made up of one or two moderate CS-inducing entertaining task(s) as well as a mild CS-inducing cognitive task implemented before and after the moderate CS task(s). Results not only showed that CS impaired cognitive control ability, represented by the degree of attentional engagement, but also revealed that combined indicators from all three HVN domains could together establish the best regression relationship with CS ratings. More importantly, we found that every HVN domain had its unique advantage with the dynamic changes in CS severity and time. These results provide evidence for involvement of the HVN in CS and indicate the necessity of HVN-based CS detection. © 2013 IEEE.",cognitive control ability; cybersickness; multimodal sensing; vestibular network; Virtual reality,Humans; Virtual Reality; Electroencephalography; Electrophysiology; Frequency modulation; Job analysis; Cognitive control; Cognitive control ability; Cybersickness; Forehead; Multi-modal; Multimodal sensing; Network domains; Network-based; Task analysis; Vestibular network; adult; Article; autonomic nervous system; bioinformatics; blurred vision; clinical article; cognition; controlled study; cybersickness; disorientation; dizziness; electroencephalogram; electrooculography; eructation; eye movement control; female; forehead; frequency modulation; genetic procedures; headache; heart rate; high frequency electrotherapy; human; human experiment; infrared radiation; low frequency electrotherapy; male; middle aged; multimodal biosensing; nausea; nerve cell network; plethysmography; salivation; sensorimotor function; Simulator Sickness Questionnaire; stomach cancer; vertigo; vestibular system; virtual reality; young adult; virtual reality; Virtual reality,Article,Final,All Open Access; Green Open Access,Scopus,2-s2.0-85121380921,Gaming / VR
Eckert M.; Robotham T.; Habets E.A.P.; Rummukainen O.S.,"Eckert, Marie (57210227449); Robotham, Thomas (57190864032); Habets, Emanuël A. P. (13608885900); Rummukainen, Olli S. (55354761500)",57210227449; 57190864032; 13608885900; 55354761500,Pupillary Light Reflex Correction for Robust Pupillometry in Virtual Reality,2022,Proceedings of the ACM on Computer Graphics and Interactive Techniques,5,2,18,,,,3,10.1145/3530798,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85130872653&doi=10.1145%2f3530798&partnerID=40&md5=d508b1828bb038295ba7fdb2a46696db,"Virtual reality (VR) headsets with an integrated eye tracker enable the measurement of pupil size fluctuations correlated with cognition during a VR experience. We present a method to correct for the light-induced pupil size changes, otherwise masking the more subtle cognitively-driven effects, such as cognitive load and emotional state. We explore multiple calibration sequences to find individual mapping functions relating the luminance to pupil dilation that can be employed in real-Time during a VR experience. The resulting mapping functions are evaluated in a VR-based n-back task and in free exploration of a six-degrees-of-freedom VR scene. Our results show estimating luminance from a weighted average of the fixation area and the background yields the best performance. Calibration sequence composed of either solid gray or realistic scene brightness levels shown for 6 s in a pseudo-random order proved most robust.  © 2022 Owner/Author.",Calibration; Cognitive load; Pupillometry; Virtual reality,Degrees of freedom (mechanics); Eye tracking; Luminance; Mapping; Virtual reality; Cognitive loads; Eye trackers; Light-induced; Mapping functions; Measurements of; Pupil size; Pupillometry; Size fluctuations; Virtual reality experiences; Virtual-reality headsets; Calibration,Article,Final,,Scopus,2-s2.0-85130872653,Gaming / VR
Delvigne V.; Wannous H.; Dutoit T.; Ris L.; Vandeborre J.-P.,"Delvigne, Victor (57219434009); Wannous, Hazem (23391125300); Dutoit, Thierry (36022249200); Ris, Laurence (6602720245); Vandeborre, Jean-Philippe (6507497277)",57219434009; 23391125300; 36022249200; 6602720245; 6507497277,PhyDAA: Physiological Dataset Assessing Attention,2022,IEEE Transactions on Circuits and Systems for Video Technology,32,5,,2612,2623,11.0,30,10.1109/TCSVT.2021.3061719,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85101765216&doi=10.1109%2fTCSVT.2021.3061719&partnerID=40&md5=1a65f770929b07704fac61df78624892,"Attention Deficit Hyperactivity Disorder (ADHD) is the most prevalent neurodevelopmental disorder among children. It affects patients' lives in many ways: inattention, difficulty with stimuli inhibition or motor function regulation. Different treatments exist today, but these can present side effects or are not effective for all subgroups. Neurofeedback (NF) is an innovative treatment consisting of brain activity display. NF training could consist of a virtual reality (VR) video-game in which the participant's attention affects the game. Attention being assessed through physiological signals, one of the main steps is to design an estimator for the attention state. We present a novel framework able to record physiological signals in specific attention states and able to estimate the corresponding attention state. We propose a database composed of electroencephalography signals (EEG), and an eye-tracker labelled with a score representing the attention span for 32 healthy participants. Different features are extracted from the signals and machine learning (ML) algorithms are proposed. Our approach exhibits high accuracy for attention estimation, which corroborates a correlation between attention state and physiological signals (i.e. EEG, eye-tracking signals). The dataset has been made publicly available to promote research in the domain and we encourage other scientists to use their own approach for attention estimation.  © 1991-2012 IEEE.",attention estimation; Brain-computer interface; deep learning; virtual reality,Brain; Electroencephalography; Electrophysiology; Machine learning; Attention deficit hyperactivity disorder; Attention estimations; Brain activity; Different treatments; High-accuracy; Motor function; Neurofeedback; Physiological signals; Eye tracking,Article,Final,All Open Access; Green Open Access,Scopus,2-s2.0-85101765216,Gaming / VR
Chen B.; Gong C.; Li S.,"Chen, Bingjing (57225891546); Gong, Chen (57444741400); Li, Shuhua (35205661700)",57225891546; 57444741400; 35205661700,Looking at buildings or trees? Association of human nature relatedness with eye movements in outdoor space,2022,Journal of Environmental Psychology,80,,101756,,,,28,10.1016/j.jenvp.2022.101756,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124309870&doi=10.1016%2fj.jenvp.2022.101756&partnerID=40&md5=e0e5784d88617d6760750984b7871b6c,"This study explored how nature relatedness (NR) was linked with human eye movements in outdoor space. We created a 360-degree virtual reality scene that contained half urban setting of buildings and half nature area of trees, and recorded 112 participants' eye movements (total visit duration and total fixation duration ratio) during 1 min of free viewing. It was found that individuals’ NR score correlated with their eye movements on the area of interests (AOIs), i.e., total fixation duration ratio on Trees (r (110) = 0.341, p < .001). Further analyses of the correlations between three subscales of NR scale and eye-tracking data revealed that NR-Self was related to eye movements significantly, but not for NR-Perception and NR-Experience. The paired t-tests result showed that people with Low NR score spent more time looking at buildings than trees, while the High NR group did the opposite. For the medium NR group, no distinct eye movements difference between trees and buildings was detected. The outcomes shed light on the divergence of individual visual attention in a mix of natural and built environment. Our finding also determined the efficacy of NR scale with physical measurements of behavior. © 2022 The Authors",Eye-tracking; Immersive virtual environment; Nature relatedness; Top-down effect; Virtual reality; Visual attention,,Article,Final,All Open Access; Hybrid Gold Open Access,Scopus,2-s2.0-85124309870,Gaming / VR
Han Y.; Miao Y.; Lu J.; Guo M.; Xiao Y.,"Han, Yu (57202210958); Miao, Yu (57672757600); Lu, Jie (57223403325); Guo, Mei (57149623500); Xiao, Yi (57673583100)",57202210958; 57672757600; 57223403325; 57149623500; 57673583100,Exploring Intervention Strategies for Distracted Students in VR Classrooms,2022,Conference on Human Factors in Computing Systems - Proceedings,,,354,,,,17,10.1145/3491101.3519627,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85129766232&doi=10.1145%2f3491101.3519627&partnerID=40&md5=c9518d0f019e9f3051c39f202e114e4e,"As one of the manifestations of virtual reality (VR) in education, virtual classroom allows students to enjoy a near-real classroom experience. VR class creates much better engagement and helps to stimulate interest and motivation in learning, making it an ideal solution to online teaching and learning activities, especially during the COVID-19 pandemic. Distraction is an unavoidable problem in immersive virtual classes, which has a great detrimental impact on learning. However, how to intervene in students' distraction behaviors in immersive virtual environments has not been thoroughly investigated up to now. In this paper, inspired by teachers' instructional techniques in real-life classes, we propose three intervention strategies, namely eye contact, verbal warning and text warning, and explore the intervening effects of these strategies on the inattention of students seated at the front or back of a virtual classroom via eye tracking. Our results show that all of the proposed intervention strategies have positive impacts on the attention of students. This research gives evidence that teachers' instructional techniques in the real world can be transferred to the virtual class, which provides a new insight for the future design of educational VR. © 2022 ACM.",attention; distraction; intervention; virtual reality,Computer aided instruction; E-learning; Eye tracking; Teaching; Virtual reality; Attention; Distraction; Education virtual; Ideal solutions; Instructional techniques; Intervention; Intervention strategy; Teachers'; Virtual class; Virtual Classroom; Students,Conference paper,Final,,Scopus,2-s2.0-85129766232,Gaming / VR
Campanaro D.M.; Landeschi G.,"Campanaro, Danilo M. (57254516200); Landeschi, Giacomo (55189941500)",57254516200; 55189941500,Re-viewing Pompeian domestic space through combined virtual reality-based eye tracking and 3D GIS,2022,Antiquity,96,386,,479,486,7.0,13,10.15184/aqy.2022.12,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85127477606&doi=10.15184%2faqy.2022.12&partnerID=40&md5=8492263a695484e863e5700984334e4f,"This article presents an innovative methodology in which virtual reality-based eye-tracking techniques and 3D Geographical Information Systems are employed to record and measure human visual attention within the virtually reconstructed space of a Pompeian house.  Copyright © The Author(s), 2022. Published by Cambridge University Press on behalf of Antiquity Publications Ltd.",3D GIS; eye tracking; Pompeii; virtual reality,,Review,Final,All Open Access; Hybrid Gold Open Access,Scopus,2-s2.0-85127477606,Gaming / VR
Mitchell D.; Choi H.,"Mitchell, Daxton (57672520400); Choi, HeeSun (55802606300)",57672520400; 55802606300,Assessing the Spatial Distribution of Visual Attention in a Virtual Environment: Development and Validation of a Novel VR-based Attentional Visual Field (AVF) Task,2022,Conference on Human Factors in Computing Systems - Proceedings,,,379,,,,2,10.1145/3491101.3519698,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85129723775&doi=10.1145%2f3491101.3519698&partnerID=40&md5=da11d1428bf3e97c563fbda0026a2869,"Visual attention is critical for everyday task performance and safety. The Attentional Visual Field Task (AVF) is an established, computerized method for assessing the distribution of visual attention across a wide visual field. High-fidelity virtual reality (VR) presents an opportunity for more ecological methods for assessing and training visual attention; however, this novel approach has not been examined. We developed a new VR-based AVF task, AVF-VE, using a Head-Mounted Display (HMD) VR device with an integrated eye-tracker, and conducted a study to validate this newly developed visual attention task. We further examined how visual attention is distributed in a virtual visual field. The findings suggest that the VR-based visual attention task is a valid and useful tool that can be used for future attention research and training. Unique characteristics of the spatial distribution of visual attention in the virtual environment observed in the current evaluation study are discussed. © 2022 ACM.",Attentional visual field; Head-mounted display; Virtual reality; Visual attention,Behavioral research; Eye tracking; Helmet mounted displays; Spatial distribution; Attentional visual field; Computerized methods; Ecological methods; Eye trackers; Head-mounted-displays; High-fidelity; Task performance; Virtual reality devices; Visual Attention; Visual fields; Virtual reality,Conference paper,Final,,Scopus,2-s2.0-85129723775,Gaming / VR
Cha G.-E.; Min B.-C.,"Cha, Go-Eum (57219691264); Min, Byung-Cheol (39161762500)",57219691264; 39161762500,Correlation between Unconscious Mouse Actions and Human CognitiveWorkload,2022,Conference on Human Factors in Computing Systems - Proceedings,,,243,,,,2,10.1145/3491101.3519658,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85129745749&doi=10.1145%2f3491101.3519658&partnerID=40&md5=99f1cfb7c1126cd1318150ff53a0ab76,"Unconscious behaviors are one of the indicators of the human perception process from a psychological perspective. As a result of perception responses, hand gestures show behavioral responses from given stimuli. Mouse usages in Human-Computer Interaction (HCI) show hand gestures that individuals perceive information processing. This paper presents an investigation of the correlation between unconscious mouse actions and human cognitive workload. We extracted mouse behaviors from a Robot Operating System (ROS) file-based dataset that user responses are reproducible. We analyzed redundant mouse movements to complete a dual n-back game by solely pressing the left and right buttons. Starting from a hypothesis that unconscious mouse behaviors predict different levels of cognitive loads, we statistically analyzed mouse movements. We also validated mouse behaviors with other modalities in the dataset, including self-questionnaire and eye blinking results. As a result, we found that mouse behaviors that occur unconsciously and human cognitive workload correlate. © 2022 Owner/Author.",Gesture; Human cognitive load; Mouse dynamics; Unconscious behavior,Human computer interaction; Human robot interaction; Mammals; Cognitive loads; Cognitive workloads; Gesture; Hand gesture; Human cognitive load; Human perception; Mouse behaviors; Mouse dynamics; Mouse movements; Unconscious behavior; Robot Operating System,Conference paper,Final,All Open Access; Green Open Access,Scopus,2-s2.0-85129745749,Gaming / VR
Gilbertson R.J.; Leff D.J.; Downs E.,"Gilbertson, Rebecca J. (8366039500); Leff, Dustyn J. (57209138878); Downs, Edward (15058608900)",8366039500; 57209138878; 15058608900,Eye Tracking of Attentional Allocation During Processing of Game Technologies: Association with Daily Playtime and Gaming Consequences,2022,Addicta: the Turkish Journal on Addictions,9,1,,56,62,6.0,1,10.5152/ADDICTA.2022.21068,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85159911510&doi=10.5152%2fADDICTA.2022.21068&partnerID=40&md5=23124e85182e33cd12120627683705d9,"This study used eye tracking as a measure of attention to examine how preference for video gaming images may change across a continuum of video gamers (casual to heavy). During the eye tracking procedure, participants (ages 18–26, N = 73; 43 men, 30 women) viewed 45 image pairs, presented in random order, com-posed of video gaming, alcohol, and neutral images. Following, participants completed questionnaires about video gaming and alcohol behavior. Findings showed that self-reported measures of experience, including video gaming quantity (i.e., daily playtime) and consequences, showed a significant positive relationship to eye tracking metrics of initial fixation and dwell time toward video game images. Results also showed that participants who reported loss of control (i.e., binge gaming) also demonstrated greater preference for video game images. The findings suggest that the attentional allocation during the processing of video game images in young adults may be enhanced by experience such as daily playtime. Changes in attentional allocation and cue reactivity may further contribute to the development of problematic video gaming behavior through prioritization of video gaming over other appetitive behaviors. These findings have basic science and clinical implications, including treatment for gaming disorder. © by 2022 Türkiye Yeşilay Cemiyeti (Turkish Green Crescent Society).",Addictive behavior; attentional biases; eye tracking; internet gaming disorder; substance addiction; video games,,Article,Final,,Scopus,2-s2.0-85159911510,Gaming / VR
Stokes J.D.; Rizzo A.; Geng J.J.; Schweitzer J.B.,"Stokes, Jared D. (36905714000); Rizzo, Albert (57189943380); Geng, Joy J. (7102538686); Schweitzer, Julie B. (7102296230)",36905714000; 57189943380; 7102538686; 7102296230,Measuring Attentional Distraction in Children With ADHD Using Virtual Reality Technology With Eye-Tracking,2022,Frontiers in Virtual Reality,3,,855895,,,,23,10.3389/frvir.2022.855895,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85131192821&doi=10.3389%2ffrvir.2022.855895&partnerID=40&md5=a0a6c82668904aa824881921ad4ca8e6,"Objective: Distractions inordinately impair attention in children with Attention-Deficit Hyperactivity Disorder (ADHD) but examining this behavior under real-life conditions poses a challenge for researchers and clinicians. Virtual reality (VR) technologies may mitigate the limitations of traditional laboratory methods by providing a more ecologically relevant experience. The use of eye-tracking measures to assess attentional functioning in a VR context in ADHD is novel. In this proof of principle project, we evaluate the temporal dynamics of distraction via eye-tracking measures in a VR classroom setting with 20 children diagnosed with ADHD between 8 and 12 years of age. Method: We recorded continuous eye movements while participants performed math, Stroop, and continuous performance test (CPT) tasks with a series of “real-world” classroom distractors presented. We analyzed the impact of the distractors on rates of on-task performance and on-task, eye-gaze (i.e., looking at a classroom whiteboard) versus off-task eye-gaze (i.e., looking away from the whiteboard). Results: We found that while children did not always look at distractors themselves for long periods of time, the presence of a distractor disrupted on-task gaze at task-relevant whiteboard stimuli and lowered rates of task performance. This suggests that children with attention deficits may have a hard time returning to tasks once those tasks are interrupted, even if the distractor itself does not hold attention. Eye-tracking measures within the VR context can reveal rich information about attentional disruption. Conclusions: Leveraging virtual reality technology in combination with eye-tracking measures is well-suited to advance the understanding of mechanisms underlying attentional impairment in naturalistic settings. Assessment within these immersive and well-controlled simulated environments provides new options for increasing our understanding of distractibility and its potential impact on the development of interventions for children with ADHD. Copyright © 2022 Stokes, Rizzo, Geng and Schweitzer.",ADHD (attention deficit and hyperactivity disorder); attention; distraction; eye tracking; virtual reality,,Article,Final,All Open Access; Gold Open Access; Green Open Access,Scopus,2-s2.0-85131192821,Gaming / VR
Soret R.; Charras P.; Hurter C.; Peysakhovich V.,"Soret, Rébaï (57210112083); Charras, Pom (24483147000); Hurter, Christophe (24740874700); Peysakhovich, Vsevolod (56644829800)",57210112083; 24483147000; 24740874700; 56644829800,Attentional Orienting in Front and Rear Spaces in a Virtual Reality Discrimination Task,2022,Vision (Switzerland),6,1,3,,,,1,10.3390/vision6010003,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123921248&doi=10.3390%2fvision6010003&partnerID=40&md5=7f8edfef5fe57b455fdecd3226cc2677,"Recent studies on covert attention suggested that the visual processing of information in front of us is different, depending on whether the information is present in front of us or if it is a reflection of information behind us (mirror information). This difference in processing suggests that we have different processes for directing our attention to objects in front of us (front space) or behind us (rear space). In this study, we investigated the effects of attentional orienting in front and rear space consecutive of visual or auditory endogenous cues. Twenty-one participants performed a modified version of the Posner paradigm in virtual reality during a spaceship discrimination task. An eye tracker integrated into the virtual reality headset was used to make sure that the participants did not move their eyes and used their covert attention. The results show that informative cues produced faster response times than non-informative cues but no impact on target identification was observed. In addition, we observed faster response times when the target occurred in front space rather than in rear space. These results are consistent with an orienting cognitive process differentiation in the front and rear spaces. Several explanations are discussed. No effect was found on subjects’ eye movements, suggesting that participants did not use their overt attention to improve task performance. © 2022 by the authors. Licensee MDPI, Basel, Switzerland.",Attentional orienting; Discrimination task; Eye tracking; Front space; Posner paradigm; Rear space; Spatial cognition; Virtual reality,,Article,Final,All Open Access; Gold Open Access; Green Open Access,Scopus,2-s2.0-85123921248,Gaming / VR
Tzi-Dong Ng J.; Hu X.; Que Y.,"Tzi-Dong Ng, Jeremy (57201810212); Hu, Xiao (55496358400); Que, Ying (57215926050)",57201810212; 55496358400; 57215926050,Towards Multi-modal Evaluation of Eye-tracked Virtual Heritage Environment,2022,ACM International Conference Proceeding Series,,,,451,457,6.0,15,10.1145/3506860.3506881,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85126215509&doi=10.1145%2f3506860.3506881&partnerID=40&md5=4566515423f64f2bfd34eca225d6b7d6,"In times of pandemic-induced challenges, virtual reality (VR) allows audience to learn about cultural heritage sites without temporal and spatial constraints. The design of VR content is largely determined by professionals, while evaluations of content often rely on learners' self-report data. Learners' attentional focus and understanding of VR content might be affected by the presence or absence of different multimedia elements including text and audio-visuals. It remains an open question which design variations are more conducive for learning about heritage sites. Leveraging eye-tracking, a technology often adopted in recent multimodal learning analytics (MmLA) research, we conducted an experiment to collect and analyze 40 learners' eye movement and self-reported data. Results of statistical tests and heatmap elicitation interviews indicate that 1) text in the VR environment helped learners better understand the presented heritage sites, regardless of having audio narration or not, 2) text diverted learners' attention away from other visual elements that contextualized the heritage sites, 3) exclusively having audio narration best simulated the experience of a real-world heritage tour, 4) narration accompanying text prompted learners to read the text faster. We make recommendations for improving the design of VR learning materials and discuss the implications for MmLA research.  © 2022 ACM.",Cultural heritage; Eye-tracking; Multimedia learning; Multimodal learning analytics; Virtual reality,E-learning; Eye movements; Virtual reality; Audio narrations; Cultural heritages; Eye-tracking; Heritage sites; Learn+; Multi-media learning; Multi-modal; Multi-modal learning; Multimodal learning analytic; Virtual heritage; Eye tracking,Conference paper,Final,,Scopus,2-s2.0-85126215509,Gaming / VR
Olsen J.K.; Guneysu Ozgur A.; Sharma K.; Johal W.,"Olsen, Jennifer K. (55417042300); Guneysu Ozgur, Arzu (57201504713); Sharma, Kshitij (55903734200); Johal, Wafa (55912460600)",55417042300; 57201504713; 55903734200; 55912460600,"Leveraging eye tracking to understand children's attention during game-based, tangible robotics activities",2022,International Journal of Child-Computer Interaction,31,,100447,,,,11,10.1016/j.ijcci.2021.100447,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85121934762&doi=10.1016%2fj.ijcci.2021.100447&partnerID=40&md5=fa7df630fd5a943fe5e30927e8e76c4a,"The difficulty in maintaining attention can interfere with the acquisition of critical academic skills. Recently, researchers have used embodied and game-based learning to support skill acquisition for children with learning difficulties. In this context, robots can be an interesting asset to foster engagement and investigate game dynamics. However, it is still an open question of how to develop adaptive learning environments for children with learning difficulties. Before one can provide effective adaptation, a first step is needed to understand the differentiating behaviors during the activity for children with attention difficulties. Three such differentiating behaviors are how a child divides his or her attention during the learning activity, the child's level of cognitive load, and the child's physiological fatigue, which are the focus of our study Using a robot assisted, gamified activity, we conducted a user study with 18 children having difficulty in maintaining attention. Using process mining techniques and eye-tracking data, we found the importance of integrating the autonomous robots into the attention patterns to successfully complete a game and the influence their behaviors can have on the participant's attention. This importance was supported by the cognitive load of participants decreasing the more they focused on the autonomous robots in successful games. This work contributes to the understanding of children's behaviors during tangible game-based activities and can be used to build effective adaptation for children with attention difficulties. © 2021 Elsevier B.V.",Attention; Cognitive load; Gaze; Process analysis; Robot,,Article,Final,,Scopus,2-s2.0-85121934762,Gaming / VR
Kim N.; Lee H.,"Kim, Nayeon (57216898685); Lee, Hyunsoo (54393374900)",57216898685; 54393374900,Exploring spatial design cognition using VR and eye-tracking,2022,Design Computing and Cognition'20,,,,593,603,10.0,4,10.1007/978-3-030-90625-2_35,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85184096670&doi=10.1007%2f978-3-030-90625-2_35&partnerID=40&md5=f662cc621d4777357d7513f601ee2266,"This paper aims to contribute to the understanding of the spatial design cognition in virtual reality environments. Biometrics are used to assess human psychological responses to architectural spaces. This study explores the use of eye-tracking in virtual reality to observe participants' cognitive sequence, behavior, visual attention, and emotional arousal during spatial experience. The preliminary experiment presents a case study of six participants who were confronted with two different virtual representations of architectural spaces while their gaze was tracked by an integrated eye-tracking system. This paper reports pupillary response as well as visual attention on architectural design features and the sequence of spatial observations. The results present that the spatial elements which attract visual attention were not consistent with emotional arousal by the pupillary response. Findings from comparison by gender indicate that there are significant gender differences in cognitive sequence, visual attention, and emotional arousal. The result of the experiment can provide evidence-based insights in designing human-centric spaces. © Springer Nature Switzerland AG 2022. All rights reserved.",,,Book chapter,Final,,Scopus,2-s2.0-85184096670,Gaming / VR
Brand J.; Lansigan R.K.; Thomas N.; Emond J.; Gilbert-Diamond D.,"Brand, John (55258376100); Lansigan, Reina Kato (56426570400); Thomas, Natalie (57221287613); Emond, Jennifer (56402794100); Gilbert-Diamond, Diane (34969411400)",55258376100; 56426570400; 57221287613; 56402794100; 34969411400,Completing a Sustained Attention Task Is Associated With Decreased Distractibility and Increased Task Performance Among Adolescents With Low Levels of Media Multitasking,2022,Frontiers in Psychology,12,,804931,,,,5,10.3389/fpsyg.2021.804931,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124993883&doi=10.3389%2ffpsyg.2021.804931&partnerID=40&md5=dddb56a1398d2427b6c272a75d70c5c3,"Objective: To assess distracted attention and performance on a computer task following completion of a sustained attention and acute media multitasking task among adolescents with varying self-reported usual media multitasking. Methods: Ninety-six 13- to 17-year-olds played the video game Tetris following completion of a Go/No-go paradigm to measure sustained attention in the presence of distractors, an acute media multitasking, or a passive viewing condition. Adolescents completed the conditions on separate visits in randomized order. Sustained attention was measured within the Go/No-go task by measuring errors of omission. Distracted attention while playing the Tetris task was measured by computing eye tracking measures of attention (first fixation duration, cumulative fixation duration) to irrelevant distractor images that bordered the Tetris game. Participants also self-reported their daily media multitasking. Results: The Go/No-go task revealed important qualitative differences in sustained attention among low and high usual media multitaskers. There was a uniform improvement in sustained attention among low usual media multitaskers, demonstrated by a consistent linear decrease in omission errors (β = −0.01; P < 0.05). Among high usual media multitaskers, there was initially a decrease in sustained attention (β = −0.01; P = 0.05) followed by an increase (β = 0.16; P < 0.001). Completing the Go/No-go task also statistically significantly reduced distractibility and increased performance while playing Tetris compared to the passive viewing condition, but only among those with low usual media multitasking (all Ps ≤ 0.05). There was a non-statistically significant trend that completing the acute media multitask increased subsequent distractibility and performance while playing Tetris among high media multitaskers. Conclusion: In this sample of adolescents, practicing a sustained attention task reduces distractibility and improves task performance among those who have low levels of usual media multitasking. Copyright © 2022 Brand, Lansigan, Thomas, Emond and Gilbert-Diamond.",adolescents; attention; cognition; distraction; media multitasking,,Article,Final,,Scopus,2-s2.0-85124993883,Gaming / VR
Wang J.; Stebbins A.; Ferdig R.E.,"Wang, Jiahui (57191491117); Stebbins, Abigail (57373186200); Ferdig, Richard E. (13411366400)",57191491117; 57373186200; 13411366400,Examining the effects of students' self-efficacy and prior knowledge on learning and visual behavior in a physics game,2022,Computers and Education,178,,104405,,,,23,10.1016/j.compedu.2021.104405,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85121208328&doi=10.1016%2fj.compedu.2021.104405&partnerID=40&md5=c4d889b5a49ffc2a6d6324cb1638c0ac,"Research has provided evidence of the significant promise of using educational games for learning. However, there is limited understanding of how individual differences (e.g., self-efficacy and prior knowledge) affect visual processing of game elements and learning from an educational game. This study aimed to address these gaps by: a) examining the effects of students' self-efficacy and prior knowledge on learning from a physics game; and b) exploring how learners with distinct levels of self-efficacy and prior knowledge differ in their visual behavior with respect to the game elements. The visual behavior of 69 undergraduate students was recorded as they played an educational game focusing on Newtonian mechanics. Individual differences in self-efficacy in learning physics and prior knowledge were assessed prior to the game, while a comprehension test was administered immediately after gameplay. Wilcoxon signed-rank tests showed that all participants significantly improved in their understanding of Newtonian mechanics. Mann-Whitney U tests indicated learning gains were not significantly different between the groups with varying levels of prior knowledge or self-efficacy. Additionally, a series of Mann-Whitney U tests of the eye tracking data suggested the learners with high self-efficacy tended to pay more attention to the motion map - a critical navigation component of the game. Further, the high prior knowledge individuals excelled in attentional control abilities and exhibited effective visual processing strategies. The study concludes with important implications for the future design of educational games and developing individualized instructional support in game-based learning. © 2021 Elsevier Ltd",Eye tracking; Game-based learning; Prior knowledge; Self-efficacy; Visual behavior,Students; Testing; Educational game; Eye-tracking; Game elements; Game-based Learning; Individual Differences; Newtonian mechanics; Prior-knowledge; Self efficacy; Visual behavior; Visual-processing; Eye tracking,Article,Final,,Scopus,2-s2.0-85121208328,Gaming / VR
Gupta A.S.,"Gupta, Anoopum S. (58896850500)",58896850500,Digital Phenotyping in Clinical Neurology,2022,Seminars in Neurology,42,1,,48,59,11.0,14,10.1055/s-0041-1741495,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123206271&doi=10.1055%2fs-0041-1741495&partnerID=40&md5=414cc14cc47946f86807b9a45625bc0a,"Internet-connected devices, including personal computers, smartphones, smartwatches, and voice assistants, have evolved into powerful multisensor technologies that billions of people interact with daily to connect with friends and colleagues, access and share information, purchase goods, play games, and navigate their environment. Digital phenotyping taps into the data streams captured by these devices to characterize and understand health and disease. The purpose of this article is to summarize opportunities for digital phenotyping in neurology, review studies using everyday technologies to obtain motor and cognitive information, and provide a perspective on how neurologists can embrace and accelerate progress in this emerging field. © 2022 SAE International. All rights reserved.",biomarkers; digital phenotyping; outcome measures; quantitative phenotyping,Humans; Neurologists; Neurology; Smartphone; arm movement; Article; behavior; body equilibrium; clinical research; cognition; digital technology; eye movement; gait; gaze; human; Internet; motor performance; neurologist; neurology; nonhuman; phenotype; speech and language; smartphone,Article,Final,All Open Access; Green Open Access,Scopus,2-s2.0-85123206271,Gaming / VR
Liu R.; Xu X.; Yang H.; Li Z.; Huang G.,"Liu, Rui (57451015500); Xu, Xiang (57658306800); Yang, Hairu (57208328724); Li, Zhenhua (57192168531); Huang, Guan (59596490600)",57451015500; 57658306800; 57208328724; 57192168531; 59596490600,Impacts of Cues on Learning and Attention in Immersive 360-Degree Video: An Eye-Tracking Study,2022,Frontiers in Psychology,12,,792069,,,,32,10.3389/fpsyg.2021.792069,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124554148&doi=10.3389%2ffpsyg.2021.792069&partnerID=40&md5=3466b333be543f5e8835e6bc85f660ac,"Immersive 360-degree video has become a new learning resource because of its immersive sensory experience. This study examined the effects of textual and visual cues on learning and attention in immersive 360-degree video by using eye-tracking equipment integrated in a virtual reality head-mounted display. Participants (n = 110) were randomly assigned to one of four conditions: (1) no cues, (2) textual cues in the initial field of view (FOV), (3) textual cues outside the initial FOV, and (4) textual cues outside the initial FOV + visual cues. The results showed that the cues (annotations or annotations + arrows) helped learners achieve better learning outcomes and spend more time focusing on the areas with cues. In addition, the study found a serious imbalance in the distribution of learners’ attention in each region of the video. The attention directed to textual cues in the initial FOV is much higher than the attention directed to textual cues outside the initial FOV. Adding visual cues can effectively direct attention to textual cues outside the initial FOV and alleviate the imbalance of attention distribution. Consequently, adding cues to immersive 360-degree video can be an appropriate approach to promote learning and guide attention in immersive 360-degree video learning environments. This study provided new insights into the design and development of immersive 360-degree video instructional resources. Copyright © 2022 Liu, Xu, Yang, Li and Huang.",attention allocation; cues; eye-tracking technologies; immersive 360-degree video; learning outcome; signaling,,Article,Final,,Scopus,2-s2.0-85124554148,Gaming / VR
Alcañiz M.; Chicchi-Giglioli I.A.; Carrasco-Ribelles L.A.; Marín-Morales J.; Minissi M.E.; Teruel-García G.; Sirera M.; Abad L.,"Alcañiz, Mariano (36921902100); Chicchi-Giglioli, Irene Alice (56994284500); Carrasco-Ribelles, Lucía A. (57221713820); Marín-Morales, Javier (57191977544); Minissi, Maria Eleonora (57207617337); Teruel-García, Gonzalo (57216431113); Sirera, Marian (57215562711); Abad, Luis (6507261181)",36921902100; 56994284500; 57221713820; 57191977544; 57207617337; 57216431113; 57215562711; 6507261181,Eye gaze as a biomarker in the recognition of autism spectrum disorder using virtual reality and machine learning: A proof of concept for diagnosis,2022,Autism Research,15,1,,131,145,14.0,83,10.1002/aur.2636,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85119596316&doi=10.1002%2faur.2636&partnerID=40&md5=60e7a3d67bda7a1b3abecc38babbe8be,"The core symptoms of autism spectrum disorder (ASD) mainly relate to social communication and interactions. ASD assessment involves expert observations in neutral settings, which introduces limitations and biases related to lack of objectivity and does not capture performance in real-world settings. To overcome these limitations, advances in technologies (e.g., virtual reality) and sensors (e.g., eye-tracking tools) have been used to create realistic simulated environments and track eye movements, enriching assessments with more objective data than can be obtained via traditional measures. This study aimed to distinguish between autistic and typically developing children using visual attention behaviors through an eye-tracking paradigm in a virtual environment as a measure of attunement to and extraction of socially relevant information. The 55 children participated. Autistic children presented a higher number of frames, both overall and per scenario, and showed higher visual preferences for adults over children, as well as specific preferences for adults' rather than children's faces on which looked more at bodies. A set of multivariate supervised machine learning models were developed using recursive feature selection to recognize ASD based on extracted eye gaze features. The models achieved up to 86% accuracy (sensitivity = 91%) in recognizing autistic children. Our results should be taken as preliminary due to the relatively small sample size and the lack of an external replication dataset. However, to our knowledge, this constitutes a first proof of concept in the combined use of virtual reality, eye-tracking tools, and machine learning for ASD recognition. Lay Summary: Core symptoms in children with ASD involve social communication and interaction. ASD assessment includes expert observations in neutral settings, which show limitations and biases related to lack of objectivity and do not capture performance in real settings. To overcome these limitations, this work aimed to distinguish between autistic and typically developing children in visual attention behaviors through an eye-tracking paradigm in a virtual environment as a measure of attunement to, and extraction of, socially relevant information. © 2021 The Authors. Autism Research published by International Society for Autism Research and Wiley Periodicals LLC.",,"Adult; Autism Spectrum Disorder; Biomarkers; Child; Fixation, Ocular; Humans; Machine Learning; Virtual Reality; biological marker; biological marker; Article; autism; brain depth stimulation; child; clinical assessment; clinical feature; controlled study; diagnostic accuracy; diagnostic test accuracy study; eye movement; eye tracking; female; gaze; human; machine learning; major clinical study; male; proof of concept; psychologic assessment; sensitivity and specificity; social interaction; virtual reality; visual attention; adult; eye fixation; machine learning",Article,Final,All Open Access; Green Open Access,Scopus,2-s2.0-85119596316,Gaming / VR
Chen Y.; Zulnaidi H.; Syed Ali S.K.B.,"Chen, Yanlan (57760433400); Zulnaidi, Hutkemri (55352069000); Syed Ali, Syed Kamaruzaman Bin (57190983051)",57760433400; 55352069000; 57190983051,Study on the eye movement characteristics of the badminton practitioners of different levels regarding visual attention,2022,Frontiers in Psychology,13,,1026006,,,,7,10.3389/fpsyg.2022.1026006,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85149622368&doi=10.3389%2ffpsyg.2022.1026006&partnerID=40&md5=e37fcebfd61f5d3d3d0abac84fcefa9c,"Badminton is a highly sophisticated, fierce, and competitive tactical game. It requires the same action of hitting a ball with different landing points. Therefore, the complexity of badminton practitioner's sports decision-making is relatively high. Accordingly, it is extremely important to study the difference between the eye movement characteristics of different levels of badminton athletes and the difference between the eye movement characteristics of different sports levels of amateur athletes. Overall, 15 students from the badminton professional training team of the Physical Education College of the Jiangxi Science and Technology Normal University and 15 students from the common public sports and badminton course were included as experimental participants in the present study. The laboratory experimental test on the virtual sports situation in badminton was conducted using an eye tracker. The eye movement index of both the badminton professionals and the experimental participants was recorded for statistical analysis, and the following results were obtained: (1) In the cognitive decision-masking task, the reaction time of the professional badminton practitioners was faster than that of the amateur practitioners. Similarly, in the intuitive decision-masking task, the reaction time and accuracy of the former were better than those of the latter. (2) The professional badminton practitioners' group was able to process and integrate the searched information in the process of sports attention selection information; although the amateur group was able to search and filter information, they were passive and could not actively process and assimilate the searched information. (3) The professional badminton practitioners could allocate their attention reasonably and process information in the process of attention transfer, while their amateur counterparts were affected easily by external interference factors. The level of motor intelligence of badminton practitioners in the professional group was higher than that of the amateur practitioners. Thus, these two groups of different levels showed attention transfer. (4) The mental skills of the professional group were higher than those of the amateur group. Copyright © 2023 Chen, Zulnaidi and Syed Ali.",attention choice; badminton practitioner; cognitive decision-making; different sports levels; eye movements; intuitive decision-making,,Article,Final,All Open Access; Gold Open Access; Green Open Access,Scopus,2-s2.0-85149622368,Gaming / VR
Xiangmin G.; Weiqiang C.; Tiantian L.; Shumeng H.,"Xiangmin, Guo (36571734100); Weiqiang, Cui (57226125168); Tiantian, Lo (57223112124); Shumeng, Hou (57226110017)",36571734100; 57226125168; 57223112124; 57226110017,Research on dynamic visual attraction evaluation method of commercial street based on eye movement perception,2022,Journal of Asian Architecture and Building Engineering,21,5,,1779,1791,12.0,16,10.1080/13467581.2021.1944872,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85110414008&doi=10.1080%2f13467581.2021.1944872&partnerID=40&md5=e820e0d9acfc8f1dce2d5d46fe262cbe,"Commercial streets play an essential role in urban public life, and their appearance constitutes an important visual attraction, which can attract people to form high-quality and active public spaces. However, in the traditional design analysis methods, limited by the existing evaluation tools, the key content of visual attractiveness analysis under dynamic conditions has rarely been paying attention to. Given this, this research adopts both objectivity and real-time eye movement perception technology to construct an evaluation method based on dynamic visual attractiveness-through the application of virtual reality technology to construct a virtual scene of a typical commercial street and detect the user’s actions in it Eye movement at a time and physiological parameters such as the number of fixation points, fixation duration, and pupil diameter changes.This paper takes the main commercial street in Huanggaihu Town in Hubei Province of China as an example to analyze the visual attraction caused by different design methods.Therefore, from the four aspects of material, shape, line, and vegetation, design suggestions for the reasonable distribution of visual attractiveness in the commercial street space design are proposed to provide useful enlightenment for the vast towns’ commercial street space design. © 2021 The Author(s). Published by Informa UK Limited, trading as Taylor & Francis Group on behalf of the Architectural Institute of Japan, Architectural Institute of Korea and Architectural Society of China.",Commercial street design; evaluation of visual attractiveness; eye movement perception; virtual reality,Physiological models; Design analysis method; Design suggestions; Dynamic condition; Evaluation tool; Fixation duration; Number of fixations; Physiological parameters; Virtual reality technology; Eye movements,Article,Final,All Open Access; Gold Open Access,Scopus,2-s2.0-85110414008,Gaming / VR
Marchezini F.; Claessens P.M.E.; Carthery-Goulart M.T.,"Marchezini, Fernanda (57444750800); Claessens, Peter Maurice Erna (12807756800); Carthery-Goulart, Maria Teresa (6506112063)",57444750800; 12807756800; 6506112063,Word and pseudoword reading in young adults: An eye-tracking study; [Leitura de palavras e pseudopalavras em adultos jovens: Um estudo de rastreamento ocular],2022,CODAS,34,4,e20200333,,,,3,10.1590/2317-1782/20212020333,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124326243&doi=10.1590%2f2317-1782%2f20212020333&partnerID=40&md5=db4efaf725b7139435802da94bdaea37,"Purpose: To evaluate and characterize the oculomotor behavior during the reading of words and pseudowords in Brazilian Portuguese organized by frequency, length and regularity and verify its association with performance on neuropsychological tests. Methods: 21 university students, with a mean age of 20.9 years, were submitted to a word and pseudoword reading task (TLPP) from the Anele Battery, in addition to verbal fluency and phonological working memory tests. The patterns of first fixation duration, gaze duration and rate of refixation were studied. Results: The first fixation duration and the gaze duration were significantly lower for words if compared to pseudowords and the gaze duration was also lower for high-frequency and short words. Significant interactions were also found between verbal fluency performance and the first fixation duration. Conclusion: Our results demonstrate the applicability of eye tracking to study reading patterns at the word-level in Brazilian Portuguese. The eye tracker can be an additional tool in the investigation of acquired and developmental reading disorders and can assist in the detection of reading difficulties based on comparisons of the oculomotor behavior between fluent and non-fluent readers. © 2022",Dislexia; Dyslexia; Eye Movement Measurements; Eye Movements; Language; Leitura; Linguagem; Medições dos Movimentos Oculares; Movimentos Oculares; Psicolinguística; Psycholinguistics; Reading,"Adult; Cognition; Eye Movements; Eye-Tracking Technology; Humans; Linguistics; Memory, Short-Term; Young Adult; adult; cognition; eye movement; human; linguistics; short term memory; young adult",Article,Final,All Open Access; Gold Open Access; Green Open Access,Scopus,2-s2.0-85124326243,Gaming / VR
Gutierrez-Maldonado J.; Clua i Sánchez M.; Porras-Garcia B.; Ferrer-Garcia M.; Serrano E.; Carulla M.; Meschberger-Annweiler F.; Ascione M.,"Gutierrez-Maldonado, José (9334173600); Clua i Sánchez, Mar (57772657500); Porras-Garcia, Bruno (57201200642); Ferrer-Garcia, Marta (13405944200); Serrano, Eduardo (58711117400); Carulla, Marta (57193497702); Meschberger-Annweiler, Franck (57773528000); Ascione, Mariarca (57772657700)",9334173600; 57772657500; 57201200642; 13405944200; 58711117400; 57193497702; 57773528000; 57772657700,Body-Related Attentional Bias in Anorexia Nervosa and Body Dissatisfaction in Females: An Eye-Tracking and Virtual Reality New Paradigm,2022,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),13317 LNCS,,,443,454,11.0,2,10.1007/978-3-031-05939-1_30,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85133160643&doi=10.1007%2f978-3-031-05939-1_30&partnerID=40&md5=6bb9d5a280e79b8b8e9c06c3ee2533ad,"According to recent research, eating disorder (ED) patients tend to check unattractive body parts. However, few studies have studied this attentional bias (AB) phenomenon combining virtual reality (VR) with eye-tracking (ET). This study aims to examine whether anorexia nervosa (AN) patients have a longer fixation time and a greater fixations number on the weight-related body areas compared to the healthy sample with high body dissatisfaction (HBD) and low body dissatisfaction (LBD). It will also examine whether the HBD group will have more fixations and spend more time looking at weight-related areas than those with LBD. Forty-three college women (18 with HBD and 25 with LBD) and 23 AN patients were immersed in a virtual environment and then embodied in a virtual avatar with their real body measurements and body mass index (BMI). Eye movement data were tracked using an ET device incorporated in the VR headset (FOVE). The number of fixations and the complete fixations time were registered on the weight-related areas of interest (W-AOIs) and non-weight-related areas of interest (NW-AOIs). The results showed that AN patients have a longer fixation time and a greater fixations number on W-AOIs than both HBD and LBD groups, who did not show any statistical differences in the visual selective attention to NW-AOIs and W-AOIs. © 2022, The Author(s), under exclusive license to Springer Nature Switzerland AG.",Anorexia nervosa; Attentional bias; Body dissatisfaction; Body image; Virtual reality,Diseases; Eye movements; Eye tracking; Anorexia nervosa; Area of interest; Attentional bias; Body dissatisfaction; Body image; Eating disorders; Eye-tracking; Fixation time; Lower body; Recent researches; Virtual reality,Conference paper,Final,,Scopus,2-s2.0-85133160643,Gaming / VR
Murray N.P.; Trotter B.M.; Heidner G.S.; Herman C.; Hunfalvay M.,"Murray, Nicholas P. (35900796000); Trotter, Brittany M. (57773716700); Heidner, Gustavo Sandri (57196194907); Herman, Callie (57772684300); Hunfalvay, Melissa (57188538024)",35900796000; 57773716700; 57196194907; 57772684300; 57188538024,"Eye Tracker Outcomes from Static, Mobile, Virtual Reality Eye Tracking Devices",2022,Neuromethods,183,,,113,129,16.0,1,10.1007/978-1-0716-2391-6_7,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85133194436&doi=10.1007%2f978-1-0716-2391-6_7&partnerID=40&md5=d3ffb69b2578a114e866d7d3e70dcdec,"Examining eye movement through the use of eye tracking technology provides us with copious information across a variety of human processes. The advent of static, mobile, and virtual reality (VR) based eye trackers provides flexibility in studying eye movement under an array of conditions. This chapter explores the use and application of these eye tracking devices; diving into the suitability of each with respect to study design, data collection, data quality, and data analysis. We will also identify key visual behavior measures that can be assessed using eye tracking methodology and what these measures tell us. Lastly, we will examine the use of different eye trackers in relevant case studies. © 2022, The Author(s), under exclusive license to Springer Science+Business Media, LLC, part of Springer Nature.",Fixation; Mobile eye tracker; Remote eye tracker; Saccades; Static eye tracker; VR eye tracker,balance beam walking; calibration; cognitive load; data analysis; data collection method; data quality; eye fixation; eye tracking; human; mobile eye tracking; saccadic eye movement; smooth pursuit eye movement; static eye tracking; virtual reality eye tracking; visual system function; walking,Book chapter,Final,,Scopus,2-s2.0-85133194436,Gaming / VR
Wang C.C.; Chen H.C.; Hung J.C.,"Wang, Chun Chia (7501632228); Chen, Hsuan Chu (56078732800); Hung, Jason C. (7201963626)",7501632228; 56078732800; 7201963626,Employing Portable Eye Tracking Technology in Visual Attention of Cognitive Process: A Case Study of Digital Game-Based Learning,2022,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),13449 LNCS,,,480,490,10.0,0,10.1007/978-3-031-15273-3_53,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85137991982&doi=10.1007%2f978-3-031-15273-3_53&partnerID=40&md5=3933bd87816c67bcae4eb0254726d521,"In recent years, digital game-based learning (DGBL) has received more and more attention as a form of digital learning. This study used a portable eye tracker to explore the learners’ visual attention and cognitive process during DGBL. The study recruited 38 students in the third year of study in the school of information and design at a university in southern Taiwan. According to students’ performance in the English courses at the university, participants were divided into High Competence Group (n = 5), Intermediate Competence Group (n = 8), and Low Competence Group (n = 25). By collecting the participants’ eye movement data with eye tracker, eye movement indicators were acquired to investigate the correlation between the participants’ external behavior and cognitive process among regions of interest (ROIs). The finding results of this study were as follows: (1) The fixation sequence of ROIs from participants of different groups is different; (2) During the experiment, ROI3 (English definitions of the vocabulary) was the first ROI of fixation for participants of Intermediate and Low Competence Groups, showing a difference from participants of the High Competence Group in terms of visual attention during the first fixation; (3) By examining the total fixation duration and total fixation count of the ROIs from all participants, the same pattern can be found, indicating that as participants answered the questions in this DGBL context, their visual attention was distributed more towards the English definitions of the vocabulary, in order to successfully answer the vocabulary-related questions. © 2022, The Author(s), under exclusive license to Springer Nature Switzerland AG.",Cognitive process; Eye tracking; Game-based learning; Visual attention,Behavioral research; Cognitive systems; Computer games; E-learning; Eye tracking; Cognitive process; Digital game-based learning; Eye trackers; Eye tracking technologies; Eye-tracking; Game-based Learning; Portable eye-tracking; Region-of-interest; Regions of interest; Visual Attention; Eye movements,Conference paper,Final,,Scopus,2-s2.0-85137991982,Gaming / VR
Ko H.; Wang B.; Lim J.S.,"Ko, Hansol (57638541900); Wang, Bohyun (56142751600); Lim, Joon. S. (7403454472)",57638541900; 56142751600; 7403454472,A Study for ADHD Identification using Eye Movement Data,2022,"2022 International Conference on Electronics, Information, and Communication, ICEIC 2022",,,,,,,1,10.1109/ICEIC54506.2022.9748230,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85128862254&doi=10.1109%2fICEIC54506.2022.9748230&partnerID=40&md5=3c9d314fe06ee8d5fc9bd3c435ea021b,"Attention deficit hyperactivity disorder (ADHD) is a disorder that occurs a lot in childhood, and is a state of distraction, hyperactivity, and impulsiveness due to a continuous lack of attention. We classified ADHD and control using eye movement data by deep learning. Through seven kinds of VR game, we collected eye movement data from subjects. As deep learning algorithm, we used Zoom in Neural Network(ZNN) based on NEWFM. As classification accuracy, 82.1 % and 83.6% were obtained for two groups, respectively.  © 2022 IEEE.",ADHD; deep learning; ZNN,Deep learning; Diseases; Learning algorithms; Virtual reality; Attention deficit hyperactivity disorder; Classification accuracy; Classifieds; Deep learning; Eye movement datum; Network-based; Neural-networks; Zoom in neural network; Eye movements,Conference paper,Final,,Scopus,2-s2.0-85128862254,Gaming / VR
Fei C.; Sun B.; Li Y.; Zhang Q.,"Fei, Chao (57841155800); Sun, Baiqing (55413100100); Li, Yong (59069206100); Zhang, Qiuhao (7407964126)",57841155800; 55413100100; 59069206100; 7407964126,A Study of Virtual Reality Systems for Attention Stabilization,2022,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),13455 LNAI,,,105,113,8.0,0,10.1007/978-3-031-13844-7_11,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85135835955&doi=10.1007%2f978-3-031-13844-7_11&partnerID=40&md5=ce720bc379009e3923f4cd3529a4a15f,"The main idea of this paper is to design an attention quality testing system based on the combination of virtual reality technology and eye-tracking technology, through which a visual attention stability testing method is established and the feasibility of the system and method is verified. The relationship between attention and eye movements and prefrontal cerebral blood flow was investigated by simultaneous acquisition of prefrontal blood oxygen concentration in the attentional stability test experiment. After the experimental data analysis, the eye-movement signal can accurately capture the information of attention shifting occurred, and find the evidence that attention shifting excited the prefrontal brain area, and find the relationship between attention, eye-movement and prefrontal. It verified the rationality of the combined method of virtual reality technology and eye-tracking technology in the content of attentional stability test, and provided scientific data support and experimental suggestions for future attention deficit disorder to conduct attentional stability test. © 2022, The Author(s), under exclusive license to Springer Nature Switzerland AG.",Attention stabilization; Eye tracking; Virtual reality,Behavioral research; Blood; Brain; Eye tracking; Stabilization; Testing; Virtual reality; Attention shifting; Attention stabilization; Eye tracking technologies; Eye-tracking; Quality testing systems; Stability testing; Stability tests; Virtual reality system; Virtual reality technology; Visual Attention; Eye movements,Conference paper,Final,,Scopus,2-s2.0-85135835955,Gaming / VR
Azizi E.; Fielding J.; Abel L.A.,"Azizi, Elham (54683453100); Fielding, Joanne (7202507959); Abel, Larry A. (7103217203)",54683453100; 7202507959; 7103217203,Video game training in traumatic brain injury patients: an exploratory case report study using eye tracking,2022,Journal of Eye Movement Research,15,1,6,,,,11,10.16910/jemr.15.1.6,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85133426668&doi=10.16910%2fjemr.15.1.6&partnerID=40&md5=ff2d6bb980ced298a56ad64e50c1e89a,"Remediation of attentional impairments is an essential component of cognitive rehabilitation after traumatic brain injury (TBI). Evidence from healthy participants has demonstrated attentional improvement following playing an action video game. This exploratory study investigated its application in TBI participants in a multiple baselines single case experimental design (SCED). Saccadic eye movements, recognized as the visible indicators of visual attention, were assessed to evaluate the effectiveness of the game training. Three severe TBI participants were trained in an action game for 10 hours. Saccadic eye movements during a self-paced saccade and an abstract visual search task were investigated during baseline, mid training and post-training. Using Percentage of Non-overlapping Data (PND), analysis showed consistent increase in the rate of the self-paced saccades in participants 1 (PND=80%) and 2 (PND=70%). In abstract search, fixation duration showed a minimally effective decrease for participant 2 (PND= 60%) and a moderately effective reduction in participant 3 (PND= 80%). Search time showed a highly effective reduction in participant 2 (PND = 100%) and moderately effective decrease in participant 3 (PND=70%). Overall, video game training might modify allocation of attention in eye movements. More evidence is required to validate the usefulness of this novel method of the cognitive training © This article is licensed under a Creative Commons Attribution 4.0 International license",Antisaccades; Attention; Convergence; Eye movement; Eye tracking; Microsaccades; Saccades; Scanpath; Smooth pursuit,,Article,Final,All Open Access; Gold Open Access; Green Open Access,Scopus,2-s2.0-85133426668,Gaming / VR
Gao H.; Frommelt L.; Kasneci E.,"Gao, Hong (57217014766); Frommelt, Lasse (58059126600); Kasneci, Enkelejda (56059892600)",57217014766; 58059126600; 56059892600,The Evaluation of Gait-Free Locomotion Methods with Eye Movement in Virtual Reality,2022,"Proceedings - 2022 IEEE International Symposium on Mixed and Augmented Reality Adjunct, ISMAR-Adjunct 2022",,,,530,535,5.0,9,10.1109/ISMAR-Adjunct57072.2022.00112,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85146055546&doi=10.1109%2fISMAR-Adjunct57072.2022.00112&partnerID=40&md5=1fc5a1c242d5b511dd8e4f37936c6a6f,"As VR becomes increasingly popular in the entertainment industry, VR locomotion, a technique that allows users to navigate virtual environments beyond the spatial confines of the real world, is being increasingly studied by developers and researchers. Previous work has examined the effects of locomotion methods on various aspects of users, such as user experience, motion sickness, and task performance. However, how locomotion methods affect users' eye movements that might indicate cognitive load has not yet been in-vestigated, although several relevant works have addressed these effects as being important to study. To contribute to this area of research, in this work we investigate the evaluation of five common gait-free locomotion methods using eye movements during VR nav-igation. The results show that locomotion methods significantly affect participants' eye movement behavior (i.e., blinks, fixations, and saccades), suggesting that different cognitive responses were elicited with different locomotion methods. Our research provides a viable tool for future studies evaluating locomotion methods, thus providing further in-depth insights for developing more effective and enjoyable VR locomotion methods.  © 2022 IEEE.",cognitive load; eye tracking; Human-centered computing-Human computer interaction (HCI)-Interaction paradigms-Virtual reality; locomotion methods; Virtual reality,Eye tracking; Human computer interaction; User interfaces; Virtual reality; Cognitive loads; Entertainment industry; Eye-tracking; Human-centered computing; Human-centered computing-human computer interaction -interaction paradigm-virtual reality; Interaction paradigm; Locomotion method; Motion sickness; Real-world; Users' experiences; Eye movements,Conference paper,Final,,Scopus,2-s2.0-85146055546,Gaming / VR
Susanty E.; Sijbrandij M.; van Dijk W.; Srisayekti W.; de Vries R.; Huizink A.C.,"Susanty, Eka (57222245862); Sijbrandij, Marit (14066824800); van Dijk, Willeke (57216354726); Srisayekti, Wilis (55834996600); de Vries, Ralph (57201342966); Huizink, Anja C. (6603105993)",57222245862; 14066824800; 57216354726; 55834996600; 57201342966; 6603105993,The effects of psychological interventions on neurocognitive functioning in posttraumatic stress disorder: a systematic review; [心理干预对创伤后应激障碍神经认知功能的影响：一项系统综述]; [Los efectos de intervenciones psicológicas en el funcionamiento neurocognitivo en Trastorno de Estrés Postraumático: Una revisión sistemática],2022,European Journal of Psychotraumatology,13,1,2071527,,,,15,10.1080/20008198.2022.2071527,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85131147342&doi=10.1080%2f20008198.2022.2071527&partnerID=40&md5=f7104599a0ab8aafdfbb220f0eb8523f,"Background: Posttraumatic stress disorder (PTSD) is a serious mental disorder, which is associated with emotional and cognitive functioning problems. Psychological interventions, such as trauma-focused cognitive behavioural therapy (tf-CBT) and eye movement desensitization and reprocessing (EMDR) are effective in reducing PTSD symptoms. Despite evidence showing that PTSD is associated with neurocognitive deficits, there is no systematic overview available on neurocognitive outcomes following treatment for PTSD. The current systematic review examined whether psychological treatments for PTSD improve neurocognitive functioning outcomes related to memory, attention, information processing, and executive functioning. Method: A literature search in PubMed, PsycINFO, PTSDpubs, and Cochrane Library was performed up to March 7, 2022, in collaboration with a medical information specialist. Eligible PTSD treatment studies examining neurocognitive outcomes (memory, attention, information processing and executive function) in patients with a DSM-IV or ICD diagnosis of PTSD were included. Results: Of the 3023 titles and abstracts identified, 9 articles met inclusion criteria, of which 5 randomized controlled trials (RCTs) and 4 non-randomized studies. Treatments included were cognitive behavioural therapy (CBT), cognitive processing therapy (CPT), brief eclectic psychotherapy (BEP), eye movement desensitization and reprocessing (EMDR), virtual reality graded exposure therapy (VR-GET), and resilience-oriented treatment (ROT). Conclusions: This systematic review showed that psychological treatments for PTSD do not affect most neurocognitive functions, with exception of the memory outcomes. Future research, high-quality studies are needed to provide evidence of the effect of psychological treatment in improving neurocognitive functioning in PTSD. HIGHLIGHTS: This systematic review investigated the effects of psychological treatments on neurocognitive functioning in adults with PTSD. This review showed that most studies were very heterogeneous in design, method, and analysis. This review supports the evidence for psychological treatments for PTSD on improving memory outcomes. © 2022 The Author(s). Published by Informa UK Limited, trading as Taylor & Francis Group.",attention; executive function; information processing; memory; Neurocognitive functioning; psychological treatment; PTSD,"Adult; Cognitive Behavioral Therapy; Eye Movement Desensitization Reprocessing; Humans; Psychosocial Intervention; Psychotherapy; Stress Disorders, Post-Traumatic; adult; cognitive behavioral therapy; eye movement desensitization and reprocessing; human; posttraumatic stress disorder; procedures; psychotherapy",Review,Final,All Open Access; Gold Open Access; Green Open Access,Scopus,2-s2.0-85131147342,Gaming / VR
Bian S.; Liu X.; Zhao H.; Gong X.; Jing S.,"Bian, Siyu (58164174500); Liu, Xiwei (55193285400); Zhao, Hongxia (56438159100); Gong, Xiaoyan (26651229400); Jing, Sifeng (55555982500)",58164174500; 55193285400; 56438159100; 26651229400; 55555982500,An Immersive Learning System with Multimodal Cognitive Processes Inference,2022,"Proceedings - 2022 Chinese Automation Congress, CAC 2022",2022-January,,,6633,6637,4.0,3,10.1109/CAC57257.2022.10055676,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85151290129&doi=10.1109%2fCAC57257.2022.10055676&partnerID=40&md5=5e18cad7c147b75a32709aa45405f773,"Using multimodal technology to infer learners' cognitive processes is an important means to support personalized learning. Previous cognitive process analysis is more about subjective reasoning, and less about combining physiological signals. With the rapid development of virtual reality technology, new requirements and more immersive schemes for learner's cognitive process inferring have been proposed. Based on this, this paper proposes an immersive learning system with multimodal cognitive processes inference, which is composed of macro, mesoscopic and micro environments. The system analyzes learner's actions, emotions and attention to infer learner's cognitive process through more comprehensive and objective multimodal data. The system also combines eye movement, brainwaves and other data, and proposes a multimodal data acquisition and analysis scheme using HMD-based VR. This paper also provides an immersive learning cognitive processes inference scheme for individual online learning, offline classroom learning scenarios.  © 2022 IEEE.",cognitive process; immersive learning; learning system; multimodal; virtual reality,Cognitive systems; Data acquisition; E-learning; Eye movements; Modal analysis; Physiological models; Virtual reality; Cognitive process; Immersive; Immersive learning; Macro-environment; Multi-modal; Multi-modal data; Personalized learning; Physiological signals; Process analysis; Virtual reality technology; Learning systems,Conference paper,Final,,Scopus,2-s2.0-85151290129,Gaming / VR
Huy T.H.N.; Carlon M.K.J.; Cross J.S.,"Huy, Tran Huu Nhat (57815916300); Carlon, May Kristine Jonson (57215307959); Cross, Jeffrey S. (7402349007)",57815916300; 57215307959; 7402349007,Simulated Gaze Tracking using Computer Vision for Virtual Reality,2022,"Proceedings of 2022 8th International Conference of the Immersive Learning Research Network, iLRN 2022",,,,,,,1,10.23919/iLRN55037.2022.9815891,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85134740386&doi=10.23919%2fiLRN55037.2022.9815891&partnerID=40&md5=f7edf13af4fecafa0c6b2bcdaa30d2eb,"As Virtual Reality (VR) is becoming more viable for creating immersive learning environments, a promising area of study is in monitoring and assessing learner's engagement. Several related works in this area have used gaze tracking. Unfortunately, gaze tracking in VR is currently limited by hardware availability. This paper presents an approach for engagement monitoring that does not require the traditional costs of more sophisticated hardware setup. A simple combination of video processing techniques was used to show which contents capture the user's attention, similar to the information current gaze trackers provide. However, success with this method was only evident with a sample video; refinements are still needed to cater to unpredictable changes in field of view. After fine-tuning the process, this approach may potentially benefit instructional VR content developers in determining which content grabs their learners' attention effectively. © 2022 Immersive Learning Research Network.",computer vision; eye tracking; gaze tracking; immersive learning environment; virtual reality,Computer aided instruction; Computer hardware; Computer vision; E-learning; Video signal processing; Virtual reality; Eye-tracking; Gaze-tracking; Immersive learning; Immersive learning environment; Learning environments; Processing technique; Related works; Simple++; User attention; Video processing; Eye tracking,Conference paper,Final,,Scopus,2-s2.0-85134740386,Gaming / VR
Reddy G.S.R.; Spencer C.A.; Durkee K.; Cox B.; Fox Cotton O.; Galbreath S.; Meyer S.; Natali M.; Seech T.; Severe-Valsaint G.; Zimmerman G.; Hirshfield L.,"Reddy, G. S. Rajshekar (57377147400); Spencer, Cara A. (57740523400); Durkee, Kevin (36025070300); Cox, Brennan (57209158496); Fox Cotton, Olivia (57219608708); Galbreath, Sheila (57741010900); Meyer, Sarah (57740846000); Natali, Michael (57208304461); Seech, Todd (55502164700); Severe-Valsaint, Gabriella (57741326200); Zimmerman, Gavin (57741165800); Hirshfield, Leanne (16303680200)",57377147400; 57740523400; 36025070300; 57209158496; 57219608708; 57741010900; 57740846000; 57208304461; 55502164700; 57741326200; 57741165800; 16303680200,Estimating Cognitive Load and Cybersickness of Pilots in VR Simulations via Unobtrusive Physiological Sensors,2022,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),13318 LNCS,,,251,269,18.0,12,10.1007/978-3-031-06015-1_18,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85131912797&doi=10.1007%2f978-3-031-06015-1_18&partnerID=40&md5=e9033b61640a32ea9fa8f14970c57abf,"Predicting real-time estimates of cognitive load in pilots assists intelligent flight systems in alleviating high workloads, thereby averting accidents and directly impacting safety in aviation. Virtual Reality (VR) flight simulations provide an immersive stage to evaluate physiological measures and identify their cognitive correlates. In this work, unobtrusive sensors such as eye-tracking, pupillometry, and photoplethysmography (PPG) record physiological data while six participants perform six flying tasks of varying complexity in VR. The extracted feature sets such as pupil diameter change, number of fixations and saccades, and heart rate variability (HRV) are compared to the Pilot Inceptor Workload (PIW) measures, specifically duty cycle and aggressiveness. The PIW, number of saccades, and the self-reported workload measures were significantly affected by the tasks. However, the number of saccades measure demonstrated a significant negative correlation with the PIW’s measures, contradicting prior work. The remaining feature sets, including the pupil diameter change and the number of fixations, display a nearly identical trend to the PIW measure, though no significance was detected. © 2022, Springer Nature Switzerland AG.",Aviation; Cognitive load; Cybersickness; Virtual reality,Cognitive systems; Eye tracking; Flight simulators; Physiological models; Real time systems; Virtual reality; Cognitive loads; Cybersickness; Features sets; Flight systems; Inceptor; Number of fixations; Physiological sensors; Pupil diameter; Real- time; Virtual reality simulations; Eye movements,Conference paper,Final,,Scopus,2-s2.0-85131912797,Gaming / VR
Chikha H.B.; Zoudji B.; Khacharem A.,"Chikha, Houssem Ben (58108865200); Zoudji, Bachir (6504620730); Khacharem, Aïmen (55628220800)",58108865200; 6504620730; 55628220800,Coaches’ pointing gestures as means to convey tactical information in basketball: an eye-tracking study,2022,International Journal of Sport and Exercise Psychology,22,1,,236,249,13.0,9,10.1080/1612197X.2022.2138498,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85141186046&doi=10.1080%2f1612197X.2022.2138498&partnerID=40&md5=0993d8e0f54edfdbd2a856aabc179d49,"The question of how information of play should be presented so that tactical memorisation is optimised is still under discussion. To explore how players’ visual attentional resources are allocated when learning from a game system in basketball, eye movements were registered in the absence and presence of coaches’ pointing gestures. Expert (N = 48) and novice (N = 48) participants watched a video in which the coach either (i) described the basketball game system without using pointing gestures or (ii) described the basketball game system while pointing to the relevant aspects of it. The results of analyses of variance showed that the coach’s pointing gestures improved novice players’ performance (they achieved better recall performances, invested less cognitive load and showed more efficient visual search). However, this effect disappeared in expert participants, indicating a partial expertise reversal effect. Findings suggest that the effectiveness of coach’s pointing gestures depends on levels of players’ expertise. © 2022 International Society of Sport Psychology.",expertise; eye tracking; Gesture; memory; pointing,,Article,Final,,Scopus,2-s2.0-85141186046,Gaming / VR
Chen P.; Mitake H.; Hasegawa S.,"Chen, Pinjung (57671808000); Mitake, Hirnori (14058759600); Hasegawa, Shoichi (8410343100)",57671808000; 14058759600; 8410343100,Gaze Capture based Considerate Behaviour Control of Virtual Guiding Agent,2022,"Proceedings - 2022 IEEE Conference on Virtual Reality and 3D User Interfaces Abstracts and Workshops, VRW 2022",,,,562,563,1.0,0,10.1109/VRW55335.2022.00132,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85129589970&doi=10.1109%2fVRW55335.2022.00132&partnerID=40&md5=0f9ea5b786dd0ceb28920fbb2fa93144,"Agents in VR have wide application like guidance. Most current agents are passive, so that people should suspend their current tasks and request agents with explicit demand. It is necessary to make agent more actively open the interaction naturally but without being bothering. We propose a virtual guidance agent which provide voice explanation in appropriate timing, using gaze tracking, attention amount estimation and attention driven state machine. We used time-decayed moving average of angle between gaze direction and face front direction. We implemented the method in VR and evaluated effectiveness in virtual guiding tour experimentally. © 2022 IEEE.",Human-centered computing; Human-centered computing; Treemaps; Visualization; Visualization; Visualization design and evaluation methods; Visualization techniques,Eye tracking; Virtual reality; 'current; Behavior control; Design and evaluation methods; Gaze-tracking; Human-centered computing; Treemap; Visualization design and evaluation method; Visualization designs; Visualization technique; Visualization,Conference paper,Final,,Scopus,2-s2.0-85129589970,Gaming / VR
Lu T.; Tang M.; Guo Y.; Zhou C.; Zhao Q.; You X.,"Lu, Tianjiao (57209296270); Tang, Menghan (57216561125); Guo, Yu (57201457400); Zhou, Chenchen (57195101524); Zhao, Qingxian (57480725200); You, Xuqun (36952116700)",57209296270; 57216561125; 57201457400; 57195101524; 57480725200; 36952116700,Effect of video game experience on the simulated flight task: the role of attention and spatial orientation,2022,Australian Journal of Psychology,74,1,,1,18,17.0,5,10.1080/00049530.2021.2007736,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85125911547&doi=10.1080%2f00049530.2021.2007736&partnerID=40&md5=7e2eb4906630ded46ebe5c08d6171746,"Objective: In this study, we investigated the advantages of attention allocation and spatial orientation among video game players (VGPs) and non-video game players (NVGPs) and explored the performance differences between the two groups in flight simulations. Method: Thirty candidates from a Chinese university were categorised as VGPs and NVGPs before participating in all tests. Results: The comparison of flight performance and eye movement indicators between the two groups showed that the flight performance of VGPs was significantly better than that of NVGPs. We then found that an attention shift task and group factors predicted flight performance during take-off, while spatial orientation and tracking tasks have direct and indirect effects on cruise task performance, respectively. Eye movement indicators can directly predict flight performance to a certain extent. Conclusion: We believe that the transfer effect of game experience in simulated flight tasks assisted VGPs in using top-down processing strategies in the flight process, and in better allocating cognitive resources. KEY POINTS What is already known about this topic: The advantages of attention allocation and spatial orientation among video game players (VGPs) and non-video game players (NVGPs). This difference not only caused by gaming experience, but also comes from the interaction of gaming experience and individual differences. The VGPs perform better on simulated flight mission. We speculated that gaming practice had assisted VGPs in using top-down processing strategies in the flight process, and in better allocating cognitive resources. VGPs did not depend solely on low-level cognitive abilities to finish tasks, but rather could rapidly understand the relevant situation and task in order to choose a proper attention allocation strategy, that visual search patterns must be adapted according to different situations, as opposed to reliance on a single parallel or sequence search. What this topic adds: The promotional effects of gaming experience occurred in high-level cognitive processing, but this effect was also observed in low-level ability tests. The key to promoting and maintaining excellent long-term performance are comprehensive cognitive skills, advanced cognitive skills, the establishment of mental models, and adaptive top-down processing strategies. The aviation training system design should not stop at elemental training to improve general cognitive abilities, but should be a comprehensive ability-training game in order to improve training efficiency for pilots. In order to establish a cognitive ability video game training system for pilots in the future, further exploration is needed to understand whether the transfer effect would occur in real world flight missions. © 2022 The Author(s). Published by Informa UK Limited, trading as Taylor & Francis Group.",eye movement indicators; flight performance; non-video game players; Video game players,,Article,Final,All Open Access; Gold Open Access,Scopus,2-s2.0-85125911547,Gaming / VR
Mirault J.,"Mirault, Jonathan (57200959324)",57200959324,The contribution of virtual reality to reading research; [L'APPORT DE LA RÉALITÉ VIRTUELLE POUR LES RECHERCHES SUR LA LECTURE],2022,Annee Psychologique,122,4,,687,702,15.0,0,10.3917/anpsy1.224.0687,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85143862260&doi=10.3917%2fanpsy1.224.0687&partnerID=40&md5=6b746a4bab0c1dd05911455042db86a3,"The great developpement of the virtual reality these last years gives us hope of an integration in numerous domains like scientific research. In the field of cognitive psychology, the use of this new methodology allows to greatly increase the attention of the participants during experiments. A surprising concerned domain by virtual reality remains Language's sciences. Indeed, text presentation in virtual environments pasted on the real world allows to better understand cognitive processes that subtend Reading in ecologically valid places while controling various variables. This new technology of investigation has been recently used in studies with children (lexical decision) and in order to record eye movements during reading tasks. © 2022 Presses Universitaires de France. All rights reserved.",cognition; immersion; reading; virtual reality,article; attention; child; cognition; cognitive psychology; eye movement; female; human; human experiment; immersion; language; male; reading test; virtual reality,Article,Final,All Open Access; Green Open Access,Scopus,2-s2.0-85143862260,Gaming / VR
Mendez-Encinas D.; Sujar A.; Delgado-Gómez D.,"Mendez-Encinas, David (58027691200); Sujar, Aaron (57202589141); Delgado-Gómez, David (16202707400)",58027691200; 57202589141; 16202707400,Assessment of attention through user's performance in a virtual reality game,2022,CEUR Workshop Proceedings,3305,,,,,,0,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85144592389&partnerID=40&md5=5cade568f46e79b007955a1bf39725b9,"Serious games can help traditional evaluation methods by creating an objective system. There are computerised tests that measure user performance through reaction time, hits, and misses but they are focused on discrete variables. A virtual reality game has been developed where the user must maintain attention to get a target fish among non-target fishes. The game has been divided into sequences where speeds and interstimulus times vary uniformly. Distractors have been included in some sequences. The game through the device records variables related to the user's movement, eyes movement, and performance. The variables are recorded continuously. Random forest regressor was used to infer the Attention Control Scale Test with the variables recorded. Although the sample is small, it has been found that errors made with and without distractors, together with reaction time are predictors related to the score obtained in the test. Other variables like eye gaze also suggest a correlation with the attention control scale score. Virtual reality applications and new devices can help in the assessment of psychological variables. © 2022 Copyright for this paper by its authors. Use permitted under Creative Commons License Attribution 4.0 International (CC BY 4.0)",Attention; CPT; Regression; Video games; Virtual reality,Eye movements; Human computer interaction; Serious games; Virtual reality; Attention; Attention control; Computerized tests; CPT; Creative Commons; Evaluation methods; Hit and miss; Regression; User performance; Video-games; Fish,Conference paper,Final,,Scopus,2-s2.0-85144592389,Gaming / VR
Lim H.; Kim S.; Ku J.,"Lim, Hyunmi (57201758403); Kim, Sungmin (57835343800); Ku, Jeonghun (7102897693)",57201758403; 57835343800; 7102897693,Distraction Classification During Target Tracking Tasks Involving Target and Cursor Flickering Using EEGNet,2022,IEEE Transactions on Neural Systems and Rehabilitation Engineering,30,,,1113,1119,6.0,6,10.1109/TNSRE.2022.3168829,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85128670580&doi=10.1109%2fTNSRE.2022.3168829&partnerID=40&md5=fc8efe32ead9bc1da6f0b5aad70d3358,"Keeping patients from being distracted while performing motor rehabilitation is important. An EEG-based biofeedback strategy has been introduced to help encourage participants to focus their attention on rehabilitation tasks. Here, we suggest a BCI-based monitoring method using a flickering cursor and target that can evoke a steady-state visually evoked potential (SSVEP) using the fact that the SSVEP is modulated by a patient's attention. Fifteen healthy individuals performed a tracking task where the target and cursor flickered. There were two tracking sessions, one with and one without flickering stimuli, and each session had four conditions in which each had no distractor (non-D), a visual (vis-D) or cognitive distractor (cog-D), and both distractors (both-D). An EEGNet was trained as a classifier using only non-D and both-D conditions to classify whether it was distracted and validated with a leave-one-subject-out scheme. The results reveal that the proposed classifier demonstrates superior performance when using data from the task with the flickering stimuli compared to the case without the flickering stimuli. Furthermore, the observed classification likelihood was between those corresponding to the non-D and both-D when using the trained EEGNet. This suggests that the classifier trained for the two conditions could also be used to measure the level of distraction by windowing and averaging the outcomes. Therefore, the proposed method is advantageous because it can reveal a robust and continuous level of patient distraction. This facilitates its successful application to the rehabilitation systems that use computerized technology, such as virtual reality to encourage patient engagement.  © 2001-2011 IEEE.",Attention; brain-computer interface; deep learning; flickeringcursor and target; neurofeedback; steadystate visually evoked potential,"Attention; Brain-Computer Interfaces; Electroencephalography; Evoked Potentials; Evoked Potentials, Visual; Humans; Photic Stimulation; Biofeedback; Brain; Clutter (information theory); Deep learning; Electroencephalography; Electrophysiology; Patient rehabilitation; Virtual reality; Attention; Brain modeling; Condition; Deep learning; Flickering cursor and target; Motor rehabilitation; Neurofeedback; Steady-state visually evoked potential; Targets tracking; Task analysis; adult; Article; averaging; biofeedback; cognition; computer model; data analysis; deep learning; electroencephalography; eye movement; eye tracking; female; human; information processing; local field potential; machine learning; male; monitoring; motor activity; neurofeedback; normal human; signal processing; task performance; training; validation process; virtual reality; attention; brain computer interface; electroencephalography; evoked response; photostimulation; procedures; visual evoked potential; Brain computer interface",Article,Final,All Open Access; Gold Open Access,Scopus,2-s2.0-85128670580,Gaming / VR
Dong W.; Qin T.; Yang T.; Liao H.; Liu B.; Meng L.; Liu Y.,"Dong, Weihua (22233370800); Qin, Tong (57212512374); Yang, Tianyu (57209027567); Liao, Hua (56059959800); Liu, Bing (57195617998); Meng, Liqiu (56002214900); Liu, Yu (55742311000)",22233370800; 57212512374; 57209027567; 56059959800; 57195617998; 56002214900; 55742311000,Wayfinding Behavior and Spatial Knowledge Acquisition: Are They the Same in Virtual Reality and in Real-World Environments?,2022,Annals of the American Association of Geographers,112,1,,226,246,20.0,47,10.1080/24694452.2021.1894088,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85105215110&doi=10.1080%2f24694452.2021.1894088&partnerID=40&md5=52c3442cfcdec74dc1ddcc6ad46801fe,"Finding one’s way is a fundamental daily activity and has been widely studied in the field of geospatial cognition. Immersive virtual reality (iVR) techniques provide new approaches for investigating wayfinding behavior and spatial knowledge acquisition. It is currently unclear, however, how wayfinding behavior and spatial knowledge acquisition in iVR differ from those in real-world environments (REs). We conducted an RE wayfinding experiment with twenty-five participants who performed a series of tasks. We then conducted an iVR experiment using the same experimental design with forty participants who completed the same tasks. Participants’ eye movements were recorded in both experiments. In addition, verbal reports and postexperiment questionnaires were collected as supplementary data. The results revealed that individuals’ wayfinding performance is largely the same between the two environments, whereas their visual attention exhibited significant differences. Participants processed visual information more efficiently in RE but searched visual information more efficiently in iVR. For spatial knowledge acquisition, participants’ distance estimation was more accurate in iVR compared with RE. Participants’ direction estimation and sketch map results were not significantly different, however. This empirical evidence regarding the ecological validity of iVR might encourage further studies of the benefits of VR techniques in geospatial cognition research. © 2021 by American Association of Geographers.",immersive virtual reality; indoor wayfinding; spatial knowledge acquisition; visual attention; wayfinding behavior,cognition; experimental design; knowledge; questionnaire survey; research; spatial analysis; virtual reality,Article,Final,,Scopus,2-s2.0-85105215110,Gaming / VR
Kosmyna N.; Maes P.,"Kosmyna, Nataliya (56406489500); Maes, Pattie (56268463300)",56406489500; 56268463300,"Assessing internal, external and covert visuospatial attention in AR using brain sensing: a pilot study",2022,Progress in Biomedical Optics and Imaging - Proceedings of SPIE,11931,,1193106,,,,0,10.1117/12.2613493,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85131218567&doi=10.1117%2f12.2613493&partnerID=40&md5=7757f8fa9c1974f8f73eb7d192fa84b6,"Most research works featuring AR and Brain-Computer Interface (BCI) systems are not taking advantage of the opportunities to integrate the two planes of data. In this project, we propose a prototype which combines an existing AR headset HoloLens 2 with a Brain-Computer Interfaces (BCI) system and we perform several tasks to validate this concept. In the first experiment, we reduced the distraction of the user by including information about the current attentional state. A simple game was designed for the Microsoft HoloLens 2, which changed in real time according to the user's state of attention measured via electroencephalography (EEG). The system only responded if the attentional orientation was classified as ""external.""Fourteen users tested the attention-aware system; we show that the augmentation of the interface improved the usability of the system. We conclude that more systems would benefit from clearly visualizing the user's ongoing attentional state as well as further efficient integration of AR and BCI headsets. In the second prototype we propose a system based on covert visuospatial attention (CVSA) - the process of focusing attention on different regions of the visual field without overt eye movements. In the experiment we did not rely on any stimulusdriven responses. Fourteen participants were able to test the system over the course of two days. To the best of our knowledge, this system is the first AR EEG-BCI integrated prototype that explores the complementary features of the AR headset like the HoloLens 2 and the CVSA paradigm. © 2022 SPIE.",AR; attention; BCI; covert; EEG; external attention; headset; HMD; Hololens2; internal attention,Brain computer interface; Electroencephalography; Eye movements; Helmet mounted displays; 'current; AR; Attention; Covert; External attention; Headset; Hololens2; Interface system; Internal attention; Pilot studies; Electrophysiology,Conference paper,Final,,Scopus,2-s2.0-85131218567,Gaming / VR
Dzardanova E.; Kasapakis V.,"Dzardanova, Elena (57195961951); Kasapakis, Vlasios (55854154600)",57195961951; 55854154600,Preliminary evaluation of an IVR user experience design model using eye-tracking attention measurements,2022,"Proceedings - 2022 IEEE Conference on Virtual Reality and 3D User Interfaces Abstracts and Workshops, VRW 2022",,,,822,823,1.0,1,10.1109/VRW55335.2022.00262,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85129627019&doi=10.1109%2fVRW55335.2022.00262&partnerID=40&md5=12274e47b61f55acdfac2212d8ad7f53,"The present study drafts a simplified IVR user experience design model to guideline a preliminary evaluation of attention variance for semantically distinct elements. 27 participants (11 females) freely explored an interactive multi-user virtual setting, whilst equipped with full-body motion support and eye-tracking which procured attention duration measurements. Initial results confirm significant element attention discrepancy and provide the first indication toward a more detailed categorical organization of experience components for follow-up experimentation. © 2022 IEEE.","Eye Tracking; Human computer interaction (HCI)-HCI theory, concepts and models; Human-centered computing; User Experience; Virtual Reality","Computation theory; Eye tracking; Human computer interaction; User interfaces; Concept and model; Eye-tracking; Human computer interaction -human computer interaction theory, concept and model; Human-centered computing; Interaction concepts; Interaction modeling; Interaction theory; Theory concept; Theory model; Users' experiences; Virtual reality",Conference paper,Final,,Scopus,2-s2.0-85129627019,Gaming / VR
Souchet A.D.; Philippe S.; Lourdeaux D.; Leroy L.,"Souchet, Alexis D. (57205639297); Philippe, Stéphanie (57205640104); Lourdeaux, Domitile (6507194079); Leroy, Laure (19640647700)",57205639297; 57205640104; 6507194079; 19640647700,Measuring Visual Fatigue and Cognitive Load via Eye Tracking while Learning with Virtual Reality Head-Mounted Displays: A Review,2022,International Journal of Human-Computer Interaction,38,9,,801,824,23.0,129,10.1080/10447318.2021.1976509,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85116344964&doi=10.1080%2f10447318.2021.1976509&partnerID=40&md5=26e317b23e6e9af9973d2ad2d016c559,"Virtual Reality Head-Mounted Displays (HMDs) reached the consumer market and are used for learning purposes. Risks regarding visual fatigue and high cognitive load arise while using HMDs. These risks could impact learning efficiency. Visual fatigue and cognitive load can be measured with eye tracking, a technique that is progressively implemented in HMDs. Thus, we investigate how to assess visual fatigue and cognitive load via eye tracking. We conducted this review based on five research questions. We first described visual fatigue and possible cognitive overload while learning with HMDs. The review indicates that visual fatigue can be measured with blinks and cognitive load with pupil diameter based on thirty-seven included papers. Yet, distinguishing visual fatigue from cognitive load with such measures is challenging due to possible links between them. Despite measure interpretation issues, eye tracking is promising for live assessment. More researches are needed to make data interpretation more robust and document human factor risks when learning with HMDs. © 2021 Taylor & Francis Group, LLC.",,E-learning; Eye tracking; Virtual reality; Cognitive loads; Cognitive overload; Consumer market; Eye-tracking; Fatigue loads; Head-mounted-displays; Learning efficiency; Pupil diameter; Research questions; Visual fatigue; Helmet mounted displays,Review,Final,,Scopus,2-s2.0-85116344964,Gaming / VR
Saedi H.; Rice A.,"Saedi, Hossein (57750806900); Rice, Arthur (57750554900)",57750806900; 57750554900,A Deeper Understanding of the Impact on the Restorative Quality of Green Environments as Related to the Location and Duration of Visual Interaction,2022,Journal of Digital Landscape Architecture,2022,7,,412,424,12.0,1,10.14627/537724040,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85132284914&doi=10.14627%2f537724040&partnerID=40&md5=10e9d6e6cc95d0522a004b312aa38fdb,"This exploratory research aims to examine if the existence of vegetation in indoor environments affects people's attention levels. Also, do those elements receive more visual attention, or do they cause other elements in the space to stand out more visually? During experimental research, 182 resi-dents of a high-rise residential building were randomly assigned to experience one of two versions of their building's lobby as a 3D virtual reality (VR) environment, one with and one without vegetation. Participants completed the Sustained Attention to Response Task (SART) twice, once before experi-encing the lobby to establish a baseline of attention and once after, to assess possible improvement. Also, the amount of visual attention that each element received in the environment was calculated through gaze tracking. Results indicated that participants who experienced the lobby with vegetation showed meaningful improvement in their SART score. The gaze tracking heatmaps revealed that vegetation received significantly higher attention than all other elements. The analysis of the data demonstrated that those who gained the highest scores paid the highest amount of visual attention to vegetation. These findings suggest a positive relationship between the location and duration of visual attention and attention restoration level and that vegetation capturing non-voluntary attention may be among the major factors that positively impact attention restoration. © Wichmann Verlag, VDE VERLAG GMBH · Berlin.",attention restoration; gaze tracking; Immersive Virtual Reality environment (IVRE); sustained attention; vegetation,,Article,Final,,Scopus,2-s2.0-85132284914,Gaming / VR
Zhong M.; Shen L.,"Zhong, Ma (55352493600); Shen, Li (57890247900)",55352493600; 57890247900,Learning to Predict What Humans Look at: Computational Visual Attention Model for Specific Category,2022,"Proceedings - 2022 International Symposium on Electrical, Electronics and Information Engineering, ISEEIE 2022",,,,1,7,6.0,1,10.1109/ISEEIE55684.2022.00008,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85138077601&doi=10.1109%2fISEEIE55684.2022.00008&partnerID=40&md5=9205762f0116644b6276c9a54e16332d,"Humans can easily find a specific category of objects in a complex scene, simulating this mechanism has both scientific and economic impact. The existing research usually focuses on where humans will look, overlooking the effects of the category of objects on visual attention. In this paper, we proposed a computational visual model aimed at mimicking human visual attention while they were looking for a specific category of objects. First, we proposed a machine learning-based Probabilistic Latent Semantic Analysis (PLSA) model to recognize the generic category of objects and find their locations. The PLSA model parameters are learned in a supervised manner. Then, the category and location information of objects are used as high-level information, together with low-level features of the same objects, to train a Support Vector Regression (SVR) model. With this SVR model, we can predict the visual attention for a specific category. To demonstrate the effectiveness and applicability of the proposed method, we conducted comprehensive experiments. The results show that the model can provide a reasonable estimate for human visual attention under the visual search task for a specific category. Besides, we evaluated the proposed model's ability to discover a specific category of objects and their locations on the natural image dataset. The results show that besides facilitating the human visual attention mechanism research, the proposed model can also be applied to find a specific category of objects from a set of images.  © 2022 IEEE.",eye tracking; machine learning; visual attention; visual search,Behavioral research; Location; Machine learning; Semantics; Complex scenes; Economic impacts; Eye-tracking; Human visual attention; Machine-learning; Probabilistic latent semantic analysis model; Support vector regression models; Visual Attention; Visual attention model; Visual search; Eye tracking,Conference paper,Final,,Scopus,2-s2.0-85138077601,Gaming / VR
Jiang T.; Goh J.; Xu Z.; Zheng N.; Fang Y.,"Jiang, Tanghan (57758917000); Goh, Jiantsen (57209798281); Xu, Zheng (57220859583); Zheng, Nan (55253878400); Fang, Yihai (55648722300)",57758917000; 57209798281; 57220859583; 55253878400; 55648722300,Analyzing the impact of simulation fidelity on VR-enabled hazard detection on construction sites - A case study on crane lift operation,2022,IOP Conference Series: Earth and Environmental Science,1101,9,92021,,,,4,10.1088/1755-1315/1101/9/092021,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85144140134&doi=10.1088%2f1755-1315%2f1101%2f9%2f092021&partnerID=40&md5=c73da68516046e69108cf62f6bca371e,"Crane operator training is an essential part of construction safety and is attracting extensive attention from researchers worldwide. Virtual reality (VR) is considered an effective tool to improve training outcomes by providing users with an immersive, risk-free experience in various environments. However, previous VR-based training platforms mainly focused on the scenario and task design; few studies attempted to investigate the impact of simulation fidelity on training efficiency. This research aims to explore the effect of simulation fidelity on training outcomes by comparing user performance in two scenarios. A typical construction site was modelled in a game engine using two rendering approaches; an eye-tracking system was adopted for data collection. The results from a subject experiment indicated the high efficiency of VR in operator safety training and demonstrated the usefulness of eye-tracking in measuring hazard detection performance. Findings showed that a higher level of simulation fidelity might not significantly improve the training efficiency, especially in hazard detection aspects. © Published under licence by IOP Publishing Ltd.",,,Conference paper,Final,All Open Access; Gold Open Access,Scopus,2-s2.0-85144140134,Gaming / VR
Guan H.; Chao C.; Shuangjie D.; Yihan W.,"Guan, Huang (57681945000); Chao, Chen (57747644400); Shuangjie, Deng (57748620700); Yihan, Wang (59783835300)",57681945000; 57747644400; 57748620700; 59783835300,Research on the Effect of Pre-problems on the Learning Effect of Declarative Knowledge in VR Videos,2022,"2022 10th International Conference on Information and Education Technology, ICIET 2022",,,,70,73,3.0,0,10.1109/ICIET55102.2022.9778955,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85132155133&doi=10.1109%2fICIET55102.2022.9778955&partnerID=40&md5=87a2b8f78d83ea126157dda089777d63,"In order to explore whether pre-embedding can better support learning in VR videos, this paper takes 30 college students as the research object, and conducts a single-factor randomized experiment of 'pre-embedding problems and non-embedding problems in VR videos' The results show that pre-problems can effectively increase the time learners pay attention to the corresponding learning content, have a positive impact on learning retention and long-Term memory, and increase the cognitive load in the learning process and have no significant impact on knowledge transfer. The research conclusions of this article can provide reference for the development of VR video teaching resources. © 2022 IEEE.",eye tracking; learning effect; preliminary questions; VR video,Eye tracking; Knowledge management; Students; College students; Declarative knowledge; Embedding problems; Embeddings; Eye-tracking; Learning effects; Preliminary question; Research object; Support learning; VR video; Embeddings,Conference paper,Final,,Scopus,2-s2.0-85132155133,Gaming / VR
Cutting J.; Cairns P.,"Cutting, Joe (57197781877); Cairns, Paul (35228796000)",57197781877; 35228796000,Investigating game attention using the Distraction Recognition Paradigm,2022,Behaviour and Information Technology,41,5,,981,1001,20.0,7,10.1080/0144929X.2020.1849402,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85090980991&doi=10.1080%2f0144929X.2020.1849402&partnerID=40&md5=178155d765d781ca89176b0b559c73b6,"Digital games are well known for holding players’ attention and stopping them from being distracted by events around them. Being able to quantify how well games hold attention provides a behavioral foundation for measures of game engagement and a link to existing research on attention. We developed a new behavioral measure of how well games hold attention, based on players’ post-game recognition of irrelevant distractors which are shown around the game. This is known as the Distractor Recognition Paradigm (DRP). In two studies we show that the DRP is an effective measure of how well self-paced games hold attention. We show that even simple self-paced games can hold players’ attention completely and the consistency of attentional focus is moderated by game engagement. We compare the DRP to existing measures of both attention and engagement and consider how practical it is as a measure of game engagement. We find no evidence that eye tracking is a superior measure of attention to distractor recognition. We discuss existing research on attention and consider implications for areas such as motivation to play and serious games. © 2020 Informa UK Limited, trading as Taylor & Francis Group.",attention; distraction; engagement; Games; self-paced,Eye tracking; Behavioral measures; Digital games; Effective measures; article; attention; eye tracking; human; motivation; Serious games,Article,Final,All Open Access; Green Open Access,Scopus,2-s2.0-85090980991,Gaming / VR
(Abdolreza) Oboudi B.; Elahi A.; Akbari Yazdi H.; Pyun D.Y.,"(Abdolreza) Oboudi, Behnam (57834922400); Elahi, Alireza (57207882270); Akbari Yazdi, Hossein (57834538600); Pyun, Do Young (26644414600)",57834922400; 57207882270; 57834538600; 26644414600,Impacts of game attractiveness and color of message on sport viewers' attention to prosocial message: an eye-tracking study,2022,"Sport, Business and Management: An International Journal",,,,,,,6,10.1108/SBM-11-2021-0143,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85135581730&doi=10.1108%2fSBM-11-2021-0143&partnerID=40&md5=f6ec69c9b647eb1521ff27659c801d0d,"Purpose: In recent years, neurophysiological tools have been vastly applied in sport marketing research. Eye tracking, a pervasive sensor technology, has received a growing interest to examine the effects of advertising through sport on viewer attention. While there is a plethora of evidence in advertising that supports the positive effects of various advertising types and locations on viewer attention in various sport contexts, little is known about the role of a prosocial overlay ad on viewer attention when watching televised football matches. Therefore, this research aims to examine the differences in viewers' attention (i.e. fixation and duration) with regard to game attractiveness and colors of the prosocial message during televised football matches. Design/methodology/approach: To identify the research gap, the authors first reviewed the relevant sport marketing and neuroscience research on advertising effectiveness. The authors selected a prosocial message displayed. Adopting an experimental research design and using eye tracking, this study examined the impacts of game attractiveness and colors of message on viewer attention to the prosocial message displayed on an overlay advertisement during a football match. Findings: The authors found that the colors of prosocial messages and game attractiveness had significant effects on viewer attention to the prosocial message. Originality/value: In this study, the authors sought to add advertisement color, as well as game attractiveness, to the extant knowledge in marketing literature as effective advertising factors in capturing viewers' attention. These variables can offer marketers new insights in designing effective advertisements for the context of televised sports events in a specialized field. © 2022, Emerald Publishing Limited.",Advertisement effectiveness; Eye tracking; Social responsibility; Sport advertisement,,Article,Article in press,All Open Access; Green Open Access,Scopus,2-s2.0-85135581730,Gaming / VR
Souchet A.D.; Xie W.; Lourdeaux D.,"Souchet, Alexis D. (57205639297); Xie, Weifei (57671220900); Lourdeaux, Domitile (6507194079)",57205639297; 57671220900; 6507194079,"Distinguishing Visual Fatigue, Mental Workload and Acute Stress in Immersive Virtual Reality with Physiological Data: pre-test results",2022,"Proceedings - 2022 IEEE Conference on Virtual Reality and 3D User Interfaces Abstracts and Workshops, VRW 2022",,,,720,721,1.0,3,10.1109/VRW55335.2022.00211,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85129611826&doi=10.1109%2fVRW55335.2022.00211&partnerID=40&md5=29af48c3bf3a491de2fd5229d92089ed,"Virtual Reality Induced Symptoms and Effects (VRISE) can arise. Experimental paradigms are heterogeneous to assess them, and var-ious factors can induce physiological variations. Therefore, we developed a Stroop task to study and distinguish VRISE. We use eye tracking, ECG, EDA, and VRSQ, NASA-TLX, and STAI-6 questionnaires. Pre-tests have been conducted with 6 subjects exposed to 4 experimental conditions: control, dual task, stressful, and stere-oscopy. Subjects report different subjective visual fatigue and mental workload but not stress between conditions. Several physiological features are different between conditions. A VRISE detector can be envisioned based on physiological data and questionnaires as an index. © 2022 IEEE.",Human-centered computing-Human computer interaction (HCI)-HCI design and evaluation methods-Laboratory experiments,Eye tracking; Human computer interaction; NASA; Physiological models; Physiology; Surveys; Design and evaluation methods; Human-centered computing; Human-centered computing-human computer interaction -human computer interaction design and evaluation method-laboratory experiment; Human-computer-interaction designs; Interaction design methods; Interaction evaluations; Laboratory experiments; Mental workload; Methods:laboratory; Visual fatigue; Virtual reality,Conference paper,Final,,Scopus,2-s2.0-85129611826,Gaming / VR
Kim N.; Ahn C.R.; Miller A.; Dibello R.; Lobello D.; Oh S.; McNamara A.,"Kim, Namgyun (57218884102); Ahn, Changbum R. (55417817400); Miller, Austin (57644098300); Dibello, Robert (57644098400); Lobello, Daniel (57646326500); Oh, Somyung (57192991456); McNamara, Ann (35253845600)",57218884102; 55417817400; 57644098300; 57644098400; 57646326500; 57192991456; 35253845600,Enhancing Workers Vigilance to Electrical Hazards through a Virtually Simulated Accident,2022,"Construction Research Congress 2022: Health and Safety, Workforce, and Education - Selected Papers from Construction Research Congress 2022",4-D,,,651,659,8.0,5,10.1061/9780784483985.066,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85128915852&doi=10.1061%2f9780784483985.066&partnerID=40&md5=8778779e5a9ee062dd5ab8ee7b75d8fc,"Electrocution is one of the major causes of fatalities in the construction industry. Despite periodic safety training aimed at retaining workers' vigilance (i.e., sustained attention) to electrical hazards, workers tend to fail to maintain vigilance toward frequent encounters with electrical hazards. Providing an effective intervention that restores workers' vigilance is thus critical to reducing electrocution accidents. To this end, this study proposes a Virtual Reality (VR) safety training environment that exposes workers to repeated electrical hazards and simulates an electrocution accident when workers come in contact with the hazards. A pilot experiment was conducted, and participants' vigilance (i.e., eye fixations on the hazard) was measured using eye-Tracking sensors. The results reveal the potential effect of experiencing VR-simulated electrocution on enhancing workers' vigilance to electrical hazards. The outcomes of this study will lay the foundation for further studies to employ VR as a safety training environment that allows workers to experience a simulated electrocution, thereby contributing to a potential reduction in fatal electrocutions. © 2022 ASCE.",,Accidents; Construction industry; Eye tracking; Hazards; Occupational risks; Electrical hazards; Eye fixations; Eye-tracking sensors; Fatal electrocutions; Pilot experiment; Potential effects; Potential reduction; Safety training; Sustained attention; Workers'; Virtual reality,Conference paper,Final,,Scopus,2-s2.0-85128915852,Gaming / VR
Zou S.; Hu X.; Ban Y.; Warisawa S.,"Zou, Shangyin (57216461455); Hu, Xianyin (57216456819); Ban, Yuki (53866114000); Warisawa, Shin'Ichi (6603601285)",57216461455; 57216456819; 53866114000; 6603601285,Simulating Olfactory Cocktail Party Effect in VR: A Multi-odor Display Approach Based on Attention,2022,"Proceedings - 2022 IEEE Conference on Virtual Reality and 3D User Interfaces, VR 2022",,,,474,482,8.0,7,10.1109/VR51125.2022.00067,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85129453625&doi=10.1109%2fVR51125.2022.00067&partnerID=40&md5=a14c0c377b1f753812b3b2daa680a379,"Odor display has been a popular approach in virtual reality (VR) to enhance users' multi-sensory experience. The existing multi-odor presentation methods in VR are mostly based on spatiality of scent sources to produce mixed scents, which will possibly compromise users' olfactory experience because humans normally have poor ability to analyze distinct odorant components from a mixture. To tackle this problem, we present a VR multi-odor display approach that dynamically changes the intensity combinations of different scent sources in the virtual environment according to the user's attention, hence simulating a virtual cocktail party effect of smell. We acquire the user's gaze information as attention from the eye-tracking sensors embedded in the head mounted display (HMD), and increase the display intensity of the scent that the user is focusing on to simulate the cocktail party effect of smell, enabling the user to distinguish their desired scent source. We conducted a user study to validate the perception and experience of 2 ways of intensity settings in response to the user's attention shift, which were a strong level of focused scent mixed with weak levels of non-focused scents and strong focused scent only. The results showed that both of the two intensity settings were able to improve olfactory experience in VR compared to the non-dynamic odor display method. Meanwhile, only the method of presenting strengthened focused scent while maintaining the weaker mixture of background scents showed significant improvement on simulating the olfactory cocktail party effect by giving the users an enhanced sense of their own olfactory sensitivity.  © 2022 IEEE.",attention; odor presentation; virtual reality,Eye tracking; Helmet mounted displays; Mixtures; Odors; Attention; Attention shifts; Cocktail party effects; Eye-tracking sensors; Head-mounted-displays; Multi-Sensory; Odor presentations; Sensory experiences; User attention; User study; Virtual reality,Conference paper,Final,,Scopus,2-s2.0-85129453625,Gaming / VR
Khokhar A.; Borst C.W.,"Khokhar, Adil (57210910170); Borst, Christoph W. (9736479200)",57210910170; 9736479200,Evaluating Modifying Teacher Avatar Clip Sequencing Based on Eye-Tracked Visual Attention in Educational VR,2022,"Proceedings - 2022 IEEE International Symposium on Mixed and Augmented Reality Adjunct, ISMAR-Adjunct 2022",,,,613,614,1.0,0,10.1109/ISMAR-Adjunct57072.2022.00127,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85146054738&doi=10.1109%2fISMAR-Adjunct57072.2022.00127&partnerID=40&md5=6c5c65d47b789c55cf6d684470c38752,"We evaluate a novel virtual reality (VR) avatar animation architecture that is responsive to a viewer's eye gaze and we demonstrate that it can be used to improve teacher agents that are based on the playback of a recorded teacher. Our architecture modifies the playing, seeking, rewinding, stopping, and pausing of a teacher recording to create appropriate teacher avatar behavior based on a viewer's eye-tracked visual attention. We present preliminary results from our experiment that evaluated how subjects perceived three different teacher avatar behavioral responses.  © 2022 IEEE.",HCI design and evaluation methods; Human computer interaction (HCI); Human-centered computing; Interaction paradigms; User studies; Virtual reality,Behavioral research; Eye tracking; Human computer interaction; User interfaces; Design and evaluation methods; Human computer interaction; Human computer interaction design and evaluation method; Human-centered computing; Human-computer-interaction designs; Interaction design methods; Interaction evaluations; Interaction paradigm; Teachers'; User study; Virtual reality,Conference paper,Final,,Scopus,2-s2.0-85146054738,Gaming / VR
Nelson J.B.; Navarro A.; Balea P.; Sanjuan M.D.C.,"Nelson, James Byron (7404794993); Navarro, Anton (8253506800); Balea, Paula (57204038768); Sanjuan, Maria del Carmen (10141196500)",7404794993; 8253506800; 57204038768; 10141196500,The Effects of Stimulus Pre-Exposure and Conditioning on Overt Visual Attention,2022,Journal of Experimental Psychology: Animal Learning and Cognition,48,1,,29,45,16.0,4,10.1037/xan0000313,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124445730&doi=10.1037%2fxan0000313&partnerID=40&md5=99b2c63b999cd769013083d886808359,"Three experiments (a, b, c) combined to provide a well-powered examination of the effects of stimulus pre-exposure and conditioning on visual attention using an eye tracker and a space-shooter video game where a colored flashing light predicted an attacking spaceship. In each, group “control” received no pre-exposure to the light, group “same” received pre-exposure in the same context as conditioning, and group “different” received pre-exposure in a different context. Experiments differed in visual details regarding the game (1a vs. 1b and 1c) or minor details in the setup of the eye tracker (1a and 1b vs. 1c). Overall, pre-exposure retarded acquisition of keyboard responding. That effect was enhanced, rather than attenuated, by a context change. Separating participants by sign and goal trackers showed the context change enhanced the pre-exposure effect in goal trackers and reduced it in sign trackers. Visual attention to the light declined during pre-exposure and did not recover with either conditioning or a context switch. Visual attention to the light decreased during conditioning. Visual goal tracking toward where the spaceship would appear was also retarded with pre-exposure. Unlike the keyboard responding, a context change led to more normal goal-tracking acquisition. Results are discussed in terms of theories of attention and the potential effects of demand characteristics on the task. © 2022 American Psychological Association",Eye tracking; Goal tracking; Latent inhibition; Sign tracking; Visual attention,"Conditioning, Classical; Humans; Motivation; Psychomotor Performance; conditioned reflex; human; motivation; psychomotor performance",Article,Final,,Scopus,2-s2.0-85124445730,Gaming / VR
,,,"2nd International Conference on Advanced Research in Technologies, Information, Innovation and Sustainability, ARTIIS 2022",2022,Communications in Computer and Information Science,1675 CCIS,,,,,986.0,0,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85144201598&partnerID=40&md5=5d8dc7dcbd2189bf5d1e3b0082f27649,"The proceedings contain 72 papers. The special focus in this conference is on Advanced Research in Technologies, Information, Innovation and Sustainability. The topics include: A Control Problem with Passive Particles Driven by Point Vortices on the Sphere; the Dangers of Gamification; mobile Applications in Tourism: A Tale of Two Perspectives; accessibility Study in Hotels in the City of Quito; a Comprehensive Study on Remote Laboratories Based on Pneumatic and Electro-Pneumatic Systems; Augmented Reality in Clothing Consumer Customization in COVID-19 Pandemic: A Preliminary Study; identification of Mango Fruit Maturity Using Robust Industrial Devices and Open-Source Devices Applying Artificial Vision; FHSS Classification System in the Spectrum Using SDR Generators for Signal Inhibitors; Front-End Framework to Improve HCI, Evaluated Using an Eye-Tracking; GraphQL or REST for Mobile Applications?; advanced Exploratory Data Analysis for Moroccan Shopping Places in TripAdvisor; content Richness, Perceived Price, and Perceived Ease of Use in Relation to the Satisfaction Level and Brand Equity in Streaming Platforms; augmented Virtual Reality in Data Visualization; big Data Analytics to Measure the Performance of Higher Education Students with Online Classes; COVID-19 Fake News Detection Using Joint Doc2Vec and Text Features with PCA; eye Tracking and Visual Attention in a Retail Context: Can Point-of-Sale Stickers Guide Customers?; error Classification Using Automatic Measures Based on n-grams and Edit Distance; understanding and Predicting Process Performance Variations of a Balanced Manufacturing Line at Bosch; a Comparative Analysis on the Summarization of Legal Texts Using Transformer Models; determination of the Factors Influencing Proper Face Recognition in Faces Protected by Face Masks, an Analysis of Their Algorithms and the Factors Affecting Recognition Success; Evaluation Metrics in Explainable Artificial Intelligence (XAI).",,,Conference review,Final,,Scopus,2-s2.0-85144201598,Gaming / VR
Hebbar P.A.; Vinod S.; Shah A.K.; Pashilkar A.A.; Biswas P.,"Hebbar, P Archana (56102250400); Vinod, Sanjana (57441219000); Shah, Aumkar Kishore (57918560100); Pashilkar, Abhay A (11439939700); Biswas, Pradipta (14007579800)",56102250400; 57441219000; 57918560100; 11439939700; 14007579800,Cognitive Load Estimation in VR Flight Simulator,2022,Journal of Eye Movement Research,15,3,11,,,,14,10.16910/JEMR.15.3.11,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85166933035&doi=10.16910%2fJEMR.15.3.11&partnerID=40&md5=949a72de2242711dd807cd317eedb773,"This paper discusses the design and development of a low-cost virtual reality (VR) based flight simulator with cognitive load estimation feature using ocular and EEG signals. Focus is on exploring methods to evaluate pilot’s interactions with aircraft by means of quantifying pilot’s perceived cognitive load under different task scenarios. Realistic target tracking and context of the battlefield is designed in VR. Head mounted eye gaze tracker and EEG headset are used for acquiring pupil diameter, gaze fixation, gaze direction and EEG theta, alpha, and beta band power data in real time. We developed an AI agent model in VR and created scenarios of interactions with the piloted aircraft. To estimate the pilot’s cognitive load, we used low-frequency pupil diameter variations, fixation rate, gaze distribution pattern, EEG signal-based task load index and EEG task engagement index. We compared the physiological measures of workload with the standard user’s inceptor control-based workload metrics. Results of the piloted simulation study indicate that the metrics discussed in the paper have strong association with pilot’s perceived task difficulty. © This article is licensed under a Creative Commons Attribution 4.0 International license.",cognitive load; EEG; eye gaze; flight simulator; Human factors; ocular parameters; task engagement; virtual reality,,Article,Final,All Open Access; Gold Open Access; Green Open Access,Scopus,2-s2.0-85166933035,Gaming / VR
Mascarenhas D.R.D.; Birtwhistle J.; Martindale A.,"Mascarenhas, D.R.D. (8432020000); Birtwhistle, J. (57952949900); Martindale, A. (10045154700)",8432020000; 57952949900; 10045154700,First-person video recordings with eye tracking glasses and cognitive task analysis as a framework for referee decision training,2022,Managing Sport and Leisure,,,,,,,5,10.1080/23750472.2022.2134186,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85141151305&doi=10.1080%2f23750472.2022.2134186&partnerID=40&md5=8950bf2108dd4276a8b1a6dc89106c6d,"Rationale: In comparison to players little is known about how sports officials integrate perception and cognition to manage in-game decisions. Design: Using a naturalistic approach this paper uses first-person eye-tracked video footage to document the attentional demands and situation awareness (SA) of expert touch (rugby/football) referees in their real-world environment to inform decision training for amateur officials. Drawing directly from match performances, an applied cognitive task analysis (ACTA) technique investigated how three international referees manage complex attentional demands, to see what lessons could be learned for less-experienced referees. Findings: Referees emphasised the importance of role clarity and game understanding as the foundation for effective match officiating. They used advanced cues such as player body language and movement patterns (SA1) to interpret game status (SA2) and predict likely actions and movement patterns (SA3). Ordering abstraction, preventive communication and early positioning were used to lessen cognitive load and encourage game flow. Practical Implications: The merits of using first-person, eye-tracked, audio-visual footage with ACTA for training less experienced sports officials through expert verbal elicitation or self-reflection are discussed. Research Contribution: The paper proposes a decision tree for touch refereeing which emphasises a hierarchical ordering of cognitive decision points that provides the basis for training amateur referees. © 2022 The Author(s). Published by Informa UK Limited, trading as Taylor & Francis Group.",cognitive task analysis; naturalistic decision making; touch (rugby/football) referees; visual gaze,,Article,Article in press,All Open Access; Green Open Access; Hybrid Gold Open Access,Scopus,2-s2.0-85141151305,Gaming / VR
De Cock L.; Van de Weghe N.; Ooms K.; Saenen I.; Van Kets N.; Van Wallendael G.; Lambert P.; De Maeyer P.,"De Cock, Laure (57204432138); Van de Weghe, Nico (8973773700); Ooms, Kristien (35176370900); Saenen, Ignace (36976475800); Van Kets, Niels (56786203200); Van Wallendael, Glenn (35749154000); Lambert, Peter (35262123100); De Maeyer, Philippe (8973773500)",57204432138; 8973773700; 35176370900; 36976475800; 56786203200; 35749154000; 35262123100; 8973773500,"Linking the cognitive load induced by route instruction types and building configuration during indoor route guidance, a usability study in VR",2022,International Journal of Geographical Information Science,36,10,,1978,2008,30.0,11,10.1080/13658816.2022.2032080,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85126071584&doi=10.1080%2f13658816.2022.2032080&partnerID=40&md5=461eec14d01166109e2ed02dd7c2eacc,"Every route instruction type (e.g. map, symbol, photo) induces a specific cognitive load. However, when these types are used at different decision points in a building, the building configuration of these points also influences the induced cognitive load. Therefore, the process of route guidance results in an interaction between the instruction type and the decision point, which determines the induced cognitive load. One way of reducing cognitive load during route guidance is by using adaptive systems that show specific route instruction types at specific decision points. Therefore, in this VR experiment, the usability of such an adaptive indoor route guidance system is tested by tracking the wayfinding and gaze behavior of the users. First, the difference in wayfinding and gaze behavior between all route instruction types is compared. Next, the building configuration at the decision points is quantified through the architectural theory of space syntax, and the correlation with the wayfinding and gaze behavior is determined. Our findings indicate that adapting the route instruction type does make a difference for the user. © 2022 Informa UK Limited, trading as Taylor & Francis Group.",eye tracking; indoor route guidance; route instruction types; space syntax; Virtual reality,adaptive management; building; cognition; computer simulation; experimental study; GIS; guideline; quantitative analysis; tracking; virtual reality,Article,Final,,Scopus,2-s2.0-85126071584,Gaming / VR
Davies H.J.; Williams I.; Mandic D.P.,"Davies, Harry J. (57213609195); Williams, Ian (57206923926); Mandic, Danilo P. (7006513328)",57213609195; 57206923926; 7006513328,Tracking Cognitive Workload in Gaming with In-Ear SpO2,2022,"Proceedings of the Annual International Conference of the IEEE Engineering in Medicine and Biology Society, EMBS",2022-July,,,4913,4916,3.0,0,10.1109/EMBC48229.2022.9871448,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85138127417&doi=10.1109%2fEMBC48229.2022.9871448&partnerID=40&md5=796af50fd23535b4f668a494246146f0,"The feasibility of using in-ear SpO2 to track cognitive workload induced by gaming is investigated. This is achieved by examining temporal variations in cognitive workload through the game Geometry Dash, with 250 trials across 7 subjects. The relationship between performance and cognitive load in Dark Souls III boss fights is also investigated followed by a comparison of the cognitive workload responses across three different genres of game. A robust decrease in in-ear SpO2 is observed in response to cognitive workload induced by gaming, which is consistent with existing results from memory tasks. The results tentatively suggest that in-ear SpO2 may be able to distinguish cognitive workload alone, whereas heart rate and breathing rate respond similarly to both cognitive workload and stress. This study demonstrates the feasibility of low cost wearable cognitive workload tracking in gaming with in-ear SpO2, with applications to the play testing of games and biofeedback in games of the future. © 2022 IEEE.",,Cognition; Heart Rate; Humans; Video Games; Workload; Computer programming; Breathing rate; Cognitive loads; Cognitive workload and stress; Cognitive workloads; Heart-rate; Low-costs; Performance; Play testing; Temporal variation; cognition; heart rate; human; video game; workload; Biofeedback,Conference paper,Final,,Scopus,2-s2.0-85138127417,Gaming / VR
,,,"Proceedings - 2022 11th International Conference of Educational Innovation through Technology, EITT 2022",2022,"Proceedings - 2022 11th International Conference of Educational Innovation through Technology, EITT 2022",,,,,,166.0,0,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85158848554&partnerID=40&md5=c73e6bc701f51a8f3f25c95d4e001b58,The proceedings contain 27 papers. The topics discussed include: the practice and research on the development of university curriculum alliance from the perspective of self-organization theory; learning attention level prediction via multimodal physiological data using wearable wrist devices; using the six-hat thinking technique to facilitate pre-service teachers' learning of instructional design in a flipped classroom; what makes good blended collaborative learning: a case study; the impact of two mediums based on virtual reality technology in the education of practical courses; research on the construction and application of digital education resources from the perspective of motion graphic animation; is the teacher's classroom interactive decision-making ethical? evidence from eye-tracking; under the condition of stress arousal: the influence of positive emotion design on multimedia learning; and investigating undergraduates' perceptions of digital citizenship: a survey from China.,,,Conference review,Final,,Scopus,2-s2.0-85158848554,Gaming / VR
Rubin M.; Muller K.; Hayhoe M.M.; Telch M.J.,"Rubin, Mikael (56459512900); Muller, Karl (57204293789); Hayhoe, Mary M. (7004513666); Telch, Michael J. (7006365823)",56459512900; 57204293789; 7004513666; 7006365823,Attention guidance augmentation of virtual reality exposure therapy for social anxiety disorder: a pilot randomized controlled trial,2022,Cognitive Behaviour Therapy,51,5,,371,387,16.0,22,10.1080/16506073.2022.2053882,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85129223219&doi=10.1080%2f16506073.2022.2053882&partnerID=40&md5=a0884803cda727c6242622448d128f72,"Biased attention to social threats has been implicated in social anxiety disorder. Modifying visual attention during exposure therapy offers a direct test of this mechanism. We developed and tested a brief virtual reality exposure therapy (VRET) protocol using 360°-video and eye tracking. Participants (N = 21) were randomized to either standard VRET or VRET + attention guidance training (AGT). Multilevel Bayesian models were used to test (1) whether there was an effect of condition over time and (2) whether post-treatment changes in gaze patterns mediated the effect of condition at follow-up. There was a large overall effect of the intervention on symptoms of social anxiety, as well as an effect of the AGT augmentation on changes in visual attention to audience members. There was weak evidence against an effect of condition on fear of public speaking and weak evidence supporting a mediation effect, however these estimates were strongly influenced by model priors. Taken together, our findings suggest that attention can be modified within and during VRET and that modification of visual gaze avoidance may be casually linked to reductions in social anxiety. Replication with a larger sample size is needed. © 2022 Swedish Association for Behaviour Therapy.",attention; exposure therapy; social anxiety; Virtual reality,"Bayes Theorem; Humans; Phobia, Social; Pilot Projects; Virtual Reality Exposure Therapy; Bayes theorem; controlled study; human; pilot study; procedures; randomized controlled trial; social phobia; virtual reality exposure therapy",Article,Final,All Open Access; Green Open Access,Scopus,2-s2.0-85129223219,Gaming / VR
Cai M.; Epp C.D.,"Cai, Minghao (57824447700); Epp, Carrie Demmans (40660928200)",57824447700; 40660928200,Modeling Cognitive Load and Affect to Support Adaptive Online Learning,2022,"Proceedings of the 15th International Conference on Educational Data Mining, EDM 2022",,,,,,,0,10.5281/zenodo.6853020,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85174816491&doi=10.5281%2fzenodo.6853020&partnerID=40&md5=af9c64f4ddf28fdf6fd60637f533e0da,"Online learning has been spreading with the increasing availability and diversity of digital resources. Understanding how students’ cognitive load and affect changes when using learning technologies will help us decipher the learning process and understand student needs. In this research, we focus on modeling learner’s cognitive load and affect using real-time physiological reactions. We explore what affect modeling contributes to the modeling of cognitive load, and how real-time cognitive load changes alongside learning activities. We want to further investigate if cognitive load modeling helps diagnose learner knowledge and facilitates improvement. We have designed two case studies: one where students are learning python with an e-learning system and another where they are practicing literacy skills with a web-based learning game. To collect learner data, we have implemented a sensing prototype consisting of an eye tracker and a wireless wristband. © 2022 Copyright is held by the author(s).",Affect; Cognitive load; Online learning; Student modeling,Computer aided instruction; E-learning; Eye tracking; Learning systems; Python; Adaptive online learning; Affect; Cognitive loads; Digital resources; Learning process; Learning technology; Model learners; Online learning; Real- time; Student Modeling; Students,Conference paper,Final,,Scopus,2-s2.0-85174816491,Gaming / VR
Woodworth J.W.; Broussard D.; Borst C.W.,"Woodworth, Jason W. (57192676048); Broussard, David (57215364589); Borst, Christoph W. (9736479200)",57192676048; 57215364589; 9736479200,Redirecting Desktop Interface Input to Animate Cross-Reality Avatars,2022,"Proceedings - 2022 IEEE Conference on Virtual Reality and 3D User Interfaces, VR 2022",,,,843,851,8.0,10,10.1109/VR51125.2022.00106,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85129384473&doi=10.1109%2fVR51125.2022.00106&partnerID=40&md5=0649968cebc984c16cf54647712fde54,"We present and evaluate methods to redirect desktop inputs such as eye gaze and mouse pointing to a VR-embedded avatar. We use these methods to build a novel interface that allows a desktop user to give presentations in remote VR meetings such as conferences or classrooms. Recent work on such VR meetings suggests a substantial number of users continue to use desktop interfaces due to ergonomic or technical factors. Our approach enables desk-top and immersed users to better share virtual worlds, by allowing desktop-based users to have more engaging or present cross-reality avatars. The described redirection methods consider mouse pointing and drawing for a presentation, eye-tracked gaze towards audience members, hand tracking for gesturing, and associated avatar motions such as head and torso movement. A study compared different levels of desktop avatar control and headset-based control. Study results suggest that users consider the enhanced desktop avatar to be human-like and lively and draw more attention than a conventionally animated desktop avatar, implying that our interface and methods could be useful for future cross-reality remote learning tools.  © 2022 IEEE.",Human computer interaction (HCI); Human-centered computing; Human-centered computing; Interaction design; Interaction design pro-cess and methods; Interaction Paradigms; User interface design; Virtual Reality,Human computer interaction; Mammals; Virtual reality; Desktop interfaces; Eye mouse; Human computer interaction; Human-centered computing; Interaction design; Interaction design pro-cess and method; Interaction paradigm; User interface designs; User interfaces,Conference paper,Final,,Scopus,2-s2.0-85129384473,Gaming / VR
Schenkluhn M.; Peukert C.; Weinhardt C.,"Schenkluhn, Marius (58034358800); Peukert, Christian (58077908100); Weinhardt, Christof (6604021298)",58034358800; 58077908100; 6604021298,A Look Behind the Curtain: Exploring the Limits of Gaze Typing,2022,Lecture Notes in Information Systems and Organisation,58,,,251,259,8.0,0,10.1007/978-3-031-13064-9_26,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85144962475&doi=10.1007%2f978-3-031-13064-9_26&partnerID=40&md5=e449a94c3f57274b1cc96b64ed831458,"Text entry in Augmented and Virtual Reality applications remains a major challenge for a comprehensive and ubiquitous usability of this technology. Therefore, researchers have proposed several solutions for text entry in Augmented Reality. Hands-free gaze typing is a popular approach to ensure mobility and both fast and accurate typing. However, prior studies struggle with the missing expertise of participants. We propose a study design that eliminates the learning process and gives an outlook to future gaze typing performance. In particular, we focus on the time-dependent effects of different dwell times on typing performance. By focusing on user-specific limitations, the results of this study are a prerequisite to user-adaptive gaze typing without fatigue. © 2022, The Author(s), under exclusive license to Springer Nature Switzerland AG.",Augmented reality; Dwell time; Eye tracking; Gaze typing; NeuroIS,,Conference paper,Final,,Scopus,2-s2.0-85144962475,Gaming / VR
,,,"Optical Architectures for Displays and Sensing in Augmented, Virtual, and Mixed Reality (AR, VR, MR) III",2022,Progress in Biomedical Optics and Imaging - Proceedings of SPIE,11931,,,,,231.0,0,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85131620795&partnerID=40&md5=f563c3565658774a3fd9aa078981c69f,"The proceedings contain 27 papers. The topics discussed include: compact electric pupil steering for Maxwellian-type augmented reality systems; autonomous calibration for gaze detection using Bayesian estimation and canonical correlation analysis; hyperchromatic multifocal 3D display for augmented reality applications; an augmented reality measurement tool for clinical procedures; assessing internal, external and covert visuospatial attention in AR using brain sensing: a pilot study; sub-millisecond in-plane only phase modulation by an SSD liquid crystal technology to AR/VR devices; inside-out tracking and projection mapping for robot-assisted transcranial magnetic stimulation; the path towards mass manufacturing of optical waveguide combiners via large-area nanoimprinting; and quadrant detector-based method for eye point alignment of augmented and virtual reality head mounted displays.",,,Conference review,Final,,Scopus,2-s2.0-85131620795,Gaming / VR
Zhagufarova A.; Banakou D.; Salam H.,"Zhagufarova, Alima (60008195400); Banakou, Domna (35190013200); Salam, Hanan (39862037100)",60008195400; 35190013200; 39862037100,Adaptive Virtual Reality Meditation for Adults with ADHD,2026,"Lecture Notes of the Institute for Computer Sciences, Social-Informatics and Telecommunications Engineering, LNICST",651 LNICST,,,112,131,19.0,0,10.1007/978-3-031-97257-7_8,https://www.scopus.com/inward/record.uri?eid=2-s2.0-105011259282&doi=10.1007%2f978-3-031-97257-7_8&partnerID=40&md5=f03f3cbd15fb7b5216dc4de76dd2bd36,"Attention-deficit/hyperactivity disorder (ADHD) is a prevalent neurodevelopmental disorder marked by inattention, impulsiveness, and hyperactivity. Mindfulness practices like meditation are associated with reduced ADHD symptoms. Virtual Reality (VR) offers a more immersive and engaging experience than traditional meditation. This research explores VR’s potential for developing immersive and adaptive interventions for adults with ADHD, leveraging VR’s adaptability to create interactive environments that enhance engagement and focus. 16 adults with ADHD participated in a VR meditation study using a between-group design. Participants listened to a 12-minute meditation audio in VR. Eye tracking data were recorded in real-time and were used to determine attention thresholds, based on which auditory and visual cues were triggered to prompt participants to refocus their attention on the meditation (Adaptive group). The Control group attended VR meditation without any adaptive elements. Results show differences in mindfulness and attention between groups and changes in eye tracking patterns. © ICST Institute for Computer Sciences, Social Informatics and Telecommunications Engineering 2026.",adaptive systems; ADHD; Affective Computing; Human-Computer Interaction; Virtual Reality,Adaptive control systems; Eye movements; Eye tracking; Human computer interaction; Affective Computing; Attention deficit hyperactivity disorder; Auditory cues; Computer interaction; Eye-tracking; Group Design; Immersive; Interactive Environments; Real- time; Tracking data; Virtual reality,Conference paper,Final,,Scopus,2-s2.0-105011259282,Gaming / VR
Qin D.; Long Y.; Zhang X.; Zhou Z.; Jin Y.; Wang P.,"Qin, Dantong (59565081300); Long, Yang (57089680300); Zhang, Xun (58802432800); Zhou, Zhibin (57211970144); Jin, Yuting (59565047900); Wang, Pan (57827203900)",59565081300; 57089680300; 58802432800; 57211970144; 59565047900; 57827203900,Towards stereoscopic vision: Attention-guided gaze estimation with EEG in 3D space,2025,Neurocomputing,648,,130577,,,,0,10.1016/j.neucom.2025.130577,https://www.scopus.com/inward/record.uri?eid=2-s2.0-105007728737&doi=10.1016%2fj.neucom.2025.130577&partnerID=40&md5=77d0d26c0b01841e75253b871db089c1,"Since traditional gaze-tracking methods rely on line-of-sight estimation, spatial attention modeling from neural activity offers an alternative perspective to gaze estimation. This paper presents a proof-of-concept study on attention-guided gaze estimation with Electroencephalography (EEG), investigating whether brain signals can be leveraged to estimate attentional focus within a controlled 3D environment. We first conducted a preliminary survey to gather public opinions, revealing a generally positive attitude towards EEG-driven gaze tracking. Building on this insight, we collected an EEG dataset in VR, where participants engaged with stimuli presented at predefined spatial locations. We introduce a deep learning model that estimates the relative saliency of candidate positions, enabling gaze estimation through optimization within the learned representation. Our results demonstrate that attentional focus was successfully mapped in a 3D coordinate space from 5 participants, and low-frequency oscillations contributed more significantly to predictive performance. The model achieved robust accuracy in distinguishing gaze locations, highlighting the potential of EEG-based gaze estimation for attention tracking in 3D environments. © 2025 The Authors",3D; Brain–computer interface; Gaze estimation; Spatial attention prediction; Virtual reality (VR),Three dimensional computer graphics; Virtual reality; 3-D environments; 3d; 3D spaces; Gaze estimation; Gaze-tracking; Spatial attention; Spatial attention prediction; Stereoscopic vision; Tracking method; Virtual reality; adult; Article; brain function; brain signal; convolutional neural network; deep learning; electroencephalogram; female; functional magnetic resonance imaging; gaze; human; human experiment; independent component analysis; male; neural activity; neuroscience; oscillation; recurrent neural network; stereoscopic vision; support vector machine; virtual reality; Deep learning,Article,Final,,Scopus,2-s2.0-105007728737,Gaming / VR
Zhang J.; Kong W.; Ma M.; Yang X.; Li W.; Song A.,"Zhang, Jun (55875816600); Kong, Wei (59710875600); Ma, Ming (57198921428); Yang, Xi (59711056800); Li, Weifeng (58845409600); Song, Aiguo (59037308400)",55875816600; 59710875600; 57198921428; 59711056800; 58845409600; 59037308400,A review of eye-tracking technology and its application in stroke diagnosis and assessment,2025,Measurement: Journal of the International Measurement Confederation,252,,117325,,,,1,10.1016/j.measurement.2025.117325,https://www.scopus.com/inward/record.uri?eid=2-s2.0-105001122874&doi=10.1016%2fj.measurement.2025.117325&partnerID=40&md5=fff9b8c99d45aa54a34dace1508e09f7,"The eyes are the windows of the soul, providing essential information through eye movement. With the rapid development of eye-tracking technology (ETT), its application in health assessment, including diagnosing and treating neurological diseases, has expanded significantly. Stroke is a leading cause of adult death and disability worldwide, and studies have shown that eye movement information can serve as a quantitative indicator for stroke diagnosis and assessment. Despite significant research on ETT-based stroke diagnosis, a comprehensive review is still lacking. This paper reviews 238 papers from the past twenty years, focusing on recent advancements in ETT and its application in stroke diagnosis. The studies were selected through a systematic review process following PRISMA-ScR guidelines. This paper provides a systematic overview of ETT principles, methods, and systems, detailing the entire application process in stroke diagnosis and assessment, from data acquisition to symptom evaluation. It also compares various methods and discusses the latest advancements. Statistical analysis methods remain the majority in ETT-based stroke research, while machine learning methods are increasingly attracting attention as alternative approaches. Appearance-based deep learning methods show potential for future stroke diagnosis but require accuracy improvements. Future directions in stroke diagnosis and clinical applications mainly include synthetic data generation, VR integration, wearable ETT devices, multimodal data fusion, and expanding application scenarios. Finally, we propose an active perception strategy and a stroke medical system framework for ETT application. This paper aims to provide researchers with a rapid understanding of core technologies and comprehensive knowledge. © 2025 Elsevier Ltd",Active perception; Eye tracking; Eye-tracking devices; Human-computer interaction; Neurological disorder diagnosis; Stroke assessment; Video-oculography,Clinical research; Neurons; Active perceptions; Computer interaction; Eye tracking devices; Eye tracking technologies; Eye-tracking; Neurological disorder diagnose; Neurological disorders; Stroke assessment; Technology application; Video oculography; Eye movements,Review,Final,,Scopus,2-s2.0-105001122874,Gaming / VR
Luo Y.; Seo J.; Hasanzadeh S.,"Luo, Yanfang (57938022600); Seo, JoonOh (55924450700); Hasanzadeh, Sogand (57190003680)",57938022600; 55924450700; 57190003680,Understanding hazard recognition behavior using eye-tracking metrics in a VR-simulated environment: Learning from successful and failed conditions,2025,KSCE Journal of Civil Engineering,29,8,100173,,,,1,10.1016/j.kscej.2025.100173,https://www.scopus.com/inward/record.uri?eid=2-s2.0-105008531932&doi=10.1016%2fj.kscej.2025.100173&partnerID=40&md5=f0d3a5cd1da25eae4788a3e961e6645c,"Effective hazard recognition is critical for ensuring safety in complex construction sites. Workers often encounter distinct hazard recognition conditions, influenced by limited cognitive abilities and attentional resources. However, the cognitive processes such as attention allocation, cognitive load, and emotional arousal between successful and failed (misidentified, overlooked) conditions have not been fully investigated. This study employed decision time and eye movement metrics to investigate the cognitive processes among these three recognition conditions. The results revealed that successful hazard recognitions involved increased attentional resources, lower cognitive load, and more emotional arousal, evidenced by proper decision time, a prolonged fixation duration, smaller average pupil diameter, and pupil dilation while gazing at areas of interest (AOI). Conversely, participants in the misidentified condition appeared to have limited information extraction and comprehension abilities with long fixation duration and more saccade count, while those in the overlooked condition demonstrated poor visual search skills with less saccade count. Both misidentified and overlooked conditions exhibited high cognitive load and low emotional arousal, as indicated by larger average pupil size and pupil contraction. These findings offer valuable insights for developing targeted training strategies to address the deficiencies of cognitive ability associated with each recognition condition. © 2025 The Author(s)",Attention allocation; Cognitive load; Cognitive process; Emotion arousal; Eye-tracking metrics; Hazard recognition,Behavioral research; Cognitive systems; Eye movements; Hazards; Information retrieval; Occupational risks; Personnel training; Time and motion study; Attention allocation; Cognitive ability; Cognitive loads; Cognitive process; Condition; Decision time; Emotion arousal; Eye-tracking; Eye-tracking metric; Hazard recognition; Eye tracking,Article,Final,All Open Access; Hybrid Gold Open Access,Scopus,2-s2.0-105008531932,Gaming / VR
Shi J.; Ding N.; Wang H.; Wang Y.,"Shi, Jiguang (57833343800); Ding, Ning (57188846463); Wang, Hao (59682593100); Wang, Yang (58724731800)",57833343800; 57188846463; 59682593100; 58724731800,How risk preference affects evacuees’ route choice in buildings: An IVR-based experimental study,2025,Safety Science,187,,106840,,,,0,10.1016/j.ssci.2025.106840,https://www.scopus.com/inward/record.uri?eid=2-s2.0-86000777374&doi=10.1016%2fj.ssci.2025.106840&partnerID=40&md5=26922b25389c52ca67b7512609a392dc,"During building evacuations, evacuees often tend to enter areas with smoke and flames, which contradicts established safety principles for evacuation. This paper investigates how individuals with different risk preferences process evacuation information and make route choices. 72 participants were categorized into two groups based on a risk preference questionnaire: one-third were identified as Risk Seeking Group (RSG) and the rest as Risk Averse Group (RAG). Subsequently, eye-tracking technology and immersive virtual reality (IVR) were employed to analyze the variations in behavior between these groups. The findings show that: (1) RAG exhibited a general attention bias toward risk-related information; (2) Significant differences were observed in route choice among RAG based on varying cognitive approaches; (3) While all participants acknowledged the importance of safety factors, approximately 40% behaviorally chose routes involving flames; (4) RSG prioritizes evacuation distance and evacuation efficiency in the evacuation process, achieving an average evacuation time that was 23.85% faster than that of RAG. Conversely, RAG displayed a tendency to avoid harm, even at the cost of evacuation efficiency. This paper deconstructs complex evacuation behaviors from a psychological perspective, providing a more comprehensive understanding of route choices among evacuees with different risk preferences. It serves as a reference for optimizing evacuation strategies and designing building safety features with consideration of psychological factors. © 2025 Elsevier Ltd",Attention process; Building safety design; IVR experiment; Risk preference; Route choice,Attention process; Building safety; Building safety design; Group-based; Immersive virtual reality; Immersive virtual reality experiment; Risk averse; Risk preference; Route choice; Safety design; adult; Article; attentional bias; avoidance behavior; building; cognition; construction work and architectural phenomena; decision making; emergency evacuation; eye tracking; flame; human; human experiment; normal human; psychological aspect; questionnaire; risk aversion; risk behavior; risk seeking; safety; smoke; time; virtual reality,Article,Final,,Scopus,2-s2.0-86000777374,Gaming / VR
Zhang Z.; Guo B.H.W.; Feng Z.; Goh Y.M.,"Zhang, Zhe (57561760400); Guo, Brian H.W. (56602167900); Feng, Zhenan (57203837932); Goh, Yang Miang (57022370200)",57561760400; 56602167900; 57203837932; 57022370200,"Investigating the interplay of bottom-up and top-down attention in hazard recognition: Insights from immersive virtual reality, eye-tracking and electroencephalography",2025,Safety Science,187,,106841,,,,0,10.1016/j.ssci.2025.106841,https://www.scopus.com/inward/record.uri?eid=2-s2.0-86000748576&doi=10.1016%2fj.ssci.2025.106841&partnerID=40&md5=6e4743bb405c8a0e5e8a26017b11155f,"The construction industry's high-risk environment demands effective hazard recognition strategies. Attention, a critical cognitive process, plays a crucial role in this task. Previous research focused on individual attention process, such as sustained attention, selective and divided attention. However, no research has been conducted to investigate the effects of the interplay between endogenous and exogenous factors on hazard recognition in construction settings. This paper aims to investigate the effects of the interplay between top-down (T-D) and bottom-up (B-U) attention networks on hazard recognition, using immersive virtual reality (IVR), eye tracking (ET), and electroencephalography (EEG). Two safety interventions—augmented stimuli and toolbox meetings—were tested in a dynamic IVR construction site. The results showed that both augmented stimuli and the safety toolbox meeting significantly affected B-U, T-D, and hazard recognition. This paper provided evidence that the interplay between B-U and T-D can significantly improve workers’ hazard recognition performance. The results improved our understanding of the mechanisms that control selective attention and the source of guidance over attention orientation. By demonstrating that T-D and B-U processes can work together rather than in isolation, this research contributes a key theoretical insight: attentional orientation in hazardous construction environments is neither fully determined by external stimuli nor entirely controlled by internal cognitive sets. In addition, this paper highlights and calls for an integrated approach to improving worker's hazard recognition performance, by combining digital-technology-enabled stimuli with safety-goal-oriented training and managerial practices. © 2025 The Author(s)",Bottom-up attention (B-U); Electroencephalography (EEG); Event-related potential (ERP); Eye tracking; Hazard recognition; Immersive virtual reality (IVR); Top-down attention (T-D),Augmented reality; Concrete construction; Hazards; Human resource management; Virtual environments; Bottom-up attention; Electroencephalography; Event related potentials; Event-related potential; Eye-tracking; Hazard recognition; Immersive virtual reality; Top-down attention; Topdown; adult; article; attention; attention network; building industry; cognition; controlled study; digital technology; electroencephalogram; electroencephalography; event related potential; evoked response; eye tracking; eye-tracking technology; human; recognition; selective attention; virtual reality; Construction industry,Article,Final,,Scopus,2-s2.0-86000748576,Gaming / VR
Fromm A.E.; Trujillo-Llano C.; Grittner U.; Meinzer M.; Flöel A.; Antonenko D.,"Fromm, Anna Elisabeth (57726531000); Trujillo-Llano, Catalina (57226718001); Grittner, Ulrike (12759777500); Meinzer, Marcus (22735009900); Flöel, Agnes (6603724144); Antonenko, Daria (54958476600)",57726531000; 57226718001; 12759777500; 22735009900; 6603724144; 54958476600,Increased variability in response to transcranial direct current stimulation in healthy older compared to young adults: A systematic review and meta-analysis,2025,Brain Stimulation,18,4,,1257,1265,8.0,0,10.1016/j.brs.2025.06.005,https://www.scopus.com/inward/record.uri?eid=2-s2.0-105010421809&doi=10.1016%2fj.brs.2025.06.005&partnerID=40&md5=c1b05862288bcd4e428e462779252f7b,"Background: Healthy aging is associated with a decline in cognitive and motor functions, affecting daily activities and quality of life. Combining transcranial direct current stimulation (tDCS) with behavioral training may be a promising intervention against this decline. However, individual response variability may obscure group-level effects and mislead conclusions about tDCS efficacy. Quantifying this variability is crucial for accurately assessing stimulation effects and understanding individual response factors, like age. Yet, no study has quantitatively compared tDCS variability across age groups. This systematic review and meta-analysis examine age-related variability in cognitive and motor responses to tDCS. Methods: Following PRISMA guidelines, we searched PubMed and Cochrane for studies directly comparing young and healthy older adults under similar experimental conditions. Across 19 studies comprising 390 older adults (mean ± SD age: 67 ± 5 years) and 384 young adults (mean ± SD age: 24 ± 3 years) receiving transcranial direct current (tDCS), we quantified behavioral variability using the log-transformed coefficient of variation ratio (lnCVR). Results: Results revealed substantially higher response variability in healthy older compared to young adults during active tES (21 %, lnCVRactive = −0.24 [-0.43, −0.04], p = 0.02), but not during sham conditions (lnCVRsham = −0.18 [-0.42, 0.05], p = 0.13). Conclusion: These findings provide the first quantitative evidence that advanced age increases behavioral tDCS response variability, highlighting the need to develop personalized tDCS approaches to optimize their efficacy in older populations. © 2025",Aging; Meta-analysis; Stimulation effect; Transcranial direct current stimulation (tDCS); Transcranial electrical stimulation (tES); Variability,adult; aged; Article; associative memory; behavioral training; body position; cognition; dorsolateral prefrontal cortex; episodic memory; eye tracking; frontoparietal cortex; Go No Go task; grid test; groups by age; healthy aging; human; inferior frontal gyrus; language development; meta analysis; Mini Mental State Examination; Montreal cognitive assessment; motor cortex; motor performance; movement (physiology); normal human; prefrontal cortex; primary motor cortex; primary somatosensory cortex; proprioceptive feedback; publication bias; saccadic eye movement; semantics; supplementary motor area; systematic review; temporal cortex; transcranial direct current stimulation; treadmill exercise; verbal memory; visual memory; visuomotor coordination; working memory; young adult,Article,Final,,Scopus,2-s2.0-105010421809,Gaming / VR
Guo Z.; Lv J.; Liu X.; Pan W.; Song D.-A.,"Guo, Zhicong (59762867200); Lv, Jian (57208089799); Liu, Xiang (59662923500); Pan, Weijie (35211388700); Song, Ding-an (57191431995)",59762867200; 57208089799; 59662923500; 35211388700; 57191431995,"Exploring virtual reality as an anxiety-inducing paradigm: Multimodal insights from subjective, behavioral and neurophysiological measures",2025,Behavioural Brain Research,489,,115610,,,,0,10.1016/j.bbr.2025.115610,https://www.scopus.com/inward/record.uri?eid=2-s2.0-105004045491&doi=10.1016%2fj.bbr.2025.115610&partnerID=40&md5=60f50e9547d18be40f32ab2b2c8847d8,"Anxiety results from the complex interplay between innate defensive responses to perceived threats and higher-order cognitive processes, mediated by specialized circuits in the human neocortex. Traditional methods of anxiety induction often fail to replicate the inherent unpredictability of threats or maintain ecological validity, thereby limiting their ability to fully elucidate the underlying mechanisms of anxiety. To overcome these limitations, this study aimed to explore the effectiveness of virtual reality (VR) as an innovative anxiety-inducing tool. By further using its ability to simulate real world scenes, the neural activities inducing anxiety in VR scenes were studied. VR is used to induce anxiety through customized scenarios, while a range of data, including subjective self-reports, objective performance measures, eye movement data, and EEG signals, are collected. The findings indicate that VR is efficacious in induced anxiety, which manifests through the arousal of anxious emotions, alterations in cognitive processes, and distinct neurophysiological patterns, particularly increased theta and alpha activity in the frontal and parietal regions. This research reinforces the ecological validity of VR as a research tool, contributing to a deeper understanding of the neurophysiological basis of anxiety and providing a more nuanced framework for both anxiety research and interventions in real-world contexts. © 2025 Elsevier B.V.",Anxiety induction; Electroencephalogram; Eye movement; Hemispheric asymmetry; Neurophysiological analysis; Virtual Reality,Adult; Anxiety; Electroencephalography; Emotions; Female; Humans; Male; Virtual Reality; Young Adult; adult; alpha rhythm; anxiety; arousal; article; brain asymmetry; cognition; controlled study; ecological validity; electroencephalogram; electroencephalography; emotion; eye movement; female; human; male; neocortex; self report; simulation; virtual reality; electroencephalography; pathophysiology; physiology; young adult,Article,Final,,Scopus,2-s2.0-105004045491,Gaming / VR
Lozzi D.; Di Pompeo I.; Marcaccio M.; Alemanno M.; Krüger M.; Curcio G.; Migliore S.,"Lozzi, Daniele (58181039100); Di Pompeo, Ilaria (58108299900); Marcaccio, Martina (58317396600); Alemanno, Michela (59752214100); Krüger, Melanie (35573518900); Curcio, Giuseppe (6701578294); Migliore, Simone (50161958900)",58181039100; 58108299900; 58317396600; 59752214100; 35573518900; 6701578294; 50161958900,AI-Powered Analysis of Eye Tracker Data in Basketball Game,2025,Sensors,25,11,3572,,,,0,10.3390/s25113572,https://www.scopus.com/inward/record.uri?eid=2-s2.0-105007795162&doi=10.3390%2fs25113572&partnerID=40&md5=f5f876d12b1dbce0f16ed55c303e974b,"This paper outlines a new system for processing of eye-tracking data in basketball live games with two pre-trained Artificial Intelligence (AI) models. blueThe system is designed to process and extract features from data of basketball coaches and referees, recorded with the Pupil Labs Neon Eye Tracker, a device that is specifically optimized for video analysis. The research aims to present a tool useful for understanding their visual attention patterns during the game, what they are attending to, for how long, and their physiological responses, blueas is evidenced through pupil size changes. AI models are used to monitor events and actions within the game and correlate these with eye-tracking data to provide understanding into referees’ and coaches’ cognitive processes and decision-making. This research contributes to the knowledge of sport psychology and performance analysis by introducing the potential of Artificial Intelligence (AI)-based eye-tracking analysis in sport with wearable technology and light neural networks that are capable of running in real time. © 2025 by the authors.",Artificial Intelligence; basketball; cognitive psychology; Computer Vision; eye tracking; sport psychology,Basketball games; Cognitive psychology; Eye trackers; Eye-tracker data; Eye-tracking; Intelligence models; Sport psychology; Tracking data; Video analysis; Visual Attention,Article,Final,,Scopus,2-s2.0-105007795162,Gaming / VR
Gao H.; Gao Y.; Kasneci E.,"Gao, Hong (57217014766); Gao, Yapeng (57207999028); Kasneci, Enkelejda (56059892600)",57217014766; 57207999028; 56059892600,An Explainable Machine Learning Approach for Cognitive Load Detection in Virtual Reality Using Eye Tracking Data,2025,ICMR 2025 - Proceedings of the 2025 International Conference on Multimedia Retrieval,,,,340,348,8.0,0,10.1145/3731715.3733275,https://www.scopus.com/inward/record.uri?eid=2-s2.0-105011591678&doi=10.1145%2f3731715.3733275&partnerID=40&md5=98d228bce37a8af9fbb1969d4c10de06,"Accurate cognitive load (CL) detection during virtual reality (VR) locomotion is critical for enhancing user experience and improving interaction design. Traditional CL assessment methods, such as self-reports and physiological measures, face challenges in VR environments. Eye tracking has shown potential as a reliable indicator of CL across various human-computer interaction (HCI) tasks. It offers significant promise as a discriminative feature for predictive models in VR. This study explores the feasibility of detecting CL induced by VR locomotion using an explainable machine-learning approach along with eye-tracking techniques. A comparative user study employing a within-subjects design evaluated five unique gait-free locomotion techniques. Statistical analysis revealed distinct CL levels across these locomotion techniques. Several machine learning models were developed for CL detection using eye-tracking data, with the Light Gradient Boosting Machine (LightGBM) achieving the highest accuracy of 0.78. The SHAP approach was employed to analyze the importance of features to provide interpretability, offering insights into the machine learning model's decision-making process. Our findings highlight the potential of using eye-tracking-based machine learning techniques as a practical approach for cognitive load detection in VR, contributing to the growing research in multimedia analytics, human perception, and user intent within immersive environments. Additionally, our work demonstrates how eye-tracking data can be leveraged to improve user interactions and optimize immersive multimedia experiences based on cognitive load analysis.  © 2025 ACM.",cognitive load; explainable ai; eye tracking; machine learning; virtual reality,Biped locomotion; Decision making; E-learning; Eye movements; Human computer interaction; Learning algorithms; Learning systems; Machine learning; Multimedia systems; User experience; User interfaces; Cognitive loads; Explainable ai; Eye-tracking; Load detection; Locomotion technique; Machine learning approaches; Machine learning models; Machine-learning; Tracking data; Users' experiences; Eye tracking; Virtual reality,Conference paper,Final,,Scopus,2-s2.0-105011591678,Gaming / VR
Argilés M.; Rovira-Gay C.; Pérez-Mañà L.; Sunyer-Grau B.; Gantz L.,"Argilés, Marc (56946819500); Rovira-Gay, Cristina (58221843800); Pérez-Mañà, Luis (57209010557); Sunyer-Grau, Bernat (57909941700); Gantz, Liat (59950744600)",56946819500; 58221843800; 57209010557; 57909941700; 59950744600,Performance of teenaged action video game players on the Developmental Eye Movement and King-Devick tests,2025,PLOS ONE,20,6-Jun,e0324862,,,,0,10.1371/journal.pone.0324862,https://www.scopus.com/inward/record.uri?eid=2-s2.0-105008400493&doi=10.1371%2fjournal.pone.0324862&partnerID=40&md5=7b091b21f1eb771ec0a74d6f0c92447c,"Action video game (AVG) players have been reported to demonstrate improved perceptual, cognitive, and motor skills, as well as enhanced eye movements such as saccade latency and accuracy. This study examined if these improvements are also observed in two common clinical eye movement assessments, the Developmental Eye Movement (DEM) and King–Devick tests. Ninth and tenth graders (15–16 years old) without learning disabilities or oculomotor dysfunctions, and who do not regularly play ball sports were tested in the DEM and King–Devick tests in random order. Those playing AVG ≥ 5 hours per week were included in the AVG group, and those playing ≤ 1 h per week were included in the non-video game group (NVG). Participants were asked to read quickly while reading speed and errors were recorded. Two DEM subtests were read vertically and one subtest was read from left to right. All King–Devick subtests were read from left to right. Forty participants (20 AVG, 20 NVG, 15 girls, mean age: 15.7 ± 0.1 years) were included. AVG players were significantly faster in their horizontal DEM times (AVG: 26 ± 5, NVG: 34 ± 7 sec, p < 0.001) and King–Devick times (AVG: 14 ± 2, NVG: 16 ± 4 sec, p = 0.034), but not in their vertical DEM times (AVG: 26 ± 3, NVG: 28 ± 5 sec p = 0.063). King-Devick and horizontal DEM times were significantly correlated for the entire sample (p < 0.01). Teenagers who regularly played AVGs performed better on the DEM and King–Devick tests, indicating that regularly playing this genre of AVGs may enhance oculomotor skills and visual processing speed. Clinicians should take this into account when interpreting results of these clinical tests. © 2025 Argilés et al. This is an open access article distributed under the terms of the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.",,Adolescent; Eye Movements; Female; Humans; Male; Motor Skills; Psychomotor Performance; Video Games; adolescent; Article; attention deficit hyperactivity disorder; autism; ball sport; binocular convergence; clinical article; cognition; dyslexia; error; eye development; eye movement; female; human; human experiment; male; motor performance; questionnaire; video game; visual acuity; physiology; psychomotor performance,Article,Final,,Scopus,2-s2.0-105008400493,Gaming / VR
Wang H.; Liu W.; Chen K.; Sun Q.; Zhang S.Q.,"Wang, Haiyu (59933940100); Liu, Wenxuan (59491997600); Chen, Kenneth (57909754800); Sun, Qi (57190442743); Zhang, Sai Qian (57205361814)",59933940100; 59491997600; 57909754800; 57190442743; 57205361814,Process Only Where You Look: Hardware and Algorithm Co-optimization for Efficient Gaze-Tracked Foveated Rendering in Virtual Reality,2025,Proceedings - International Symposium on Computer Architecture,,,,344,358,14.0,0,10.1145/3695053.3731110,https://www.scopus.com/inward/record.uri?eid=2-s2.0-105009595455&doi=10.1145%2f3695053.3731110&partnerID=40&md5=281f272c456b741408b1aab6e2c93431,"Virtual reality (VR) plays a crucial role in advancing immersive, interactive experiences that transform learning, work, and entertainment by enhancing user engagement and expanding possibilities across various fields. Image rendering is one of the most crucial application in VR, as it produces high-quality, realistic visuals that are vital for maintaining immersive user experiences and preventing visual discomfort or motion sickness. However, the cost of image rendering in VR environment is considerable, primarily due to the demands of high-quality visual experiences from users. This challenge is even greater in real-time applications, where maintaining low latency further increases the complexity of the rendering process. On the other hand, VR devices, such as head-mounted displays (HMDs), are intrinsically linked to human behavior, using insights from perception and cognition to enhance user experience. In this work, we aim to reduce the high computational costs of the rendering process in VR by leveraging natural human eye dynamics and focusing on processing only where you look (POLO). This involves co-optimizing AI algorithms with underlying hardware for greater efficiency.We introduce POLONet, an efficient multitask deep learning framework designed to track human eye movements with minimal latency. Integrated with the POLO accelerator as a plug-in for VR HMD SoCs, this approach significantly lowers image rendering costs, achieving up to a 3.9× reduction in end-to-end latency compared to the latest gaze tracking methods.  © 2025 Copyright held by the owner/author(s).",Foveated Rendering; Gaze Tracking; Hardware Accelerator; Saccade Detection; Virtual Reality,Behavioral research; Computer hardware; Cost reduction; Deep learning; Eye movements; Eye tracking; Helmet mounted displays; Human computer interaction; Image understanding; Interactive computer graphics; Interactive computer systems; Rendering (computer graphics); User experience; User interfaces; Foveated rendering; Gaze-tracking; Hardware accelerators; Head-mounted-displays; High quality; Image rendering; Immersive; Rendering process; Saccade detections; Users' experiences; Virtual reality,Conference paper,Final,,Scopus,2-s2.0-105009595455,Gaming / VR
Singh S.J.; Raman S.; Lim X.; Rajakumar V.; Razak R.A.; Chu S.Y.,"Singh, Susheel Joginder (57664818900); Raman, Suvalaxmi (59930029700); Lim, XinTong (59929288900); Rajakumar, Vinitaa (59930327500); Razak, Rogayah A. (56609064100); Chu, Shin Ying (35168924300)",57664818900; 59930029700; 59929288900; 59930327500; 56609064100; 35168924300,Eye-tracking as a measure of receptive vocabulary in non-verbal children with cerebral palsy,2025,PLOS ONE,20,6-Jun,e0325183,,,,0,10.1371/journal.pone.0325183,https://www.scopus.com/inward/record.uri?eid=2-s2.0-105007339017&doi=10.1371%2fjournal.pone.0325183&partnerID=40&md5=37c72348b1711d84504c4db5c23ecfc0,"Children with cerebral palsy (CP) often struggle to participate in traditional language assessments due to their limited mobility, making it challenging for speech-language therapists (SLTs) to accurately assess their language abilities. In recent years, there has been evidence that eye-tracking is an effective way of measuring the receptive language abilities of children who demonstrate difficulties with traditional language assessments. This study aimed to (i) develop eye-tracking assessment materials based on a receptive vocabulary subtest of a Malay language assessment and (ii) evaluate the performance of children with CP on the receptive vocabulary assessment conducted via eye-tracking, compared to their performance on a traditional receptive vocabulary assessment. The first phase of the study focused on developing eye-tracking receptive vocabulary assessment materials from the Malay Preschool Language Assessment Tool and trialling the materials and assessment protocol. This phase involved 15 typically developing children aged 4–6 years. The finalized materials and protocol were administered to 15 children with CP in the second phase. Each child attended two assessment sessions: the first was a traditional receptive vocabulary assessment, and the second utilized eye-tracking technology. Children practiced eye-tracking through online games. Results showed that eight children with CP performed better in the eye-tracking assessment, two scored similarly across both methods, and five scored lower during eye-tracking. The Wilcoxon Signed Rank Test conducted revealed no significant difference in scores across both assessment methods (p>.05). Furthermore, most children exhibited poor consistency in their scores across the two methods. These findings suggest that while some children with CP may benefit from receptive vocabulary assessments conducted via eye-tracking, no single assessment method is optimal for all children with CP. Instead, children with CP may benefit from a combination of assessment methods, including eye-tracking, to increase the accuracy of assessment results. © 2025 Singh et al. This is an open access article distributed under the terms of the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.",,"Cerebral Palsy; Child; Child, Preschool; Eye Movements; Eye-Tracking Technology; Female; Humans; Language Tests; Male; Vocabulary; Article; assessment of humans; cerebral palsy; child; clinical article; communication disorder; eye tracking; eye-tracking technology; fatigue; female; human; language ability; Malay Preschool Language Assessment Tool; male; nonverbal communication; physical disability; preschool child; receptive vocabulary; school child; speech language pathologist; vocabulary; eye movement; eye-tracking technology; language test; pathophysiology",Article,Final,All Open Access; Gold Open Access,Scopus,2-s2.0-105007339017,Gaming / VR
Li X.; Zhou Y.; Zhao W.; Fu C.; Huang Z.; Li N.; Xu H.,"Li, Xiuyi (57909483100); Zhou, Yue (57210314660); Zhao, Weiwei (57218139599); Fu, Chuanyun (49663195400); Huang, Zhuocheng (59963965700); Li, Nianqian (59962835700); Xu, Haibo (59963058300)",57909483100; 57210314660; 57218139599; 49663195400; 59963965700; 59962835700; 59963058300,Predicting Landing Position Deviation in Low-Visibility and Windy Environment Using Pilots’ Eye Movement Features,2025,Aerospace,12,6,523,,,,0,10.3390/aerospace12060523,https://www.scopus.com/inward/record.uri?eid=2-s2.0-105009069526&doi=10.3390%2faerospace12060523&partnerID=40&md5=514d367d8a3f9975761a919783398e69,"Eye movement features of pilots are critical for aircraft landing, especially in low-visibility and windy conditions. This study conducts simulated flight experiments concerning aircraft approach and landing under three low-visibility and windy conditions, including no-wind, crosswind, and tailwind. This research collects 30 participants’ eye movement data after descending from the instrument approach to the visual approach and measures the landing position deviation. Then, a random forest method is used to rank eye movement features and sequentially construct feature sets by feature importance. Two machine learning models (SVR and RF) and four deep learning models (GRU, LSTM, CNN-GRU, and CNN-LSTM) are trained with these feature sets to predict the landing position deviation. The results show that the cumulative fixation duration on the heading indicator, altimeter, air-speed indicator, and external scenery is vital for landing position deviation under no-wind conditions. The attention allocation required by approaches under crosswind and tailwind conditions is more complex. According to the MAE metric, CNN-LSTM has the best prediction performance and stability under no-wind conditions, while CNN-GRU is better for crosswind and tailwind cases. RF also performs well as per the RMSE metric, as it is suitable for predicting landing position errors of outliers. © 2025 by the authors.",CNN-LSTM model; deep learning; eye movement features; landing position deviation; random forest algorithm,Air navigation; Aircraft landing; Aneroid altimeters; Fighter aircraft; Learning systems; Long short-term memory; Meteorological instruments; Random forests; Visibility; CNN-LSTM model; Deep learning; Eye movement feature; Features sets; Landing position deviation; Low visibility; Low visibility conditions; Random forest algorithm; Wind conditions; Windy conditions; Eye movements,Article,Final,,Scopus,2-s2.0-105009069526,Gaming / VR
Li X.; Lan Y.-J.; Pi Z.; Qi G.Y.; Grant S.; Sun J.,"Li, Xiaofei (58657890900); Lan, Yu-Ju (16230465000); Pi, Zhongling (56704731200); Qi, Grace Yue (57200006742); Grant, Scott (55925611200); Sun, Jinmei (59393251300)",58657890900; 16230465000; 56704731200; 57200006742; 55925611200; 59393251300,Pedagogical agent positioning in external videos improves English academic presentation proficiency in desktop virtual reality settings,2025,British Journal of Educational Technology,56,4,,1507,1529,22.0,0,10.1111/bjet.13531,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85208068648&doi=10.1111%2fbjet.13531&partnerID=40&md5=3ded3f18fbe58f74ec3947b377a014f4,"English academic presentation (EAP) is an indispensable skill set of academic communication for university students. With the rapid development of desktop virtual reality (DVR), its application in language learning is worth exploring. The present study aimed to examine whether there is an improvement and difference in students' EAP by learning from the DVR with an in-video pedagogical agent (PA) or an out-of-video PA. Adopting a between-subject experimental design, a total of 64 students were randomly assigned to one of two group conditions depending on whether the PA was inside or outside the lecture video embedded in DVR. Participants' EAP performance, attention allocation and behavioural patterns were measured and analysed. As hypothesized, t-tests, repeated ANOVA and lag sequence analysis showed that the participants who learned from the DVR with an out-of-video PA showed better learning performance, less attention allocation on content and more frequent behavioural patterns than those with an in-video PA. Overall, our findings suggest that in a VR educational environment of video lectures, instructors should consider using an out-of-video PA to increase their social presence and improve students' learning experience. Practitioner notes What is already known about this topic EAP is an indispensable skill set of academic communication for university students. PA is an effective social cue in video lectures to promote learning. VR has been widely applied in language learning. What this paper adds Reveals the relationship between the PA's positioning and the learners' EAP performance and deepens the understanding of the PA's positioning in video lectures of a DVR learning environment. Provides empirical analysis of natural eye-tracking during the video learning in DVR scene and EAP data during the experimental condition. Students who learned from the DVR with an out-of-video PA showed better learning performance, less attention allocation on content and more frequent behavioural patterns than those with an in-video PA. Implications for practice and/or policy Designers are encouraged to use DVR with an out-of-video PA to enhance students' social presence and learning experience. © 2024 British Educational Research Association.",attention allocation; desktop virtual reality; English academic presentation; pedagogical agent; positioning; video lectures,Adversarial machine learning; Behavioral research; Computer simulation languages; Contrastive Learning; Digital video broadcasting (DVB); Economic and social effects; Interactive computer graphics; Teaching; Three dimensional computer graphics; Video analysis; Videodisks; Virtual environments; Virtual reality; Academic presentations; Attention allocation; Behavioral patterns; Desktop virtual reality; English academic presentation; Pedagogical agents; Positioning; Skill sets; University students; Video lectures; Students,Article,Final,,Scopus,2-s2.0-85208068648,Gaming / VR
Pokkula S.K.; Amresh A.; Duval J.,"Pokkula, Saroj Kaashyap (59953354300); Amresh, Ashish (36715908000); Duval, Jared (57195217976)",59953354300; 36715908000; 57195217976,Gaze Entropy as a Measure of Player Performance and Its Correlation with Focus and Attention,2025,Eye Tracking Research and Applications Symposium (ETRA),,,84,,,,0,10.1145/3715669.3725884,https://www.scopus.com/inward/record.uri?eid=2-s2.0-105008489122&doi=10.1145%2f3715669.3725884&partnerID=40&md5=1943e78b7b66f9a7ac94542c5e642b11,"Gaze entropy quantifies the randomness in eye movements and serves as a proxy for visual attention and cognitive engagement. This study investigates its relationship with player focus and learning within a pattern matching memory game designed to challenge attention and working memory. Using an iPad-based gaze-tracking system, we analyzed gaze entropy trends over 21 days. The results suggest that frequent players exhibit lower entropy, indicating structured gaze behavior and improved focus, whereas infrequent players display higher entropy linked to erratic attention. Although promising, these findings are currently limited to pattern matching games and may not be generalized to other genres of games. The study highlights the potential of gaze entropy to inform adaptive game mechanics and real-time cognitive feedback. © 2025 Copyright held by the owner/author(s).",Adaptive Gaming; Cognitive Load; Eye Tracking; Gaze Entropy; Player Performance; Visual Attention,Behavioral research; Entropy; Eye movements; Human computer interaction; Interactive computer graphics; Pattern matching; Adaptive gaming; Cognitive loads; Eye-tracking; Gaze entropy; Gaze tracking system; Pattern-matching; Performance; Player performance; Visual Attention; Working memory; Eye tracking,Conference paper,Final,,Scopus,2-s2.0-105008489122,Gaming / VR
Galli J.; Loi E.; Zanardini F.; Baldoni G.; Novara F.; Panigada S.; Ciccone R.; Cutrì M.R.; Bertoletti A.; Pinelli L.; Fazzi E.,"Galli, Jessica (23978191300); Loi, Erika (57226611952); Zanardini, Federica (59543281100); Baldoni, Giovanna (59543084400); Novara, Francesca (57190369673); Panigada, Serena (59543281200); Ciccone, Roberto (22833289700); Cutrì, Maria Rosa (49761138600); Bertoletti, Alice (57271387600); Pinelli, Lorenzo (7004252603); Fazzi, Elisa (7004236903)",23978191300; 57226611952; 59543281100; 59543084400; 57190369673; 59543281200; 22833289700; 49761138600; 57271387600; 7004252603; 7004236903,"Clinical Insights Into Nabais Sá-De Vries Syndrome due to a Novel SPOP Mutation: Neuromotor, Cognitive, Adaptive, Behavioral, and Neurovisual Features",2025,"American Journal of Medical Genetics, Part A",197,6,e64007,,,,0,10.1002/ajmg.a.64007,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85216987412&doi=10.1002%2fajmg.a.64007&partnerID=40&md5=82910ef6fde150437675a564a2971c68,"Nabais Sá-De Vries syndrome (NSDVS) is an extremely rare autosomal dominant disorder caused by SPOP mutations. To date, only 10 cases have been described presenting with intellectual disability, neurological signs and symptoms, and a variable association of dysmorphic features. In this article, we report a new case of NSDVS involving a novel pathogenic variant of the SPOP gene. We describe the patient's motor, cognitive, adaptive, behavioral, and neurovisual features, as well as her developmental trajectory. The girl, followed-up from the first months of life to 11 years of age, presented with a de novo heterozygous missense in Exon 5 of the SPOP gene (NM_001007228.2:c.361C>T, p.Arg121Trp) and, thus, classified as NSDVS Type 1. Along with a global developmental delay, she showed microcephaly, dysmorphic features (such as narrow forehead, highly arched eyebrows, and blepharophimosis), moderate intellectual disability, adaptive difficulties, language disorder, and several neurovisual signs and symptoms (such as refractive errors, strabismus, nystagmus, altered oculomotor functions and deficits of visual acuity, and contrast sensitivity). These findings suggest a predominant involvement of the central nervous system in NSDVS and expand the phenotypic spectrum of this syndrome. © 2025 The Author(s). American Journal of Medical Genetics Part A published by Wiley Periodicals LLC.",intellectual disability; Nabais Sá-De Vries syndrome; neurodevelopmental disorder; SPOP mutation; visual impairment,"Child; Child, Preschool; Developmental Disabilities; Facies; Female; Humans; Infant; Intellectual Disability; Mutation; Mutation, Missense; Nuclear Proteins; Phenotype; Repressor Proteins; nuclear protein; repressor protein; SPOP protein, human; adaptive behavior; Article; astigmatism; autosomal dominant disorder; birth weight; blepharophimosis; case report; central incisor; child; clinical article; cognition; communication skill; contrast sensitivity; developmental delay; dysphasia; exon; eye movement control; eyebrow; female; forehead; gene; gestational age; growth disorder; human; hypermetropia; intellectual impairment; introspection; language disability; mental disease; microcephaly; motoneuron; mutation; nabais sa de vries syndrome; neurologic disease; nuclear magnetic resonance imaging; oculomotor nerve disease; pathologic nystagmus; phenotype; psychomotor disorder; refraction error; Sanger sequencing; school child; SPOP mutation; strabismus; telecanthus; Vineland Adaptive Behavior Scale; visual acuity; visual impairment; Wechsler intelligence scale; whole exome sequencing; developmental disorder; facies; genetics; infant; intellectual impairment; missense mutation; mutation; pathology; pathophysiology; preschool child",Article,Final,All Open Access; Hybrid Gold Open Access,Scopus,2-s2.0-85216987412,Gaming / VR
Sharma K.; Lee-Cultura S.; Papavlasopoulou S.; Giannakos M.,"Sharma, Kshitij (55903734200); Lee-Cultura, Serena (57203855458); Papavlasopoulou, Sofia (57063398000); Giannakos, Michail (36462343600)",55903734200; 57203855458; 57063398000; 36462343600,"Multimodal Effort Profiles and Children's Performance: Cognitive, Physiological and Physical Dimensions",2025,Journal of Computer Assisted Learning,41,3,e70033,,,,0,10.1111/jcal.70033,https://www.scopus.com/inward/record.uri?eid=2-s2.0-105002133821&doi=10.1111%2fjcal.70033&partnerID=40&md5=a433f05680a7aee2c225e79f625b73ef,"Background: Effort measurement is essential for adaptation to interactive learning technologies. Most contemporary technologies measure effort through the log data (reaction time and correctness). Some adaptive technologies use facial expressions and attention to adapt. Objectives: We present a novel, complementary, and multimodal definition of effort that can be used to adapt not only the content but also the interaction in learning technologies for children. Methods: We propose a 3D view of an effort measurement, that is, cognitive, physiological, and physical. We use eye-tracking, heart rate monitoring, and motion tracking to define these three dimensions. We then apply this measurement within two motion-based educational games and show how this 3D effort varies across the different phases of the educational games and children's performance. Results and Conclusions: The results show an interaction effect of the phase and performance on each of the three effort dimensions. We discuss how these results can inspire the design of multi-sensory adaptive learning systems for children. © 2025 John Wiley & Sons Ltd.",cognitive load; eye-tracking; motion based games; multimodal learning analytics,,Article,Final,,Scopus,2-s2.0-105002133821,Gaming / VR
D’Angelo G.; Clerico V.; Bartolozzi C.; Hoffmann M.; Furlong P.M.; Hadjiivanov A.,"D’Angelo, Giulia (59928514600); Clerico, Victoria (58304053700); Bartolozzi, Chiara (22940171400); Hoffmann, Matej (34971244000); Furlong, P Michael (56260625800); Hadjiivanov, Alexander (57192668774)",59928514600; 58304053700; 22940171400; 34971244000; 56260625800; 57192668774,Wandering around: a bioinspired approach to visual attention through object motion sensitivity,2025,Neuromorphic Computing and Engineering,5,2,24019,,,,0,10.1088/2634-4386/addc90,https://www.scopus.com/inward/record.uri?eid=2-s2.0-105008463792&doi=10.1088%2f2634-4386%2faddc90&partnerID=40&md5=7769cd56310a1c4a61a75f0a4001c135,"Abstract Active vision enables dynamic and robust visual perception, offering an alternative to the static, passive nature of feedforward architectures commonly used in computer vision, which depend on large datasets and high computational resources. Biological selective attention mechanisms allow agents to focus on salient regions of interest (ROIs), reducing computational demand while maintaining real-time responsiveness. Event-based cameras, inspired by the mammalian retina, further enhance this capability by capturing asynchronous scene changes, enabling efficient, low-latency processing. To distinguish moving objects while the event-based camera is also in motion, the agent requires an object motion segmentation mechanism to accurately detect targets and position them at the centre of the visual field (fovea). Integrating event-based sensors with neuromorphic algorithms represents a paradigm shift, using spiking neural networks (SNNs) to parallelise computation and adapt to dynamic environments. This work presents a spiking convolutional neural network bioinspired attention system for selective attention through object motion sensitivity. The system generates events via fixational eye movements using a dynamic vision sensor integrated into the Speck neuromorphic hardware, mounted on a Pan-Tilt unit, to identify the ROI and saccade toward it. The system, characterised using ideal gratings and benchmarked against the event camera motion segmentation dataset, reaches a mean IoU of 82.2% and a mean structural similarity index of 96% in multi-object motion segmentation. Additionally, the detection of salient objects reaches an accuracy of 88.8% in office scenarios and 89.8% in challenging indoor and outdoor low-light conditions, as evaluated on the event-assisted low-light video object segmentation dataset. A real-time demonstrator showcases the system’s capabilities of detecting the salient object through object motion sensitivity in 0.124 s in dynamic scenes. Its learning-free design ensures robustness across diverse perceptual scenes, making it a reliable foundation for real-time robotic applications and serving as a basis for more complex architectures. Media: The accompanying video can be found online7 7 https://youtu.be/dcAJlDgVR0o. . © 2025 The Author(s). Published by IOP Publishing Ltd.",active vision; event-driven cameras; neuromorphic; object motion segmentation; visual attention,,Article,Final,,Scopus,2-s2.0-105008463792,Gaming / VR
Guzman-Jimenez R.; Prem D.; Saldívar A.; Escotto-Córdova E.A.,"Guzman-Jimenez, Rosario (58028548900); Prem, Dhavit (59932168600); Saldívar, Alvaro (58028134200); Escotto-Córdova, Eduardo Alejandro (55806991100)",58028548900; 59932168600; 58028134200; 55806991100,Yupana Inka Tawa Pukllay arithmetic eye tracking analysis: novices,2025,Frontline Learning Research,13,3,,29,52,23.0,0,10.14786/flr.v13i3.1569,https://www.scopus.com/inward/record.uri?eid=2-s2.0-105007467415&doi=10.14786%2fflr.v13i3.1569&partnerID=40&md5=e92845422982ed6c4b920f5766e38023,"The concept of number emerges from the interaction of psychological, behavioral, and material elements of numerical cognition, collapsing the distinction between ""abstract"" and ""concrete."" This dual nature is evident in the Inca numerical system, where tools like the yupana integrate abstract numerical concepts with concrete materials. The Yupana Inka Tawa Pukllay (YITP), a Peruvian arithmetic method, enhances mathematical and visual-spatial skills through tile-based board games. While effective with children, its impact on university students is unexplored. This research used eye tracking to study gaze and attention during YITP operations, comparing novices and experts. Eight university students and two experts participated, with eye-tracking data and scatter plot (dispersion plot) analyses collected using Tobii Pro Glasses. The study introduced the Variation Ratio Tokens (VRT) metric to assess visual attention efficiency, showing significant improvements in VRT dispersion and attention during the arithmetic learning process. These findings suggest YITP's potential in higher education for improving cognitive processes and arithmetic performance, laying a foundation for future research and innovative educational practices. This work establishes a foundation for cross-cultural cognitive studies and innovative STEM education approaches leveraging ancestral knowledge systems. © 2025, European Association for Research on Learning and Instruction. All rights reserved.",arithmetic; eye tracking; inca abacus; inca number system; pattern recognition,,Article,Final,,Scopus,2-s2.0-105007467415,Gaming / VR
Koren O.; Di Via Ioschpe A.; Wilf M.; Dahly B.; Ravona-Springer R.; Plotnik M.,"Koren, Or (57487834900); Di Via Ioschpe, Anais (57212895958); Wilf, Meytal (36515832500); Dahly, Bailasan (59937896400); Ravona-Springer, Ramit (6506431035); Plotnik, Meir (6603202801)",57487834900; 57212895958; 36515832500; 59937896400; 6506431035; 6603202801,Validation of an Automated Scoring Algorithm That Assesses Eye Exploration in a 3-Dimensional Virtual Reality Environment Using Eye-Tracking Sensors,2025,Sensors,25,11,3331,,,,0,10.3390/s25113331,https://www.scopus.com/inward/record.uri?eid=2-s2.0-105007799324&doi=10.3390%2fs25113331&partnerID=40&md5=59c3c6ed7f53c1a6a2f4b9eab4e8fdfc,"Eye-tracking studies in virtual reality (VR) deliver insights into behavioral function. The gold standard of evaluating gaze behavior is based on manual scoring, which is labor-intensive. Previously proposed automated eye-tracking algorithms for VR head mount display (HMD) were not validated against manual scoring, or tested in dynamic areas of interest (AOIs). Our study validates the accuracy of an automated scoring algorithm, which determines temporal fixation behavior on static and dynamic AOIs in VR, against subjective human annotation. The interclass-correlation coefficient (ICC) was calculated for the time of first fixation (TOFF) and total fixation duration (TFD), in ten participants, each presented with 36 static and dynamic AOIs. High ICC values (≥0.982; p < 0.0001) were obtained when comparing the algorithm-generated TOFF and TFD to the raters’ annotations. In sum, our algorithm is accurate in determining temporal parameters related to gaze behavior when using HMD-based VR. Thus, the significant time required for human scoring among numerous raters can be rendered obsolete with a reliable automated scoring system. The algorithm proposed here was designed to sub-serve a separate study that uses TOFF and TFD to differentiate apathy from depression in those suffering from Alzheimer’s dementia. © 2025 by the authors.",cognition; eye tracking; fixation time; virtual reality,Virtual environments; Virtual reality; Area of interest; Cognition; Eye-tracking; Fixation duration; Fixation time; Gaze behaviours; Head-mount displays; Interclass correlation coefficients; Scoring algorithms; Statics and dynamics; Virtualization,Article,Final,,Scopus,2-s2.0-105007799324,Gaming / VR
Abeysinghe Y.; Cauchi K.; Ashok V.; Jayarathna S.,"Abeysinghe, Yasasi (57208207245); Cauchi, Kevin (59954288900); Ashok, Vikas (55633359800); Jayarathna, Sampath (36052654200)",57208207245; 59954288900; 55633359800; 36052654200,Framework for Measuring Visual Attention in Gaze-Driven VR Learning Environments Using Meta Quest Pro,2025,Eye Tracking Research and Applications Symposium (ETRA),,,54,,,,0,10.1145/3715669.3726795,https://www.scopus.com/inward/record.uri?eid=2-s2.0-105008497876&doi=10.1145%2f3715669.3726795&partnerID=40&md5=476e1a1d21c32f2387ea96b26181635d,"The growing availability of consumer-grade devices equipped with eye-tracking optics, including Augmented/virtual reality (AR/VR) headsets, has brought eye-tracking technology into wider use than ever before. Understanding the focus and visual scanning behavior of users can help optimize users’ engagement in immersive environments. In this study, we present a framework for measuring visual attention in a gaze-driven VR learning environment using a consumer-grade Meta Quest Pro VR headset. This system generates and presents basic and advanced eye-tracking measures such as fixation duration, saccade amplitude, and ambient/focal attention coefficient K as indicators of visual attention in the VR environment. © 2025 Copyright held by the owner/author(s).",Advanced Gaze Measures; Eye Tracking; Meta Quest Pro; Virtual Reality; Visual Attention,Augmented reality; Behavioral research; Eye movements; Interactive computer graphics; Advanced gaze measure; Augmented/virtual reality; Eye tracking technologies; Eye-tracking; Immersive environment; Learning environments; Meta quest pro; User engagement; Visual Attention; Visual scanning; Eye tracking; Virtual reality,Conference paper,Final,,Scopus,2-s2.0-105008497876,Gaming / VR
Gollan B.; Raggam P.,"Gollan, Benedikt (48361077700); Raggam, Philipp (57203240560)",48361077700; 57203240560,Beyond Gaze: Quantifying Conscious Perception Through an Innovative Eye Tracking Biomarker,2025,Proceedings of the ACM on Human-Computer Interaction,9,3,ARTETRA06,,,,0,10.1145/3725831,https://www.scopus.com/inward/record.uri?eid=2-s2.0-105006516929&doi=10.1145%2f3725831&partnerID=40&md5=9c7538030bdcf04093c4eeb041c1b67f,"This paper introduces a novel digital biomarker based on eye tracking data, the Conscious Perception Index (CPI), designed to measure conscious perception by leveraging established eye gaze metrics. The CPI builds upon foundational eye-tracking markers, including saccades and fixation statistics, coefficient K, and cognitive load, to provide a continuous, quantified computation of perceptual awareness in real time. To evaluate CPI's effectiveness, a change blindness study was conducted in a VR setting, allowing the analysis of conscious perception within an explicit interaction context. Findings demonstrate that the CPI provides a reliable measure of conscious engagement, with significant results in statistical analysis supporting its robustness. Classification via LogisticRegression is able to separate conscious interaction from observational behavior with an accuracy of 83.3%. This research underscores CPI's potential to enhance eye-tracking applications in cognitive science and human-computer interaction, opening new avenues for measuring perceptual awareness and refining interactive technologies based on user perception. © 2025 ACM.",cognitive modeling; consciousness of perception; eye-tracking,Interactive computer systems; Change blindness; Cognitive loads; Cognitive model; Consciousness of perception; Eye-gaze; Eye-tracking; Interaction context; Real- time; Tracking application; Tracking data; Vision,Article,Final,,Scopus,2-s2.0-105006516929,Gaming / VR
Kiderman A.; Coto J.; Gibson L.C.; Ashmore R.C.; Braverman A.; Williams E.; Finamore A.M.F.; Yunis V.; Hoffer M.E.,"Kiderman, Alex (6506842178); Coto, Jennifer (57204269549); Gibson, Laura C. (57191759205); Ashmore, Robin C. (59847137300); Braverman, Alexandr (56921632300); Williams, Erin (57222079019); Finamore, Angela M. Flamm (59739052000); Yunis, Valerie (59395972400); Hoffer, Michael E. (7006165812)",6506842178; 57204269549; 57191759205; 59847137300; 56921632300; 57222079019; 59739052000; 59395972400; 7006165812,"Oculomotor, vestibular, reaction time, and cognitive (OVRT-C) responses in 7- to 17-year-old children",2025,Experimental Brain Research,243,5,110,,,,0,10.1007/s00221-025-07005-y,https://www.scopus.com/inward/record.uri?eid=2-s2.0-105002747351&doi=10.1007%2fs00221-025-07005-y&partnerID=40&md5=f046d933f3374cc9d5a18885603386a4,"Several aspects of oculomotor, vestibular, reaction time, and cognitive (OVRT-C) abilities improve throughout childhood at varying rates and become adult-like at different ages. However, developmental testing of these abilities often focuses on limited age ranges and does not elucidate clear developmental trajectories. The present study utilized high-resolution eye-tracking to evaluate 40 children aged 7–17 years on a comprehensive battery of OVRT-C tests to better understand how and when these abilities develop across childhood. As expected, mean responses on OVRT-C tests showed consistent improvement as subject age increased. We report a high prevalence of saccadic intrusions during smooth pursuit in children and adolescents, more self-paced saccades in older children, decreased auditory and visual RT with age, and fewer errors on the anti-saccade test in older children. We also used the Akaike information criterion (AIC) and Bayesian information criterion (BIC) modelling to determine whether a two- or three age group division would be most appropriate for each OVRT-C test. For all key OVRT-C metrics, our data support a separation of children into two age groups as opposed to three. While the age group divide varied by OVRT-C test, these data suggest these abilities mature at differing rates, and optimal separations into two age groups rather than three may reflect a slowing of rapid development as OVRT-C performance becomes more adult-like. © The Author(s) 2025.",Development; Eye-tracking; Oculomotor,"Adolescent; Age Factors; Child; Child Development; Cognition; Eye Movements; Eye-Tracking Technology; Female; Humans; Male; Psychomotor Performance; Reaction Time; Saccades; Vestibule, Labyrinth; adolescent; adult; Article; artifact; Black person; Caucasian; child; child development; clinical article; cognition; comparative study; eye tracking; eye-tracking technology; female; Hispanic; human; male; oculomotor system; pathologic nystagmus; prevalence; questionnaire; reaction time; saccadic eye movement; school child; smooth pursuit eye movement; vestibular function; vestibular system; visual stimulation; voice; young adult; age; child development; eye movement; physiology; psychomotor performance; reaction time; vestibular labyrinth",Article,Final,All Open Access; Hybrid Gold Open Access,Scopus,2-s2.0-105002747351,Gaming / VR
Maquiling V.; Zhaoping L.; Kasneci E.,"Maquiling, Virmarie (58202025500); Zhaoping, Li (6602091539); Kasneci, Enkelejda (56059892600)",58202025500; 6602091539; 56059892600,Imperceptible Gaze Guidance Through Ocularity in Virtual Reality,2025,Proceedings of the ACM on Human-Computer Interaction,9,3,ARTETRA14,,,,0,10.1145/3725839,https://www.scopus.com/inward/record.uri?eid=2-s2.0-105006524593&doi=10.1145%2f3725839&partnerID=40&md5=aa94d8cccbd22943fb9498b1227f62f9,"We introduce to VR a novel imperceptible gaze guidance technique from a recent discovery that human gaze can be attracted to a cue that contrasts from the background in its perceptually non-distinctive ocularity, defined as the relative difference between inputs to the two eyes. This cue pops out in the saliency map in the primary visual cortex without being overtly visible. We tested this method in an odd-one-out visual search task using eye tracking with 31 participants in VR. When the target was rendered as an ocularity singleton, participants' gaze was drawn to the target faster. Conversely, when a background object served as the ocularity singleton, it distracted gaze from the target. Since ocularity is nearly imperceptible, our method maintains user immersion while guiding attention without noticeable scene alterations and can render object's depth in 3D scenes, creating new possibilities for immersive user experience across diverse VR applications. © 2025 Copyright held by the owner/author(s).",Eye tracking; Gaze guidance; Saliency; V1 Saliency Hypothesis; Virtual reality,Discrete event simulation; Three dimensional computer graphics; 3D scenes; Background objects; Eye-tracking; Gaze guidances; Primary visual cortex; Saliency; Saliency map; Search tasks; V1 saliency hypothesis; Visual search; Virtual reality,Article,Final,,Scopus,2-s2.0-105006524593,Gaming / VR
Stasch S.-M.; Mack W.,"Stasch, Sophie-Marie (57226831860); Mack, Wolfgang (56248614100)",57226831860; 56248614100,Exploring Task Prioritization in VR Flight Environments: Can Eye-Tracking Uncover Cognitive Control?,2025,Eye Tracking Research and Applications Symposium (ETRA),,,76,,,,0,10.1145/3715669.3725898,https://www.scopus.com/inward/record.uri?eid=2-s2.0-105008498711&doi=10.1145%2f3715669.3725898&partnerID=40&md5=5cd52c6739b726716d6f318d226c9a87,"Cognitive control is important for multitasking performance, which is essential for safety and efficiency in human-machine interaction domains such as aviation. Previous research has shown that the stability-flexibility dilemma of cognitive control can be manipulated through task prioritization in low-fidelity flight environments, with eye-tracking metrics effectively classifying this dilemma in a low-fidelity flight environment using a machine learning approach. However, it is unclear whether these results extend to higher-fidelity environments. This study examines the applicability of this approach to eye-tracking metrics assessed in a VR flight simulator. Linear mixed-effects models reveal significant differences in fixation duration, relative number of fixations, coefficient K, stationary entropy, and explore-exploit ratio across flight scenarios with varying task prioritization. Additionally, these metrics were used as input features in a machine learning model to classify the mission scenarios. The findings have important implications for designing adaptive assistance systems aiming at supporting multitasking performance in safety-critical environments. © 2025 Copyright held by the owner/author(s).",aviation; multitasking; virtual-reality,Adaptive control systems; Aviation; Eye movements; Eye tracking; Flight control systems; Flight simulators; Human computer interaction; Learning systems; Machine learning; Multitasking; Cognitive control; Eye-tracking; Flight environment; Human machine interaction; Interaction domain; Low fidelities; Machine learning approaches; Performance; Prioritization; Safety and efficiencies; Virtual reality,Conference paper,Final,,Scopus,2-s2.0-105008498711,Gaming / VR
Reich D.R.; Prasse P.; Jäger L.A.,"Reich, David R. (57392440200); Prasse, Paul (55376814000); Jäger, Lena A. (56503248300)",57392440200; 55376814000; 56503248300,Evaluating Gaze Event Detection Algorithms: Impacts on Machine Learning-based Classification and Psycholinguistic Statistical Modeling,2025,Proceedings of the ACM on Human-Computer Interaction,9,3,ARTETRA10,,,,0,10.1145/3725835,https://www.scopus.com/inward/record.uri?eid=2-s2.0-105006603092&doi=10.1145%2f3725835&partnerID=40&md5=4e1fbc0da41d130c8f5a25b64a57e971,"Eye movements offer valuable, non-invasive insights into cognitive processes and are widely used in both psycholinguistic research and machine-learning applications, such as assessing reading comprehension and cognitive load. These applications typically rely on fixations and saccades detected through gaze event algorithms, which may be either proprietary or open-source. The impact of different gaze event detection algorithms on subsequent analysis is underexplored and often overlooked. This study investigates how two threshold-based algorithms, I-DT and I-VT, influence both machine-learning classification tasks and psycholinguistic statistical modeling. Using diverse datasets-including stationary, remote, and VR eye-tracking data across multiple sampling frequencies-our findings show significant differences in downstream performance. For ML tasks, I-DT generally outperforms I-VT, with I-VT being highly sensitive to threshold choices. In psycholinguistic analysis, results confirm established findings only when thresholds align with established fixation metrics, emphasizing the importance of appropriate threshold selection for meaningful analysis. © 2025 Copyright held by the owner/author(s).",eye movements; I-DT; I-VT; machine learning; psycholinguistic analysis; psycholinguistic statistical modeling,Cognitive process; Event detection algorithm; I-DT; I-VT; Machine learning applications; Machine-learning; On-machines; Psycholinguistic analyze; Psycholinguistic statistical modeling; Statistic modeling; Psychoacoustic,Article,Final,,Scopus,2-s2.0-105006603092,Gaming / VR
Rohaly T.; Tweedell A.; Tam J.; Stauff A.; Callahan-Flintoft C.,"Rohaly, Thomas (58068876100); Tweedell, Andrew (56215688500); Tam, Joyce (55599771900); Stauff, Alexander (59912949500); Callahan-Flintoft, Chloe (57191728837)",58068876100; 56215688500; 55599771900; 59912949500; 57191728837,"A Bayesian Model to Describe the Relationship between Gaze Pattern, Task Familiarity, and Search Efficiency in a Visual Foraging Task",2025,Proceedings of the ACM on Human-Computer Interaction,9,3,ETRA02,,,,0,10.1145/3725827,https://www.scopus.com/inward/record.uri?eid=2-s2.0-105006675032&doi=10.1145%2f3725827&partnerID=40&md5=778eca4121a5d9a4dd585cde56e09130,"Foraging is common in everyday life and studying search for multiple targets can shed light on underlying mechanisms of multiple visual processes. The current study had participants forage in virtual reality to allow for more naturalist eye and head movement, shooting targets amongst distractors. Eye-tracking metrics were used to unpack what strategies were related to search efficiency in earlier versus later trials where performance improved with task familiarity. A Bayesian hierarchical model showed that, consistent with previous findings, saccade frequency and fixation duration of targets were related to more efficient foraging. The relationship with saccade frequency was also stronger in later trials compared to earlier. Interestingly, longer fixation durations on distractors also related to more efficient foraging, but this pattern disappeared or was reversed when individual distractor types were examined. Our results highlight how gaze patterns may be altered to coordinate with other body movements in unconstrained search. © 2025 Copyright is held by the owner/author(s).",eye movements and cognition; virtual reality; visual search behavior,Virtual reality; Visualization; Bayesian modelling; Eye movement and cognition; Fixation duration; Foraging task; Multiple targets; Search behavior; Search efficiency; Visual process; Visual search; Visual search behavior; Virtualization,Article,Final,,Scopus,2-s2.0-105006675032,Gaming / VR
Sankar B.; Sen D.,"Sankar, B. (57191344691); Sen, Dibakar (57203031055)",57191344691; 57203031055,Designing with the Eyes: Implicit Attention-based Form Ideation in Virtual Reality,2025,Eye Tracking Research and Applications Symposium (ETRA),,,41,,,,0,10.1145/3715669.3726819,https://www.scopus.com/inward/record.uri?eid=2-s2.0-105008489610&doi=10.1145%2f3715669.3726819&partnerID=40&md5=2be77f5c24ee30667aea5d65f0ffd712,"Traditional form ideation methods rely heavily on explicit user input, manual curation, and subjective interpretation, often limiting creativity and increasing cognitive load for novice product designers. We introduce ""EUPHORIA"", a novel approach that leverages implicit visual attention through real-time gaze tracking and feature extraction in a virtual reality (VR) environment to automatically generate mood boards for product form ideation. Using an immersive space containing 150 visual stimuli, participants freely explored images while their gaze behaviours were recorded implicitly. Analysis from this initial self-attention phase (Phase 1) demonstrated a significant correlation (r = 0.816, p<0.01) between gaze fixation durations and subjective user preferences. Results validate the hypothesis that implicit gaze metrics reliably reflect user preferences. Then, participants went on to explore visual stimuli after pre-conditioning their minds with emotional phrases (Phase 2 - stimuli-attention). The preconditioned emotional stimuli in Phase 2 yielded clear emotional category ratings (positive, neutral, negative), with reduced fixation times indicating faster image selection. Ongoing research will extend this framework to the strategic attention phase (Phase 3), offering novice designers an intuitive, efficient, and personalized tool for automated form generation. © 2025 Copyright held by the owner/author(s).",Form Design; Gaze-Tracking; Moodboard; Virtual Reality,Behavioral research; Eye tracking; Human engineering; Interactive computer graphics; Product design; Virtual environments; Cognitive loads; Curation; Form design; Gaze-tracking; Ideation methods; Moodboard; Phase 2; User input; User's preferences; Visual stimulus; Virtual reality,Conference paper,Final,,Scopus,2-s2.0-105008489610,Gaming / VR
Diotaiuti P.; Marotta G.; Di Siena F.; Vitiello S.; Di Prinzio F.; Rodio A.; Di Libero T.; Falese L.; Mancone S.,"Diotaiuti, Pierluigi (56263490400); Marotta, Giulio (59751558800); Di Siena, Francesco (59252195700); Vitiello, Salvatore (59751706300); Di Prinzio, Francesco (57212512003); Rodio, Angelo (6603475587); Di Libero, Tommaso (57235284300); Falese, Lavinia (35310081000); Mancone, Stefania (36337979300)",56263490400; 59751558800; 59252195700; 59751706300; 57212512003; 6603475587; 57235284300; 35310081000; 36337979300,Eye Tracking in Parkinson’s Disease: A Review of Oculomotor Markers and Clinical Applications,2025,Brain Sciences,15,4,362,,,,1,10.3390/brainsci15040362,https://www.scopus.com/inward/record.uri?eid=2-s2.0-105003478454&doi=10.3390%2fbrainsci15040362&partnerID=40&md5=fe5ff0e44e2385e7231702fcd32e3718,"(1) Background. Eye movement abnormalities are increasingly recognized as early biomarkers of Parkinson’s disease (PD), reflecting both motor and cognitive dysfunction. Advances in eye-tracking technology provide objective, quantifiable measures of saccadic impairments, fixation instability, smooth pursuit deficits, and pupillary changes. These advances offer new opportunities for early diagnosis, disease monitoring, and neurorehabilitation. (2) Objective. This narrative review explores the relationship between oculomotor dysfunction and PD pathophysiology, highlighting the potential applications of eye tracking in clinical and research settings. (3) Methods. A comprehensive literature review was conducted, focusing on peer-reviewed studies examining eye movement dysfunction in PD. Relevant publications were identified through PubMed, Scopus, and Web of Science, using key terms, such as “eye movements in Parkinson’s disease”, “saccadic control and neurodegeneration”, “fixation instability in PD”, and “eye-tracking for cognitive assessment”. Studies integrating machine learning (ML) models and VR-based interventions were also included. (4) Results. Patients with PD exhibit distinct saccadic abnormalities, including hypometric saccades, prolonged saccadic latency, and increased anti-saccade errors. These impairments correlate with executive dysfunction and disease progression. Fixation instability and altered pupillary responses further support the role of oculomotor metrics as non-invasive biomarkers. Emerging AI-driven eye-tracking models show promise for automated PD diagnosis and progression tracking. (5) Conclusions. Eye tracking provides a reliable, cost-effective tool for early PD detection, cognitive assessment, and rehabilitation. Future research should focus on standardizing clinical protocols, validating predictive AI models, and integrating eye tracking into multimodal treatment strategies. © 2025 by the authors.",cognitive impairment; eye tracking; fixation instability; machine learning; neurorehabilitation; Parkinson’s disease; pupillary response; saccadic dysfunction,biological marker; clinical protocol; cognition; cognitive defect; disease exacerbation; eye movement; eye tracking; human; machine learning; neurorehabilitation; oculomotor system; Parkinson disease; pathophysiology; Review; saccadic eye movement,Review,Final,,Scopus,2-s2.0-105003478454,Gaming / VR
Liu S.; Mädche A.; Feick M.,"Liu, Shi (57913265700); Mädche, Alexander (57194237390); Feick, Martin (57201559467)",57913265700; 57194237390; 57201559467,GazeClass: Towards Gaze-Adaptive Cross-Device Learning Support for Virtual Classrooms,2025,Conference on Human Factors in Computing Systems - Proceedings ,,,299,,,,0,10.1145/3706599.3720232,https://www.scopus.com/inward/record.uri?eid=2-s2.0-105005745711&doi=10.1145%2f3706599.3720232&partnerID=40&md5=8eac330e75b279a2754994c6c87de26b,"Immersive virtual classrooms provide engaging learning experiences but often lack individualized support, particularly in asynchronous settings where students independently engage with prerecorded lectures. This absence of live teacher interaction can hinder personalized guidance and self-reflection. To address this challenge, we present GazeClass, a system enabling cross-device interaction by combining eye tracking in Virtual Reality (VR) with a learning assistant powered by large language models (LLMs) on a separate mobile or desktop device, delivering post-lecture, gaze-adaptive learning support. During the lecture, eye tracking monitors students’ attention, and afterward, tailored feedback and self-reflective Q&A are provided to bridge attention gaps. The cross-device interaction between VR and a mobile or desktop device fosters interactivity and active learning. Developed through co-design workshops and iterative evaluations, GazeClass was validated in a preliminary study, demonstrating its potential to enhance learning outcomes by addressing key challenges in immersive learning. © 2025 Copyright held by the owner/author(s).",adaptive support; eye tracking; mixed reality; virtual classroom,Students; Teaching; Virtual reality; Adaptive support; Cross-device interactions; Desktop devices; Eye-tracking; Immersive; Learning experiences; Learning support; Mixed reality; Teachers'; Virtual Classroom; Active learning,Conference paper,Final,,Scopus,2-s2.0-105005745711,Gaming / VR
Cavadini T.; Courbois Y.; Gentaz E.,"Cavadini, Thalia (57223026777); Courbois, Yannick (22034531800); Gentaz, Edouard (55917486200)",57223026777; 22034531800; 55917486200,Improving social-emotional abilities in children with profound intellectual and multiple disabilities through a person-centred eye-tracking-based training: A pilot study,2025,Acta Psychologica,255,,104928,,,,0,10.1016/j.actpsy.2025.104928,https://www.scopus.com/inward/record.uri?eid=2-s2.0-105000573879&doi=10.1016%2fj.actpsy.2025.104928&partnerID=40&md5=b90603db39a57f17a1d3dc3269c8a214,"Individuals with profound intellectual and multiple disabilities (PIMD) are characterized by a combination of a profound intellectual disabilities and a profound motor disability frequently associated with a number of additional severe secondary disabilities or impairments. The aim of this pilot study was to evaluate the effects of an innovative person-centred training based on eye-tracking computerised serious games on the social-emotional abilities in these individuals with PIMD. Nine participants aged 7–18 years were followed over a period of 1 year. A pre-test (T1) – training – post-test (T2) design was used. During T1 and T2, visual attention and six social-emotional abilities (preferential attention to biological motion, social orienting, facial expression exploration, emotional faces discrimination, joint attention and socio-moral evaluations) were assessed using an eye-tracking-based experimental paradigm combining various visual preference tasks. During the training, each participant benefited from personalized one-to-one sessions tailored to their skills based on results of T1 and the observations of their practitioners. To implement person-centred training, the experimenter chose from a set of serious games to train these social-emotional abilities, those he felt were best suited to the participant's current state of heath and alertness, personal skills and specific needs. All participants improved their visual exploration between T1 and T2. In addition, they all made progress on at least one of the six social-emotional competencies. These results showed preliminary evidence that it is possible to increase some social-emotional abilities in these individuals with an adapted training, thus indicating that they also have unsuspected learning abilities. © 2025",Eye-tracking; Learning abilities; Profound intellectual and multiple disabilities; Social-emotional training,Adolescent; Attention; Child; Children with Disabilities; Emotions; Eye-Tracking Technology; Facial Expression; Female; Humans; Intellectual Disability; Male; Pilot Projects; Social Skills; Video Games; adolescent; attention; child; disabled child; emotion; eye-tracking technology; facial expression; female; human; intellectual impairment; male; physiology; pilot study; psychology; rehabilitation; social competence; video game,Article,Final,,Scopus,2-s2.0-105000573879,Gaming / VR
Schott E.; López García I.; Semple L.A.; Froehlich B.,"Schott, Ephraim (57219867259); López García, Irene (57222255973); Semple, Lauren August (59907461200); Froehlich, Bernd (18433968300)",57219867259; 57222255973; 59907461200; 18433968300,Estimating Detection Thresholds of Being Looked at in Virtual Reality for Avatar Redirection,2025,Conference on Human Factors in Computing Systems - Proceedings ,,,1125,,,,0,10.1145/3706598.3714041,https://www.scopus.com/inward/record.uri?eid=2-s2.0-105005755223&doi=10.1145%2f3706598.3714041&partnerID=40&md5=5e26f98ecad36b4f2ed78df83ebcb1c1,"The human face and eyes provide crucial conversational cues about a person's focus of attention. In virtual reality applications, avatar faces are typically simplified, and eye movements often neglected. This paper explores how VR users perceive the look-at direction of other avatars and estimates the range within which an avatar's averted gaze goes unnoticed. Through two-alternative forced choice experiments, we investigate different gaze offsets to quantify thresholds for perceived gaze aversion across three conditions: gaze side (left/right), stimulus duration, and avatar distance. Additionally, we assess the impact of averted gaze on social presence during interactions with an embodied conversational agent in a social game. A user study (N=40) revealed that social presence is significantly affected by averted gaze when noticed, and that detection thresholds are particularly impacted by stimuli duration and interactions between side and distance. Our findings provide a foundation for understanding gaze perception in social virtual reality. © 2025 Copyright held by the owner/author(s).",avatar redirection; averted gaze; embodied conversational agents; gaze direction; gaze perception; threshold detection; virtual reality,Virtual environments; Virtual reality; Avatar redirection; Averted gaze; Detection threshold; Embodied conversational agent; Gaze direction; Gaze perception; Human eye; Human faces; Social presence; Threshold detection; Virtualization,Conference paper,Final,,Scopus,2-s2.0-105005755223,Gaming / VR
Jeppesen A.C.E.; Andresen J.; Parvaiz R.; Clemmensen L.; Jepsen J.R.M.; Hansen D.W.; Glenthøj L.B.,"Jeppesen, Alberte Cathrine Ehrhardt (59525096200); Andresen, Johannes (59525620800); Parvaiz, Rizwan (57990265400); Clemmensen, Lars (57209036755); Jepsen, Jens Richardt Møllegaard (26532977800); Hansen, Dan Witzner (15063910800); Glenthøj, Louise Birkedal (34976586900)",59525096200; 59525620800; 57990265400; 57209036755; 26532977800; 15063910800; 34976586900,Study protocol for the EYEdentify project: An examination of gaze behaviour in autistic adults using a virtual reality-based paradigm,2025,PLoS ONE,20,4-Apr,e0316502,,,,0,10.1371/journal.pone.0316502,https://www.scopus.com/inward/record.uri?eid=2-s2.0-105002407426&doi=10.1371%2fjournal.pone.0316502&partnerID=40&md5=46137e7532638ea7d2d9496b25eb60f6,"Introduction Autism Spectrum Condition (ASC) is characterised by difficulties in social communication and interaction, which may pose significant challenges to daily functioning throughout life. While current diagnostic methods for ASC often rely on measures based on subjective reports, there is a growing need for objective, quantifiable measures to support current clinical assessment of ASC. Eye-tracking technology records eye and gaze movements in real time and provides a direct and objective method for assessing social attention. Integrating eye-tracking within virtual reality (VR) environments presents a novel approach for capturing gaze behaviour in dynamic, ecologically valid social scenarios. This study aims to investigate whether VR-based eye information can reveal group differences in gaze behaviour between autistic adults and neurotypical controls in simulated social interactions. Methods This case-control study will include 140 adults diagnosed with ASC and 50 neurotypical controls, matched by age and gender. Participants will engage in six VR-based social scenarios, which vary in social complexity and the presence of non-social distractors. Eye information will be measured using eye-tracking technology integrated into a head-mounted display. Gaze behaviour will be analysed through fixation-based metrics on parameters including number of fixations, mean fixation time, and dwell time, on predetermined Areas of Interest. Analysis Statistical analyses will assess between-group differences in gaze behaviour as well as correlations between gaze metrics and clinical measures of social functioning, social cognition and symptom severity. Discussion This study utilises VR-based eye-tracking to investigate novel paradigms for assessing gaze behaviour in ASC in immersive, interactive environments and aims to advance the current understanding of visual social attention in ASC. Positive outcomes from this study may support further research into VR-based eye-tracking to supplement existing clinical assessment methods. © 2025 Jeppesen et al. This is an open access article distributed under the terms of the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.",,"Adolescent; Adult; Attention; Autism Spectrum Disorder; Autistic Disorder; Case-Control Studies; Eye Movements; Eye-Tracking Technology; Female; Fixation, Ocular; Humans; Male; Middle Aged; Social Interaction; Virtual Reality; Young Adult; adhd self report scale; adolescent; Article; autism; autism assessment; Beck Depression Inventory; behavior assessment; camouflaging autistic traits questionnaire; cantab emotion recognition task; case control study; child; cognition assessment; controlled study; cybersickness; dwell time; empathy; eye tracking; female; gaze; high risk social challenge task; hinting task; human; interview; major clinical study; male; mental disease; personal and social performance scale; psychometry; randomized controlled trial (topic); repetitive behaviour questionnaire; sensory reactivity in autism spectrum questionnaire; social interaction; Social Interaction Anxiety Scale; Social Responsiveness Scale-second edition; social skills performance assessment; social support; temple university community participation; virtual reality; visual attention; Wechsler adult intelligence scale; adult; attention; eye fixation; eye movement; eye-tracking technology; middle aged; pathophysiology; physiology; psychology; young adult",Article,Final,,Scopus,2-s2.0-105002407426,Gaming / VR
Zhu H.; Kong Y.; Zhang H.; Gu Z.; Ohno R.,"Zhu, Haipeng (59798348800); Kong, Yuhang (54403029900); Zhang, Hong (59378631800); Gu, Zongchao (57212190966); Ohno, Ryuzo (59089982500)",59798348800; 54403029900; 59378631800; 57212190966; 59089982500,Effects of scenery frame on visual depth perception in classical Chinese gardens: A case study of the Lvyin Pavilion in Lingering Garden,2025,Frontiers of Architectural Research,14,2,,402,415,13.0,0,10.1016/j.foar.2024.08.009,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85207105087&doi=10.1016%2fj.foar.2024.08.009&partnerID=40&md5=ed3d4d5c6c48045944c5ec8b154ee0a4,"Visual depth (distance) perception is a fundamental aspect of environmental cognition, as it allows people to judge the spatial scale of their surroundings. However, estimating the depth of classical Chinese gardens is challenging, especially from static viewpoints that frame the scenery. Previous studies have examined how the internal components of the scenery frame affect depth perception. Still, the role of the frame and its peripheral information as environmental background have been largely overlooked. This study investigates how depth perception at viewpoints is influenced by viewing position displacement, frame geometry, and environmental context. The authors created nine stimulus materials in a cave virtual reality environment (three image treatments × three positions). Seventy-one participants were asked to evaluate depth perception using the magnitude estimation and adjustment methods. Their eye movement behavior was also recorded using an eye-movement instrument (SensoMotoric Instruments (SMI) eye-tracking glasses, 120 Hz). The results showed that participants could perceive spatial depth differences between viewing positions even when the internal viewpoint displacement was small; frame shape did not significantly affect depth perception and gaze behavior; and peripheral visual information of the frame enhanced depth perception significantly. Moreover, the form of the environmental background, especially the position of the scenery window, strongly guided the participants’ gaze. These findings suggest that ambient visual information significantly impacts environmental experience, which landscape designers should consider. © 2024 The Author(s)",Ambient visual information; China; Eye-tracking technology; Frame; Landscape design; Lvyin Pavilion of Lingering Garden; Visual depth perception; VR experiment,,Article,Final,,Scopus,2-s2.0-85207105087,Gaming / VR
Hwang E.; Lee J.,"Hwang, Eugene (57542512000); Lee, Jeongmi (37034338500)",57542512000; 37034338500,Looking but Not Focusing: Defining Gaze-Based Indices of Attention Lapses and Classifying Attentional States,2025,Conference on Human Factors in Computing Systems - Proceedings ,,,754,,,,0,10.1145/3706598.3714269,https://www.scopus.com/inward/record.uri?eid=2-s2.0-105005745118&doi=10.1145%2f3706598.3714269&partnerID=40&md5=fb192af22df5486c994113757b76d4f3,"Identifying objective markers of attentional states is critical, particularly in real-world scenarios where attentional lapses have serious consequences. In this study, we identified gaze-based indices of attentional lapses and validated them by examining their impact on the performance of classification models. We designed a virtual reality visual search task that encouraged active eye movements to define dynamic gaze-based metrics of different attentional states (zone in/out). The results revealed significant differences in both reactive ocular features, such as first fixation and saccade onset latency, and global ocular features, such as saccade amplitude, depending on the attentional state. Moreover, the performance of the classification models improved significantly when trained only on the proven gaze-based and behavioral indices rather than all available features, with the highest prediction accuracy of 79.3%. We highlight the importance of the preliminary studies before model training and provide generalizable gaze-based indices of attentional states for practical applications. © 2025 Copyright held by the owner/author(s).",Attention Lapses; Classification Model; Eye-tracking; Gaze-based Indices; Sustained Attention; Zoning out,Attention lapse; Classification models; Eye-tracking; Gaze-based index; Performance; Real-world scenario; Search tasks; Sustained attention; Visual search; Zoning outs; Virtual reality,Conference paper,Final,,Scopus,2-s2.0-105005745118,Gaming / VR
Aksoy M.E.; Izzetoglu K.; Utkan N.Z.; Agrali A.; Yoner S.I.; Bishop A.; Shewokis P.A.,"Aksoy, Mehmet Emin (23970353200); Izzetoglu, Kurtulus (6603128234); Utkan, Nihat Zafer (6602831324); Agrali, Atahan (57194279742); Yoner, Serhat Ilgaz (57193696035); Bishop, Ashley (57219441834); Shewokis, Patricia A. (6602855729)",23970353200; 6603128234; 6602831324; 57194279742; 57193696035; 57219441834; 6602855729,Comparing Behavioral and Neural Activity Changes During Laparoscopic and Robotic Surgery Trainings,2025,Journal of Surgical Education,82,5,103486,,,,1,10.1016/j.jsurg.2025.103486,https://www.scopus.com/inward/record.uri?eid=2-s2.0-86000276586&doi=10.1016%2fj.jsurg.2025.103486&partnerID=40&md5=b4a534adad4a29a288499e8ff6ec871b,"Objective: This study aims to compare the cognitive workload levels of general surgery residents by measuring prefrontal cortex hemodynamic activity while performing a similar task using robotic-assisted surgery (RAS) and laparoscopic surgery simulators. Design: The study was conducted with 22 general surgery residents who completed a peg transfer task in simulated laparoscopic and RAS training environments. Participants' performance and neurophysiological data were collected over a 1-month period. Setting: The study was conducted at Acibadem Mehmet Ali Aydinlar University- CASE (Center of Advanced Simulation and Education), utilizing a laparoscopic training simulator (CAE Lap VR) and robotic surgery simulator (Da Vinci Surgical System Si console with Backpack). Participants: Twenty-two general surgery residents (mean [SD] age, 29.45 [2.40] years; 18 [81.82%] male) volunteered for the study. None of the participants had prior experience with RAS or RAS simulators, whereas most had varying degrees of laparoscopic surgery experience. Results: Significant differences were observed between RAS and laparoscopic simulations in terms of performance time and neural activity. Peg transfer times were shorter in RAS simulations compared to laparoscopic simulations (χ2(3) = 134.805, p < 0.001). Mean oxygenated hemoglobin (ΔHbO) levels in the prefrontal cortex were lower in RAS simulations (χ2(3) = 20.695, p < 0.001), indicating reduced cognitive workload. Relative Neural Efficiency (RNE) and Relative Neural Involvement (RNI) scores were higher in RAS tasks (χ2(1) = 55.765, p < 0.001), suggesting greater efficiency and involvement during robotic-assisted procedures. Conclusions: The findings indicate that RAS tasks are associated with lower cognitive workload and improved efficiency compared to laparoscopic tasks. Incorporating neural indices alongside performance metrics may enhance training assessments and provide deeper insights into trainees’ experiences in simulation-based surgical education. © 2025 The Author(s)",functional near infrared spectroscopy efficiency index; minimal invasive surgery; robotic surgery; training,"Adult; Clinical Competence; Cognition; Education, Medical, Graduate; Female; General Surgery; Humans; Internship and Residency; Laparoscopy; Male; Prefrontal Cortex; Robotic Surgical Procedures; Simulation Training; Task Performance and Analysis; Workload; hemoglobin; adult; Article; female; functional near-infrared spectroscopy; general surgery; hemodynamics; human; job experience; laparoscopic surgery; male; medical education; mental performance; neurophysiology; physician engagement; post hoc analysis; prefrontal cortex; productivity; resident; robot assisted surgery; scoring system; simulation; statistically significant result; treatment duration; volunteer; work environment; workload; clinical competence; cognition; comparative study; education; laparoscopy; physiology; procedures; simulation training; task performance; vascularization; workload",Article,Final,,Scopus,2-s2.0-86000276586,Gaming / VR
Majidi M.H.; Borhani K.,"Majidi, Mohammad Hossein (59728768700); Borhani, Khatereh (56786167000)",59728768700; 56786167000,The Impact of Threatening Facial Expressions on Negotiation: An Eye-Tracking Study,2025,Brain and Behavior,15,4,e70461,,,,0,10.1002/brb3.70461,https://www.scopus.com/inward/record.uri?eid=2-s2.0-105002114234&doi=10.1002%2fbrb3.70461&partnerID=40&md5=acf03794dbd34cb0d87aebe3aec32943,"Introduction: Facial expressions play a crucial role in social interactions, influencing trust, and decision-making. In negotiations, threatening expressions may convey dominance or hostility, potentially reducing cooperation. This study explores how threatening facial expressions and autistic traits (ATs) affect social decision-making in the Ultimatum Game (UG), focusing on their main effects on UG offers. Method: Fifty adults participated in the study. A Linear Mixed Model (LMM) was conducted to analyze the main effects of threat level and ATs on UG proposals. In addition, eye-tracking technology was used to investigate participants' visual attention toward different facial areas. Results: The results revealed a significant main effect of threatening facial expressions, as participants made lower offers in response to high-threat faces. However, ATs did not show a significant main effect on UG proposals. Eye-tracking data showed that participants focused more on the eyes of high-threat faces compared to low-threat faces. Conclusion: These findings support the Emotion-as-Social-Information (EASI) model, suggesting that emotional expressions, particularly threatening ones, influence negotiation behavior. The study enhances understanding of how facial cues and individual differences in ATs affect cooperation and decision-making in social interactions. © 2025 The Author(s). Brain and Behavior published by Wiley Periodicals LLC.",autistic traits; eye-tracking; facial expressions; social decision-making; threat; Ultimatum Game,Adult; Decision Making; Emotions; Eye-Tracking Technology; Facial Expression; Facial Recognition; Female; Humans; Interpersonal Relations; Male; Negotiating; Social Interaction; Young Adult; adult; anxiety; Article; autism; autism-spectrum quotient; behavior; behavior assessment; calibration; cognition; computer graphics; data base; decision making; emotion; eye tracking; eye-tracking technology; facial expression; female; hostility; human; human experiment; male; maximum likelihood method; mouth; negotiation; normal human; nose; prisoner dilemma; psychology; questionnaire; self report; social interaction; social media; surface analysis; threat; trustworthiness; university student; visual attention; visual stimulation; young adult; decision making; emotion; eye-tracking technology; facial recognition; human relation; physiology,Article,Final,,Scopus,2-s2.0-105002114234,Gaming / VR
Razzak R.; (Joy) Li Y.; (Selena) He J.; Jung S.; Mei C.; Huang Y.,"Razzak, Rehma (57218544379); (Joy) Li, Yi (59525623500); (Selena) He, Jing (59525099300); Jung, Sungchul (56963120200); Mei, Chao (56421487500); Huang, Yan (57196142018)",57218544379; 59525623500; 59525099300; 56963120200; 56421487500; 57196142018,Using virtual reality to enhance attention for autistic spectrum disorder with eye tracking,2025,High-Confidence Computing,5,1,100234,,,,3,10.1016/j.hcc.2024.100234,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85198971489&doi=10.1016%2fj.hcc.2024.100234&partnerID=40&md5=4325c0ebd37d78b504c11272f3ac493d,"Attention deficit disorder is a frequently observed symptom in individuals with autism spectrum disorder (ASD). This condition can present significant obstacles for those affected, manifesting in challenges such as sustained focus, task completion, and the management of distractions. These issues can impede learning, social interactions, and daily functioning. This complexity of symptoms underscores the need for tailored approaches in both educational and therapeutic settings to support individuals with ASD effectively. In this study, we have expanded upon our initial virtual reality (VR) prototype, originally created for attention therapy, to conduct a detailed statistical analysis. Our objective was to precisely identify and measure any significant differences in attention-related outcomes between sessions and groups. Our study found that heart rate (HR) and electrodermal activity (EDA) were more responsive to attention shifts than temperature. The ‘Noise’ and ‘Score’ strategies significantly affected eye openness, with the ASD group showing more responsiveness. The control group had smaller pupil sizes, and the ASD group's pupil size increased notably when switching strategies in Session 1. Distraction log data showed that both ‘Noise’ and ‘Object Opacity’ strategies influenced attention patterns, with the ‘Red Vignette’ strategy showing a significant effect only in the ASD group. The responsiveness of HR and EDA to attention shifts and the changes in pupil size could serve as valuable physiological markers to monitor and guide these interventions. These findings further support evidence that VR has positive implications for helping those with ASD, allowing for more tailored personalized interventions with meaningful impact. © 2024 The Author(s)",Attention training; Autistic spectrum disorders; Human–computer interaction; Virtual reality,Diseases; Virtual reality; Virtualization; Attention deficit disorder; Attention shifts; Attention training; Autism spectrum disorders; Autistic spectrum disorders; Computer interaction; Electrodermal activity; Eye-tracking; Heart-rate; Pupil size; Virtual environments,Article,Final,,Scopus,2-s2.0-85198971489,Gaming / VR
Eudave L.; Vourvopoulos A.,"Eudave, Luis (57189999067); Vourvopoulos, Athanasios (48762198300)",57189999067; 48762198300,Multimodal mapping of spatial attention for unilateral spatial neglect in VR: a proof of concept study using eye-tracking and mobile EEG,2025,Virtual Reality,29,1,24,,,,2,10.1007/s10055-025-01103-6,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85217744270&doi=10.1007%2fs10055-025-01103-6&partnerID=40&md5=bb5b0ce11a4686f6b630481c05446bf0,"Unilateral spatial neglect (USN) is a complex spatial attentional disorder consisting of a failure to attend to the contralesional side of space, frequently seen after a stroke. However, the majority of cases go undiagnosed due to the lack of a valid and reliable tool that is able to assess USN and its many variants. Recent technological advances in virtual reality (VR) and physiological sensors, allow for the study of this disorder under controlled, and ecologically-valid environments, which hold the promise of reliable and early detection. This proof of concept study aims to evaluate the feasibility of a system for discriminating different attentional states using a multimodal dataset derived from a spatial attention task conducted in VR. Nine healthy young adults underwent two experimental conditions: a Control condition and a Left Occlusion condition. Participants performed a visual search task while their behavioral data, including performance metrics, eye-gaze, head, and controller movement data, were recorded. Additionally, electroencephalography data was synchroniously collected to capture neural correlates of attentional processing. Analysis of results of this within-subjects study found worse performance (higher RT), changes in behavior (right-ward gaze bias, left-ward bias in head and controller movement) in the Left Occlusion condition. Neural differences were found (parieto-occipital mean alpha band power and event related potentials) between the two conditions. If validated, this system could be utilized as a diagnostic VR tool, while it holds the potential to facilitate the participation of stroke patients with USN in VR-driven rehabilitation. © The Author(s) 2025.",Electroencephalography; Eye-tracking; Hemineglect; Stroke; Unilateral spatial neglect; Virtual reality,Brain mapping; Diseases; Electrotherapeutics; Eye controlled devices; Eye movements; Patient rehabilitation; Physiological models; Concept studies; Condition; Eye-tracking; Hemineglect; Multi-modal; Proof of concept; Spatial attention; Stroke; Technological advances; Unilateral spatial neglect; Electroencephalography,Article,Final,All Open Access; Gold Open Access,Scopus,2-s2.0-85217744270,Gaming / VR
Basset G.; Testa A.; Turati C.; Quadrelli E.; Bulf H.,"Basset, Giada (59376917400); Testa, Alessia (58476267600); Turati, Chiara (6602812426); Quadrelli, Ermanno (55907700000); Bulf, Hermann (23033295000)",59376917400; 58476267600; 6602812426; 55907700000; 23033295000,Ostracism affects children’s behavioral reactivity and gaze cueing of attention,2025,PLoS ONE,20,3-Mar,e0320338,,,,0,10.1371/journal.pone.0320338,https://www.scopus.com/inward/record.uri?eid=2-s2.0-105001325925&doi=10.1371%2fjournal.pone.0320338&partnerID=40&md5=c9e1ba62e42d100a20cbeb5cfa3e2dd0,"Being ostracized is a negative experience that threatens important psychological needs, inducing considerable cognitive and behavioral changes and influencing the processing of social signals such as gaze-cueing. Yet, little is known about how self-experienced ostracism affects children’s behavior and attentional processes. The present study aims to explore whether the social experience of being included or ostracized can modulate gaze-cueing of attention and behavioral reactivity in 6- (N = 40) and 10-year-old children (N = 40) and adults (N = 50). Participants were video-recorded while playing an online ball-tossing game (i.e., Cyberball), where they could be either included or ostracized. They then participated in a gaze cueing task, where the cue was provided by the eye-gaze of a central human face, and the target could appear in a congruent or incongruent position. Results revealed that ostracism affected both adults’ and children’s ability to follow another’s gaze, as they were slower to respond to incongruent targets when ostracized compared to when included. Additionally, ostracism impaired 10-year-old children’s accuracy in responding to the target. Behavioral reactivity results demonstrated that both children and adults were more disappointed during the ostracism vs. inclusion condition. Overall, current findings demonstrate that self-experienced ostracism modulates children’s and adults’ behavioral reactivity and processing of social signals such as gaze cueing. © 2025 Basset et al. This is an open access article distributed under the terms of the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.",,"Adult; Attention; Child; Child Behavior; Cues; Female; Fixation, Ocular; Humans; Male; Social Isolation; adult; anxiety; Article; attention; behavior change; child; child behavior; disappointment; facial expression; female; frustration; gaze; human; human experiment; male; ostracism; questionnaire; reaction time; school child; task performance; videorecording; association; child behavior; eye fixation; physiology; psychology; social isolation",Article,Final,,Scopus,2-s2.0-105001325925,Gaming / VR
Cont C.; Stute N.; Galli A.; Schulte C.; Wojtecki L.,"Cont, Celine (57221111160); Stute, Nathalie (57219193012); Galli, Anastasia (57909809500); Schulte, Christina (36618974800); Wojtecki, Lars (7801526694)",57221111160; 57219193012; 57909809500; 36618974800; 7801526694,Eye Tracking as Biomarker Compared to Neuropsychological Tests in Parkinson Syndromes: An Exploratory Pilot Study Before and After Deep Transcranial Magnetic Stimulation,2025,Brain Sciences,15,2,180,,,,0,10.3390/brainsci15020180,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85218686652&doi=10.3390%2fbrainsci15020180&partnerID=40&md5=1f0b092886d44a92c6dce87b508cc8ea,"Background/Objectives: Neurodegenerative diseases such as Parkinson’s disease (PD) are becoming increasingly prevalent, necessitating diverse treatment options to manage symptoms. The effectiveness of these treatments depends on accurate and sensitive diagnostic methods. This exploratory pilot study explores the use of eye tracking and compares it to neuropsychological tests on patients treated with deep transcranial magnetic stimulation (dTMS). Methods: We used the HTC Vive Pro Eye VR headset with Tobii eye tracker to measure eye movements in 10 Parkinson syndrome patients while viewing three 360-degree scenes. Eye movements were recorded pre- and post-dTMS, focusing on Fixation Duration, Longest Fixation Period, Saccade Rate, and Total Fixations. Neuropsychological assessments (MoCA, TUG, BDI) were conducted before and after stimulation. dTMS was performed using the Brainsway device with the H5 helmet, targeting the motor cortex (1 Hz) and the prefrontal cortex (10 Hz) for 7–12 sessions. Results: ROC analysis indicated a moderate ability to differentiate between states using eye movement parameters. Significant correlations were found between changes in the longest fixation period and MoCA scores (r = 0.65, p = 0.025), and between fixation durations and BDI scores (r = −0.55, p = 0.043). Paired t-tests showed no significant differences in eye movement parameters, but BDI scores significantly reduced post-dTMS (t(5) = 2.57, p = 0.049). Conclusions: Eye-tracking parameters, particularly the Longest Fixation Duration and Saccade Rate, could serve as sensitive and feasible biomarkers for cognitive changes in Parkinson’s Syndrome, offering a quick alternative to traditional methods. Traditional neuropsychological tests showed a significant improvement in depressive symptoms after dTMS. Further research with larger sample sizes is necessary to validate these findings and explore the diagnostic utility of eye tracking. © 2025 by the authors.",biomarker; deep transcranial magnetic stimulation; eye tracking; neuromodulation; Parkinson’s disease,biological marker; area under the curve; Article; Beck Depression Inventory; bradykinesia; clinical article; cognition; confidence interval; controlled study; deep transcranial magnetic stimulation; degenerative disease; depression; diagnostic test accuracy study; diagnostic value; disease duration; disease severity; emotion; exploratory factor analysis; eye movement; eye tracking; female; gait disorder; Hamilton Depression Rating Scale; helmet; human; Likert scale; male; Mini Mental State Examination; Montreal cognitive assessment; motor cortex; muscle rigidity; neuromodulation; neuropsychological assessment; Parkinson disease; pilot study; prefrontal cortex; prevalence; receiver operating characteristic; saccadic eye movement; sample size; timed up and go test; transcranial magnetic stimulation; tremor; Unified Parkinson Disease Rating Scale,Article,Final,All Open Access; Gold Open Access,Scopus,2-s2.0-85218686652,Gaming / VR
Fadrigon B.; Tseng A.; Weisenburger R.L.; Levihn-Coon A.; McNamara M.E.; Shumake J.; Smits J.A.J.; Dennis-Tiwary T.A.; Beevers C.G.,"Fadrigon, Beatrice (59152002800); Tseng, Ariel (59494263200); Weisenburger, Rachel L. (57219712974); Levihn-Coon, Andrew (57202510642); McNamara, Mary E. (57217564907); Shumake, Jason (6506740948); Smits, Jasper A.J. (7101674924); Dennis-Tiwary, Tracy A. (16678691300); Beevers, Christopher G. (7004578857)",59152002800; 59494263200; 57219712974; 57202510642; 57217564907; 6506740948; 7101674924; 16678691300; 7004578857,Efficacy of traditional and gamified attention bias modification for depression: Study protocol for a randomized controlled trial,2025,Contemporary Clinical Trials,149,,107797,,,,1,10.1016/j.cct.2024.107797,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85213551598&doi=10.1016%2fj.cct.2024.107797&partnerID=40&md5=c1c61c6f1d2e1a55bb204b5dc99e93ac,"Cognitive models posit that negatively biased attention toward dysphoric information has a causal role in the maintenance of depression-related psychopathology. Attention bias modification (ABM) tests this idea by altering an attentional bias and examining subsequent effects on depression. Prior work finds that ABM alters negatively biased attention for dysphoric information and reduces depression; however, a number of studies have failed to show these effects. Other research suggests that adding game-like elements (i.e.game play, achievements, levels, challenges, and points) to cognitive training can enhance participant engagement. No prior work has examined the efficacy of gamified ABM for depression. The goal of this study is to conduct a large (N = 600) efficacy trial comparing gamified, mobile ABM and traditional, web-based ABM to traditional, web-based sham ABM among adults with elevated symptoms of depression. Participants in all conditions are asked to complete 16 ABM sessions across a four week period (i.e., 4 training sessions per week). We hypothesize that gamified and traditional ABM will lead to significantly greater reductions in self-reported and interviewer-rated depression symptoms than traditional sham ABM. We further hypothesize that gamified ABM will be non-inferior to traditional ABM. Our third hypothesis is that people with a strong attentional bias will experience greater reductions in depression in response to either gamified or traditional ABM compared to sham ABM. Secondary analyses will examine putative mediators of ABM. Finally, we will estimate the durability of ABM by collecting post-treatment symptom data 2-, 3-, and 6-months after the acute ABM period. ClinicalTrials.gov ID: NCT06361095 © 2024",ABM; Attention bias modification; Clinical trial; Depression,Adult; Attentional Bias; Cognitive Behavioral Therapy; Depression; Female; Humans; Male; Research Design; Video Games; antidepressant agent; achievement; adult; aged; Article; attentional bias; cognitive model; depression; eye tracking; facial expression; follow up; game-based learning; human; major clinical study; patient engagement; psychometry; randomized controlled trial (topic); reaction time; Sheehan Disability Scale; symptom; tactile feedback; treatment outcome; treatment response; visual feedback; visual field; cognitive behavioral therapy; controlled study; depression; female; male; methodology; procedures; psychology; randomized controlled trial; therapy; video game,Article,Final,,Scopus,2-s2.0-85213551598,Gaming / VR
Huang Y.; Hansen M.; Richter E.; Kleickmann T.; Scheiter K.; Richter D.,"Huang, Yizhen (57214226921); Hansen, Mira (59413991200); Richter, Eric (57204350319); Kleickmann, Thilo (36992400200); Scheiter, Katharina (6507519353); Richter, Dirk (36195062400)",57214226921; 59413991200; 57204350319; 36992400200; 6507519353; 36195062400,Enhancing preservice teachers’ noticing via adaptive feedback in a virtual reality classroom,2025,Learning and Instruction,95,,102053,,,,0,10.1016/j.learninstruc.2024.102053,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85209349503&doi=10.1016%2fj.learninstruc.2024.102053&partnerID=40&md5=0bef8989fefcf2970c89c7c819b711d6,"Background: Despite the relevance of adaptive performance feedback in teacher education, it remains unclear if it improves preservice teachers’ noticing abilities. Aims: This study aimed to investigate the influence of feedback on teachers' noticing abilities, specifically in the context of visually attending to disruptions within a virtual reality (VR) classroom. We examined the effect of feedback conditions (adaptive/static/no feedback) on three aspects of teachers’ visual attention performance (VAP): selective visual attention, visual scan scope, and visual sensitivity to significant events. Sample: The sample consisted of 98 preservice teachers who were randomly assigned to one of three conditions. Methods: We used linear mixed-effects modeling to examine feedback effects in a VR classroom with eye tracking. VAP was measured by both subjective (self-report) and objective (the number of fixations on students versus on objects; the degree of dispersions of fixation locations; the number of seen disruptions and the average time to first fixate on a disruption) measures. Adaptive feedback was based on real-time process data from eye tracking and provided participants with individualized evaluations of their actions, while static feedback only offered generic recommendations. Results: We found that participants in both feedback conditions perceived their (subjective) VAP to be improving compared to the control group. But the actual objective VAP only improved for teachers receiving adaptive feedback. Conclusions: This study provides empirical support for using adaptive feedback systems based on real-time process data in enhancing preservice teachers’ professional competence in noticing significant classroom events. © 2024 Elsevier Ltd",Augmented and virtual reality; Data-driven applications in education; Improving classroom teaching; Simulations; Teacher education,,Article,Final,,Scopus,2-s2.0-85209349503,Gaming / VR
Levy O.; Korisky A.; Zvilichovsky Y.; Zion Golumbic E.,"Levy, Orel (58902662000); Korisky, Adi (57216159628); Zvilichovsky, Yair (57246389800); Zion Golumbic, Elana (18435708300)",58902662000; 57216159628; 57246389800; 18435708300,The Neurophysiological Costs of Learning in a Noisy Classroom: An Ecological Virtual Reality Study,2025,Journal of cognitive neuroscience,37,2,,300,316,16.0,1,10.1162/jocn_a_02249,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85216606870&doi=10.1162%2fjocn_a_02249&partnerID=40&md5=110379b7b57c183bb14a1f9b7dad2f90,"Many real-life situations can be extremely noisy, which makes it difficult to understand what people say. Here, we introduce a novel audiovisual virtual reality experimental platform to study the behavioral and neurophysiological consequences of background noise on processing continuous speech in highly realistic environments. We focus on a context where the ability to understand speech is particularly important: the classroom. Participants (n = 32) experienced sitting in a virtual reality classroom and were told to pay attention to a virtual teacher giving a lecture. Trials were either quiet or contained background construction noise, emitted from outside the classroom window. Two realistic types of noise were used: continuous drilling and intermittent air hammers. Alongside behavioral outcomes, we measured several neurophysiological metrics, including neural activity (EEG), eye-gaze and skin conductance (galvanic skin response). Our results confirm the detrimental effect of background noise. Construction noise, and particularly intermittent noise, was associated with reduced behavioral performance, reduced neural tracking of the teacher's speech and an increase in skin conductance, although it did not have a significant effect on alpha-band oscillations or eye-gaze patterns. These results demonstrate the neurophysiological costs of learning in noisy environments and emphasize the role of temporal dynamics in speech-in-noise perception. The finding that intermittent noise was more disruptive than continuous noise supports a ""habituation"" rather than ""glimpsing"" hypothesis of speech-in-noise processing. These results also underscore the importance of increasing the ecologically relevance of neuroscientific research and considering acoustic, temporal, and semantic features of realistic stimuli as well as the cognitive demands of real-life environments. © 2024 Massachusetts Institute of Technology.",,Adult; Brain; Electroencephalography; Female; Galvanic Skin Response; Humans; Learning; Male; Noise; Speech Perception; Virtual Reality; Young Adult; adult; brain; electrodermal response; electroencephalography; female; human; learning; male; noise; physiology; speech perception; virtual reality; young adult,Article,Final,,Scopus,2-s2.0-85216606870,Gaming / VR
Du X.; Jia L.; Wu J.; Peng N.; Zhou X.; Xue C.,"Du, Xiaoxi (57222361522); Jia, Lesong (57219549255); Wu, Jinchun (57222358934); Peng, Ningyue (57189886548); Zhou, Xiaozhou (56925623200); Xue, Chengqi (14053333400)",57222361522; 57219549255; 57222358934; 57189886548; 56925623200; 14053333400,How bare-hand clicking influences eye movement behavior in virtual button selection tasks: a comparative analysis with keyboard control,2025,Journal on Multimodal User Interfaces,,,,,,,0,10.1007/s12193-025-00455-2,https://www.scopus.com/inward/record.uri?eid=2-s2.0-105009942035&doi=10.1007%2fs12193-025-00455-2&partnerID=40&md5=6f5d90675d565301b4a36b9f93b71bfb,"The flexibility of virtual reality systems has fostered the development of various interaction techniques, yet our understanding of users’ eye movement behaviors with different interaction methods remains limited. This study compares eye movement details in goal-driven and stimulus-based virtual button selection tasks, using both direct bare-hand interaction and indirect keyboard control in virtual environments. Our findings show that both interface type and interaction mode significantly affect multiple eye movement metrics, including saccades, fixations, and blinks. These insights enhance our understanding of visual attention and action execution in virtual selection tasks, offering data-driven conclusions that can serve as a foundation for developing design guidelines for interaction techniques and interface designs in virtual reality. We believe these results will contribute to the creation of more natural, efficient, and user-friendly virtual reality systems, further advancing the technology. © The Author(s), under exclusive licence to Springer Nature Switzerland AG 2025.",Bare-hand Interaction; Eye Tracking; Selection Task; Virtual Buttons; Virtual Reality,,Article,Article in press,,Scopus,2-s2.0-105009942035,Gaming / VR
Minissi M.E.; Antzaka A.; Mancini S.; Lallier M.,"Minissi, Maria Eleonora (57207617337); Antzaka, Alexia (57188829783); Mancini, Simona (43661431500); Lallier, Marie (26634009300)",57207617337; 57188829783; 43661431500; 26634009300,Can playing video games enhance reading skills through more efficient serial visual search mechanisms? Insights from an eye tracking study,2025,"Language, Cognition and Neuroscience",40,2,,209,230,21.0,1,10.1080/23273798.2024.2411696,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85206464065&doi=10.1080%2f23273798.2024.2411696&partnerID=40&md5=abd9859e3d2fe0e2f415a39070687560,"Reading disorders are associated with atypical top-down visual attention (VA) processes like reduced VA span and slower serial visual search (SVS). In contrast, expert action video game (AVG) players, known for their efficient top-down VA, exhibit improved reading abilities. It is unclear whether these benefits stem solely from AVGs or apply to other gaming experiences. To explore this, AVG players (AVGPs), players of genres excluding AVGs (VGPs), and non-players were evaluated on their VA span, and behavioural and oculomotor performance in SVS. VGPs, but not AVGPs, demonstrated enhanced performance and oculomotor behaviour in SVS compared to non-players, while both player groups showed a trend towards better VA span skills. Notably, reading-related skills were enhanced in the two player groups, but particularly more so in VGPs. These findings support the existence of potential benefits of playing video games different from classical AVGs for the development of top-down VA and reading-related abilities. © 2024 Informa UK Limited, trading as Taylor & Francis Group.",Action video games; eye movements; reading skills; serial visual search; visual attention,action video game; adult; Article; behavior; behavior assessment; cognition; dyslexia; eye movement; eye tracking; eye-tracking technology; female; human; human experiment; literacy; male; music; oculomotor behavior; reaction time; serial visual search; sport; video game; visual attention; visual system,Article,Final,,Scopus,2-s2.0-85206464065,Gaming / VR
Tsumagari S.; Hachisuka S.; Kurita K.; Warisawa S.,"Tsumagari, Shunki (58886634600); Hachisuka, Satori (44161205200); Kurita, Kayoko (36891417000); Warisawa, Shinichi (58760185500)",58886634600; 44161205200; 36891417000; 58760185500,Examining the Influence of Perceptual Discrepancy for Visual Information on Memory Consolidation Using the Method of Loci,2025,"2025 13th International Conference on Information and Education Technology, ICIET 2025",,,,332,336,4.0,0,10.1109/ICIET66371.2025.11046259,https://www.scopus.com/inward/record.uri?eid=2-s2.0-105010676792&doi=10.1109%2fICIET66371.2025.11046259&partnerID=40&md5=b0cf84e88827a17af0e5e5f52e7fc14d,"In this paper, we show the results of examining what kind of information should be visualized in VR to improve the usability and effectiveness of the Method of Loci (MoL), a memory method that associates memory targets with familiar locations. Previous studies have explored the use of VR in memory tasks employing the MoL. However, the effectiveness of VR remains inconsistent. Because it is insufficient control of how to use VR across studies, due to it is not clear what kind of visual information is utilized during memorization. Therefore, we conducted an experiment based on the hypothesis that the perceptual discrepancies caused by visual attributes that differ from reality in visualized locations improve the effectiveness of the MoL. In the experiment, based on a VR environment modeled after a familiar location, three conditions were set up: VR with the same color as reality, completely white VR, and VR with different colors. Also, Eye movements during memorization were measured using a wearable eye tracker, Tobii Pro Glasses 3. As a result, it was confirmed that VR-based information visualization significantly enhanced the usability of the MoL. While color differences were impressive information that attracted attention, they may not be useful for association linking memory targets with locations. © 2025 IEEE.",discrepancy; eye tracking; method of loci; mnemonic techniques; virtual reality; visual attention,Behavioral research; Color; Eye movements; Information analysis; Information systems; Information use; Location; Condition; Discrepancy; Eye trackers; Eye-tracking; Memory consolidation; Method of locus; Mnemonic technique; Visual Attention; Visual attributes; Visual information; Eye tracking; Virtual reality,Conference paper,Final,,Scopus,2-s2.0-105010676792,Gaming / VR
Ji Y.,"Ji, Yuwei (59541008400)",59541008400,Evaluation of Eye Movement Features and Visual Fatigue in Virtual Reality Games,2025,International Journal of Advanced Computer Science and Applications,16,1,,1027,1038,11.0,0,10.14569/IJACSA.2025.0160199,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85216892646&doi=10.14569%2fIJACSA.2025.0160199&partnerID=40&md5=a6594d71a727168d9ad863d69a852031,"VR games make people happy physically and mentally, but also lead to eye health problems. At present, the existing VR systems lack fatigue detection technology, which makes it difficult to help users use their eyes reasonably. In order to improve the user experience of VR gamers, this paper proposes a visual fatigue detection algorithm based on eye movement features, which uses the relationship between the lateral and longitudinal displacements of the human head and the displacement of the center point of the human eye to locate the position of the human eye. Moreover, in this paper, the human eye position tracking model is input into the three-frame difference algorithm to detect eye movement features. In addition, for tiny motion interference such as eyebrows, the image opening operation of eroding first and then expanding is used to remove it. Through experiments, it is found that the eye movement feature detection method adopted in this paper can greatly improve the detection speed with less accuracy loss, meet the sensitivity requirements of eye movement feature capture, improve the real- time performance of the system, and effectively improve the real- time analysis of player status. Therefore, integrating this algorithm into the virtual game system can help players adjust their own state, which has a positive effect on improving the game experience and reducing eye damage. © (2025), (Science and Information Organization). All rights reserved.",eye movement features; games; Virtual reality; visual fatigue,Eye movements; Interactive computer graphics; Medical problems; Real time systems; Virtual reality; Detection algorithm; Detection technology; Eye health; Eye movement feature; Fatigue detection; Game; Human eye; Users' experiences; Visual fatigue; VR systems; Virtual environments,Article,Final,All Open Access; Gold Open Access,Scopus,2-s2.0-85216892646,Gaming / VR
Zhou X.; Viola I.; Rossi S.; Cesar P.,"Zhou, Xuemei (57221563326); Viola, Irene (57192590535); Rossi, Silvia (57201133915); Cesar, Pablo (16237880000)",57221563326; 57192590535; 57201133915; 16237880000,Comparison of Visual Saliency for Dynamic Point Clouds: Task-free vs. Task-dependent,2025,IEEE Transactions on Visualization and Computer Graphics,31,5,,2964,2974,10.0,0,10.1109/TVCG.2025.3549863,https://www.scopus.com/inward/record.uri?eid=2-s2.0-105004015477&doi=10.1109%2fTVCG.2025.3549863&partnerID=40&md5=936b23082ea9bad5a7d019cc907c4e1d,"This paper presents a Task-Free eye-tracking dataset for Dynamic Point Clouds (TF-DPC) aimed at investigating visual attention. The dataset is composed of eye gaze and head movements collected from 24 participants observing 19 scanned dynamic point clouds in a Virtual Reality (VR) environment with 6 degrees of freedom. We compare the visual saliency maps generated from this dataset with those from a prior task-dependent experiment (focused on quality assessment) to explore how high-level tasks influence human visual attention. To measure the similarity between these visual saliency maps, we apply the well-known Pearson correlation coefficient and an adapted version of the Earth Mover's Distance metric, which takes into account both spatial information and the degrees of saliency. Our experimental results provide both qualitative and quantitative insights, revealing significant differences in visual attention due to task influence. This work enhances our understanding of the visual attention for dynamic point cloud (specifically human figures) in VR from gaze and human movement trajectories, and highlights the impact of task-dependent factors, offering valuable guidance for advancing visual saliency models and improving VR perception. © 1995-2012 IEEE.",dynamic point cloud; eye-tracking; similarity measurement; task-free; visual saliency metric,"Adult; Attention; Computer Graphics; Eye Movements; Eye-Tracking Technology; Female; Fixation, Ocular; Head Movements; Humans; Male; Virtual Reality; Visual Perception; Young Adult; Clouds; Eye movements; Virtual reality; Dynamic point cloud; Eye-tracking; Gaze movements; Point-clouds; Saliency metric; Similarity measurements; Task-free; Visual Attention; Visual saliency; Visual saliency metric; adult; attention; computer graphics; eye fixation; eye movement; eye-tracking technology; female; head movement; human; male; physiology; virtual reality; vision; young adult; Virtual environments",Article,Final,,Scopus,2-s2.0-105004015477,Gaming / VR
Nolte D.; Vidal De Palol M.; Keshava A.; Madrid-Carvajal J.; Gert A.L.; von Butler E.-M.; Kömürlüoğlu P.; König P.,"Nolte, Debora (57210123336); Vidal De Palol, Marc (59207563800); Keshava, Ashima (57202213703); Madrid-Carvajal, John (59168788700); Gert, Anna L. (55016203000); von Butler, Eva-Marie (59168788800); Kömürlüoğlu, Pelin (59168696500); König, Peter (7102563952)",57210123336; 59207563800; 57202213703; 59168788700; 55016203000; 59168788800; 59168696500; 7102563952,Combining EEG and eye-tracking in virtual reality: Obtaining fixation-onset event-related potentials and event-related spectral perturbations,2025,"Attention, Perception, and Psychophysics",87,1,116117,207,227,20.0,8,10.3758/s13414-024-02917-3,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85197711873&doi=10.3758%2fs13414-024-02917-3&partnerID=40&md5=52d1a76ae692d1cf75e9e79f7a13b099,"Extensive research conducted in controlled laboratory settings has prompted an inquiry into how results can be generalized to real-world situations influenced by the subjects' actions. Virtual reality lends itself ideally to investigating complex situations but requires accurate classification of eye movements, especially when combining it with time-sensitive data such as EEG. We recorded eye-tracking data in virtual reality and classified it into gazes and saccades using a velocity-based classification algorithm, and we cut the continuous data into smaller segments to deal with varying noise levels, as introduced in the REMoDNav algorithm. Furthermore, we corrected for participants' translational movement in virtual reality. Various measures, including visual inspection, event durations, and the velocity and dispersion distributions before and after gaze onset, indicate that we can accurately classify the continuous, free-exploration data. Combining the classified eye-tracking with the EEG data, we generated fixation-onset event-related potentials (ERPs) and event-related spectral perturbations (ERSPs), providing further evidence for the quality of the eye-movement classification and timing of the onset of events. Finally, investigating the correlation between single trials and the average ERP and ERSP identified that fixation-onset ERSPs are less time sensitive, require fewer repetitions of the same behavior, and are potentially better suited to study EEG signatures in naturalistic settings. We modified, designed, and tested an algorithm that allows the combination of EEG and eye-tracking data recorded in virtual reality. © The Author(s) 2024.",EEG; Eye tracking; Fixation-onset event-related potential; Fixation-onset event-related spectral perturbation; Saccade classification; Virtual reality,"Adult; Algorithms; Attention; Electroencephalography; Evoked Potentials; Eye-Tracking Technology; Female; Fixation, Ocular; Humans; Male; Saccades; Virtual Reality; Young Adult; adult; algorithm; attention; electroencephalography; evoked response; eye fixation; eye-tracking technology; female; human; male; physiology; saccadic eye movement; virtual reality; young adult",Article,Final,All Open Access; Hybrid Gold Open Access,Scopus,2-s2.0-85197711873,Gaming / VR
Xie Z.; Duan H.; Zhu Y.; Wang P.; Min X.; Zhai G.,"Xie, Zongyi (59995121400); Duan, Huiyu (57195265438); Zhu, Yuxin (58533077400); Wang, Pengfei (59708904700); Min, Xiongkuo (56030205300); Zhai, Guangtao (15847120000)",59995121400; 57195265438; 58533077400; 59708904700; 56030205300; 15847120000,Visual Saliency Prediction for Augmented Reality Videos,2025,Proceedings - IEEE International Symposium on Circuits and Systems,,,,,,,0,10.1109/ISCAS56072.2025.11043345,https://www.scopus.com/inward/record.uri?eid=2-s2.0-105010657870&doi=10.1109%2fISCAS56072.2025.11043345&partnerID=40&md5=56e8c6dba25c1f89019f26f0f1dc7f69,"Augmented Reality (AR) is an emerging technology that allows users to perceive both virtual-world contents and real-world scenes simultaneously. It has numerous applications in industrial manufacturing, entertainment, gaming, education, etc. In AR environments, the visual confusion phenomenon caused by the overlay of augmented content and real backgrounds is evident, yet the understanding and research of visual saliency under the AR visual confusion condition remains limited. This paper primarily analyzes the interaction between real-world scenes and AR content, explores human visual saliency when using AR devices. First, we conduct a large-scale eye-tracking experiment based on a head-mounted AR device, and construct an AR saliency dataset containing 2160 videos, with corresponding collected eye movement data. Through qualitative analysis of the visual attention heat maps, we conclude that visual confusion significantly influences visual attention in AR video. Additionally, we quantitatively evaluate the performance of a series of classical saliency models and deep neural network saliency models on the dataset constructed in this project. For better predicting saliency in AR, we propose a general saliency prediction model, InternSal, which achieves state-of-the-art performance compared to other methods. The database and codes will be released to facilitate future research. © 2025 IEEE.",Augmented Reality (AR); dataset; saliency prediction; visual attention,Behavioral research; Data visualization; Eye movements; Eye tracking; Industrial research; Video analysis; Virtual reality; Visualization; Augmented reality; Dataset; Emerging technologies; Industrial manufacturing; Real-world; Saliency modeling; Saliency prediction; Virtual worlds; Visual Attention; Visual saliency; Augmented reality,Conference paper,Final,,Scopus,2-s2.0-105010657870,Gaming / VR
Luo W.; Yuan Y.; Wang L.; Wu L.,"Luo, Weijing (57216921374); Yuan, Yuan (56390696800); Wang, Linting (58570014000); Wu, Lijuan (59522942500)",57216921374; 56390696800; 58570014000; 59522942500,The effects of visual characteristics of waterfront spaces on attention and emotion：Evidence from an eye-tracking experiment of college students; [滨水空间视觉特征的注意力和情绪效应——基于高校在校学生眼动追踪实验的证据],2025,Zhongshan Daxue Xuebao/Acta Scientiarum Natralium Universitatis Sunyatseni,64,1,,207,217,10.0,0,10.13471/j.cnki.acta.snus.ZR20240187,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85215686171&doi=10.13471%2fj.cnki.acta.snus.ZR20240187&partnerID=40&md5=c71c4532fcc45fac34232a6705435a6b,The intense pressure of social competition and the fast-paced urban living have been increasingly exacerbating mental health issues among college students. As one of the everyday therapeutic landscapes，green-blue spaces are crucial nature prescriptions and environmental assets that could improve mental health. Drawing upon visual landscape perception theory，this study conducted a virtual reality eye-tracking experiment and collected eye movement and emotional data of 51 college students to explore the effect of visual characteristics of waterfront spaces on college students’ attention and emotions. The results indicated that waterfront spaces within public open spaces triggered longer fixation duration and greater pleasure. The participants’mean pupil diameters and pleasure levels varied across different behavior perspectives while perceiving these spaces. Various levels of green density，plant structure，and water visibility had different effects on attention and emotion. Notably，significant interaction effects were observed between visual characteristics and environmental type（or behavior perspective）. The study extends the human-place perception theory regarding green-blue space through experimental exploration and nuanced analyses. Its results provide valuable insights and empirical evidence for the design of meticulously crafted waterfront spaces and the construction of youth-friendly waterfront spaces at a humanistic scale. © 2025 Journal of Zhongshan University. All rights reserved.,attention; emotion; eye-tracking experiment; waterfront space; youth-friendly,,Article,Final,,Scopus,2-s2.0-85215686171,Gaming / VR
Chen M.; Li J.; Zhang Y.; Luo Y.; Wang X.; Zhang Z.; Xing R.; Xue C.,"Chen, Miaomiao (59937714100); Li, Jing (59937533100); Zhang, Yu (59938248100); Luo, Ying (59938017500); Wang, Xiaoxiao (56531031300); Zhang, Ziyi (57671826700); Xing, Rui (59561197300); Xue, Chengqi (14053333400)",59937714100; 59937533100; 59938248100; 59938017500; 56531031300; 57671826700; 59561197300; 14053333400,Virtual Reality-Based Analysis of Urban Waterfront Lighting Elements’ Influence on Visual Behavior,2025,Lecture Notes in Computer Science,15788 LNCS,,,193,203,10.0,0,10.1007/978-3-031-93700-2_13,https://www.scopus.com/inward/record.uri?eid=2-s2.0-105007806421&doi=10.1007%2f978-3-031-93700-2_13&partnerID=40&md5=7d945a5f2b0f0abc6383b6f0df88ee4f,"This study explores the impact of waterfront interface lighting on visual behavior using eye-tracking data and virtual reality, focusing on the Xuanwu Lake waterfront in Nanjing, China. We examine how various brightness combinations of focal building, core buildings, secondary buildings, city walls, and vegetation affect fixation counts and durations. Thirty-one participants viewed 130 panoramic images, and eye-tracking data were collected using Varjo XR-3 mixed reality devices. The results show that the brightness of focal and core buildings significantly affects fixation counts, while the city wall’s brightness and its interaction with other lighting parameters influence fixation duration. Secondary buildings had little impact due to their lower brightness. Lighting configurations with higher contrast and multi-layered brightness schemes increased fixation time, suggesting that well-designed lighting can attract more attention and enhance visual experience. This study provides data-driven insights for optimizing urban waterfront lighting design, helping create more engaging and visually appealing nighttime environments to support the nighttime economy.  © The Author(s), under exclusive license to Springer Nature Switzerland AG 2025.",Eye movement; Eye-tracking; Virtual reality; Visualization and image rendering; Waterfront interface lighting,Lighting; Visualization; Eye-tracking; Image rendering; Lighting elements; Nanjing; Panoramic images; Tracking data; Visual behavior; Visualization and image rendering; Waterfront interface lighting; Waterfront interfaces; Walls (structural partitions),Conference paper,Final,,Scopus,2-s2.0-105007806421,Gaming / VR
Zhao J.; Shi C.; Zhang X.; Ma S.; Sun W.; Tian F.; Wang P.; Li J.; Du J.; Zhao X.; Wan Z.,"Zhao, Jing (57204546509); Shi, Chong (59554810000); Zhang, Xucheng (58905837900); Ma, Shaochen (59554878200); Sun, Wei (59807906800); Tian, Feng (57202049684); Wang, Peifu (55575950900); Li, Jilai (26025120200); Du, Jichen (26025034700); Zhao, Xingquan (59811295800); Wan, Zhirong (57208026972)",57204546509; 59554810000; 58905837900; 59554878200; 59807906800; 57202049684; 55575950900; 26025120200; 26025034700; 59811295800; 57208026972,Eye movement and pupillary response abnormalities measured using virtual reality as biomarkers in the diagnosis of early-stage Parkinson’s disease,2025,Frontiers in Neurology,16,,1537841,,,,0,10.3389/fneur.2025.1537841,https://www.scopus.com/inward/record.uri?eid=2-s2.0-105004655680&doi=10.3389%2ffneur.2025.1537841&partnerID=40&md5=79d9623d8ec645533b1f00e460568bed,"Objective: Characteristic ocular symptoms are expected to serve as potential biomarkers for early diagnosis of Parkinson’s disease (PD). However, possible ocular impairments in PD patients are rarely studied. The study aimed to investigate eye movement characteristics and pupil diameter changes in early-stage PD patients using virtual reality (VR)-based system and explore their contribution in the diagnosis of early-stage PD. Methods: Forty-three early-stage PD patients and 25 healthy controls were included. Eye movements and pupillary response of all subjects were recorded and evaluated by wearing VR glasses. All subjects completed pro-saccade and anti-saccade tasks. Saccadic eye movement and pupillary response parameters were analyzed. Random Forests method was used for classification task, the performance of the classification model in differentiating early-stage PD patients from healthy controls were evaluated. Results: PD patients exhibited reduced pro-saccade velocity and accuracy, longer average time to complete the pro-saccade, and lower anti-saccade error correction rate than healthy controls (all p < 0.05). Significant differences were found in the trajectories of changes in pupil diameter between the two groups. After extraction of frequency-amplitude features of pupil constriction from the spectra of the eye movement signals of PD patients, it can be seen that the amplitudes of movement signals of both the left and right eyes at different frequencies during pro-saccade and anti-saccade tasks were significant. The number of significant amplitude frequencies in both eyes at low (0–6 Hz), medium (7–12 Hz) and high frequencies (13–19 Hz) was 23, 9, and 16, respectively, during pro-saccade task, which was 10, 29, and 43, respectively, during anti-saccade task. The model with all features achieved an accuracy of up to 79%. Conclusion: This study presents a non-invasive approach toward the diagnosis of early-stage PD with VR technology. Eye movement and pupillary response abnormalities measured using VR may be used as effective biomarkers for the diagnosis of early-stage PD. Copyright © 2025 Zhao, Shi, Zhang, Ma, Sun, Tian, Wang, Li, Du, Zhao and Wan.",diagnosis; eye movement; Parkinson’s disease; pupillary response; virtual reality,biological marker; accuracy; adult; Article; bladder dysfunction; clinical article; cognition; controlled study; disease duration; disease severity; early stage parkinson disease; eye movement; female; Hamilton Depression Rating Scale; human; hypotension; male; measurement; middle aged; Mini Mental State Examination; Montreal cognitive assessment; neurologic examination; Parkinson disease; pupillary response abnormalities; Unified Parkinson Disease Rating Scale; velocity; virtual reality,Article,Final,,Scopus,2-s2.0-105004655680,Gaming / VR
Jeong J.; Kwak M.; Kang H.,"Jeong, Jongwook (57204434883); Kwak, Myeongseok (59681882400); Kang, HyeongYeop (56553325600)",57204434883; 59681882400; 56553325600,Visual Interfaces to Mitigate Eye Problems in a Virtual Environment via Triggering Eye Blinking and Movement,2025,IEEE Transactions on Human-Machine Systems,55,2,,278,288,10.0,0,10.1109/THMS.2025.3542452,https://www.scopus.com/inward/record.uri?eid=2-s2.0-105002325420&doi=10.1109%2fTHMS.2025.3542452&partnerID=40&md5=bf9e9d3fc21527107548530bd94ea37b,"With the increase of virtual reality (VR) applications in daily life, protecting the comfort and health of VR users has become increasingly important. The immersive nature of VR often results in decreased eye blinking and movement, putting users at risk of developing conditions such as dry eye syndrome and eye strain. In this article, we propose visual interfaces to induce temporary eye blinks or movements by drawing users' attention temporarily in order to mitigate the negative effects of VR on eye health. Our proposed interfaces can induce eye blinking and movement, which are known to mitigate eye problems in VR. The experimental results confirmed that our interfaces increase the frequency of eye blinking and movement in VR users. © 2013 IEEE.",Computer vision syndrome (CVS); eye blinking; eye health; eye movement; virtual reality (VR),Electronic health record; Eye movements; Eye protection; Virtual reality; Computer vision syndrome; Computer vision syndromes; Condition; Daily lives; Dry eye; Eye health; Eye-blinking; Immersive; Virtual reality; Visual Interface; Virtual environments,Article,Final,,Scopus,2-s2.0-105002325420,Gaming / VR
Qiang W.; Yang L.; Zhang X.; Liu N.; Wang Y.; Zhang J.; Long Y.; Xu W.; Sun W.,"Qiang, Wei (58450867600); Yang, Lin (59834465000); Zhang, Xucheng (58905837900); Liu, Na (59831109400); Wang, Yanyong (15823129500); Zhang, Jipeng (59829427700); Long, Yixin (59835303800); Xu, Weiwei (59713266100); Sun, Wei (57211764593)",58450867600; 59834465000; 58905837900; 59831109400; 15823129500; 59829427700; 59835303800; 59713266100; 57211764593,Detecting emotional disorder with eye movement features in sports watching,2025,Frontiers in Neurology,16,,1562785,,,,0,10.3389/fneur.2025.1562785,https://www.scopus.com/inward/record.uri?eid=2-s2.0-105004759719&doi=10.3389%2ffneur.2025.1562785&partnerID=40&md5=0ddd5c515015bf6471832a902ada6008,"Introduction: Digital technologies have significantly advanced the detection of emotional disorders (EmD) in clinical settings. However, their adoption for long-term monitoring remains limited due to reliance on fixed testing formats and active user participation. This study introduces a novel approach utilizing common ball game videos–table tennis–to implicitly capture eye movement trajectories and identify EmD through natural viewing behavior. Methods: An eye movement data collection system was developed using VR glasses to display sports videos while recording participants' eye movements. Based on prior research and collected data, four primary eye movement behaviors were identified, along with 14 associated features. Statistical significance was assessed using t-tests and U-tests, and machine learning models were employed for classification (SVM for single-feature analysis and a decision tree for significant features) with k-fold validation. The reliability of the proposed paradigm and extracted features was evaluated using intraclass correlation coefficient (ICC) analysis. Results: Significance tests revealed 11 significant features in table tennis videos, encompassing exploration, fixation, and saccade behaviors, while only 3 features in tennis videos, which served as a supplemental stimulus, were salient in the re-testing. GazeEntropy emerged as the most predictive feature, achieving an accuracy of 0.88 with a significance p-value of 0.0002. A decision tree model trained on all significant features achieved 0.92 accuracy, 0.80 precision, and an AUC of 0.94. ICC analysis further confirmed the high reliability and significance of key features, including GazeEntropy and fixation metrics (average, maximum, and standard deviation). Discussion: This study highlights the potential of ball game video viewing as a natural and effective paradigm for EmD identification, particularly focusing on two key characteristics of EmD: curiosity exploration and psychomotor function. Additionally, participant preferences for video content significantly influenced diagnostic performance. We propose that future in-home, long-term monitoring of psychological conditions can leverage interactions with daily digital devices, integrating behavioral analysis seamlessly into everyday life. Copyright © 2025 Qiang, Yang, Zhang, Liu, Wang, Zhang, Long, Xu and Sun.",diagnosis; emotional disorder; eye movement; machine learning; sports watching,Article; cognition; correlation coefficient; decision making; emotional disorder; engagement; entropy; exploratory behavior; eye movement; Hamilton Depression Rating Scale; human; machine learning; Pittsburgh Sleep Quality Index; recognition; saccadic eye movement; sport; support vector machine; television viewing; tennis; videorecording,Article,Final,,Scopus,2-s2.0-105004759719,Gaming / VR
Yan S.; Zhong S.; Lyu S.; Lai S.; Zhang Y.; Luo Y.; Ran H.; Duan M.; Jia Y.,"Yan, Shuya (57228231300); Zhong, Shuming (55845361600); Lyu, Sihui (58629739700); Lai, Shunkai (56982319800); Zhang, Yiliang (57210568737); Luo, Yange (57227684800); Ran, Hanglin (57227860600); Duan, Manying (57976814200); Jia, Yanbin (54406029000)",57228231300; 55845361600; 58629739700; 56982319800; 57210568737; 57227684800; 57227860600; 57976814200; 54406029000,A Randomized Trial of Virtual Reality Eye Movement Desensitization and Reprocessing Therapy for Major Depressive Disorder With Childhood Trauma: A 3-Month Follow-Up Study,2025,"Psychological Trauma: Theory, Research, Practice, and Policy",17,5,,1127,1139,12.0,0,10.1037/tra0001830,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85215682684&doi=10.1037%2ftra0001830&partnerID=40&md5=40a1431a95a6e54ad4e9d7edf92eebc4,"Objective: Eye movement desensitization and reprocessing therapy (EMDR) is effective in treating major depressive disorder (MDD) with childhood trauma, and virtual reality (VR) can further extend its application form. However, the utilization of VR-EMDR in treating MDD with childhood trauma is still in its infancy, and whether it can improve depressive symptoms and traumatic experience remains unknown. Method: Seventy-two MDD patients were randomly allocated to the intervention group and the wait-list control group on a 1:1 basis. The intervention group received 12-session VR-EMDR, while another group received no intervention. We used Patient Health Questionnaire–9 (PHQ-9) and Hamilton Depression Rating Scale–24 Version (24-HDRS) to assess the patient’s subjective and objective depressive symptoms, the Posttraumatic Stress Disorder Check List–Civilian (PCL-C) to assess the patient’s traumatic experience, the Massachusetts General Hospital Cognitive and Physical Functioning Questionnaire, and the MATRICS Consensus Cognitive Battery to assess the patient’s subjective and objective cognitive performance. Results: After VR-EMDR, the linear mixed model revealed significantly lower scores in PHQ-9, 24-HDRS total and factor score (including anxiety/somatization, weight, and block), Posttraumatic Stress Disorder Check List–Civilian, and Massachusetts General Hospital Cognitive and Physical Functioning Questionnaire and significantly higher scores in information processing speed, attention/alertness, and working memory in the intervention group (p <.05). Improvements can be maintained in the 3-month follow-up, except for 24-HDRS anxiety/somatization factor score, which showed significantly higher scores in the 3-month follow-up compared with postintervention (p <.05). Conclusions: VR-EMDR is effective in improving depressive symptoms, traumatic experience, and cognitive performance in MDD with childhood trauma. Part of the effects can be maintained 3 months after the intervention. © 2025 American Psychological Association",childhood trauma; eye movement desensitization and reprocessing therapy; immersive virtual reality; major depressive disorder,"Adult; Adverse Childhood Experiences; Depressive Disorder, Major; Eye Movement Desensitization Reprocessing; Female; Follow-Up Studies; Humans; Male; Middle Aged; Stress Disorders, Post-Traumatic; Treatment Outcome; Virtual Reality Exposure Therapy; adult; childhood adversity; complication; controlled study; eye movement desensitization and reprocessing; female; follow up; human; major depression; male; middle aged; posttraumatic stress disorder; procedures; psychology; randomized controlled trial; therapy; treatment outcome; virtual reality exposure therapy",Article,Final,,Scopus,2-s2.0-85215682684,Gaming / VR
Mazloum S.; Khademi N.; Ahmadi A.; Afand K.; Zabihpour A.; Rafe A.; Singleton P.A.,"Mazloum, Sajjad (59205408000); Khademi, Navid (55058710100); Ahmadi, Ali (59763585000); Afand, Kimia (59984483000); Zabihpour, Amirmohammad (59206592000); Rafe, Amir (57219819636); Singleton, Patrick A. (57215476386)",59205408000; 55058710100; 59763585000; 59984483000; 59206592000; 57219819636; 57215476386,Exploring the Role of Leadership in Pedestrian Evacuations Using a Virtual Reality (VR) Environment: An Eye-Tracking Study,2025,International Conference on Transportation and Development 2025: Transportation Planning and Operations - Selected Papers from the International Conference on Transportation and Development 2025,,,,407,418,11.0,0,10.1061/9780784486207.035,https://www.scopus.com/inward/record.uri?eid=2-s2.0-105010225384&doi=10.1061%2f9780784486207.035&partnerID=40&md5=0698a1f720600982dbd9348977333945,"Effective crowd management during evacuations depends on timely information and leadership that guides evacuees to safety. This study investigates factors influencing pedestrian behavior in outdoor evacuations using immersive virtual reality (VR). Data from 27 participants, collected through eye-tracking and physiological sensors, examined leader credibility, leadership style (visual versus visual-verbal), stress levels, leader gender, familiarity, and crowd behavior. Mann-Whitney U tests analyzed attention and physiological responses, while a mixed binary logistic regression model identified predictors of leader-following. Results indicated that leader credibility and environmental familiarity were key predictors of leader-following. Participants were more likely to follow leaders in scenarios that included both visual and verbal leadership, male leaders, and normal conditions. Female participants showed a greater likelihood to follow leaders. Analysis of eye-tracking measures revealed increased attention and scanning spans among followers, particularly at night and under normal conditions. These findings highlight the importance of leadership dynamics and inform targeted strategies for effective evacuation management.  © ASCE.",Eye-Tracking; Immersive Virtual Reality; Leadership Dynamics; Pedestrian Evacuation; Physiological Sensors,Behavioral research; Eye movements; Information management; Logistic regression; Pedestrian safety; Physiological models; Virtual reality; Eye-tracking; Eye-tracking studies; Follow leader; Immersive virtual reality; Leader following; Leadership dynamic; Normal condition; Pedestrian evacuations; Physiological sensors; Virtual-reality environment; Eye tracking,Conference paper,Final,,Scopus,2-s2.0-105010225384,Gaming / VR
Vona F.; Schorlemmer J.; Kaulard P.; Fischer S.; Stemann J.; Voigt-Antons J.-N.,"Vona, Francesco (57204721437); Schorlemmer, Julia (57202641379); Kaulard, Paulina (59955401200); Fischer, Sebastian (55761108600); Stemann, Jessica (59566858300); Voigt-Antons, Jan-Niklas (57194940462)",57204721437; 57202641379; 59955401200; 55761108600; 59566858300; 57194940462,Comparing User Behavior in Real vs. Virtual Supermarket Shelves: An Eye-Tracking Study Using Tobii 3 Pro and Meta Quest Pro,2025,Communications in Computer and Information Science,2528 CCIS,,,395,405,10.0,0,10.1007/978-3-031-94168-9_39,https://www.scopus.com/inward/record.uri?eid=2-s2.0-105008649834&doi=10.1007%2f978-3-031-94168-9_39&partnerID=40&md5=6b0da0d2dc9f4b9a2ad520c2281a7c0b,"This study compares user behaviors between real and virtual supermarket shelves, using eye-tracking technology to assess behavior in both environments. A sample of 29 participants was randomly assigned to undergo two conditions: a real-world supermarket shelf with Tobii eye-tracking and a virtual shelf using Meta Quest Pro’s eye-tracker. In both scenarios, participants were asked to select three packs of cereals belonging to specific categories (healthy/tasty). The aim was to explore whether virtual environments could realistically replicate real-world experiences, particularly regarding consumer behavior. By analyzing eye-tracking data, researchers examined how attention and product selection strategies varied between real and virtual conditions. Participants’ attention differed across product types and shopping environments. Consumers focus on lower shelves in real settings, especially when looking for healthy products. In VR, attention shifted to eye-level shelves, particularly for tasty items, aligning with optimal product placement strategies in supermarkets. Generally, sweet products received less visual attention overall. © The Author(s), under exclusive license to Springer Nature Switzerland AG 2025.",Consumer behavior; Tobii Eye Tracker; Virtual Reality,Consumer behavior; Eye movements; Eye tracking; Retail stores; Virtual environments; Condition; Eye trackers; Eye tracking technologies; Eye-tracking; Eye-tracking studies; Real-world; Real-world experience; Tobii eye tracker; Tracking data; User behaviors; Virtual reality,Conference paper,Final,,Scopus,2-s2.0-105008649834,Gaming / VR
Andreou G.; Argatzopoulou A.,"Andreou, Georgia (55884721900); Argatzopoulou, Ariadni (58852640300)",55884721900; 58852640300,Unraveling ADHD Through Eye-Tracking Procedures: A Scoping Review,2025,Journal of Attention Disorders,,,1.08705E+16,,,,1,10.1177/10870547251344731,https://www.scopus.com/inward/record.uri?eid=2-s2.0-105009693959&doi=10.1177%2f10870547251344731&partnerID=40&md5=5e473810d2af514f5463d5b2b3e61cb3,"Objective: This scoping review aimed to examine the application of eye-tracking technology in children with Attention-Deficit/Hyperactivity Disorder (ADHD), focusing on the scientific fields involved, methodologies employed, research goals, and outcomes related to its effectiveness. Method: Following PRISMA guidelines for Scoping Reviews, a total of 22 studies using eye-tracking with children diagnosed with ADHD were identified and analyzed. Data were extracted regarding study aims, methodological approaches, disciplinary origins, and key findings. Results: The majority of studies originated from neuroscience and psychiatry, with contributions from artificial intelligence, machine learning, virtual reality, and biomedical engineering. Eye-tracking technology was used for three main purposes: (1) identification and diagnosis of ADHD, (2) investigation of cognitive and behavioral mechanisms—particularly attention and inhibitory control, and (3) as an intervention tool to improve cognitive functions. Findings indicated that eye-tracking, especially when integrated with virtual reality or machine learning, may support efficient ADHD diagnosis. Moreover, studies reported that distinct eye movement patterns are associated with attention deficits, inhibitory control issues, impaired working memory, and challenges in emotional and social processing. Interventions using eye-tracking demonstrated potential in training attention control and motor coordination. Conclusion: Eye-tracking technology holds promise as both a diagnostic and interventional tool for children with ADHD. Future research should address methodological limitations, explore long-term effectiveness, and further investigate multimodal integration with emerging technologies. © The Author(s) 2025",ADHD; attention; cognitive functions; eye-tracking; inhibitory control,,Article,Article in press,,Scopus,2-s2.0-105009693959,Gaming / VR
Iida Y.; Kameda Y.,"Iida, Yusuke (59565963000); Kameda, Yoshinari (8342036900)",59565963000; 8342036900,EEG Analysis for Cognition During Soccer Play Experience Using Head-Mounted Display,2025,Proceedings of SPIE - The International Society for Optical Engineering,13510,,135101M,,,,0,10.1117/12.3058028,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85218346762&doi=10.1117%2f12.3058028&partnerID=40&md5=217cd0567a205a2172cd63b3781f5db1,"In soccer games, the cognitive ability to understand a situation is essential.In this study, we use an HMD-type VR system in combination with an EEG-measuring device to measure brain waves while experiencing a specific soccer situation in a VR space and analyze the brain wave characteristics during cognition.To obtain accurate EEG measurements, we have to cope with the unexpected influence of body movements that occur while wearing the HMD on EEG measurements, including head rotation and gaze movement, which typically happen in HMD VR experience.In this research, we build a preliminary system that can measure EEG together with head rotation and gaze movement.Based on the alpha and beta band power of EEG frequency analysis, we discuss the influence of body movements on EEG analysis during different types of pass experiences.We found that the band powers of active experiences are larger than those of passive experiences. © 2024 SPIE.",electroencephalography; frontal lobe; head mounted display; Pass experience on soccer game,Brain; Electrophysiology; Eye movements; Head-up displays; Helmet mounted displays; Body movements; Brain wave; EEG analysis; Frontal lobes; Gaze movements; Head rotation; Head-mounted-displays; Pass experience on soccer game; Power; Soccer games; Electroencephalography,Conference paper,Final,,Scopus,2-s2.0-85218346762,Gaming / VR
Wang X.; Qu J.; Bu L.; Zhu S.,"Wang, Xinyi (59680729600); Qu, Jing (57762827400); Bu, Lingguo (57190024447); Zhu, Shantong (59004692600)",59680729600; 57762827400; 57190024447; 59004692600,Investigating Virtual Reality for Alleviating Human-Computer Interaction Fatigue: A Multimodal Assessment and Comparison with Flat Video,2025,IEEE Transactions on Visualization and Computer Graphics,31,5,,3580,3590,10.0,0,10.1109/TVCG.2025.3549581,https://www.scopus.com/inward/record.uri?eid=2-s2.0-105003745277&doi=10.1109%2fTVCG.2025.3549581&partnerID=40&md5=77564c481d0ba265631b1bfcae247c9f,"Studies have shown that prolonged Human-Computer Interaction (HCI) fatigue can increase the risk of mental illness and lead to a higher probability of errors and accidents during operations. Virtual Reality (VR) technology can simultaneously stimulate multiple senses such as visual, auditory, and tactile, providing an immersive experience that enhances cognition and understanding. Therefore, this study collects multimodal data to develop evaluation methods for HCI fatigue and further explores the fatigue-relieving effects of VR technology by comparing it with flat video. Using a modular design, electroencephalogram (EEG) and functional near-infrared spectroscopy (fNIRS) data in the resting, fatigue-induced, and recovery states, eye movement data in the resting and fatigue-induced states, as well as subjective scale results after each state were collected from the participants. Preprocessing and statistical analysis are performed through data flow architecture. After fatigue induction, it was found that the degree of activation of brain areas, especially the Theta band of prefrontal cortex, occurred significantly higher, the effective connectivity in the Alpha and Theta bands occurred significantly lower, the subjects' pupil diameters decreased, the blink frequency increased, and subjective questionnaire scores increased, which verified the validity of the multimodal data for assessing HCI fatigue. Analyzing fatigue relief through subgroups, it was found that when using the natural grassland scene with soothing music, both flat video and VR had the ability to alleviate fatigue, which was manifested as a significant decrease in the Alpha band in the LPFC brain area and a decrease in the questionnaire score. Moreover, during the recovery state, it was found that compared to the video group, the VR group had significantly higher activation in the Alpha and Theta bands of the prefrontal cortex, while the video group had significantly higher effective connectivity than the VR group in the Alpha band. This study delved deeply into the multidimensional characterization of fatigue and investigated new scenarios for the use of VR, which can help to promote the use of VR and can be migrated to scenarios that require fatigue management and productivity enhancement.  © 1995-2012 IEEE.",EEG; Evaluation methods; Eye-tracking; fNIRS; Software architectures; VR,"Adult; Brain; Computer Graphics; Electroencephalography; Fatigue; Female; Humans; Male; Spectroscopy, Near-Infrared; User-Computer Interface; Virtual Reality; Young Adult; Audition; Data flow analysis; Electroencephalography; Electrotherapeutics; Eye movements; Risk assessment; Risk perception; Video analysis; Virtual environments; Virtual reality; Virtualization; Brain areas; Computer interaction; Effective connectivities; Evaluation methods; Eye-tracking; Functional near infrared spectroscopy; Mental illness; Multi-modal; Prefrontal cortex; Virtual reality technology; adult; brain; computer graphics; computer interface; diagnostic imaging; electroencephalography; fatigue; female; human; male; near infrared spectroscopy; pathophysiology; physiology; virtual reality; young adult; Near infrared spectroscopy",Article,Final,,Scopus,2-s2.0-105003745277,Gaming / VR
Zhang K.; Chen J.; Yang Z.; Ji Y.; Min Y.; Wang G.; Liu X.,"Zhang, Kai (57215349729); Chen, Jingying (55139504300); Yang, Zhiyi (57443981300); Ji, Yanfeng (59477055100); Min, Yuandong (58084744100); Wang, Guangshuai (57204149575); Liu, Xiaodi (57200138498)",57215349729; 55139504300; 57443981300; 59477055100; 58084744100; 57204149575; 57200138498,Investigating joint attention in children with autism spectrum disorder through virtual reality and eye-tracking: a comparative study,2025,Education and Information Technologies,,,,,,,0,10.1007/s10639-025-13554-0,https://www.scopus.com/inward/record.uri?eid=2-s2.0-105001969209&doi=10.1007%2fs10639-025-13554-0&partnerID=40&md5=dd695cb7376ab0ebcb135f7b38087c84,"Joint attention is essential for establishing effective social communication. However, children with Autism Spectrum Disorder (ASD) often exhibit deficits in joint attention, which hinder their social interactions with peers and adults. Traditional assessment methods predominantly rely on expert observation, lacking objective measures of children’s joint attention responses. To overcome these limitations, digital learning environments utilizing virtual reality technology and eye-tracking offer new opportunities for effectively assessing joint attention in children with ASD. This study explores the joint attention performance of children with ASD under social and non-social stimuli by tracking their eye movements in virtual environments. The study results indicate that: (1) Under social stimulus conditions alone, the joint attention abilities of children with ASD are significantly influenced by the environment, while typically developing (TD) children’s abilities remain unaffected; (2) the additional of non-social stimuli significantly enhanced the joint attention scores for both groups. The TD group responded more effectively to non-social stimuli in the VR environment; (3) using TD children’s joint attention scores under the Ssocial condition as a baseline, the additional of non-social stimuli helped ASD group approach the joint attention levels of TD. The conclusion suggests that VR-based multi-stimulus environments provide meaningful joint attention opportunities for children. By comparing with TD group, the study offers new insights and references for future interventions aimed at improving joint attention in children with ASD. © The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature 2025.",Children with ASD; Eye-tracking; Joint attention; Multi-stimulus; Virtual reality,,Article,Article in press,,Scopus,2-s2.0-105001969209,Gaming / VR
Chen X.; Chen M.; Chen Y.; Lin Y.; Ke B.; Ni B.,"Chen, Xuanhong (57214245182); Chen, Muchun (57214246935); Chen, Yugang (59529941500); Lin, Yinxin (59256842800); Ke, Bilian (56256766900); Ni, Bingbing (57188713282)",57214245182; 57214246935; 59529941500; 59256842800; 56256766900; 57188713282,Large Generative Model Impulsed Lightweight Gaze Estimator via Deformable Approximate Large Kernel Pursuit,2025,IEEE Transactions on Image Processing,34,,,1149,1162,13.0,0,10.1109/TIP.2025.3529379,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85216069807&doi=10.1109%2fTIP.2025.3529379&partnerID=40&md5=9a25208ff45445f9bc9c996b889930d5,"Efficient and highly accurate lightweight gaze estimation method has been receiving increasing research attention due to the emergence of mobile interactive platforms such as mobile device and AR/VR. State-of-the-art deep learning based gaze estimation models suffer from either heavy computational architecture which is infeasible for mobile deployment or limited generalization capability which cannot deal with large diversity in eye texture or distinguish subtle/frequent pupil movement. To mitigate the above challenges, we propose a novel lightweight network structure featuring a deformable approximate large kernel which can effectively extend the receptive field to handle complicated eye movement and highly varying eye/gaze region appearance with very tight computational budget. In the meantime, we embed the training of the gaze estimator into a control information extraction module, which serves as a gaze-parameter input that modularizes a large generative model (Stable Diffusion V1.5) to output gaze-specific eye images. In this way, the great generalization capability of large generative model could be implicitly distilled/pursued into our lightweight gaze model. Extensive comparisons with various state-of-the-art gaze estimation methods demonstrate the superiority of our proposed model and training scheme in terms of both accuracy and model complexity.  © 1992-2012 IEEE.",channel self-attention; computational budget; deformable kernel; Gaze estimation; large generative model; large receptive field; lightweight model; reconstruction loss; spatial self-attention,Channel estimation; Deep learning; Eye movements; Generative adversarial networks; Higher order statistics; Image segmentation; Channel self-attention; Computational budget; Deformable kernel; Gaze estimation; Generative model; Large generative model; Large receptive field; Lightweight model; Receptive fields; Reconstruction loss; Spatial self-attention; article; deep learning; diffusion; eye movement; gaze; generative model; human; human experiment; pupil; receptive field; Budget control,Article,Final,,Scopus,2-s2.0-85216069807,Gaming / VR
Gulhan D.; Kaur Suri H.; Mascarenhas-Whitman I.; Tennyson A.; Durant S.,"Gulhan, Doga (57221762876); Kaur Suri, Harsimran (59943011500); Mascarenhas-Whitman, Isobel (59943390100); Tennyson, Alex (58178096200); Durant, Szonya (14019318600)",57221762876; 59943011500; 59943390100; 58178096200; 14019318600,The Effect of Particle Turbulence on Guiding Attention in a 360° VR Art Piece,2025,Lecture Notes in Computer Science,15789 LNCS,,,23,41,18.0,0,10.1007/978-3-031-93712-5_2,https://www.scopus.com/inward/record.uri?eid=2-s2.0-105007969258&doi=10.1007%2f978-3-031-93712-5_2&partnerID=40&md5=0ba7d8a79904b842924fb227f622f1c0,"In 3D immersive experiences, while users can explore freely, designers often aim for specific interactions and viewpoints. This study employs eye tracking to analyse how particle systems, a motion graphics technique for forming dynamic objects, affect attention in a 360° immersive experience. While visual factors like colour, size, and luminance influence attention, the role of motion properties, especially turbulence (unpredictability in particle movements, including variations in direction, speed, and flow), remains underexplored. By keeping other variables constant, we aimed to isolate the effects of turbulence, i.e. the aspect to which the randomness of the particle motion directions retain attention. Our case study utilised a VR narrative piece, a reimagining of Hans Christian Andersen’s “The Red Shoes”. We compared the effect of turbulence in two settings (low and high turbulence, where particles move coherently or incoherently), focusing on how the subtle manipulation of noise in particle trajectories impacts the spatial distribution of attention and the ability of moving particles to retain attention. Findings indicate a small advantage for the less turbulent particle clouds in retaining attention, and in general the swarm was attended to when no other object motion was visible to the viewer. Exploratory analyses showed how attention moved as the particle systems’ locations changed and how differences manifested in maintaining attention over time. Our findings and methodological approach offer valuable guidance for designing targeted VR experiences, particularly in utilising eye tracking in 3D to select different scene properties to enhance the effectiveness of attention guidance. © The Author(s), under exclusive license to Springer Nature Switzerland AG 2025.",Eye Tracking; Human Computer Interaction (HCI); Particle System; Virtual Reality; Visual Attention,Arts computing; Behavioral research; Eye movements; Human computer interaction; Motion tracking; Particle separators; Three dimensional computer graphics; Turbulence; Computer interaction; Eye-tracking; Graphic techniques; Human computer interaction; Immersive; Motion graphics; Particles system; Property; Specific interaction; Visual Attention; Eye tracking; Virtual reality,Conference paper,Final,,Scopus,2-s2.0-105007969258,Gaming / VR
Navrozidou E.; Karavidas L.; Katsanos C.; Tsiatsos T.,"Navrozidou, Eleni (59949194200); Karavidas, Lampros (57218701051); Katsanos, Christos (36118039900); Tsiatsos, Thrasyvoulos (6602843005)",59949194200; 57218701051; 36118039900; 6602843005,Comparing Gaze-Based Interaction Methods in a Game Environment,2025,Lecture Notes in Computer Science,15816 LNCS,,,288,297,9.0,0,10.1007/978-3-031-92578-8_19,https://www.scopus.com/inward/record.uri?eid=2-s2.0-105008266055&doi=10.1007%2f978-3-031-92578-8_19&partnerID=40&md5=421a1a0df29c4a4ba7dfc16d1f6fa07b,"Eye tracking is a valuable tool in human-computer interaction providing insights into user behavior via gaze data or opportunities for novel gaze-based interaction methods. These new interaction methods can result in greater immersion and accessibility, which are particularly important in gaming. Several gaze-based interaction methods, such as dwell time and blinking, have been proposed in an attempt to balance interaction efficiency and accuracy with player fatigue. This study compares two such gaze-based interaction methods, the point-and-click and the drag-and-drop, in the context of a time-management game. In specific, 30 participants used a Tobii Pro Fusion eye tracker for both interaction methods and data were collected about their user experience and in-game efficiency. Statistical analysis showed that the drag-and-drop method led to significantly better perceived user experience in the dimensions of Attractiveness, Efficiency and Dependability. No significant difference was observed in terms of in-game efficiency. © The Author(s), under exclusive license to Springer Nature Switzerland AG 2025.",Eye tracking; Game Design; Gaming Interface; Gaze-based Interaction; Input Methods; Interaction Methods,Behavioral research; Computer games; Drops; Efficiency; Eye movements; Game design; Human computer interaction; User experience; User interfaces; Computer interaction; Drag and drop; Eye-tracking; Game design; Game environment; Gaming interface; Gaze-based interaction; Input methods; Interaction methods; Users' experiences; Eye tracking,Conference paper,Final,,Scopus,2-s2.0-105008266055,Gaming / VR
Veerkamp K.; Müller D.; Pechler G.A.; Mann D.L.; Olivers C.N.L.,"Veerkamp, Kirsten (57163697400); Müller, Daniel (57224580953); Pechler, Gwyneth A. (58310768000); Mann, David L. (24464168800); Olivers, Christian N. L. (6603803862)",57163697400; 57224580953; 58310768000; 24464168800; 6603803862,The effects of simulated central and peripheral vision loss on naturalistic search,2025,Journal of Vision,25,8,6,,,,0,10.1167/jov.25.8.6,https://www.scopus.com/inward/record.uri?eid=2-s2.0-105010427412&doi=10.1167%2fjov.25.8.6&partnerID=40&md5=cae4f1763b273021a6f421a55c76f381,"Worldwide, millions of people experience central or peripheral vision loss. The consequences on daily visual functioning are not completely known, in particular because previous studies lacked real-life representativeness. Our aim was to examine the effects of simulated central or peripheral impairment on a range of measures underlying performance in a naturalistic visual search task in a three-dimensional (3D) environment. The task was performed in a 3D virtual reality (VR) supermarket environment while being seated in a swivel chair. We used gaze-contingent masks to simulate vision loss. Participants were allocated to one of three conditions: full vision, central vision loss (a 6° mask), or peripheral vision loss (a 6° aperture) in a between-subject design. Each participant performed four search sequences, each consisting of four target products from a memorized shopping list, under varying contrast levels. Besides search time and accuracy, we tracked navigational, oculomotor, head and torso movements to assess which cognitive and motor components contributed to performance differences. Results showed increased task completion times with simulated central and peripheral vision loss, but more so with peripheral loss. With central vision loss, navigation was less efficient and it took longer to verify targets. Furthermore, participants made more and shorter fixations. With peripheral vision loss, navigation was even less efficient, and it took longer to find and verify a target. Additionally, saccadic amplitudes were reduced. Lowcontrast particularly affected search with peripheral vision loss. Memory failure, indicating cognitive load, did not differ between conditions. Thus we demonstrate that simulations of central and peripheral vision loss lead to differential search profiles in a naturalistic 3D environment. © 2025 The Authors",gaze-contingent display; low vision; virtual reality; visual field loss; visual search,Adult; Eye Movements; Female; Humans; Male; Middle Aged; Virtual Reality; Visual Fields; Young Adult; adult; eye movement; female; human; male; middle aged; physiology; virtual reality; visual field; young adult,Article,Final,,Scopus,2-s2.0-105010427412,Gaming / VR
Wang J.; Hao L.; Yang C.,"Wang, Jing (58610570300); Hao, Liangliang (59937165400); Yang, Chaoxiang (56242464000)",58610570300; 59937165400; 56242464000,The Cognitive Performance of Chinese Characters Complexity and Layout Elements with Dark Patterns on Spacecraft Interfaces,2025,Lecture Notes in Computer Science,15777 LNAI,,,355,372,17.0,0,10.1007/978-3-031-93721-7_25,https://www.scopus.com/inward/record.uri?eid=2-s2.0-105007686568&doi=10.1007%2f978-3-031-93721-7_25&partnerID=40&md5=5e337290a5ddb2b7dc8170aa803e368d,"Knowing cognitive performance on interfaces in risky environments is essential. The complexity of spacecraft interfaces causes astronauts’ cognitive loads, affecting mission completion and threatening safety. Now that the Chinese Space Station is undertaking global cooperation, Chinese Characters have become highly usable as the standard language of spacecraft systems. Targeting these concerns, based on healthful dark patterns, this paper explores the impact mechanism of Chinese Characters complexity and layout elements on cognitive performance. Combining task assessment, eye-movement feedback and subjective evaluation as indicators, and utilizing virtual reality technology to simulate space. This paper explores the intrinsic relationship between different interfaces and subjects’ cognitive load. In Experiment I, determined the legibility of Chinese characters on structures, strokes, sizes, spacing. Obtained their threshold ranges within the spacecraft interface. In Experiment II, selected cognitively beneficial interface layouts by element permutation test. Additionally, using multiple linear regression, discerned cognitive preferences for datas, labels, auxiliary lines, and buttons. Based on the findings, this study proposed optimization insights for spacecraft interfaces, designed schemes and validated usability. Confirmed the enhancement in astronauts’ recognition efficiency and mission success rate. © The Author(s), under exclusive license to Springer Nature Switzerland AG 2025.",Chinese Characters Complexity; Cognitive Performance; Layout Elements; Spacecraft Interface Design,Chinese character complexity; Chinese characters; Cognitive loads; Cognitive performance; Global cooperation; Interface designs; Layout element; Mission completion; Spacecraft interface; Spacecraft interface design; Space stations,Conference paper,Final,,Scopus,2-s2.0-105007686568,Gaming / VR
Sunny M.J.M.; Sai Nikitha Guthula J.; Kharel A.; Shraboni M.N.; Rashid H.; Bhandari P.; Springer J.P.; Basu A.,"Sunny, M.J.M. (57208389195); Sai Nikitha Guthula, Jayasri (59897745400); Kharel, Atit (59897950600); Shraboni, Meherun Nesa (58869798200); Rashid, Hadi (57985416000); Bhandari, Praveshika (57984883300); Springer, Jan P. (57985238200); Basu, Aryabrata (59843187600)",57208389195; 59897745400; 59897950600; 58869798200; 57985416000; 57984883300; 57985238200; 59843187600,Gaze Insights in XR: Real-Time Eye-Tracking Analytics with Elasticsearch,2025,"Proceedings - 2025 IEEE Conference on Virtual Reality and 3D User Interfaces Abstracts and Workshops, VRW 2025",,,,97,103,6.0,0,10.1109/VRW66409.2025.00028,https://www.scopus.com/inward/record.uri?eid=2-s2.0-105005156563&doi=10.1109%2fVRW66409.2025.00028&partnerID=40&md5=c010312e36f8edafdd75cc38a6761107,"Gaze tracking serves as a critical component in human-computer interaction, significantly enhancing the capabilities of immersive extended-reality (XR) applications. Despite the integration of advanced eye-tracking technologies in modern XR devices, challenges surrounding usability, scalability, and privacy remain unresolved. This study presents a technical framework leveraging the Elastic Stack (Elasticsearch, Logstash, Kibana) to enable real-time data collection, indexing, and analysis of gaze-tracking data alongside other interaction metrics. The framework integrates scalable distributed storage, efficient querying, and immersive heatmap visualizations within Unity, offering precise insights into user gaze behavior during interactive tasks. A pilot study involving a gaze-intensive VR task validated the system's capacity to identify focus regions, analyze attention shifts, and detect performance bottlenecks. Notable functionalities include real-time heatmap overlays for user engagement analysis and replay systems for assessing spatial navigation strategies. The framework employs secure indexing configurations to address ethical concerns and discusses privacy-preserving mechanisms, including homomorphic encryption, ensuring the protection of sensitive data such as user focus and intent. This pilot provides a robust, scalable, and privacy-compliant solution for gaze-driven analytics, advancing the state of usability studies and XR application development.  © 2025 IEEE.",ELK Stack; Extended reality (XR); Gaze tracking; Homomorphic encryption; Human-computer interaction; Privacy-preserving technologies,Human computer interaction; Privacy by design; Scalability; Sensitive data; Usability engineering; Computer interaction; ELK stack; Extended reality (XR); Gaze-tracking; Heatmaps; Ho-momorphic encryptions; Homomorphic-encryptions; Immersive; Privacy preserving; Privacy-preserving technology; Privacy-preserving techniques,Conference paper,Final,,Scopus,2-s2.0-105005156563,Gaming / VR
Que Y.; Hu X.,"Que, Ying (57215926050); Hu, Xiao (55496358400)",57215926050; 55496358400,Examining the Effect of Background Music on Learners’ Attention and Cognition in Virtual Reality Environments: A Psychophysiological Study,2025,International Journal of Human-Computer Interaction,,,,,,,0,10.1080/10447318.2025.2505778,https://www.scopus.com/inward/record.uri?eid=2-s2.0-105007435215&doi=10.1080%2f10447318.2025.2505778&partnerID=40&md5=51f4399af2a6f32f2b4a9d0510774bca,"Virtual Reality (VR) enriches learning and instruction, while background music (BGM) is widely employed to modulate attention and cognition. Understanding how BGM influences learning in VR is essential for optimizing VR learning environments and has the potential to improve educational outcomes, yet this area remains largely unexplored. This study collected fifty-two participants’ self-reports, electroencephalogram signals, eye movements, heart rates, and interview responses to explore their attention and cognition during studying virtual heritage sites with and without BGM, while considering individual traits as influential factors. Results showed that with BGM, participants reported higher levels of engagement and demonstrated longer fixation duration on heritage sites’ image regions, than without BGM. Moreover, participants’ self-reported familiarity with the cultural heritage site and BGM listening frequency moderated the effect of BGM on attention and cognition in VR. This study offers implications for incorporating BGM for learning in VR, and designing personalized VR learning environments. © 2025 Taylor & Francis Group, LLC.",attention; background music; cognition; psychophysiological signals; Virtual reality,Audition; Odors; Virtual reality; Virtualization; Vision; Attention; Background musics; Cognition; Electroencephalogram signals; Heart-rate; Heritage sites; Psychophysiological signals; Virtual heritage; Virtual reality learning environments; Virtual-reality environment; Virtual environments,Article,Article in press,,Scopus,2-s2.0-105007435215,Gaming / VR
Mahamadu A.-M.; Prabhakaran A.; Manu P.; Xiang Y.; Booth C.A.,"Mahamadu, Abdul-Majeed (56084000400); Prabhakaran, Abhinesh (57222107163); Manu, Patrick (35756460600); Xiang, Yiming (59959079600); Booth, Colin A. (35239692900)",56084000400; 57222107163; 35756460600; 59959079600; 35239692900,A Virtual Reality Approach to Enhancing Hazard Perception in Construction Using Eye-Tracking and Deep Learning,2025,"Applications of Immersive Technology in Architecture, Engineering and Construction: A Handbook",,,,71,81,10.0,0,10.1201/9781032662909-8,https://www.scopus.com/inward/record.uri?eid=2-s2.0-105008839701&doi=10.1201%2f9781032662909-8&partnerID=40&md5=aa51e3abf1e9dc47167e8e7b5a12d27a,"Over 50% of onsite hazards remain unrecognised, primarily due to poor hazard perception and attention deficits. Eye-tracking can help in understanding human visual attention and cognitive response, but its application in construction has been restricted by the reliance on two-dimensional (2D) stimulus, such as pictures, which often lack realism. This study proposes a novel immersive virtual reality (VR) system integrated with computer vision (eye-tracking) technology for visual attention measurement. The study further explores the characterisation of VR eye-tracking data using Deep Learning Techniques. Based on tests with (n = 15) construction professionals across four experiments, VR eye-tracking was found to be a viable platform for understanding visual behaviour in hazard perception tasks. A Long Short-Term Memory (LSTM) Deep Learning model was further applied to the eye-tracking data and was found to effectively predict data peculiarities, as well as hazard identification performance of participants. The study provides preliminary evidence of the usefulness of eye-tracking data in VR simulations for enhancing safety training and provides pathways for real-time visual attention data measurement for safety interventions on construction sites. © 2025 selection and editorial matter, Abhinesh Prabhakaran, Abdul-Majeed Mahamadu, Colin A. Booth and Patrick Manu; individual chapters, the contributors.",,Behavioral research; Deep learning; Hazards; Learning systems; Personnel training; Virtual reality; Attention deficit; Cognitive response; Eye-tracking; Hazard perceptions; Human visual attention; Immersive virtual reality; ITS applications; Tracking data; Two-dimensional; Visual Attention; Eye tracking,Book chapter,Final,,Scopus,2-s2.0-105008839701,Gaming / VR
Zhang R.; Yan W.; Liu J.; Zhao Q.,"Zhang, Rui (57222028270); Yan, Weidong (57217132794); Liu, Jiaxi (57218513213); Zhao, Qingyan (58955195700)",57222028270; 57217132794; 57218513213; 58955195700,Experimental and simulation research on evacuation with signage in mind: application of eye-tracking in virtual reality,2025,International Journal of System Assurance Engineering and Management,,,,,,,0,10.1007/s13198-025-02867-5,https://www.scopus.com/inward/record.uri?eid=2-s2.0-105010946807&doi=10.1007%2fs13198-025-02867-5&partnerID=40&md5=de4d8287597903cdcd1820d077eef6b9,"Signage systems play a vital role in facilitating safe and efficient evacuation during emergencies in complex buildings. However, existing studies often rely on idealized assumptions and fail to capture individuals’ actual cognitive and perceptual responses to signage, resulting in discrepancies between simulation predictions and real-world evacuation behavior. To address this gap, this study constructs an experimental platform integrating virtual reality (VR) and eye-tracking technologies to objectively record participants’ visual attention distribution and signage recognition during evacuation scenarios. Experimental results indicate that the effectiveness of signage is significantly constrained by individual visual-cognitive mechanisms. Based on these findings, an improved multi-signage evacuation guidance model is developed, which comprehensively accounts for factors such as static and dynamic signage, fire-induced environmental changes, and guidance field intensity. The model further optimizes the effective influence range of signage. Simulation results show strong consistency with empirical data (p = 0.5244 > 0.05), confirming the model’s validity and reliability. In addition, simulations under varying signage densities reveal that higher signage density does not always enhance evacuation efficiency; instead, there exists an optimal threshold, with the best guidance performance observed at a density of ρ = 2. This study provides theoretical foundations and empirical evidence for the scientific deployment of evacuation signage and the optimization of evacuation strategies, offering valuable insights for practical applications in building safety and emergency management. © The Author(s) under exclusive licence to The Society for Reliability Engineering, Quality and Operations Management (SREQOM), India and The Division of Operation and Maintenance, Lulea University of Technology, Sweden 2025.",Eye-tracking instruments; Improved evacuation modeling; Sign density; Virtual reality experiments,Behavioral research; Civil defense; Disasters; Eye tracking; Risk management; Virtual addresses; Complex buildings; Evacuation models; Experimental research; Eye-tracking; Eye-tracking instrument; Improved evacuation modeling; Sign density; Simulation prediction; Simulation research; Virtual reality experiment; Virtual reality,Article,Article in press,,Scopus,2-s2.0-105010946807,Gaming / VR
Nafe Assafi M.; Wang J.; Ma J.; Cotton J.,"Nafe Assafi, Mohammad (57656967000); Wang, Jun (57211090713); Ma, Junfeng (56057308800); Cotton, Joshua (58369459000)",57656967000; 57211090713; 56057308800; 58369459000,Assessing the impact of eye-tracking features on construction hazard identification skills using virtual reality and machine learning,2025,Construction Innovation,,,,,,,0,10.1108/CI-10-2024-0315,https://www.scopus.com/inward/record.uri?eid=2-s2.0-105005800401&doi=10.1108%2fCI-10-2024-0315&partnerID=40&md5=2a7b328424d7df254dd0ee96a7a5e856,"Purpose: Identifying hazards in construction sites is essential for ensuring safety. This study aims to propose a machine learning-based approach to identify the most effective set of eye-tracking features for assessing hazard identification skills on construction sites. By determining which features best contribute to hazard identification across various hazard types, the approach seeks to improve workplace safety. Design/methodology/approach: Four hazard types and 11 eye-tracking features were identified, and 18 strategies were developed to evaluate hazard identification skills. Support vector machine (SVM) and artificial neural network (ANN) models were applied to assess these strategies. Virtual reality simulations were used to gather eye-tracking data and evaluate hazard identification. Findings: The SVM and ANN models effectively identified the most impactful set of eye-tracking features for each hazard type individually and all hazard types together. The results indicated that the proposed approach can accurately assess hazard identification skills in construction, offering a data-driven means of reducing injuries. Research limitations/implications: This study advances safety research by identifying the most relevant eye-tracking features linked to different types of construction hazards using a machine learning approach. The results can help develop personalized VR-based training and evaluation systems that improve workers’ visual attention for hazard identification, leading to safer construction practices. Originality/value: While previous studies have explored eye-tracking features for assessing hazard identification skills, the effectiveness of certain features has been unclear. This study clarifies which set of eye-tracking features is most effective for assessing hazard identification skills based on specific hazard types, contributing to enhanced workplace safety. © 2025, Emerald Publishing Limited.",ANN; Construction safety; Eye-tracking features; Hazard identification skills; Machine learning; SVM; Virtual reality,Health hazards; Virtual environments; Construction hazards; Construction safety; Eye-tracking; Eye-tracking feature; Hazard identification; Hazard identification skill; Machine-learning; Neural-networks; Support vectors machine; Tracking feature; Support vector machines,Article,Article in press,,Scopus,2-s2.0-105005800401,Gaming / VR
Ascione M.; Glashouwer K.A.; Meschberger-Annweiler F.-A.; Mendoza-Medialdea M.T.; Porras-Garcia B.; Ferrer-Garcia M.; Gutierrez-Maldonado J.,"Ascione, Mariarca (57772657700); Glashouwer, Klaske A. (32667729000); Meschberger-Annweiler, Franck-Alexandre (57773528000); Mendoza-Medialdea, María Teresa (57202075038); Porras-Garcia, Bruno (57201200642); Ferrer-Garcia, Marta (13405944200); Gutierrez-Maldonado, José (9334173600)",57772657700; 32667729000; 57773528000; 57202075038; 57201200642; 13405944200; 9334173600,"Self-Disgust, Body-Related Attentional Bias and Body Dissatisfaction: A Virtual Reality and Eye-Tracking Exploration",2025,International Journal of Eating Disorders,,,,,,,0,10.1002/eat.24467,https://www.scopus.com/inward/record.uri?eid=2-s2.0-105005789623&doi=10.1002%2feat.24467&partnerID=40&md5=f8a4dd5fa7c781aaed913c8b9d73aef8,"Objective: This study examines the relationships between self-disgust, body dissatisfaction (BD), and attentional biases (AB) toward weight-related body areas, exploring whether self-disgust predicts attentional avoidance and moderates the relationship between BD and AB. Method: Using virtual reality and eye-tracking technology, 78 female students viewed their virtual bodies in a mirror to assess gaze patterns as an indicator of attentional bias. Results: BD was positively associated with both AB and self-disgust. Contrary to expectations, self-disgust correlated with increased attention to weight-related areas rather than avoidance and did not moderate the BD–AB relationship. Discussion: These findings suggest that self-disgust may reinforce attention toward weight-related areas, contributing to negative body image. Future research should explore these mechanisms in clinical populations to inform targeted interventions. © 2025 The Author(s). International Journal of Eating Disorders published by Wiley Periodicals LLC.",attentional bias; body dissatisfaction; eye-tracking; self-disgust; virtual reality,,Article,Article in press,,Scopus,2-s2.0-105005789623,Gaming / VR
Maddalon L.; Minissi M.E.; Alcañiz M.,"Maddalon, Luna (57456455300); Minissi, Maria Eleonora (57207617337); Alcañiz, Mariano (36921902100)",57456455300; 57207617337; 36921902100,Early diagnosis of ASD using biomarkers: a narrative review; [DIAGNÓSTICO TEMPRANO DEL TEA UTILIZANDO BIOMARCADORES: UNA REVISIÓN NARRATIVA],2025,Medicina (Buenos Aires),85,,,3,8,5.0,0,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-86000112062&partnerID=40&md5=c3837b141c3dbfc3be6e547fc2ecb024,"Autism Spectrum Disorder (ASD) encompasses a range of neurodevelopmental conditions characterized by social challenges, repetitive behaviors, and communication difficulties. While diagnosis traditionally relies on behavioral observations, new biomedical approaches, such as the Research Domain Criteria (RDoC), aim to identify biomarkers that integrate genetic, neural, and behavioral factors. Notable biomarkers include genetic variants, molecular alterations such as abnormal neurotransmitter levels, and markers associated with immune dysfunction. Brain organoids have also enabled the investigation of specific neural mechanisms. In neuroimaging, techniques such as functional magnetic resonance imaging (fMRI) and functional near-infrared spectroscopy (fNIRS) have identified atypical connectivity patterns in infants at high risk for ASD. Similarly, measures like electroencephalography (EEG) and eye tracking have revealed differences in visual attention and brain activity, while physiological indicators such as electrodermal activity (EDA) and heart rate variability (HRV) reflect sensory and autonomic dysfunctions. The use of digital biomarkers is rapidly growing, with devices like tablets and virtual reality capturing data on children’s interactions. Analyzed using artificial intelligence, these data show promise for improving early ASD detection, though further validation is needed. Integrating traditional and digital approaches is essential for advancing diagnosis and intervention strategies. © 2025, Fundacion Revista Medicina (Buenos Aires). All rights reserved.",assessment; Autism Spectrum Disorder; biomarker; children; early diagnosis,"Autism Spectrum Disorder; Biomarkers; Early Diagnosis; Electroencephalography; Humans; Magnetic Resonance Imaging; Neuroimaging; Spectroscopy, Near-Infrared; biological marker; biological marker; artificial intelligence; autism; autonomic dysfunction; behavioral observation; cerebral organoid; child; compulsion; diagnosis; early diagnosis; electroencephalogram; electroencephalography; eye tracking; eye-tracking technology; female; functional magnetic resonance imaging; functional near-infrared spectroscopy; genetic variability; heart rate variability; human; immune deficiency; infant; male; narrative; neuroimaging; review; virtual reality; visual attention; near infrared spectroscopy; nuclear magnetic resonance imaging; pathophysiology; procedures",Review,Final,,Scopus,2-s2.0-86000112062,Gaming / VR
Hao S.; Hou R.; Zhang J.; Shi Y.; Zhang Y.; Wang C.,"Hao, Shimeng (55910412900); Hou, Rui (58976133700); Zhang, Jie (59837726200); Shi, Yang (58295770200); Zhang, Yisong (58976662000); Wang, Chen (57808523800)",55910412900; 58976133700; 59837726200; 58295770200; 58976662000; 57808523800,Visual behavior characteristics of historical landscapes based on eye-tracking technology,2025,Journal of Asian Architecture and Building Engineering,24,2,,487,506,19.0,5,10.1080/13467581.2024.2306361,https://www.scopus.com/inward/record.uri?eid=2-s2.0-105001838738&doi=10.1080%2f13467581.2024.2306361&partnerID=40&md5=b5f5eed051896aa2824c863fa1c979ca,"The subjective encounter with architecture encompasses the particular presentation of architectural elements or the overall structure to individuals, taking into account their perception, cognition, and thought processes. The purpose of this research is to investigate the intricate relationship between visual attention directed towards historical architecture and the subjective experiences engendered during the process of observation. Utilizing eye-tracking technology within the realm of virtual reality, this study delves into the observation patterns exhibited by individuals when confronted with historical architecture, specifically focusing on the traditional courtyard residences found in Beijing’s hutongs. The panoramic images, meticulously modeled and rendered, are divided into six distinct areas of interest: base, ground, window&doors, walls, roof, and eaves. The eye-tracking data of 81 participants, who engaged with 10 virtual scenes through the employment of VR headsets, along with their responses to architectural style questionnaires, were systematically gathered. Through comprehensive analysis, encompassing the examination of total fixation duration (TFD), fixation count (FC), first fixation duration (FFD), and time to first fixation (TFF) across the areas of interest (AOIs), in addition to deviations in scores among different architectural areas and styles, notable insights emerged. Results indicate that both professionals and non-professionals allocate heightened attention, as evidenced by TFD, FC, TFF, and FFD, to elements that exhibit greater score deviations in the questionnaires as opposed to those with smaller discrepancies. Moreover, the TFD and FC metrics pertaining to the Windows&doors AOI collectively constitute 40.20% and 40.71%, respectively. Undoubtedly, these figures signify the preeminent focal points within all AOIs. This underscores the pivotal role played by doors and windows in shaping individuals’ judgments pertaining to historical architectural styles. This research provides designers with valuable understanding of the cognitive patterns exhibited by individuals in engaging with aspects of history, which enable them to identify and preserve key elements associated with historical styles. Additionally, it establishes a fundamental cornerstone for future quantitative research endeavors centered around the preservation of historical architectural styles. © 2024 The Author(s). Published by Informa UK Limited, trading as Taylor & Francis Group on behalf of the Architectural Institute of Japan, Architectural Institute of Korea and Architectural Society of China.",Beijing quadrangle; Historic building protection; Historic Buildings; VR,Architecture; Doors; Eye tracking; Virtual reality; Architectural style; Area of interest; Beijing quadrangle; Building protection; Eye tracking technologies; Fixation duration; Historic building protection; Historic buildings; VR; Behavioral research,Article,Final,All Open Access; Gold Open Access,Scopus,2-s2.0-105001838738,Gaming / VR
Li Q.; Pan Y.,"Li, Qi (59950910000); Pan, Yafeng (57191922458)",59950910000; 57191922458,Mobile eye-tracking and neuroimaging technologies reveal teaching and learning on the move: Bibliometric mapping and content analysis,2025,Psychoradiology,5,,kkaf013,,,,0,10.1093/psyrad/kkaf013,https://www.scopus.com/inward/record.uri?eid=2-s2.0-105008389753&doi=10.1093%2fpsyrad%2fkkaf013&partnerID=40&md5=00ce799ebb354399531d0394405882bc,"Mobile psychophysiological technologies, such as portable eye tracking, electroencephalography, and functional near-infrared spectroscopy, are advancing ecologically valid findings in cognitive and educational neuroscience research. Staying informed on the field's current status and main themes requires continuous updates. Here, we conducted a bibliometric and text-based content analysis on 135 articles from Web of Science, specifically parsing publication trends, identifying prolific journals, authors, institutions, and countries, along with influential articles, and visualizing the characteristics of cooperation among authors, institutions, and countries. Using a keyword co-occurrence analysis, five clusters of research trends were identified: (i) cognitive and emotional processes, intelligent education, and motor learning; (ii) professional vision and collaborative learning; (iii) face-to-face social learning and real classroom learning; (iv) cognitive load and spatial learning; and (v) virtual reality-based learning, child learning, and technology-assisted special education. These trends illustrate a consistent growth in the use of portable technologies in education over the past 20 years and an emerging shift towards ""naturalistic""approaches, with keywords such as ""face-to-face""and ""real-world""gaining prominence. These observations underscore the need to further generalize the current research to real-world classroom settings and call for interdisciplinary collaboration between researchers and educators. Also, combining multimodal technologies and conducting longitudinal studies will be essential for a comprehensive understanding of teaching and learning processes.  © 2025 The Author(s).",bibliometric analysis; ecologically valid education; mobile eye tracking; mobile fNIRS; portable EEG,,Review,Final,,Scopus,2-s2.0-105008389753,Gaming / VR
Wojtkiewicz K.; Palak R.; Telec Z.; Litwinienko F.,"Wojtkiewicz, Krystian (24825891800); Palak, Rafał (57191272124); Telec, Zbigniew (34771890400); Litwinienko, Filip (58617648500)",24825891800; 57191272124; 34771890400; 58617648500,Modelling Cognitive Load of Computer Game Users - Case Study,2025,Lecture Notes in Networks and Systems,1217 LNNS,,,52,63,11.0,0,10.1007/978-3-031-78465-1_5,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85214124241&doi=10.1007%2f978-3-031-78465-1_5&partnerID=40&md5=0f5ac2835aed976b87ea48671b0aeb2d,"This study evaluated the effectiveness of various methods for measuring cognitive load (CL) in arcade racing game players and their usefulness in modelling CL. The techniques used were EEG, GSR, eye-tracking, pupilometry, and subjective surveys. The ""Trackmania Nations Forever"" game had three difficulty levels: easy, medium, and hard. Forty-three participants were involved, and data were collected on player performance, survey responses, and physiological signals. The analysis included retries per minute (RPM), advancement measures, NASA-TLX, and SEQ scores. Significant changes in physiological measures were observed corresponding to different cognitive load levels, with the most accurate classification results achieved by combining physiological and performance data. © The Author(s), under exclusive license to Springer Nature Switzerland AG 2025.",Bio-metrics; Cognitive load; User experience,Physiological models; Bio-metric; Case-studies; Cognitive loads; Eye-tracking; Game players; NASA-TLX; Performance; Physiological signals; Response signal; Users' experiences; Computer games,Conference paper,Final,,Scopus,2-s2.0-85214124241,Gaming / VR
Tütüncü E.K.; Slater M.,"Tütüncü, Esen K. (59800039400); Slater, Mel (7202932472)",59800039400; 7202932472,Perception in Flux: Investigating Memory and Attention During Gradual Environmental Transformations in Virtual Reality,2025,"Proceedings - 2025 IEEE Conference on Virtual Reality and 3D User Interfaces Abstracts and Workshops, VRW 2025",,,,1166,1169,3.0,0,10.1109/VRW66409.2025.00230,https://www.scopus.com/inward/record.uri?eid=2-s2.0-105005141064&doi=10.1109%2fVRW66409.2025.00230&partnerID=40&md5=636c3c403f6c9906ad8fa0f453a1c53d,"This study examines how imperceptible environmental transitions in virtual reality (VR) influence spatial memory and perception. Participants experience a VR environment gradually morphing from one setting (a kitchen) to another (a bedroom) through the course of 15 minutes. We compare these morphing conditions to static controls to assess whether participants unconsciously form blended spatial memories integrating elements of both scenes. Eyetracking data is used to analyze attentional shifts and their relationship to undetected changes. We hypothesize that participants in morphing conditions encode integrated spatial representations without consciously detecting transformations. This research aims to advance our understanding of how gradual transitions shape memory and perception, offering insights into designing immersive VR experiences that subtly adapt while maintaining presence.  © 2025 IEEE.",Change Blindness; Cognitive Schemas; Eye Tracking; Spatial Memory,Change blindness; Cognitive schemas; Condition; Eye-tracking; Integrating element; Morphing; Spatial memory; Spatial perception; Static control; Virtual-reality environment; Virtual reality,Conference paper,Final,,Scopus,2-s2.0-105005141064,Gaming / VR
Tosa R.; Hattori S.; Hiroi Y.; Itoh Y.; Hiraki T.,"Tosa, Rinto (59510047000); Hattori, Shingo (58042242300); Hiroi, Yuichi (57188686839); Itoh, Yuta (56154865900); Hiraki, Takefumi (57147042200)",59510047000; 58042242300; 57188686839; 56154865900; 57147042200,ChromaGazer: Unobtrusive Visual Modulation using Imperceptible Color Vibration for Visual Guidance,2025,IEEE Transactions on Visualization and Computer Graphics,31,5,,3450,3458,8.0,0,10.1109/TVCG.2025.3549173,https://www.scopus.com/inward/record.uri?eid=2-s2.0-105003780919&doi=10.1109%2fTVCG.2025.3549173&partnerID=40&md5=0f1a25beab1a6f58f97fe800d871e1d4,"Visual guidance (VG) plays an essential role in directing user attention in virtual reality (VR) and augmented reality (AR) environments. However, traditional approaches rely on explicit visual annotations, which often compromise visual clarity and increase user cognitive load. To address this issue, we propose an unobtrusive VG technique based on color vibration, a phenomenon in which rapidly alternating colors at frequencies above 25 Hz are perceived as a single intermediate color. Our work explores a perceptual state that exists between complete color fusion and visible flicker, where color differences remain detectable without conscious awareness of vibration. Through two experimental studies, we first identified the thresholds separating complete fusion, this intermediate perceptual state, and visible flicker by systematically varying color vibration parameters. Subsequently, we applied color vibrations with derived thresholds to natural image regions and validated their attention-guiding capabilities using eye-tracking measurements. The results demonstrate that controlled color vibration successfully directs user attention while maintaining low cognitive demand, providing an effective method for implementing unobtrusive VG in VR and AR systems. © 1995-2012 IEEE.",augmented reality; color perception; imperceptible color vibration; visual guidance,Adult; Attention; Augmented Reality; Color; Color Perception; Computer Graphics; Eye-Tracking Technology; Female; Humans; Male; User-Computer Interface; Vibration; Virtual Reality; Young Adult; Color vision; Eye tracking; Virtual environments; Virtual reality; Visual communication; Cognitive loads; Colour fusion; Colour perception; Imperceptible color vibration; Traditional approaches; Traditional approachs; User attention; Visual annotations; Visual clarities; Visual guidance; adult; attention; augmented reality; color; color vision; computer graphics; computer interface; eye-tracking technology; female; human; male; physiology; vibration; virtual reality; young adult; Augmented reality,Article,Final,All Open Access; Green Open Access,Scopus,2-s2.0-105003780919,Gaming / VR
Ružický E.; Lacko J.; Kozák Š.; Šramka M.; Čerešnik M.,"Ružický, Eugen (6506017418); Lacko, Ján (37011248900); Kozák, Štefan (7007128459); Šramka, Miron (7005888044); Čerešnik, Michal (57103733600)",6506017418; 37011248900; 7007128459; 7005888044; 57103733600,Using Virtual Reality to Monitor Worker Stress and Fatigue in Industry,2025,"Proceedings of the International Conference on Cybernetics and Informatics, K and I",,2025,,,,,0,10.1109/KI64036.2025.10916446,https://www.scopus.com/inward/record.uri?eid=2-s2.0-105011820814&doi=10.1109%2fKI64036.2025.10916446&partnerID=40&md5=5f22354200a008aaadc3b06c84a40cae,"This study presents a new approach to the training of production line workers in the automotive industry using virtual reality and bio-signals. This approach integrates virtual reality with audio, facial recognition, ECG and EDA signals and the use of machine learning algorithms. The aim of the study was to design a comprehensive system to analyze emotional states such as stress and fatigue and collect selected physiological signals during simulated work on a production line. For this purpose, two scenarios typical of stress and fatigue monitoring on a production line in the automotive industry were proposed. The system uses these signals to semi-automatically classify the test workers into 5 levels and, using machine learning algorithms, assigns them to the appropriate class of desired work activities according to their performance and reactions to stress and fatigue. The quality assessment of the proposed integrated methodology was successfully monitored and tested on a sample of 120 adepts during two virtual reality work activity scenarios. The classification Accuracy of the Random Tree machine learning algorithm was 83% and 81% for stress and fatigue, respectively. © 2025 IEEE.",ECG; electrodermal activity; face and eye tracking; HTC Vive Pro Eye; quantitative stress and fatigue tracking; virtual reality; virtual training,Automobiles; Automotive industry; Bioelectric phenomena; Eye tracking; Face recognition; Fatigue of materials; Learning algorithms; Learning systems; Machine learning; Electrodermal activity; Eye-tracking; Face Tracking; HTC vive pro eye; Machine learning algorithms; Production line; Quantitative stress; Quantitative stress and fatigue tracking; Virtual training; Workers'; Virtual reality,Conference paper,Final,,Scopus,2-s2.0-105011820814,Gaming / VR
Simonetti A.; Bigne E.,"Simonetti, Aline (57211819253); Bigne, Enrique (55132662600)",57211819253; 55132662600,Customer journey-based smart technology of new brands: a self-reported and eye-tracking study,2025,Journal of Consumer Marketing,,,,,,,0,10.1108/JCM-12-2024-7461,https://www.scopus.com/inward/record.uri?eid=2-s2.0-105007084651&doi=10.1108%2fJCM-12-2024-7461&partnerID=40&md5=1db7ffc82ec73a7bd3237b7aae64f41b,"Purpose: This study aims to investigate the impact of advertising, packaging and packaging information on brand performance throughout the customer journey of a new brand. Design/methodology/approach: A real new brand was assessed through a controlled three-stage study involving 100 participants. Data included eye-tracking, behavioral observations and self-reported measures. Tasks involved watching an ad (prepurchase), shopping in simulated supermarkets (purchase) and subsequent evaluations of the product in home settings (postpurchase). The framework integrates schema theory and the Limited Capacity Model of Motivated Mediated Message Processing (LC4MP), and the Touchpoints Context Qualities (TCQ) framework. Findings: Advertising in the prepurchase stage drove visual attention to the packaging in the purchase stage, with 2.5 times more attention than chance in virtual reality and e-commerce shopping tasks. A notable proportion of consumers purchased the novel product, indicating successful engagement despite competition. Multiple touchpoints enhanced brand recall and recognition and increased brand trust across the customer journey. Consumers were curious about the new brand, but the QR code on the packaging was largely ignored. Originality/value: The study uniquely combines three stages of the customer journey with multimodal data, highlighting the role of integrated marketing communication (IMC) strategies. It demonstrates how advertising primes attention and curiosity, influencing consumer behavior and trust in new brands. The findings underline the importance of aligning brand communication across touchpoints to drive positive brand outcomes. © 2025, Emerald Publishing Limited.",Advertising; Customer journey; Eye-tracking; Integrated marketing communications; Packaging; Schema theory; Shopping; Visual attention,,Article,Article in press,,Scopus,2-s2.0-105007084651,Gaming / VR
Huang S.; Wen C.; Bai X.; Li S.; Wang S.; Wang X.; Yang D.,"Huang, Siyu (59914133900); Wen, Chang (57894089000); Bai, Xueying (58524764600); Li, Sihong (57893609500); Wang, Shuining (57894334500); Wang, Xiaoxuan (56681233800); Yang, Dong (55717929400)",59914133900; 57894089000; 58524764600; 57893609500; 57894334500; 56681233800; 55717929400,Exploring the Application Capability of ChatGPT as an Instructor in Skills Education for Dental Medical Students: Randomized Controlled Trial,2025,Journal of Medical Internet Research,27,,e68538,,,,0,10.2196/68538,https://www.scopus.com/inward/record.uri?eid=2-s2.0-105006671028&doi=10.2196%2f68538&partnerID=40&md5=51df173d9ba3620fcc6ee7be3ba25d1a,"Background: Clinical operative skills training is a critical component of preclinical education for dental students. Although technology-assisted instruction, such as virtual reality and simulators, is increasingly being integrated, direct guidance from instructors remains the cornerstone of skill development. ChatGPT, an advanced conversational artificial intelligence model developed by OpenAI, is gradually being used in medical education. Objective: This study aimed to compare the effects of ChatGPT-assisted skill learning on performance, cognitive load, self-efficacy, learning motivation, and spatial ability, with the aim of evaluating the potential of ChatGPT in clinical operative skills education. Methods: In this study, 187 undergraduate dental students recruited from a first-class university in China were randomly divided into a ChatGPT group and a blank control group. Among them, the control group used videos for skill acquisition, and the ChatGPT group used ChatGPT in addition to the videos. After 1 week of intervention, skills were tested using desktop virtual reality, and cognitive load was measured by recording changes in pupil diameter with an eye tracker. In addition, a spatial ability test was administered to analyze the effect of ChatGPT on those with different spatial abilities. Finally, a questionnaire was also used to assess cognitive load and self-efficacy during the learning process. Results: A total of 192 dental undergraduates from a top-tier Chinese university were initially recruited for the experiment by October 25, 2024. Following eye-tracking calibration procedures, 5 participants were excluded, resulting in 187 eligible students successfully completing the experimental protocol by November 2, 2024. Following a short-term intervention administered through randomized allocation, superior performance (ChatGPT group: mean 73.12, SD 10.06; control group: mean 65.54, SD 12.48; P<.001) was observed among participants in the ChatGPT group, along with higher levels of self-efficacy (P=.04) and learning motivation (P=.02). In addition, cognitive load was lower in the ChatGPT group according to eye-tracking measures (ChatGPT group: mean 0.137, SD 0.036; control group: mean 0.312, SD 0.032; P<.001). The analysis of the learning performance of participants with different spatial abilities in the 2 modalities showed that compared to the learners with high spatial abilities (ChatGPT group: mean 76.58, SD 9.23; control group: mean 73.89, SD 11.75; P=.22), those with low spatial abilities (ChatGPT group: mean 70.20, SD 10.71; control group: mean 55.41, SD 13.31; P<.001) were more positively influenced by ChatGPT. ©Siyu Huang, Chang Wen, Xueying Bai, Sihong Li, Shuining Wang, Xiaoxuan Wang, Dong Yang.",artificial intelligence; ChatGPT; clinical skills; cognitive load; dental education; motivation; randomized controlled trial; self-efficacy; spatial ability; virtual reality,"Adult; Artificial Intelligence; China; Clinical Competence; Education, Dental; Female; Generative Artificial Intelligence; Humans; Male; Self Efficacy; Students, Dental; Virtual Reality; Young Adult; adult; Article; calibration; ChatGPT; clinical protocol; cognitive load; controlled study; dental education; dental student; eye tracking; female; human; learning; major clinical study; male; medical education; medical student; motivation; professional knowledge; questionnaire; self concept; artificial intelligence; China; clinical competence; dental education; dental student; generative artificial intelligence; procedures; randomized controlled trial; virtual reality; young adult",Article,Final,,Scopus,2-s2.0-105006671028,Gaming / VR
Baetzner A.S.; Hill Y.; Roszipal B.; Gerwann S.; Beutel M.; Birrenbach T.; Karlseder M.; Mohr S.; Salg G.A.; Schrom-Feiertag H.; Frenkel M.O.; Wrzus C.,"Baetzner, Anke Sabine (58028825300); Hill, Yannick (57200306879); Roszipal, Benjamin (57816712300); Gerwann, Solène (58929372900); Beutel, Matthias (59532796800); Birrenbach, Tanja (23466394300); Karlseder, Markus (57815922100); Mohr, Stefan (56689465600); Salg, Gabriel Alexander (57211522689); Schrom-Feiertag, Helmut (37017384000); Frenkel, Marie Ottilie (55079978200); Wrzus, Cornelia (37082400100)",58028825300; 57200306879; 57816712300; 58929372900; 59532796800; 23466394300; 57815922100; 56689465600; 57211522689; 37017384000; 55079978200; 37082400100,Mass Casualty Incident Training in Immersive Virtual Reality: Quasi-Experimental Evaluation of Multimethod Performance Indicators,2025,Journal of Medical Internet Research,27,,e63241,,,,1,10.2196/63241,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85216278690&doi=10.2196%2f63241&partnerID=40&md5=fa5036d54f752dee32818eeb33f3b74a,"Background: Immersive virtual reality (iVR) has emerged as a training method to prepare medical first responders (MFRs) for mass casualty incidents (MCIs) and disasters in a resource-efficient, flexible, and safe manner. However, systematic evaluations and validations of potential performance indicators for virtual MCI training are still lacking. Objective: This study aimed to investigate whether different performance indicators based on visual attention, triage performance, and information transmission can be effectively extended to MCI training in iVR by testing if they can discriminate between different levels of expertise. Furthermore, the study examined the extent to which such objective indicators correlate with subjective performance assessments. Methods: A total of 76 participants (mean age 25.54, SD 6.01 y; 45/76, 59% male) with different medical expertise (MFRs: paramedics and emergency physicians; non-MFRs: medical students, in-hospital nurses, and other physicians) participated in 5 virtual MCI scenarios of varying complexity in a randomized order. Tasks involved assessing the situation, triaging virtual patients, and transmitting relevant information to a control center. Performance indicators included eye-tracking-based visual attention, triage accuracy, triage speed, information transmission efficiency, and self-assessment of performance. Expertise was determined based on the occupational group (39/76, 51% MFRs vs 37/76, 49% non-MFRs) and a knowledge test with patient vignettes. Results: Triage accuracy (d=0.48), triage speed (d=0.42), and information transmission efficiency (d=1.13) differentiated significantly between MFRs and non-MFRs. In addition, higher triage accuracy was significantly associated with higher triage knowledge test scores (Spearman ρ=0.40). Visual attention was not significantly associated with expertise. Furthermore, subjective performance was not correlated with any other performance indicator. Conclusions: iVR-based MCI scenarios proved to be a valuable tool for assessing the performance of MFRs. The results suggest that iVR could be integrated into current MCI training curricula to provide frequent, objective, and potentially (partly) automated performance assessments in a controlled environment. In particular, performance indicators, such as triage accuracy, triage speed, and information transmission efficiency, capture multiple aspects of performance and are recommended for integration. While the examined visual attention indicators did not function as valid performance indicators in this study, future research could further explore visual attention in MCI training and examine other indicators, such as holistic gaze patterns. Overall, the results underscore the importance of integrating objective indicators to enhance trainers’ feedback and provide trainees with guidance on evaluating and reflecting on their own performance. © Anke Sabine Baetzner, Yannick Hill, Benjamin Roszipal, Solène Gerwann, Matthias Beutel, Tanja Birrenbach, Markus Karlseder, Stefan Mohr, Gabriel Alexander Salg, Helmut Schrom-Feiertag, Marie Ottilie Frenkel, Cornelia Wrzus.",disaster medicine; emergency medicine; emergency simulation; eye tracking; mass casualty incident; medical education; prehospital decision-making; virtual reality,Adult; Female; Humans; Male; Mass Casualty Incidents; Triage; Virtual Reality; Young Adult; accuracy; adult; Article; disaster management; emergency physician; eye-tracking technology; female; first responder (person); human; immersion; male; mass disaster; medical education; medical information; medical student; multimethod study; nurse; paramedical personnel; patient triage; performance indicator; physician; professional knowledge; quasi experimental study; self evaluation; simulation training; velocity; virtual reality; visual attention; young adult; patient triage; procedures,Article,Final,,Scopus,2-s2.0-85216278690,Gaming / VR
Evertsen F.W.; Landman A.; Groen E.L.; Houben M.M.J.; van Paassen M.M.; Stroosma O.; Mulder M.,"Evertsen, Fleur W. (59735926000); Landman, Annemarie (57203049697); Groen, Eric L. (6603708917); Houben, Mark M. J. (7004275624); van Paassen, M.M. (57200555382); Stroosma, Olaf (10142119300); Mulder, Max (57196395283)",59735926000; 57203049697; 6603708917; 7004275624; 57200555382; 10142119300; 57196395283,Quantifying the Impact of Spatial Disorientation on Pilot Mental Workload and Attentional Focus,2025,Human Factors,,,,,,,1,10.1177/00187208251323116,https://www.scopus.com/inward/record.uri?eid=2-s2.0-105002442469&doi=10.1177%2f00187208251323116&partnerID=40&md5=1590bd3a23a2d3481d2fb9dc414ccc40,"Objective: We aimed to find objective measures of the impact of spatially disorienting (SD) stimuli on pilot cognition in an ecologically valid environment. Background: SD frequently occurs in military rotary-wing operations and often contributes to mishaps. Effects of SD stimuli on pilots are usually quantified using control errors, but effects on cognition have not yet been successfully quantified. Method: Military helicopter pilots (n = 14) performed scenarios with six SD stimuli (SD condition) and six corresponding control stimuli (NoSD condition) in a motion-base simulator with integrated virtual reality headset. SD stimuli were: false horizon, featureless terrain, leans, brownout, a somatogyral yaw illusion, and loss of horizon due to night vision goggles (NVGs). Mental workload was measured using auditory arithmetic task performance and attentional focus was measured using eye-tracking. Results: Average arithmetic task performance was significantly impaired, and proportional gaze dwell time on the attitude indicator was significantly increased in the SD compared to the NoSD condition. Of the six SD stimuli, the featureless terrain, the leans, and the brownout induced significant effects on performance, whereas the featureless terrain, brownout, and false horizon significantly affected gaze behavior. The NVGs and somatogyral yaw stimuli did not induce significant effects. Pilots’ self-reports indicated awareness of all SD stimuli, except for the featureless terrain. Conclusion: The results indicate that SD impacts pilot mental workload and attentional focus. Application: Modern military aircraft present a large volume of mission-related information to pilots. This study shows that SD stimuli may negatively impact the processing of such information. © 2025 Human Factors and Ergonomics Society.",aerospace; aircrew; aviation; distraction; situational awareness; vertigo,Audition; Aviators; Eye tracking; Eyeglasses; Fighter aircraft; Flight simulators; Goggles; Night vision devices; Aerospace; Aircrew; Arithmetic tasks; Condition; Distraction; Mental workload; Night vision goggles; Situational awareness; Task performance; Vertigo; Military helicopters,Article,Article in press,,Scopus,2-s2.0-105002442469,Gaming / VR
Coelho F.; Rando B.; Aparício D.; Pontífice-Sousa P.; Gonçalves D.; Abreu A.M.,"Coelho, Franz (57462452200); Rando, Belén (6506759908); Aparício, David (59998951200); Pontífice-Sousa, Patrícia (57217585864); Gonçalves, Daniel (35588555000); Abreu, Ana Maria (37036768300)",57462452200; 6506759908; 59998951200; 57217585864; 35588555000; 37036768300,"The impact of educational gamification on cognition, emotions, and motivation: a randomized controlled trial",2025,Journal of Computers in Education,,,,,,,1,10.1007/s40692-025-00366-x,https://www.scopus.com/inward/record.uri?eid=2-s2.0-105010773386&doi=10.1007%2fs40692-025-00366-x&partnerID=40&md5=bbded20ca384f9440dc4a74f8074b83b,"This study examines the impact of gamification on education using a novel gamified digital learning platform and a randomized controlled trial (RCT) protocol. Following established research guidelines (CONSORT, Cochrane Collaboration, EVAT©), we assessed the individual and combined effects of points, badges, and challenges against a control group without gamification. The RCT evaluated participant characteristics (sociodemographics, game habits, player traits) and outcomes in cognition (learning, engagement, webcam-based eye-tracking for visual attention, cognitive load), emotions (affective states, webcam-based facial emotion recognition), and motivation (intrinsic motivation). Results showed significantly higher learning for participants using all game elements versus the control group, while badges alone increased cognitive load compared to the other gamification groups. These findings suggest that gamification is more effective when thoughtfully integrating game elements rather than applying elements in isolation, aligning goal-setting with feedback, and combining intrinsic and extrinsic motivational cues. The absence of significant results in other variables may reflect the novelty effect, emphasizing the importance of aligning gamification with pedagogical goals, considering individual and contextual factors, and designing systems that address usability and long-term impact. Educational implications and design recommendations are provided. © The Author(s) 2025.",Digital learning; Gamification; Human–computer interaction; Interdisciplinary projects; Media in education,,Article,Article in press,,Scopus,2-s2.0-105010773386,Gaming / VR
Schmidt-Peter T.; Wechsler T.F.; Kroczek L.O.H.; Mühlberger A.,"Schmidt-Peter, Teresa (59971137700); Wechsler, Theresa F. (57210105561); Kroczek, Leon O. H. (57199653714); Mühlberger, Andreas (6507375232)",59971137700; 57210105561; 57199653714; 6507375232,The effects of different variants of eye-tracking-based feedback of attentional processes during virtual social interactions,2025,Frontiers in Virtual Reality,6,,1556898,,,,0,10.3389/frvir.2025.1556898,https://www.scopus.com/inward/record.uri?eid=2-s2.0-105009535805&doi=10.3389%2ffrvir.2025.1556898&partnerID=40&md5=2dde12097009ab68fbb95f17f0855838,"Introduction: Enhanced self-focused attention plays an important role in the maintenance of Social Anxiety Disorder (SAD). Therefore, changing attentional processes is a major target in cognitive behavioral therapy (CBT) and recent approaches apply Virtual Reality (VR) behavioral exercises to change these processes. A promising approach to enhance such VR exposure-based exercises is implementing eye-tracking-based feedback. Methods: This experimental study investigates which characteristics of gaze-related feedback lead to a positive valence and an increase in focused attention on social stimuli. Additionally, we examine differential effects in low (LSA) vs highly socially anxious (HSA) individuals. Overall, 50 participants, who were grouped into LSA and HSA according to the median split of the SPIN, were instructed to hold eye contact with virtual agents until they received feedback either in the form of a smile, a positive tone, or a praise. Furthermore, the required duration of maintaining eye contact with virtual agents to receive feedback was manipulated. The feedback variants were evaluated during and after the experiment via ratings, and participants’ gaze was measured via eye tracking. Results: Results revealed that the smile feedback was perceived as more pleasant and elicited more eye contact in a subsequent test phase than the praise, which was associated with higher valence than the tone. In addition, LSA participants rated the social feedback variants (smile, praise) as significantly more pleasant than HSA participants, who showed reduced sensitivity to positive social feedback. Discussion: These findings suggest that socially rewarding feedback is more effective in LSA individuals and may not generalize to those with high social anxiety. Future research should therefore explore further feedback variants within individuals with SAD to further refine and optimize VR-based attentional interventions for enhanced therapeutic outcomes. Copyright © 2025 Schmidt-Peter, Wechsler, Kroczek and Mühlberger.",exposure therapy; eye tracking; feedback; self-focused attention; social anxiety; social interaction; virtual reality,,Article,Final,,Scopus,2-s2.0-105009535805,Gaming / VR
Wei J.; Siegel E.; Sundaramoorthy P.; Gomes A.; Zhang S.; Vankipuram M.; Smathers K.; Ghosh S.; Horii H.; Bailenson J.; Ballagas R.L.,"Wei, Jishang (57193524446); Siegel, Erika (59739556200); Sundaramoorthy, Prahalathan (57203140587); Gomes, Antonio (59739459600); Zhang, Shibo (55851247800); Vankipuram, Mithra (25929735300); Smathers, Kevin (8557524800); Ghosh, Sarthak (59739360300); Horii, Hiroshi (35792227500); Bailenson, Jeremy (6602840468); Ballagas, Rafael LTico' (6507278590)",57193524446; 59739556200; 57203140587; 59739459600; 55851247800; 25929735300; 8557524800; 59739360300; 35792227500; 6602840468; 6507278590,Cognitive Load Inference Using Physiological Markers in Virtual Reality,2025,"Proceedings - 2025 IEEE Conference on Virtual Reality and 3D User Interfaces, VR 2025",,,,759,769,10.0,0,10.1109/VR59515.2025.00098,https://www.scopus.com/inward/record.uri?eid=2-s2.0-105002717800&doi=10.1109%2fVR59515.2025.00098&partnerID=40&md5=3e652daf66541c347fd5d6e14728d64b,"Virtual reality (VR) has become an increasingly popular way for learning and training. The assessment of the amount of mental effort, or cognitive load required to perform a task, is essential to create adaptive VR experiences. In this work, we conducted a large-scale study (N=738) to collect behavioral and physiological measures under different cognitive load conditions in a VR environment, and developed a novel machine learning solution to predict cognitive load in real time. Our model predicts cognitive load as a continuous value in the range from 0 to 1, where 0 and 1 correspond to the lowest and highest reported cognitive loads across all participants. On top of the point estimation of cognitive load, our model quantifies prediction uncertainty using a prediction interval. We propose a novel dual-branch attention model to accurately predict the cognitive load. We achieve a MAE (Mean Absolute Error) of 0.11. The result indicates that, with a combination of behavioral and physiological indicators, we can reliably predict cognitive load in real-time, without calibration. To support further research, we are releasing a test dataset comprising data from 100 participants for use by researchers and developers interested in machine learning, virtual reality, learning & memory, cognition, or psychophysiology. This dataset includes recordings from multiple sensors (including pupillometry, eye-tracking, and pulse plethysmography), self-reported cognitive effort, behavioral task performance, and demographic information on the sample.  © 2025 IEEE.",Cognitive Load; Machine Learning; Physiological Signals; Virtual Reality,Adversarial machine learning; Behavioral measures; Cognitive loads; Large-scale studies; Learning and training; Machine-learning; Mental effort; Physiological markers; Physiological signals; Real- time; Virtual reality experiences; Virtual environments,Conference paper,Final,,Scopus,2-s2.0-105002717800,Gaming / VR
Khatt S.; Bodenheimer B.,"Khatt, Sreynit (59898731900); Bodenheimer, Bobby (56133948600)",59898731900; 56133948600,Preliminary Investigation of Real and Simulated Central Vision Loss on Visual Attention in Virtual Reality,2025,"Proceedings - 2025 IEEE Conference on Virtual Reality and 3D User Interfaces Abstracts and Workshops, VRW 2025",,,,1188,1189,1.0,0,10.1109/VRW66409.2025.00240,https://www.scopus.com/inward/record.uri?eid=2-s2.0-105005141432&doi=10.1109%2fVRW66409.2025.00240&partnerID=40&md5=94e1af30a813350fdcd436e039ac3632,"Little is known about how individuals with low vision attend to 3D environments in a virtual environment (VE). This study investigates the visual attention of normal vision under a simulated vision loss and real central vision loss in a VE. We present a qualitative comparison of the saliency maps of visual attention generated from eye-tracking data of participants with normal vision under no loss and simulated low vision, saliency maps generated by a participant with low vision with and without corrective spectacles, and a saliency map generated by a computational prediction model.  © 2025 IEEE.",central vision loss; saliency; simulated-vision loss; Visual attention,Three dimensional computer graphics; Virtual environments; Virtual reality; 3-D environments; Central vision; Central vision loss; Low vision; Saliency; Saliency map; Simulated-vision loss; Vision loss; Visual Attention; Virtualization,Conference paper,Final,,Scopus,2-s2.0-105005141432,Gaming / VR
Brideau-Duquette M.; Côté S.S.-P.; Charbonneau P.; Renaud P.,"Brideau-Duquette, Mathieu (57238716000); Côté, Sara Saint-Pierre (57190405531); Charbonneau, Philippe (57197362797); Renaud, Patrice (7103298339)",57238716000; 57190405531; 57197362797; 7103298339,When Sexy Avatars Get Weird: How Brain Asymmetry and Oculomotor Dynamics Navigate the Uncanny,2025,Lecture Notes in Computer Science,15541 LNAI,,,121,139,18.0,0,10.1007/978-981-96-3294-7_10,https://www.scopus.com/inward/record.uri?eid=2-s2.0-105002448490&doi=10.1007%2f978-981-96-3294-7_10&partnerID=40&md5=61cb61b0534ab807a73107ddc116879d,"This study explores the psychological effects of human-machine interaction, particularly focusing on the phenomenon of the uncanny valley within the context of virtual reality (VR). This study specifically investigates the relationship between oculomotor dynamics, EEG asymmetry, and perceived uncanniness in an immersive setting with a realistic human avatar that can generate uncanny feelings. Participants engaged in a VR scenario designed to optimize their sexual interest. Eye-tracking and EEG data were collected during these interactions to assess the predictors of uncanniness. The final regression model identified significant predictors, including EEG asymmetry in frontal and parietal regions and nonlinear indices of gaze behavior. The findings suggest that the more a participants’ attention was attracted towards specific cues, the higher the uncanniness. The interplay between EEG asymmetries and gaze behavior furthermore indicates that these physiological and behavioral responses are closely linked to the perception of uncanniness, offering insights into how humans interact with increasingly lifelike technologies. These insights contribute to a better understanding of HMI’s potential and may inform the development of more engaging and psychologically attuned virtual content. © The Author(s), under exclusive license to Springer Nature Singapore Pte Ltd. 2025.",Embodied Computing; Immersive; Regression Analysis; Uncanny; Virtual Reality,Brain; Regression analysis; Virtual reality; Embodied computing; Eye-tracking; Gaze behaviours; Human machine interaction; Immersive; Nonlinear index; Psychological effects; Regression modelling; Uncanny; Uncanny valley; Virtual environments,Conference paper,Final,,Scopus,2-s2.0-105002448490,Gaming / VR
Clark J.; Boz L.,"Clark, Jack (59919481400); Boz, Lila (59747094400)",59919481400; 59747094400,Exploring the Effects of Visual and Auditory Distractors in Virtual Reality on Perceived Cognitive Load and Cognitive Alertness,2025,WSEAS Transactions on Information Science and Applications,22,,,358,372,14.0,1,10.37394/23209.2025.22.30,https://www.scopus.com/inward/record.uri?eid=2-s2.0-105006499031&doi=10.37394%2f23209.2025.22.30&partnerID=40&md5=6a29a3bdfbd3bf905c7c2a7af4d5bb85,"Virtual reality (VR) has been increasingly used within training contexts. It is important to understand how users are affected by various distractions for designing more effective learning experiences in VR. In this paper, we explored how visual (i.e., virtual balls bouncing across the user’s field of view) and auditory (i.e., pure-tone click train sounds) distractors affect perceived cognitive load and the relation to cognitive alertness. We conducted a within-subjects user study (N = 48) that revolved around a visuo-spatial cup stacking task. Participants completed seven trial conditions in total (no distractor condition, three visual distractor conditions with varying proximities, and three audio distractor conditions with varying pitches). We measured perceived cognitive load through questionnaires and cognitive alertness through changes in pupil dilation. In summary, participants were negatively affected by the auditory distractors and not the visual distractors. © 2025 World Scientific and Engineering Academy and Society. All rights reserved.",Cognitive alertness; cognitive load; distraction; eye tracking; virtual reality; visual and auditory distractors,,Article,Final,,Scopus,2-s2.0-105006499031,Gaming / VR
Contreras C.A.; Rastegarpanah A.; Chiou M.; Stolkin R.,"Contreras, Cesar Alan (58876424800); Rastegarpanah, Alireza (56182153700); Chiou, Manolis (57188977049); Stolkin, Rustam (16025500800)",58876424800; 56182153700; 57188977049; 16025500800,A mini-review on mobile manipulators with Variable Autonomy,2025,Frontiers in Robotics and AI,12,,1540476,,,,0,10.3389/frobt.2025.1540476,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85219122801&doi=10.3389%2ffrobt.2025.1540476&partnerID=40&md5=f9cc7ae9d5e3b6d3dd94686cf79abfb0,"This paper presents a mini-review of the current state of research in mobile manipulators with variable levels of autonomy, emphasizing their associated challenges and application environments. The need for mobile manipulators in different environments, especially hazardous ones such as decommissioning and search and rescue, is evident due to the unique challenges and risks each presents. Many systems deployed in these environments are not fully autonomous, requiring human-robot teaming to ensure safe and reliable operations under uncertainties. Through this analysis, we identify gaps and challenges in the literature on Variable Autonomy, including cognitive workload and communication delays, and propose future directions, including whole-body Variable Autonomy for mobile manipulators, virtual reality frameworks, and large language models to reduce operators’ complexity and cognitive load in some challenging and uncertain scenarios. Copyright © 2025 Contreras, Rastegarpanah, Chiou and Stolkin.",human-robot interaction; human-robot teaming; mobile manipulators; shared control; uncertain environments; variable autonomy,,Short survey,Final,All Open Access; Gold Open Access; Green Open Access,Scopus,2-s2.0-85219122801,Gaming / VR
Wang T.; Abdul Shukor S.F.; Kozlowski M.; Wan Mohamed W.S.; Zhang T.,"Wang, Tongxu (59670891700); Abdul Shukor, Shureen Faris (24448414200); Kozlowski, Marek (57040638200); Wan Mohamed, Wan Srihani (59324069700); Zhang, Ting (59670758900)",59670891700; 24448414200; 57040638200; 59324069700; 59670758900,A systematic review on evaluating school safety signs and ways to improve its effectiveness,2025,Frontiers in Communication,10,,1550402,,,,0,10.3389/fcomm.2025.1550402,https://www.scopus.com/inward/record.uri?eid=2-s2.0-86000094058&doi=10.3389%2ffcomm.2025.1550402&partnerID=40&md5=0b0f6af26311dbd0d051ccad70b28269,"Introduction: Safety signs are essential visual communication tools that convey critical information regarding hazards, regulations, and emergency procedures across various environments. Their significance has garnered considerable attention due to their role in enhancing public safety. However, existing studies have predominantly focused on design factors, with limited research addressing the interaction of non-design factors among specific age groups and safety communication in public spaces, particularly concerning young children’s cognitive abilities and perceptions. Methods: This systematic review aims to comprehensively analyze the characteristics and effectiveness of safety signs within the school environment. A systematic review of the literature on safety signs—specifically their design within school premises—was conducted using the Systematic Review and Meta-Analyses (PRISMA) approach to identify research gaps. Query strings were utilized to extract papers, identifying 48 peer-reviewed articles from 2000 to 2024 for review. Results: The review found that visual elements, such as dynamic safety signs and the use of highly saturated colors, significantly enhance the effectiveness of warnings. Several factors were identified that influence the efficacy of safety signs, including age-related cognitive abilities and the contextual understanding of young children. Additionally, the study highlights the effectiveness of technological tools, such as eye tracking and virtual reality (VR), for assessing human perception and improving testing methodologies. The role of enjoyment and experiential learning in enhancing safety communication are beneficial. Discussion: This systematic review underscores the importance of considering both design and non-design factors in safety sign development, particularly in school environments. The findings suggest that dynamic, visually appealing signs, along with the use of advanced technological tools, can improve the effectiveness of safety messages. The integration of experiential learning also contributes to a better understanding of safety information. This review is a valuable resource for safety science researchers, visual designers, and policymakers by providing insights into the current state of research, identifying research gaps, and offering perspectives on the future of school safety signage pertinent to school-age children. Copyright © 2025 Wang, Abdul Shukor, Kozlowski, Wan Mohamed and Zhang.",behavioral feedback; risk communication; safety sign; school environment; visual elements; warning effectiveness,,Review,Final,All Open Access; Gold Open Access,Scopus,2-s2.0-86000094058,Gaming / VR
Premnath S.; Ekanayake K.; Pieris T.; Hettiarachchi E.; Hewagamage K.P.,"Premnath, S. (59794342500); Ekanayake, K. (59787265100); Pieris, T. (59788435500); Hettiarachchi, E. (55332607200); Hewagamage, K.P. (6603104486)",59794342500; 59787265100; 59788435500; 55332607200; 6603104486,A New Era of Productive Learning Focused on the Retention of Attention Span: VerbaSure,2025,"2025 5th International Conference on Advanced Research in Computing: Converging Horizons: Uniting Disciplines in Computing Research through AI Innovation, ICARC 2025 - Proceedings",,,,,,,0,10.1109/ICARC64760.2025.10963237,https://www.scopus.com/inward/record.uri?eid=2-s2.0-105004548085&doi=10.1109%2fICARC64760.2025.10963237&partnerID=40&md5=805624ffdae3a6970f75efda2f4f7dc1,"The COVID-19 pandemic has driven a major shift to online learning, creating significant challenges for educational institutions in Sri Lanka, particularly in addressing diverse learning needs and attention spans. This study highlights the need to keep students engaged in online settings through inclusive and interactive tools tailored to different learning styles. Through collaboration with students, HCI experts, psychology specialists, and lecturers, current online learning methodologies were analyzed, leading to the development of VerbaSure, a gamified web solution. VerbaSure follows Universal Design for Learning (UDL) principles, offering multimodal content and game-like features such as story-based challenges, varied question formats, avatars, hints, motivational messages, points, and leaderboards. To evaluate the effectiveness of VerbaSure, a controlled study was conducted with first-year computer science students at a leading university in Sri Lanka. Participants used VerbaSure alongside traditional methods, and their attention and focus were monitored through eye tracking, mouse-clicking, and performance analysis. Positive results in attention span and performance improvement, supported by qualitative feedback, suggest that the model used in VerbaSure successfully enhances concentration and inclusivity in online learning environments. Thus, this research emphasizes the importance of integrating gamification and multimodal content to enhance attention spans and engagement levels for undergraduates in online learning environments.  © 2025 IEEE.",Attention Retention; Evaluation techniques; Gamification; Multimodal content; Online Learning,Engineering education; Attention retention; Educational institutions; Evaluation technique; Gamification; Multi-modal; Multimodal content; Online learning; Online learning environment; Productive learning; Sri Lanka; Students,Conference paper,Final,,Scopus,2-s2.0-105004548085,Gaming / VR
,,,"10th International Conference on Immersive Learning, iLRN 2024",2025,Communications in Computer and Information Science,2271 CCIS,,,,,43.0,0,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-86000460434&partnerID=40&md5=e83044ab403554a7d8f08d24ea906815,"The proceedings contain 43 papers. The special focus in this conference is on Immersive Learning. The topics include: Scent Box: Prototyping and Instructions for Olfactory Enhancement of VR-Experiences; Measuring Cognitive Load with Eye-Tracking During Mental Rotation with 2D and 3D Visualization in AR; augmented Didactic: Interacting with 3D Models to Enhance the Memory Systems; Complexity of Agency in VR Learning Environments: Exploring Associations with Interactivity, Learning Outcomes, and Affect; ready Student One: A Framework for Avatar Design in Higher Education; A Literature Review and Taxonomy of In-VR Questionnaire User Interfaces; describing and Interpreting an Immersive Learning Case with the Immersion Cube and the Immersive Learning Brain; Exploiting the TARC Framework: The Relations Between Educators’ Attitudes Towards AR, Innovativeness, Digital Skills, and AR Skills in Education; perceptions of Higher Education Students on Immersive Virtual Reality for Communication Skills Training. The Bodyswaps Case; adaptive Learning and Instruction with Augmented Reality: A Scoping Review; an Evaluation of Headset vs Desktop Use for Accessing Virtual Worlds in a Higher Education Context; design of Virtual Reality Environments to Support Learning in History Education; perfecting the Interdisciplinary Storm: Immersive Narrative Development Workflows in Context of Meteorology Labs; is Usability Always Productive in Learning Environments?; the Application of Procedurally Generated Libraries in Immersive Virtual Reality; designing MetaHuman-Based Historical Characters in Virtual Exhibitions and Scenes: A Case Study on St Andrews; Preliminary Report: Innovations in Participatory Immersive XR Research for Transition-Aged Autistic Adults; exploring the Inclusive Design and Use of Social Multi-platform Virtual Reality for a Post-secondary Gender Diversity Workshop.",,,Conference review,Final,,Scopus,2-s2.0-86000460434,Gaming / VR
Du W.,"Du, Wenting (59995964000)",59995964000,An adaptive learning path recommendation framework based on deep learning for AI-driven vocational English education,2025,Proceedings of SPIE - The International Society for Optical Engineering,13682,,136822S,,,,0,10.1117/12.3074083,https://www.scopus.com/inward/record.uri?eid=2-s2.0-105010615363&doi=10.1117%2f12.3074083&partnerID=40&md5=3a17cca35bebd881c1294c6692dd9b97,"The rapid growth of artificial intelligence (AI) has significantly reshaped the landscape of education, particularly in vocational English training, which demands personalized and adaptive learning approaches. As a core component of Smart Education, AI-driven frameworks offer opportunities to transform traditional teaching methods into intelligent, data-driven, and learner-centered systems. This study proposes an innovative adaptive learning path recommendation framework based on deep learning, aiming to address challenges such as learner diversity, engagement, and efficiency. This study proposes an innovative adaptive learning path recommendation framework based on deep learning, aiming to address challenges such as learner diversity, engagement, and efficiency. By integrating attention mechanisms and collaborative filtering, the framework dynamically captures contextual dependencies and user-specific behaviors to generate personalized learning paths. The methodology involves preprocessing learner interaction data, embedding it into a high-dimensional space, and utilizing a deep neural network enhanced with attention mechanisms to predict resource relevance. Furthermore, the integration of optical technologies, such as eye-tracking and virtual reality (VR), provides a novel approach to capturing real-time learner engagement and interaction patterns. The framework was validated through experiments on a real-world vocational English dataset, involving over 5,000 learners. Results demonstrate that the proposed model outperforms traditional collaborative filtering and non-attention-based deep learning models, achieving superior precision, recall, and F1-Score. Additionally, learners following the recommended paths exhibited a 25% faster course completion rate and a 30% higher retention rate compared to static, non-adaptive approaches. These findings highlight the framework's potential to enhance learning outcomes and engagement in vocational English education. This work bridges the gap between AI-driven recommender systems, optical technologies, and practical educational applications, offering a scalable, dynamic, and effective solution for personalized learning in diverse educational contexts.  © COPYRIGHT SPIE. Downloading of the abstract is permitted for personal use only.",adaptive learning systems; attention mechanism; collaborative filtering; computer science; deep learning; optical technologies; recommender systems,Apprentices; Behavioral research; Collaborative learning; Curricula; Deep learning; E-learning; Education computing; Engineering education; Learning algorithms; Learning systems; Optical instruments; Teaching; Virtual reality; Adaptive learning; Adaptive learning systems; Attention mechanisms; Deep learning; English educations; Learning approach; Learning paths; Optical technology; Personalized learning; Rapid growth; Collaborative filtering; Recommender systems,Conference paper,Final,,Scopus,2-s2.0-105010615363,Gaming / VR
Min B.; Kim D.; Ko Y.J.,"Min, Byungjae (58868538500); Kim, Daehwan (57198638241); Ko, Yong Jae (35558881200)",58868538500; 57198638241; 35558881200,"Effect of in-game situations and advertisement animation in eSports on visual attention, memory, brand attitude and behavioral intentions",2025,Internet Research,,,,,,,1,10.1108/INTR-03-2024-0491,https://www.scopus.com/inward/record.uri?eid=2-s2.0-105000206986&doi=10.1108%2fINTR-03-2024-0491&partnerID=40&md5=cd3e9e93126a2c82e77dc274eef1c28d,"Purpose: The purpose of this study was to examine the effects of in-game situations (battle vs non-battle) and advertisement animation (static vs animated) on visual attention, memory, brand attitude and behavioral intention in eSports. Design/methodology/approach: A 2 (advertisement animation: static vs animated) × 2 (in-game situations: non-battle vs battle) between-subject factorial design was developed. Participants wearing eye trackers watched a 2-min-and-30-s stimulus featuring manipulated variables. Subsequently, they completed a questionnaire designed to measure variables. Findings: In-game situations significantly influenced visual attention, with non-battle contexts eliciting higher fixation duration and count. Moreover, fixation count positively affected both explicit and implicit memory. Additionally, implicit memory enhanced brand attitude, which in turn positively affected behavioral intention. Practical implications: These findings guide eSports event organizers and brand managers in optimizing advertisement placement. Advertisements should be positioned in non-battle situations, where visual attention is higher. Repeated brand exposure is more effective than prolonged single exposures for enhancing memory retention and brand attitude. Lastly, unobtrusive advertisement exposure is essential because implicit memory plays a key role in shaping consumer attitudes and behavioral intentions. Originality/value: This study advances eSports advertising research by identifying psychological mechanisms influencing consumer behavior. Moreover, the study extends the Limited Capacity Model of Motivated Mediated Message Processing (LC3MP) by demonstrating how cognitive resource allocation varies across in-game situations and advertisements. Furthermore, this research contributes to self-perception theory and the mere exposure effect by clarifying the distinct roles of explicit and implicit memory in shaping brand attitudes. © 2025, Emerald Publishing Limited.",Behavioral intention; Brand attitude; Consumer memory; eSports; LC3MP; Virtual advertising; Visual attention,,Article,Article in press,,Scopus,2-s2.0-105000206986,Gaming / VR
Andiloro A.,"Andiloro, Andrea (57930708800)",57930708800,Death as design: video games and the framing of finitude,2025,Phenomenology and the Cognitive Sciences,,,,,,,0,10.1007/s11097-025-10085-4,https://www.scopus.com/inward/record.uri?eid=2-s2.0-105007980555&doi=10.1007%2fs11097-025-10085-4&partnerID=40&md5=3f7f6d5028d2dbff2cf34cb4123c748b,"Video games uniquely mediate existential themes of mortality through their formal mechanics and narrative structures. This paper positions video games as “death media,” arguing that their affordances reshape how players confront finitude. Drawing on Martin Heidegger’s notion of “being-towards-death” the analysis examines how video games transform abstract mortality into tangible, visceral experiences. Central to this exploration is Before Your Eyes (GoodbyeWorld Games, 2021), a narrative game that employs eye-tracking technology to bind its core mechanic -blinking—to the progression of its protagonist’s life story. Every involuntary blink advances time, cutting short moments of joy or regret, thereby mirroring the inevitability of death and the fragility of temporal existence. Through this design, the game operationalizes Heideggerian authenticity, disrupting players’ tendency to “fall” into everyday distractions and instead confronting them with the irreducible fact of their finitude. Player testimonials and critical reviews underscore the emotional resonance of this mechanic, revealing how physical strain and narrative brevity evoke introspection about life’s transient nature. By framing ludic death as an embodied, interactive process, the paper challenges conventional portrayals of mortality in digital media and highlights games’ potential as existential technologies. Ultimately, it contends that video games like Before Your Eyes reframe mortality not as failure but as a phenomenological truth, inviting players to grapple with impermanence through play itself. © The Author(s) 2025.",Before Your Eyes; Embodied interaction; Heidegger; McLuhan; Mortality; Video game,,Article,Article in press,,Scopus,2-s2.0-105007980555,Gaming / VR
Li T.; Geng X.; Yamada M.,"Li, Tang (59325782600); Geng, Xuewang (57208819409); Yamada, Masanori (35191391300)",59325782600; 57208819409; 35191391300,Embodied Learning in Virtual Reality: Enhancing Japanese Psychomimetic Word Acquisition Through Emotional Experiences,2025,IEEE Access,,,,,,,0,10.1109/ACCESS.2025.3590748,https://www.scopus.com/inward/record.uri?eid=2-s2.0-105011756354&doi=10.1109%2fACCESS.2025.3590748&partnerID=40&md5=441990e5e11c07ccd0a7f1af744c9708,"Virtual reality (VR) offers the potential for embodied learning through multisensory engagement. However, empirical research on the direct impact of emotional experiences on learning outcomes remains limited. This study investigated the influence of direct emotional experiences in VR on learners’ acquisition of Japanese psychomimetic words, examining how different emotional styles, self-perception, and empathy affect learning outcomes. Based on embodied learning theory, this study explored how emotional experiences shape vocabulary acquisition. Thirty participants used a VR system designed to elicit emotions, such as joy, anger, and anxiety through scenes that reflected 13 psychomimetic words. The VR system provided immersive scenes that allowed learners to connect emotions with word meanings through emotional experience. This investigation assessed whether immersive VR enhanced the understanding of psychomimetic words and explored how self-perception and empathy influenced the relationships between emotional experiences, engagement, attention, and learning outcomes using eye-tracking and controller data. Self-perception and empathy enhance learning at different effectiveness levels. Self-perception significantly improved immediate and long-term retention, whereas empathy contributed to long-term retention. In self-perception, emotional experiences affect learning outcomes, with attention significantly influencing emotional experiences and learning outcomes. Engagement primarily affects emotional experiences. These results highlight the roles of emotional experiences in VR, suggesting their potential to enhance language education through tailored emotional engagement. The findings emphasize the importance of strategic practices and learner-centered designs for optimizing VR’s effectiveness in language learning. © 2013 IEEE.",Emotion Analytics; Language Learning; Learning Analytics; Virtual Reality,Behavioral research; E-learning; Learning systems; Mergers and acquisitions; Psychoacoustic; Virtual environments; Emotion analytic; Emotional engagements; Emotional experiences; Emotional learning; Language learning; Learning analytic; Learning outcome; Long-term retention; Virtual reality system; Word acquisition; Virtual reality,Article,Article in press,,Scopus,2-s2.0-105011756354,Gaming / VR
Malaquias P.I.D.S.; Santanna A.; Alvarenga V.F.; Miranda C.; Coelho M.N.; Delabrida S.,"Malaquias, Pedro Igor de Souza (59542292100); Santanna, Adriene (59542618500); Alvarenga, Victor Ferreira (59542403200); Miranda, Christianne (57488674800); Coelho, Mateus Nazario (55619874200); Delabrida, Saul (55320260700)",59542292100; 59542618500; 59542403200; 57488674800; 55619874200; 55320260700,Proposal for a tool for the applicability of VR and BCI in an interdisciplinary study to infer attention in individuals with ADHD,2024,ACM International Conference Proceeding Series,,,37,,,,0,10.1145/3702038.3702075,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85216929237&doi=10.1145%2f3702038.3702075&partnerID=40&md5=e18fbeb6d63b9b3cf6f2804199d75c61,"Attention Deficit Hyperactivity Disorder (ADHD) is a persistent pattern of inattention and/or hyperactivity-impulsivity and has behavioral and pharmacological treatments as alternatives. For action in behavioral treatment, there are several alternatives based on psychological techniques. The present study presents the proposal of evaluating the attention rate of an individual with ADHD by using virtual reality equipment, brainwave analysis, and eye movement tracking. The proposal emerges as a way of integrating psychometric methods aligned with current technologies that can offer data to measure parameters that determine attention. The presented method uses virtual reality as a stimulus scenario, controlled by the researchers and professionals involved, and subsequently performs the reading of brainwaves to capture data. The research’s innovation is the integration of technologies that have recently become available for mass use with psychometric methods, especially for people with ADHD, and it explores how health professionals can use data obtained from technological artifacts. The present work is a collaboration of a multidisciplinary team formed by computing and pedagogy researchers and a psychology professional. © 2024 Copyright held by the owner/author(s).",ADHD; BCI; Eye Tracking; Virtual Reality,Eye movements; Virtual reality; Attention deficit hyperactivity disorder; BCI; Brain wave; Current technology; Eye-movement tracking; Eye-tracking; Inter-disciplinary studies; Pharmacological treatment; Psychological techniques; Psychometric methods; Virtual environments,Conference paper,Final,,Scopus,2-s2.0-85216929237,Gaming / VR
Wiebe A.; Selaskowski B.; Paskin M.; Asché L.; Pakos J.; Aslan B.; Lux S.; Philipsen A.; Braun N.,"Wiebe, Annika (57659776400); Selaskowski, Benjamin (57660098000); Paskin, Martha (57831954800); Asché, Laura (57203968667); Pakos, Julian (57976920100); Aslan, Behrem (57216769975); Lux, Silke (7005576971); Philipsen, Alexandra (6603416864); Braun, Niclas (55972275100)",57659776400; 57660098000; 57831954800; 57203968667; 57976920100; 57216769975; 7005576971; 6603416864; 55972275100,"Virtual reality-assisted prediction of adult ADHD based on eye tracking, EEG, actigraphy and behavioral indices: a machine learning analysis of independent training and test samples",2024,Translational Psychiatry,14,1,508,,,,4,10.1038/s41398-024-03217-y,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85213725609&doi=10.1038%2fs41398-024-03217-y&partnerID=40&md5=cc01d299c47b311fca7ed73d406dfac2,"Given the heterogeneous nature of attention-deficit/hyperactivity disorder (ADHD) and the absence of established biomarkers, accurate diagnosis and effective treatment remain a challenge in clinical practice. This study investigates the predictive utility of multimodal data, including eye tracking, EEG, actigraphy, and behavioral indices, in differentiating adults with ADHD from healthy individuals. Using a support vector machine model, we analyzed independent training (n = 50) and test (n = 36) samples from two clinically controlled studies. In both studies, participants performed an attention task (continuous performance task) in a virtual reality seminar room while encountering virtual distractions. Task performance, head movements, gaze behavior, EEG, and current self-reported inattention, hyperactivity, and impulsivity were simultaneously recorded and used for model training. Our final model based on the optimal number of features (maximal relevance minimal redundancy criterion) achieved a promising classification accuracy of 81% in the independent test set. Notably, the extracted EEG-based features had no significant contribution to this prediction and therefore were not included in the final model. Our results suggest the potential of applying ecologically valid virtual reality environments and integrating different data modalities for enhancing robustness of ADHD diagnosis. © The Author(s) 2024.",,Actigraphy; Adult; Attention; Attention Deficit Disorder with Hyperactivity; Electroencephalography; Eye-Tracking Technology; Female; Humans; Machine Learning; Male; Support Vector Machine; Virtual Reality; Young Adult; accuracy; actimetry; adult; Article; attention deficit hyperactivity disorder; clinical article; continuous performance test; controlled study; Diagnostic and Statistical Manual of Mental Disorders; electroencephalogram; eye movement; eye tracking; female; gaze; head movement; head position; human; hyperactivity; impulsiveness; linear support vector machine; machine learning; male; nested cross validation; prediction; reaction time; saccadic eye movement; self evaluation; support vector machine; task performance; velocity; virtual reality; attention; attention deficit hyperactivity disorder; diagnosis; electroencephalography; eye-tracking technology; machine learning; pathophysiology; physiology; young adult,Article,Final,All Open Access; Gold Open Access,Scopus,2-s2.0-85213725609,Gaming / VR
Su K.; Zhou Y.; Zhan H.; Yang C.,"Su, Kainan (59207131500); Zhou, Yajing (57193738558); Zhan, Hong (57217079156); Yang, Chenguang (55671767000)",59207131500; 57193738558; 57217079156; 55671767000,A Variable Parameter LoD Model Point Cloud Compression Method Based on Attention Mechanism,2025,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics) ,15209 LNAI,,,117,131,14.0,0,10.1007/978-981-96-0789-1_9,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85218440966&doi=10.1007%2f978-981-96-0789-1_9&partnerID=40&md5=4f2609fd8a90ee8762b34cf4ddd30355,"In certain hazardous environments, teleoperation allows human operators to maintain a safe distance while controlling robots to perform tasks. Point clouds can be utilized to create an immersive environment, enhancing the accuracy and safety of remote operations. To address the inefficiency of the current 3D point cloud compression methods in robot teleoperation systems, by integrating teleoperation devices, this paper proposes a variable parameter level of detail (LoD) model point cloud compression method based on attention mechanism. By extracting the operator’s attention information, remote robots are guided to compress the point cloud, significantly improving the efficiency of point cloud compression. Firstly, a LoD compression model is designed. During runtime, operator’s eye gaze and arm stiffness information is obtained using virtual reality (VR) glasses and MYO armband, which is translated into focal point, focus range and precision requirements, dynamically adjusting the parameters of the LoD compression model accordingly. Finally, point cloud downsampling is conducted based on the LoD compression model, and serialization is performed using octrees. Experimental results demonstrate the effective improvement in point cloud compression efficiency achieved by the proposed method.  © The Author(s), under exclusive license to Springer Nature Singapore Pte Ltd. 2025.",3D point cloud; attention mechanism; LoD model; teleoperation; transmission,Behavioral research; Digital elevation model; Robots; 3D point cloud; Attention mechanisms; Compression methods; Compression model; Level of details model; Level-of-detail; Model points; Parameter levels; Point-clouds; Variable-parameters; Remote control,Conference paper,Final,,Scopus,2-s2.0-85218440966,Gaming / VR
Cragoe N.; Sprowles J.; Woodbury M.L.; Musaad S.; Enright E.; Aguiar A.; Schantz S.L.,"Cragoe, Nicholas (57190747036); Sprowles, Jenna (57191071578); Woodbury, Megan L. (57222317117); Musaad, Salma (26632133200); Enright, Elizabeth (57193710486); Aguiar, Andréa (7005379000); Schantz, Susan L. (7101834759)",57190747036; 57191071578; 57222317117; 26632133200; 57193710486; 7005379000; 7101834759,Associations of prenatal maternal urinary concentrations of triclosan and benzophenone-3 with cognition in 7.5-month-old infants,2024,Environmental Research,263,,119975,,,,1,10.1016/j.envres.2024.119975,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85204375465&doi=10.1016%2fj.envres.2024.119975&partnerID=40&md5=86fc1913c350873416aae6490792d40d,"Background: Endocrine-disrupting chemicals (EDCs) have been linked to adverse health outcomes and prenatal exposure is known to impact infant and child development. However, few studies have assessed early developmental consequences of prenatal exposure to two common phenolic compounds, benzophenone-3 (BP-3) and triclosan (TCS). Objective: We evaluated the relationship of prenatal exposure to BP-3 and TCS with infant cognition at 7.5 months via performance on a visual recognition memory (VRM) task. Methods: Drawing from the Illinois Kids Development Study (IKIDS) cohort, prenatal exposure to BP-3 and TCS was assessed in pools of five urine samples collected from each woman across pregnancy. Cognition was measured in 310 infants using a VRM task assessing information processing speed, attention, and recognition memory through infrared eye-tracking. Generalized linear regression estimated exposure-outcome associations, followed by stratification to investigate modification of associations by infant sex and stimulus set. Results: Sampled mothers were more likely to be white, college educated, and middle or high income relative to the US population. Mean chemical exposures were significantly higher than those of adult women in the NHANES cohort. In models adjusted for income, gestational age at birth, and testing age, prenatal BP-3 exposure was associated with an increase in run duration (average time spent looking at the stimuli before looking away) (β = 0.0011, CI -0.0001:0.0022), indicating slower information processing speed, while TCS was associated with significantly longer time to familiarization (time to accrue a total of 20 s of looking time to the stimuli) (β = 0.0686, CI 0.0203:0.1168, p < 0.01), indicating poorer attention. Stratum-specific analyses isolated both effects to male infants who viewed the second of two stimulus sets. Conclusion: Higher prenatal exposure to triclosan was associated with poorer attention in infancy, while benzophenone-3 may be associated with slower information processing speed, particularly among males. © 2024",Benzophenone-3; Endocrine-disrupting chemicals; Infant cognition; Triclosan; Visual recognition memory,Adult; Benzophenones; Cognition; Endocrine Disruptors; Environmental Pollutants; Female; Humans; Infant; Male; Maternal Exposure; Pregnancy; Prenatal Exposure Delayed Effects; Triclosan; Young Adult; Illinois; United States; Chlorine compounds; Linear regression; oxybenzone; triclosan; benzophenone derivative; endocrine disruptor; oxybenzone; Benzophenone-3; Endocrine disrupting chemicals; Health outcomes; Infant cognition; Prenatal exposure; Processing speed; Recognition memory; Triclosan; Visual recognition; Visual recognition memory; child health; cognition; endocrine disruptor; memory; pollution exposure; urine; Article; attention; Caucasian; child; cognition; college; controlled study; eye tracking; female; highest income group; human; infant; information processing; kidney concentrating capacity; major clinical study; male; memory; pregnancy; prenatal exposure; velocity; visual memory; adult; adverse event; drug effect; maternal exposure; pollutant; prenatal exposure delayed effect; urine; young adult; Technetium compounds,Article,Final,,Scopus,2-s2.0-85204375465,Gaming / VR
Chellali A.; Herfort L.; Loup G.; Ferrer M.-H.,"Chellali, Amine (26767578800); Herfort, Lucas (57669940800); Loup, Guillaume (56801122300); Ferrer, Marie-Hélène (56927612400)",26767578800; 57669940800; 56801122300; 56927612400,The Impact of Visual and Haptic Feedback on Keyboard Typing in Immersive Virtual Environments,2025,IEEE Transactions on Visualization and Computer Graphics,31,5,,2633,2642,9.0,1,10.1109/TVCG.2025.3549555,https://www.scopus.com/inward/record.uri?eid=2-s2.0-105003753656&doi=10.1109%2fTVCG.2025.3549555&partnerID=40&md5=f9fa377979ea62c42baf2bccd30c2b60,"Typing with a keyboard is a common task in content production in the workplace. For simulation purposes in VR environments, it is important for users to perform this task accurately, with minimal performance loss, and without distraction. One common approach is using mid-air typing on virtual keyboards. However, this method presents challenges, particularly due to the lack of haptic feedback and spatial awareness. Various solutions have been suggested in the literature to address these challenges, but several design factors that influence performance, behavior, and user experience still need to be explored. This paper investigates the effects of two types of visual feedback (hover and keypress) and three passive haptic feedback conditions (physical keyboard, physical surface, and a mid-air virtual keyboard with no haptic feedback) and the possible interactions between these factors on typing using the two index fingers in VR. Results show that keypress visual feedback enhanced typing speed and reduced workload, while hover feedback lowered error rates but negatively impacted typing speed. Additionally, using a physical keyboard to provide passive haptic feedback increased the error rate. This increase in the error rate could be attributed to inaccuracies in finger and keyboard tracking, which may have caused a misalignment between the physical and virtual environments. Regarding eye gaze behavior, participants spent more time looking at the keyboard with the keypress visual feedback and when no haptic feedback was provided. Finally, participants rated the physical keyboard as the least usable option.  © 1995-2012 IEEE.",Immersive environments; Input techniques; Text entry; Virtual keyboard,"Adult; Computer Graphics; Computer Peripherals; Feedback, Sensory; Female; Humans; Male; Touch; User-Computer Interface; Virtual Reality; Young Adult; Computer keyboards; Haptic interfaces; Virtual addresses; Virtual reality; Visual communication; Visual servoing; Content production; Error rate; Haptic feedbacks; Immersive environment; Immersive virtual environments; Input techniques; Text entry; Typing speed; Virtual Keyboards; Visual feedback; adult; computer; computer graphics; computer interface; female; human; male; physiology; sensory feedback; touch; virtual reality; young adult; Virtual environments",Article,Final,,Scopus,2-s2.0-105003753656,Gaming / VR
Patarini F.; Tamburella F.; Pichiorri F.; Mohebban S.; Bigioni A.; Ranieri A.; Di Tommaso F.; Tagliamonte N.L.; Serratore G.; Lorusso M.; Ciaramidaro A.; Cincotti F.; Scivoletto G.; Mattia D.; Toppi J.,"Patarini, Francesca (59447218100); Tamburella, Federica (23767491200); Pichiorri, Floriana (35811105600); Mohebban, Shiva (59462907900); Bigioni, Alessandra (57226288800); Ranieri, Andrea (57385385100); Di Tommaso, Francesco (57383293700); Tagliamonte, Nevio Luigi (35100967100); Serratore, Giada (58703676400); Lorusso, Matteo (57217383505); Ciaramidaro, Angela (8980333700); Cincotti, Febo (7003991802); Scivoletto, Giorgio (6701533068); Mattia, Donatella (7003828922); Toppi, Jlenia (36022079200)",59447218100; 23767491200; 35811105600; 59462907900; 57226288800; 57385385100; 57383293700; 35100967100; 58703676400; 57217383505; 8980333700; 7003991802; 6701533068; 7003828922; 36022079200,On the role of visual feedback and physiotherapist-patient interaction in robot-assisted gait training: an eye-tracking and HD-EEG study,2024,Journal of NeuroEngineering and Rehabilitation,21,1,211,,,,3,10.1186/s12984-024-01504-9,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85211385642&doi=10.1186%2fs12984-024-01504-9&partnerID=40&md5=4ff43b85fda2a095485111a3849340e7,"Background: Treadmill based Robotic-Assisted Gait Training (t-RAGT) provides for automated locomotor training to help the patient achieve a physiological gait pattern, reducing the physical effort required by therapist. By introducing the robot as a third agent to the traditional one-to-one physiotherapist-patient (Pht-Pt) relationship, the therapist might not be fully aware of the patient’s motor performance. This gap has been bridged by the integration in rehabilitation robots of a visual FeedBack (FB) that informs about patient’s performance. Despite the recognized importance of FB in t-RAGT, the optimal role of the therapist in the complex patient-robot interaction is still unclear. This study aimed to describe whether the type of FB combined with different modalities of Pht’s interaction toward Pt would affect the patients’ visual attention and emotional engagement during t-RAGT. Methods: Ten individuals with incomplete Spinal Cord Injury (C or D ASIA Impairment Scale level) were assessed using eye-tracking (ET) and high-density EEG during seven t-RAGT sessions with Lokomat where (i) three types of visual FB (chart, emoticon and game) and (ii) three levels of Pht-Pt interaction (low, medium and high) were randomly combined. ET metrics (fixations and saccades) were extracted for each of the three defined areas of interest (AoI) (monitor, Pht and surrounding) and compared among the different experimental conditions (FB, Pht-Pt interaction level). The EEG spectral activations in theta and alpha bands were reconstructed for each FB type applying Welch periodogram to data localised in the whole grey matter volume using sLORETA. Results: We found an effect of FB type factor on all the ET metrics computed in the three AoIs while the factor Pht-Pt interaction level also combined with FB type showed an effect only on the ET metrics calculated in Pht and surrounding AoIs. Neural activation in brain regions crucial for social cognition resulted for high Pht-Pt interaction level, while activation of the insula was found during low interaction, independently on the FB used. Conclusions: The type of FB and the way in which Pht supports the patients both have a strong impact on patients’ engagement and should be considered in the design of a t-RAGT-based rehabilitation session. © The Author(s) 2024.",EEG; Eye-tracking; Robot-assisted gait training; Spinal cord injury; Visual feedback,"Adult; Electroencephalography; Exercise Therapy; Eye-Tracking Technology; Feedback, Sensory; Female; Gait; Gait Disorders, Neurologic; Humans; Male; Middle Aged; Physical Therapists; Robotics; Spinal Cord Injuries; adult; aged; alpha rhythm; American Spinal Injury Association impairment scale; Article; brain size; clinical article; comparative study; electroencephalogram; emotion; eye tracking; female; gait; gray matter; human; insula; low resolution brain electromagnetic tomography; male; middle aged; physiotherapist patient interaction; professional-patient relationship; rehabilitation care; social cognition; spinal cord injury; theta rhythm; treadmill based robotic assisted gait training; treadmill exercise; very elderly; visual attention; visual feedback; devices; electroencephalography; eye-tracking technology; gait; kinesiotherapy; neurologic gait disorder; physiology; physiotherapist; procedures; rehabilitation; robotics; sensory feedback; spinal cord injury",Article,Final,All Open Access; Gold Open Access; Green Open Access,Scopus,2-s2.0-85211385642,Gaming / VR
Woo K.A.; Yoon E.J.; Kim S.; Kim H.; Kim R.; Jin B.; Lee S.; Park H.; Nam H.; Kim Y.K.; Lee J.-Y.,"Woo, Kyung Ah (57204116157); Yoon, Eun Jin (36028745400); Kim, Seoyeon (57222635123); Kim, Heejung (56610997900); Kim, Ryul (56591470700); Jin, Bora (58986307500); Lee, Seungmin (58986830000); Park, Hyunwoong (57213039617); Nam, Hyunwoo (56187220700); Kim, Yu Kyeong (55982561800); Lee, Jee-Young (57225303444)",57204116157; 36028745400; 57222635123; 56610997900; 56591470700; 58986307500; 58986830000; 57213039617; 56187220700; 55982561800; 57225303444,Cognitive Impact of β-Amyloid Load in the Rapid Eye Movement Sleep Behavior Disorder–Lewy Body Disease Continuum,2024,Movement Disorders,39,12,,2259,2270,11.0,1,10.1002/mds.30031,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85206204669&doi=10.1002%2fmds.30031&partnerID=40&md5=ec62cd7bf27bb3fc00b16cc5f7466cc6,"Background: Rapid eye movement sleep behavior disorder (RBD) is linked to the diffuse-malignant subtype and higher cognitive burden in Lewy body disease (LBD). Objective: This study explores brain β-amyloid deposition and its association with cognitive decline across the RBD–LBD continuum. Methods: Patients with isolated RBD (iRBD), Parkinson's disease with probable RBD (PDRBD), and dementia with Lewy bodies with probable RBD (DLBRBD) underwent 18F-florbetaben positron emission tomography, 3T magnetic resonance imaging scans, and comprehensive neuropsychological assessments. Subjects were categorized as cognitively normal (NC), mild cognitive impairment (MCI), or dementia. Global and regional standardized uptake value ratios (SUVR) were estimated in predefined cognitive volumes of interest (VOI) derived from voxel-wise comparison analysis among the cognitive groups, namely the prefrontal, parietal, precentral cortices, lingual gyrus, and supplementary motor area. Generalized linear models assessed the relationship between 18F-florbetaben SUVRs and neuropsychological testing, adjusting for age and sex. Subgroup analysis focused on the polysomnography-confirmed iRBD-continuum subset (n = 41) encompassing phenoconverters and nonconverters in our prospective iRBD cohort. Results: Eighty-six subjects were classified as follows: 14 NC, 54 MCI, and 18 dementia. The proportion of positive β-amyloid scans increased with advanced cognitive stages (P = 0.038). β-Amyloid signals in cognitive VOIs were elevated in subgroups showing impairment in Trail-Making Test B (TMT-B). A linear association between TMT-B z score and global cortical β-amyloid levels was observed in the iRBD-continuum subset (P = 0.013). Conclusion: Cortical β-amyloid accumulates with declines in executive function within the RBD–LBD continuum. TMT-B performance may be a useful marker associating with β-amyloid load, particularly in the iRBD population. © 2024 The Author(s). Movement Disorders published by Wiley Periodicals LLC on behalf of International Parkinson and Movement Disorder Society. © 2024 The Author(s). Movement Disorders published by Wiley Periodicals LLC on behalf of International Parkinson and Movement Disorder Society.",dementia with Lewy bodies; isolated rapid eye movement sleep behavior disorder; Lewy body disease; Parkinson's disease; β-Amyloid,"Aged; Aged, 80 and over; Amyloid beta-Peptides; Aniline Compounds; Brain; Cognitive Dysfunction; Female; Humans; Lewy Body Disease; Magnetic Resonance Imaging; Male; Middle Aged; Neuropsychological Tests; Parkinson Disease; Positron-Emission Tomography; REM Sleep Behavior Disorder; Stilbenes; amyloid beta protein; florbetaben; fluorine 18; 4-(N-methylamino)-4'-(2-(2-(2-fluoroethoxy)ethoxy)ethoxy)stilbene; aniline derivative; stilbene derivative; adult; aged; allele; Article; clinical assessment; cognition; correlation analysis; diffuse Lewy body disease; executive function; female; genotype; hippocampus; human; lingual gyrus; major clinical study; male; MDS-Unified Parkinson Disease Rating Scale; neocortex; neuropsychological assessment; nuclear magnetic resonance imaging; Parkinson disease; polysomnography; positron emission tomography; quantitative analysis; REM sleep behavior disorder; standardized uptake value ratio; supplementary motor area; trail making test; brain; cognitive defect; diagnostic imaging; etiology; metabolism; middle aged; neuropsychological assessment; Parkinson disease; pathophysiology; positron emission tomography; very elderly",Article,Final,All Open Access; Green Open Access; Hybrid Gold Open Access,Scopus,2-s2.0-85206204669,Gaming / VR
Kalkanlı Z.G.; Ünübol H.; Ülker S.V.,"Kalkanlı, Zeynep Gamze (59724677700); Ünübol, Hüseyin (57199671415); Ülker, Selami Varol (58798566800)",59724677700; 57199671415; 58798566800,Use of Distraction Continuous Performance Test and Eye Tracking Integration in the Evaluation of Attention Deficit Hyperactivity Disorder; [Dikkat Dağıtıcı Sürekli Performans Testi ve Göz Takibi Bütünleştirmesinin Dikkat Eksikliği Hiperaktivite Bozukluğunun Değerlendirilmesinde Kullanımı],2024,Cyprus Turkish Journal of Psychiatry and Psychology,6,4,,303,312,9.0,0,10.35365/ctjpp.24.4.01,https://www.scopus.com/inward/record.uri?eid=2-s2.0-105001931277&doi=10.35365%2fctjpp.24.4.01&partnerID=40&md5=4beabdddc9128b75c03a799ee40d2152,"This research was conducted to evaluate and compare the eye movements of 11-12-year-old children with Attention Deficit Hyperactivity Disorder (ADHD) and typically developing children while performing sustained attention tasks by integrating eye tracking technologies and the distraction performance test. The study sample, which was conducted on students studying in 10 public secondary schools from Istanbul Province, Üsküdar District National Education Directorate, consists of 31 students with ADHD and 31 healthy students aged 11-12. In the study, the data collection tools were ""Personal Information Form"", the results of the distraction performance test applied to the students with informed consent from their parents, and the eye tracking parameters (number of stays, number of revisits, number of fixations, first fixation duration, number of twitches) was used. Mann Whitney U and Spearman correlation analysis were used in data analysis. It was determined that there was a significant difference between the case-control groups in terms of the number of stays in all stages of the test, the number of revisits and the number of fixations (p<0.05). According to the results of the research, it would be useful to use eye-tracking technologies as an informative method for real-time assessment of attention in individuals with ADHD. It will provide social benefit by preparing a short-term video task by presenting auditory, visual and multiple distractors that students may encounter in daily life in a virtual classroom environment with virtual reality technologies and applying it to large groups. It is thought that studies conducted on groups of different ages and education levels will contribute to the literature. © 2024 The Author(s).",ADHD; Distraction Continuous Performance Test; Eye Tracking,,Article,Final,,Scopus,2-s2.0-105001931277,Gaming / VR
Galán Serrano J.; Felip-Miralles F.; Palacios-Ibáñez A.,"Galán Serrano, Julia (55977488800); Felip-Miralles, Francisco (55512579700); Palacios-Ibáñez, Almudena (57387060000)",55977488800; 55512579700; 57387060000,Examining the effect of locomotion techniques on virtual prototype assessment: Gaze analysis using a Head-Mounted Display,2024,Computers in Industry,163,,104149,,,,0,10.1016/j.compind.2024.104149,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85202343342&doi=10.1016%2fj.compind.2024.104149&partnerID=40&md5=ecc202ba363b044ce590e0f0d3d75ef9,"Improvements in the performance and graphical quality of Head-Mounted Displays (HMDs) have led to their increasing use in Virtual Reality (VR) for product presentation and virtual prototype (VP) evaluations. Various locomotion techniques in VR make it possible to move through a virtual scenario and approach the VP for evaluation purposes. The integration of eye-tracking devices into recent HMDs allows the trajectory and gaze behavior of observers to be reported during the evaluation, often more objectively than self-report questionnaires. However, very few studies have used physiological measures for the evaluation of products embedded in VR environments. Therefore, this paper offers a study in which 95 people evaluated three VPs of street furniture presented in their environment of use using Meta Quest Pro headset and explored through teleport and natural walking. The influence of the locomotion techniques on the ratings recorded using a semantic differential, sense of presence, cybersickness, and the role of eye-tracking in understanding gaze behavior while evaluating products' Areas of Interest (AOIs), are investigated. This study found no evidence that the way of approaching the product influences the evaluation of some of its features, overall product evaluation, confidence in responses, sense of presence, or cybersickness differently. On the other hand, this work evidences that the locomotion technique had an impact on how the user approached the products, which could significantly influence the viewing time of some AOIs. The study revealed that the most observed AOIs coincided with those parts closely related to important features, generally located at the top of the products, so paying special attention to these parts when designing and evaluating similar VPs is recommended. © 2024 The Authors",Eye-tracking; Natural walking; Physiological measures; Product evaluation; Teleport; Virtual reality,Blood vessels; Eye movements; Head-up displays; Physiological models; Virtual environments; Area of interest; Eye-tracking; Gaze behaviours; Head-mounted-displays; Locomotion technique; Natural walking; Physiological measures; Product evaluation; Sense of presences; Teleport; Helmet mounted displays,Article,Final,All Open Access; Hybrid Gold Open Access,Scopus,2-s2.0-85202343342,Gaming / VR
Davidson M.J.; Verstraten F.A.J.; Alais D.,"Davidson, Matthew J. (57195494428); Verstraten, Frans A. J. (7003978929); Alais, David (56962734100)",57195494428; 7003978929; 56962734100,Walking modulates visual detection performance according to stride cycle phase,2024,Nature Communications,15,1,2027,,,,6,10.1038/s41467-024-45780-4,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85187102634&doi=10.1038%2fs41467-024-45780-4&partnerID=40&md5=b5334d4f6ef717ce489c363f9d02e064,"Walking is among our most frequent and natural of voluntary behaviours, yet the consequences of locomotion upon perceptual and cognitive function remain largely unknown. Recent work has highlighted that although walking feels smooth and continuous, critical phases exist within each step for the successful coordination of perceptual and motor function. Here, we test whether these phasic demands impact upon visual perception, by assessing performance in a visual detection task during natural unencumbered walking. We finely sample visual performance over the stride cycle as participants walk along a smooth linear path at a comfortable speed in a wireless virtual reality environment. At the group-level, accuracy, reaction times, and response likelihood show strong oscillations, modulating at approximately 2 cycles per stride (~2 Hz) with a marked phase of optimal performance aligned with the swing phase of each step. At the participant level, Bayesian inference of population prevalence reveals highly prevalent oscillations in visual detection performance that cluster in two idiosyncratic frequency ranges (2 or 4 cycles per stride), with a strong phase alignment across participants. © The Author(s) 2024.",,Bayes Theorem; Gait; Humans; Locomotion; Visual Perception; Walking; detection method; locomotion; performance assessment; virtual reality; walking; accuracy; adult; Article; Bayesian network; cognition; controlled study; eye movement; female; human; human experiment; locomotion; male; motor performance; normal human; oscillation; prevalence; psychometry; reaction time; stride length; stride length variability; stride time; task performance; time series analysis; velocity; virtual reality; vision; walking; young adult; Bayes theorem; gait; physiology; vision,Article,Final,All Open Access; Gold Open Access; Green Open Access,Scopus,2-s2.0-85187102634,Gaming / VR
Fu R.; Zhang C.; Wang Y.; Yue Y.; Di Sarno L.,"Fu, Rong (58864942400); Zhang, Cheng (55703837900); Wang, Yixuan (59460663000); Yue, Yong (7101695542); Di Sarno, Luigi (6506371363)",58864942400; 55703837900; 59460663000; 7101695542; 6506371363,Building layout assessment on fire evacuation efficiency by integrating graph theory and virtual reality,2024,Journal of Building Engineering,98,,111498,,,,0,10.1016/j.jobe.2024.111498,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85211176361&doi=10.1016%2fj.jobe.2024.111498&partnerID=40&md5=0da912e9973b9ade9240544057608e6a,"The building layout design significantly impacts the fire evacuation process. Existing research focused on evaluating potential evacuation routes and investigating evacuation behaviour with different guidance systems. However, these studies have overlooked the effect of building layout design on visual attention and evacuation behaviour during fire evacuation. Therefore, this paper proposed a methodology to evaluate the building floor plans and evacuation routes by integrating Graph Theory (GT) and Virtual Reality (VR) technology. Building layouts and evacuation routes were represented as graphs and subgraphs, enabling a numerical evaluation of the efficiency of evacuation routes. To further investigate occupants’ attention allocation and evacuation behaviour during the wayfinding process, an eye-tracking function was developed and embedded in a VR platform. Moreover, evacuation behaviours and corresponding time were identified and assigned to the features of edges in the subgraphs, based on which the impact of spatial design on evacuation behaviour was analysed. The findings highlighted the significant impact of route characteristics and signage visibility on attention allocation and evacuation behaviour. Following that, recommendations were provided to building designers to improve fire evacuation efficiency. This study provides emergency behaviour observations and offers a comprehensive evaluation of building layout design, contributing to supporting designers in building design assessment in terms of fire evacuation efficiency. © 2024 Elsevier Ltd",Building layout assessment; Fire evacuation; Graph theory; Virtual reality,Building floors; Building layout assessment; Buildings layout; Evacuation process; Evacuation routes; Fire evacuation; Guidance system; Layout designs; Subgraphs; Visual Attention; Virtual environments,Article,Final,,Scopus,2-s2.0-85211176361,Gaming / VR
Geers A.L.; Seligman L.D.; Pituch K.A.; Colagiuri B.; Marusak H.A.; Rabinak C.A.; Al-Ado S.L.; Turner N.; Nedley M.,"Geers, Andrew L. (7006117410); Seligman, Laura D. (7003523797); Pituch, Keenan A. (9938791000); Colagiuri, Ben (24511982300); Marusak, Hilary A. (55812452900); Rabinak, Christine A. (21835019800); Al-Ado, Sena L. (58819837200); Turner, Natalie (58079230400); Nedley, Michael (6603097181)",7006117410; 7003523797; 9938791000; 24511982300; 55812452900; 21835019800; 58819837200; 58079230400; 6603097181,A test of pre-exposure spacing and multiple context pre-exposure on the mechanisms of latent inhibition of dental fear: A study protocol,2024,BMC Psychology,12,1,85,,,,0,10.1186/s40359-024-01580-5,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85185846605&doi=10.1186%2fs40359-024-01580-5&partnerID=40&md5=80f9cd6f5bb9953d44e6e0cc9314f78d,"Background: Latent inhibition occurs when exposure to a stimulus prior its direct associative conditioning impairs learning. Results from naturalistic studies suggest that latent inhibition disrupts the learning of dental fear from aversive associative conditioning and thereby reduces the development of dental phobia. Although theory suggests latent inhibition occurs because pre-exposure changes the expected relevance and attention directed to the pre-exposed stimulus, evidence supporting these mechanisms in humans is limited. The aim of this study is to determine if two variables, pre-exposure session spacing and multiple context pre-exposure, potentiate the hypothesized mechanisms of expected relevance and attention and, in turn, increase latent inhibition of dental fear. Methods: In a virtual reality simulation, child and adult community members (ages 6 to 35) will take part in pre-exposure and conditioning trials, followed by short- and long-term tests of learning. A 100ms puff of 60 psi air to a maxillary anterior tooth will serve as the unconditioned stimulus. Pre-exposure session spacing (no spacing vs. sessions spaced) and multiple context pre-exposure (single context vs. multiple contexts) will be between-subject factors. Stimulus type (pre-exposed to-be conditioned stimulus, a non-pre-exposed conditioned stimulus, and an unpaired control stimulus) and trial will serve as within-subject factors. Baseline pain sensitivity will also be measured as a potential moderator. Discussion: It is hypothesized that spaced pre-exposure and pre-exposure in multiple contexts will increase the engagement of the mechanisms of expected relevance and attention and increase the latent inhibition of dental fear. It is expected that the findings will add to theory on fear learning and provide information to aid the design of future interventions that leverage latent inhibition to reduce dental phobia. © The Author(s) 2024.",Dental phobia; Eye tracking; Fear learning; Latent inhibition; Pain sensitivity; Pre-exposure; Virtual reality,"Adult; Attention; Child; Conditioning, Classical; Dental Anxiety; Humans; Memory; adult; article; attention; child; controlled study; dental anxiety; dental phobia; eye tracking; eye-tracking technology; fear; human; latent inhibition; learning; maxilla; nociception; protocol; simulation; virtual reality",Article,Final,All Open Access; Gold Open Access; Green Open Access,Scopus,2-s2.0-85185846605,Gaming / VR
Geers A.L.; Seligman L.D.; Pituch K.A.; Colagiuri B.; Marusak H.A.; Rabinak C.A.; Turner N.; Al-Ado S.L.; Nedley M.,"Geers, Andrew L. (7006117410); Seligman, Laura D. (7003523797); Pituch, Keenan A. (9938791000); Colagiuri, Ben (24511982300); Marusak, Hilary A. (55812452900); Rabinak, Christine A. (21835019800); Turner, Natalie (58079230400); Al-Ado, Sena L. (58819837200); Nedley, Michael (6603097181)",7006117410; 7003523797; 9938791000; 24511982300; 55812452900; 21835019800; 58079230400; 58819837200; 6603097181,A study protocol testing pre-exposure dose and compound pre-exposure on the mechanisms of latent inhibition of dental fear,2024,BMC Psychology,12,1,36,,,,0,10.1186/s40359-024-01527-w,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85182643743&doi=10.1186%2fs40359-024-01527-w&partnerID=40&md5=f86eb89adc7ef687425007ffbbb7e7ca,"Background: Dental stimuli can evoke fear after being paired - or conditioned - with aversive outcomes (e.g., pain). Pre-exposing the stimuli before conditioning can impair dental fear learning via a phenomenon known as latent inhibition. Theory suggests changes in expected relevance and attention are two mechanisms responsible for latent inhibition. In the proposed research, we test whether pre-exposure dose and degree of pre-exposure novelty potentiate changes in expected relevance and attention to a pre-exposed stimulus. We also assess if the manipulations alter latent inhibition and explore the possible moderating role of individual differences in pain sensitivity. Methods: Participants will be healthy individuals across a wide range of ages (6 to 35 years), from two study sites. Participants will undergo pre-exposure and conditioning followed by both a short-term and long-term test of learning, all in a novel virtual reality environment. The unconditioned stimulus will be a brief pressurized puff of air to a maxillary anterior tooth. Pre-exposure dose (low vs. high) and pre-exposure novelty (element stimulus vs. compound stimuli) will be between-subject factors, with stimulus type (pre-exposed to-be conditioned stimulus, a non-pre-exposed conditioned stimulus, and an unpaired control stimulus) and trial as within-subject factors. Pain sensitivity will be measured through self-report and a cold pressor test. It is hypothesized that a larger dose of pre-exposure and compound pre-exposure will potentiate the engagement of the target mechanisms and thereby result in greater latent inhibition in the form of reduced fear learning. Further, it is hypothesized that larger effects will be observed in participants with greater baseline pain sensitivity. Discussion: The proposed study will test whether pre-exposure dose and compound stimulus presentation change expected relevance and attention to the pre-exposed stimulus, and thereby enhance latent inhibition of dental fear. If found, the results will add to our theoretical understanding of the latent inhibition of dental fear and inform future interventions for dental phobia prevention. © 2024, The Author(s).","Dental phobia; Fear learning; Latent inhibition; Pre-exposure, Pain sensitivity, virtual reality, Eye tracking","Conditioning, Classical; Dental Anxiety; Humans; Learning; Memory; Pain; classical conditioning; dental anxiety; human; learning; memory; pain; physiology; prevention and control",Article,Final,All Open Access; Gold Open Access; Green Open Access,Scopus,2-s2.0-85182643743,Gaming / VR
Yang X.; Fang X.; Gao M.; Ackert L.F.; Qi L.,"Yang, Xiaolan (56169045100); Fang, Xiaotong (59404494100); Gao, Mei (57203330843); Ackert, Lucy F. (6701651018); Qi, Li (25960296500)",56169045100; 59404494100; 57203330843; 6701651018; 25960296500,Follow the gaze: How social attention shapes gendered trading choices,2024,China Economic Review,88,,102301,,,,0,10.1016/j.chieco.2024.102301,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85208687269&doi=10.1016%2fj.chieco.2024.102301&partnerID=40&md5=b750580e31a8fc073855cab15d9fe767,"We explore how visual attention differentially impacts the trading behavior of men and women. In the laboratory, eye-tracking technology measures information gaze during a sequential trading game in which participants are asked to buy or sell an asset. Before making a decision, traders receive information on the trading decisions of other participants (others' decisions) and the redemption value of the asset (private information). Research documents that women, compared to men, pay more attention to social cues. In this study, women are more attention-driven when making financial decisions, as compared to men. We conclude that attentional priority is a cognitive mechanism that can account for the increased tendency of women, compared to men, to follow the social cues of others with disparate information and tournament incentives. © 2024 Elsevier Inc.",Gender; Herd behavior; Social influences; Visual attention,,Article,Final,,Scopus,2-s2.0-85208687269,Gaming / VR
Eunji K.I.M.; Seungwan J.I.N.; Kyungsik H.A.N.,"Eunji, K.I.M. (59415776600); Seungwan, J.I.N. (59416141100); Kyungsik, H.A.N. (59416253700)",59415776600; 59416141100; 59416253700,An Empirical Study on Social Anxiety in a Virtual Environment through Mediating Variables and Multiple Sensor Data,2024,Proceedings of the ACM on Human-Computer Interaction,8,CSCW2,438,,,,2,10.1145/3686977,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85209585750&doi=10.1145%2f3686977&partnerID=40&md5=e42da9e74cb99ad5a1a186f17dae0a4a,"Social anxiety disorder is a psychological condition characterized by excessive nervousness in social situations, such as interpersonal interactions. Exposure therapy has shown benefits in its treatment, and virtual reality (VR) technology has gained much attention for reducing physical and psychological distance and providing additional quantitative evidence from the data generated by standard VR devices (e.g., head-mounted display). Clinical psychology studies have highlighted the importance of mediating variables of social anxiety; however, existing VR-based social anxiety studies have neglected such variables with respect to user experience and data analysis in the context of VR, although these variables could provide insights into the design and use of VR for the treatment of social anxiety. In this study, we focused on two representative mediating variables of social anxiety: (1) the gap between self-presentation motivation and expectancy, and (2) self-focused attention. We used sensor data (e.g., head movement, eye movement, eye gaze, and psychological signals) to investigate the impact of these variables on users’ anxiety responses in VR. We developed VR-based Social Anxiety Support Tool (VRST) that reflects the theoretical design elements of effective anxiety provocation. Based on the results of a user study with 30 participants, we confirmed that the mediating variables were associated with social anxiety in the VR environment. We also found that the mediating variables were associated with eye gaze, eye pupil, head movement, and body temperature. Our study results provide researchers, designers, and practitioners with empirical evidence and implications for the use of VR technology and sensor data in the mental health context. © 2024 Copyright held by the owner/author(s).",interactive virtual reality design; social anxiety,Eye movements; Head-up displays; Helmet mounted displays; Virtual reality; Condition; Empirical studies; Eye-gaze; Head movements; Interactive virtual reality; Interactive virtual reality design; Multiple sensors; Sensors data; Social anxieties; Virtual reality technology,Article,Final,All Open Access; Hybrid Gold Open Access,Scopus,2-s2.0-85209585750,Gaming / VR
Li J.; Zhu J.; Guan C.,"Li, Jing (57225123478); Zhu, Jingzheng (58776056400); Guan, Cheng (57195734265)",57225123478; 58776056400; 57195734265,Mitigating Fatigue in Tunnel Construction through Illumination: Evidence from Virtual Reality,2024,Journal of Construction Engineering and Management,150,11,4024165,,,,0,10.1061/JCEMD4.COENG-14860,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85203195755&doi=10.1061%2fJCEMD4.COENG-14860&partnerID=40&md5=40ecde8b580b13fdbeaef743eda6aaf9,"Tunnel construction is crucial to the advancement of global transportation networks. Despite their importance, there have been few investigations on the safety and comfort of workers. Tunnel workers operate under various adverse conditions that lead to multiple forms of fatigue. Researchers have suggested that there is a specific type of fatigue stemming from insufficient natural or improper artificial lighting that has largely been overlooked in previous studies. To validate the existence of illumination-induced fatigue and explore control measures, this study conducted virtual reality experiments in a simulated tunnel construction setting and collected and analyzed data from scales, electroencephalogram, and eye-tracking. The results indicated significant variations in fatigue levels with changes in illumination, whereas the environment remained constant, confirming the presence of illumination fatigue. Moreover, the analysis of the scale results aligned with the EEG findings, ranking the comfort levels of the mucking illumination as 100>75>150>50>200>30>15>10>0 lx. The modeling of illumination fatigue identified three comfort levels of illumination, I: [75, 100], II: [15, 75) (100, 200], III: [0, 15], thereby establishing that illumination fatigue can be controlled through improved lighting. Subsequently, to address the issue of uneven illumination distribution at construction sites, key areas for mucking task lighting were identified using eye-tracking technology as the upper and lower edges of the bucket, debris being moved, and expected stacking area. Finally, by comparing and summarizing the divergent conclusions of previous studies, this study proposed various targeted lighting guidelines. This comprehensive study not only enriches the current understanding and experimental design reference for tunnel construction fatigue but also provides practitioners with specific and effective lighting guidance to significantly reduce workers' fatigue and improve their working conditions. © 2024 American Society of Civil Engineers.",Electroencephalogram (EEG); Eye-tracking; Illumination fatigue; Lighting environment; Tunnel construction,Air navigation; Laser beams; Lighting; Virtualization; Comfort level; Condition; Electroencephalogram; Eye-tracking; Illumination fatigue; Lighting environment; Transportation network; Tunnel construction; Tunnel workers; Workers'; Tunneling (excavation),Article,Final,,Scopus,2-s2.0-85203195755,Gaming / VR
Urbano A.; Mortimer M.; Horan B.; Stefan H.; Antlej K.,"Urbano, Adrian (59299118700); Mortimer, Michael (56406985000); Horan, Ben (57202873469); Stefan, Hans (57991792600); Antlej, Kaja (36462356200)",59299118700; 56406985000; 57202873469; 57991792600; 36462356200,Using SIM-TLX to investigate the potential impacts on cognitive load while undertaking tasks in a virtual workplace,2024,Journal of Workplace Learning,36,7,,585,604,19.0,2,10.1108/JWL-03-2024-0060,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85202054891&doi=10.1108%2fJWL-03-2024-0060&partnerID=40&md5=72ea60ba719c6629c9ca418e9c81ec97,"Purpose: The ability to measure cognitive load in the workplace provides several opportunities to improve workplace learning. In recent years, virtual reality (VR) has seen an increase in use for training and learning applications due to improvements in technology and reduced costs. This study aims to focus on the use of simulation task load index (SIM-TLX), a recently developed self-reported measure of cognitive load for virtual environments to measure cognitive load while undertaking tasks in different environments. Design/methodology/approach: The authors conducted a within-subject design experiment involving 14 participants engaged in digit-recall n-back tasks (1-back and 2-back) in two VR environments: a neutral grey environment and a realistic industrial ozone facility. Cognitive load was then assessed using the SIM-TLX. Findings: The findings revealed higher task difficulty for the 2-back task due to higher mental demand. Furthermore, a notable interaction emerged between cognitive load and different virtual environments. Research limitations/implications: This study relied solely on an n-back task and SIM-TLX self-report measure to assess cognitive load. Future studies should consider including ecologically valid tasks and physiological measurement tools such as eye-tracking to measure cognitive load. Practical implications: Identifying cognitive workload sources during VR tasks, especially in complex work environments, is considered beneficial to the application of VR training aimed at improving workplace learning. Originality/value: This study provides unique insights into measuring cognitive load from various sources as defined by the SIM-TLX sub-scales to investigate the impact of simulated workplace environments. © 2024, Emerald Publishing Limited.",Cognitive load; n-back; Realistic virtual environment; Self-assessment; SIM-TLX; Task performance; Virtual reality,,Article,Final,,Scopus,2-s2.0-85202054891,Gaming / VR
Ilo C.; Diverdi S.; Bowman D.,"Ilo, Cory (57223657508); Diverdi, Stephen (8358959900); Bowman, Doug (57203231782)",57223657508; 8358959900; 57203231782,Goldilocks Zoning: Evaluating a Gaze-Aware Approach to Task-Agnostic VR Notification Placement,2024,Proceedings - SUI 2024: ACM Symposium on Spatial User Interaction,,,13,,,,1,10.1145/3677386.3682087,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85205554166&doi=10.1145%2f3677386.3682087&partnerID=40&md5=1dcddcab790aea6ea0ebda836537ebbe,"While virtual reality (VR) offers immersive experiences, users need to remain aware of notifications from outside VR. However, inserting notifications into a VR experience can result in distraction or breaks in presence, since existing notification systems in VR use static placement and lack situational awareness. We address this challenge by introducing a novel notification placement technique, Goldilocks Zoning (GZ), which leverages a 360-degree heatmap generated using gaze data to place notifications near salient areas of the environment without obstructing the primary task. To investigate the effectiveness of this technique, we conducted a dual-task experiment comparing GZ to common notification placement techniques. We found that GZ had similar performance to state-of-the-art techniques in a variety of primary task scenarios. Our study reveals that no single technique is universally optimal in dynamic settings, underscoring the potential for adaptive approaches to notification management. As a step in this direction, we explored the potential to use machine learning to predict the task based on the gaze heatmap.  © 2024 Owner/Author.",Context-Aware; Eye tracking; Gaze-aware; Interaction techniques; Virtual Reality; Visual attention,Virtual reality; Context-Aware; Eye-tracking; Gaze-aware; Heatmaps; Immersive; Interaction techniques; Primary task; User need; Virtual reality experiences; Visual Attention; Virtual environments,Conference paper,Final,,Scopus,2-s2.0-85205554166,Gaming / VR
Zhou S.; Segawa N.,"Zhou, Shuo (57346207700); Segawa, Norihisa (24466674400)",57346207700; 24466674400,Method of Electrical Muscle Stimulation to Improve Hand-eye Coordination Training in Gaming,2024,Proceedings of the ACM on Human-Computer Interaction,8,CHI PLAY,323,,,,1,10.1145/3677088,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85207328342&doi=10.1145%2f3677088&partnerID=40&md5=004de5579c624012da01f26c7287d425,"Hand-eye coordination refers to the harmonization between visual information and hand movements. In this coordination, the information perceived by the eyes is used to guide the movements of the hands. In many games, hand-eye coordination is important for the player’s experience. However, when players are faced with an unfamiliar activity, it can be a challenge to develop hand-eye coordination in a short period. This study proposes a method for hand-eye coordination training in gaming using Electrical Muscle Stimulation (EMS). By employing EMS, it is possible to enhance attention distribution during bilateral hand movements and improve hand-eye coordination, potentially enabling individuals to train hand coordination skills in a shorter period. To test the feasibility and effectiveness of our method, we first tested participants for attentional switching while wearing the EMS device to ensure that it did not negatively affect participants. Sixteen participants were then trained in hand-eye coordination with and without EMS, and the effects of the training were tested in a customized game and a real music game. The results showed that participants trained with EMS showed more significant improvements in scores and accuracy than those without EMS in both the custom game and the real game. We also conducted a retained effect experiment, which confirmed that participants retained the training effect one week after training. Thus, our study shows the possibility of using EMS as a new medium for training hand-eye coordination in games. © 2024 Copyright held by the owner/author(s).",Attention; Electrical muscle stimulation; EMS; Game; Hand Control; Hand-eye Coordination; Motion Perception; Sensorimotor; Training,Electrotherapeutics; Eye controlled devices; Eye protection; Functional electric stimulation; Attention; Electrical muscle stimulation; Game; Hand control; Hand eye coordination; Motion perception; Muscle stimulation; Sensorimotors; Eye movements,Article,Final,All Open Access; Hybrid Gold Open Access,Scopus,2-s2.0-85207328342,Gaming / VR
Li Y.; Xu H.; Kitamura Y.; Tag B.; Fujita K.,"Li, Yi (57378088400); Xu, Hongyue (59363967000); Kitamura, Yoshifumi (7401509098); Tag, Benjamin (56747853900); Fujita, Kazuyuki (35253287600)",57378088400; 59363967000; 7401509098; 56747853900; 35253287600,Towards using Eye Gaze Redirection in Immersive Reading Tasks for Visual Fatigue Reduction,2024,UbiComp Companion 2024 - Companion of the 2024 ACM International Joint Conference on Pervasive and Ubiquitous Computing,,,,607,611,4.0,0,10.1145/3675094.3678474,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85206196685&doi=10.1145%2f3675094.3678474&partnerID=40&md5=06eacf32baa61cdfe2022c9c1799b496,"Visual fatigue poses a significant challenge in Human-Information Interaction (HII) in VR. Target redirection, a method that helps mitigate shoulder fatigue, could potentially help reduce visual fatigue by guiding our eyes to move in certain ways. This paper proposes a study using eye movement redirection in VR to reduce visual fatigue from immersive HII tasks. We highlight two primary challenges, speed and range of redirection, that directly impact the user experience and effectiveness. We further introduce two studies aimed at tackling these problems: 1) to find the best parameters for redirection and 2) to evaluate the technique's usability. We hypothesize that this innovative approach will greatly contribute to user-friendly and safe gaze-based HII in VR, enhancing user experience and performance by reducing visual fatigue. © 2024 Copyright held by the owner/author(s).",Cognitive Activity; Human-Information-Interaction; Virtual Reality; Visual Fatigue,Eye movements; Virtual reality; % reductions; Cognitive activities; Eye-gaze; Human-information interaction; Immersive; Innovative approaches; User friendly; User performance; Users' experiences; Visual fatigue; Virtual environments,Conference paper,Final,,Scopus,2-s2.0-85206196685,Gaming / VR
Mosur P.; Kimmel E.; Arora P.; Singh R.; Madiwale A.R.; Kang J.; Starner T.,"Mosur, Priyanka (58998827400); Kimmel, Ethan (59364527100); Arora, Parth (58682731700); Singh, Rajandeep (58682961800); Madiwale, Atharva Rajesh (57348629600); Kang, Jenna (59143701400); Starner, Thad (7003469397)",58998827400; 59364527100; 58682731700; 58682961800; 57348629600; 59143701400; 7003469397,Stepping into AR: Exploring Optimal Positioning for Monocular Head-Worn Displays for Reading on the Go,2024,UbiComp Companion 2024 - Companion of the 2024 ACM International Joint Conference on Pervasive and Ubiquitous Computing,,,,844,850,6.0,1,10.1145/3675094.3678383,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85206154719&doi=10.1145%2f3675094.3678383&partnerID=40&md5=9c8a4034b548575bdc15bf6127ab6dfb,"Head-worn displays (HWDs), like the Vuzix Z100 and North's Focals, are designed to be worn all day as smart eyeglasses while performing everyday tasks. These products aim to display information in our field of view (FOV), enabling monitoring information during daily activities. We conducted a study using the Quest 3, a high resolution color video pass-through virtual reality (VR) headset, to emulate everyday augmented reality glasses. Users walked on a predefined track while reading a message to explore the optimal display positioning for HWDs while walking. The use of HWDs was found to decrease walking speed, with display positions closer to the nose (between -24° and 0°) yielding better performance. Our results and observations indicate that closer-to-nose positioning reduces cognitive load and excessive head or eye movements, enhancing overall dual task performance. © 2024 Copyright held by the owner/author(s).",Augmented Reality; Glasses; Head-worn Display; Human Factors of Wearables,Augmented reality; Color computer graphics; Helmet mounted displays; Virtual reality; Virtualization; Color video; Daily activity; Field of views; Head-worn displays; High resolution; Human factor of wearable; Monitoring information; On The Go; Optimal positioning; Virtual-reality headsets; Eye movements,Conference paper,Final,,Scopus,2-s2.0-85206154719,Gaming / VR
Saboundji R.R.; Faragó K.B.; Firyaridi V.,"Saboundji, Rachid Rhyad (57984649000); Faragó, Kinga Bettina (56578404500); Firyaridi, Violetta (59380246400)",57984649000; 56578404500; 59380246400,Prediction of Attention Groups and Big Five Personality Traits from Gaze Features Collected from an Outlier Search Game,2024,Journal of Imaging,10,10,255,,,,1,10.3390/jimaging10100255,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85207239922&doi=10.3390%2fjimaging10100255&partnerID=40&md5=402e1cf02c0f8f0cc9a261a9491086f9,"This study explores the intersection of personality, attention and task performance in traditional 2D and immersive virtual reality (VR) environments. A visual search task was developed that required participants to find anomalous images embedded in normal background images in 3D space. Experiments were conducted with 30 subjects who performed the task in 2D and VR environments while their eye movements were tracked. Following an exploratory correlation analysis, we applied machine learning techniques to investigate the predictive power of gaze features on human data derived from different data collection methods. Our proposed methodology consists of a pipeline of steps for extracting fixation and saccade features from raw gaze data and training machine learning models to classify the Big Five personality traits and attention-related processing speed/accuracy levels computed from the Group Bourdon test. The models achieved above-chance predictive performance in both 2D and VR settings despite visually complex 3D stimuli. We also explored further relationships between task performance, personality traits and attention characteristics. © 2024 by the authors.",eye tracking; gaze-based interaction; personality traits; virtual reality game; visual attention,Adversarial machine learning; Contrastive Learning; Eye movements; Virtual reality; Big five; Eye-tracking; Gaze-based interaction; Group fives; Personality traits; Search games; Task performance; Virtual reality game; Virtual-reality environment; Visual Attention; Virtual environments,Article,Final,All Open Access; Gold Open Access,Scopus,2-s2.0-85207239922,Gaming / VR
Palmieri J.L.; Deutsch J.E.,"Palmieri, John L. (58608285900); Deutsch, Judith E. (7201985389)",58608285900; 7201985389,The Effects of Competition on Exercise Intensity and the User Experience of Exercise during Virtual Reality Bicycling for Young Adults †,2024,Sensors,24,21,6873,,,,0,10.3390/s24216873,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85208584231&doi=10.3390%2fs24216873&partnerID=40&md5=fc5f3b27069f6d0b2f2bc91f036cbd9f,"Background: Regular moderate–vigorous intensity exercise is recommended for adults as it can improve longevity and reduce health risks associated with a sedentary lifestyle. However, there are barriers to achieving intense exercise that may be addressed using virtual reality (VR) as a tool to promote exercise intensity and adherence, particularly through visual feedback and competition. The purpose of this work is to compare visual feedback and competition within fully immersive VR to enhance exercise intensity and user experience of exercise for young adults; and to describe and compare visual attention during each of the conditions. Methods: Young adults (21–34 years old) bicycled in three 5 min VR conditions (visual feedback, self-competition, and competition against others). Exercise intensity (cycling cadence and % of maximum heart rate) and visual attention (derived from a wearable eye tracking sensor) were measured continuously. User experience was measured by an intrinsic motivation questionnaire, perceived effort, and participant preference. A repeated-measures ANOVA with paired t-test post hoc tests was conducted to detect differences between conditions. Results: Participants exercised at a higher intensity and had higher intrinsic motivation in the two competitive conditions compared to visual feedback. Further, participants preferred the competitive conditions and only reached a vigorous exercise intensity during self-competition. Visual exploration was higher in visual feedback compared to self-competition. Conclusions: For young adults bicycling in VR, competition promoted higher exercise intensity and motivation compared to visual feedback. © 2024 by the authors.",aerobic exercise; bicycling; competition; enjoyment; eye-tracking; motivation; virtual reality; visual attention; visual feedback; wearable sensors,"Adult; Bicycling; Exercise; Feedback, Sensory; Female; Heart Rate; Humans; Male; Motivation; Surveys and Questionnaires; Virtual Reality; Young Adult; Bicycles; Aerobic exercise; Bicycling; Condition; Enjoyment; Exercise intensity; Eye-tracking; Users' experiences; Visual Attention; Visual feedback; Young adults; adult; cycling; exercise; female; heart rate; human; male; motivation; physiology; questionnaire; sensory feedback; virtual reality; young adult; Health risks",Article,Final,All Open Access; Gold Open Access,Scopus,2-s2.0-85208584231,Gaming / VR
Rafa R.R.; Rahman T.; Kobir M.H.; Yang Y.; Deb S.,"Rafa, Rafia Rahman (58953373700); Rahman, Taufiq (58916072100); Kobir, Md Humaun (57506722800); Yang, Yiran (57194105819); Deb, Shuchisnigdha (56154837700)",58953373700; 58916072100; 57506722800; 57194105819; 56154837700,Enhancing experiential learning through virtual reality: System design and a case study in additive manufacturing,2024,Human Factors and Ergonomics in Manufacturing and Service Industries,34,6,,649,666,17.0,0,10.1002/hfm.21055,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85205293483&doi=10.1002%2fhfm.21055&partnerID=40&md5=8877ace1f920efaf7564ce30670a8437,"The recent advancement in additive manufacturing (AM) leads to an extensive need for an industrial workforce in the near future. Workforce training in AM requires expensive capital investment for installing and maintaining this technology and proper knowledge about potential safety hazards. Traditional classroom settings often fail to bridge the critical gap between textbook learning and practical applications. Virtual reality (VR) training can simulate real-world scenarios in a safe and controlled environment and improve student involvement to foster practical learning. In this paper, a virtual training platform for 3D printing has been developed and studied to improve AM education. The developed environment contains a selective laser sintering printer, a preparation station with necessary supplies, a control panel for process planning, and a post-processing station. This platform provides students with excellent learning opportunities to gain hands-on experiences and critical engineering skills on operating process parameters and safety measures. Undergraduate students majoring in industrial engineering were exposed to this learning approach to enhance their engagement and cognitive processing skills. Students' attentions were measured using eye metrics (fixation duration and preference index), and their exposure experiences were collected through the simulation sickness questionnaire, presence questionnaire, and system usability scale. Pre- and post-VR training questionnaires and performance metrics (task completion time and accuracy) evaluated students' learning outcomes. Results provide valuable insights into students' attention, performance, and satisfaction with virtual training environments. Users' gaze behavior and subjective responses revealed many challenges that will help future researchers develop assistive instructions within this virtual educational platform. © 2024 Wiley Periodicals LLC.",additive manufacturing; eye tracking; student attention; virtual reality; virtual training,Gluing; Laser heating; Personnel training; Printing presses; Process control; Smart manufacturing; Students; Teaching; Virtual environments; Capital investment; Case-studies; Experiential learning; Eye-tracking; Potential safety hazards; Student attention; Virtual reality system designs; Virtual reality training; Virtual training; Workforce training; Selective laser sintering,Article,Final,,Scopus,2-s2.0-85205293483,Gaming / VR
Picard S.; Botev J.,"Picard, Stéven (57830178700); Botev, Jean (25927814800)",57830178700; 25927814800,Psychophysiology of rhythmic stimuli and time experience in virtual reality,2024,Computers and Graphics (Pergamon),124,,104097,,,,1,10.1016/j.cag.2024.104097,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85205300755&doi=10.1016%2fj.cag.2024.104097&partnerID=40&md5=ed271b652d9bb22416ff6b9807096c93,"Time experience is an essential part of one's perception of any environment, real or virtual. In this article, from a virtual environment design perspective, we explore how rhythmic stimuli can influence an unrelated cognitive task regarding time experience and performance in virtual reality. This study explicitly includes physiological data to investigate how, overall, experience correlates with psychophysiological observations. The task involves sorting 3D objects by shape, with varying rhythmic stimuli in terms of their tempo and sensory channel (auditory and/or visual) in different trials, to collect subjective measures of time estimation and judgment. The results indicate different effects on time experience and performance depending on the context, such as user fatigue and trial repetition. Depending on the context, a positive impact of audio stimuli or a negative impact of visual stimuli on task performance can be observed, as well as time being underestimated concerning tempo in relation to task familiarity. However, some effects are consistent regardless of context, such as time being judged to pass faster with additional stimuli or consistent correlations between participants’ performance and time experience, suggesting flow-related aspects. We also observe correlations between time experience with eye-tracking data and body temperature, yet some of these correlations may be due to a confounding effect of fatigue. If confirmed as separate from fatigue, these physiological data could be used as reference point for evaluating a user's time experience. This might be of great interest for designing virtual environments, as purposeful stimuli can strongly influence task performance and time experience, both essential components of virtual environment user experience. © 2024 The Authors",Human–computer interaction; Physiological data; Rhythmic stimuli; Time experience; User experience; Virtual reality,Psychophysiology; Virtual reality; 3D object; Cognitive task; Computer interaction; Environment design; Performance; Physiological data; Rhythmic stimulus; Task performance; Time experience; Users' experiences; Virtual environments,Article,Final,All Open Access; Hybrid Gold Open Access,Scopus,2-s2.0-85205300755,Gaming / VR
Artiran S.; Cohen S.; Cosman P.,"Artiran, Saygin (57419665000); Cohen, Shana (54788843400); Cosman, Pamela (7003359562)",57419665000; 54788843400; 7003359562,Virtual reality interview with feedback framework for situational practice of gaze among autistic adults,2024,Research in Autism Spectrum Disorders,118,,102494,,,,1,10.1016/j.rasd.2024.102494,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85206882902&doi=10.1016%2fj.rasd.2024.102494&partnerID=40&md5=3dda2abb9f0ad9488e72248cb3c752a4,"Background: Autistic individuals commonly seek employment; however, only a small fraction are in the workforce. In part, this might be due to mismatches between their social patterns of attention and gaze, and society's normative expectations during interviews. Method: To help mitigate such disadvantages through a solo situational practice tool, we present a framework that consists of a virtual reality (VR) based job interview simulation and a coaching component. Employing data visualization, video modeling, and VR role-play of the targeted behavior, the coaching support could be done in a self-deliverable practice manner. A participatory design session with two autistic design partners was important in the co-creation of the feedback methods, making them easier to understand, and including positive reinforcement. Results: Fourteen autistic individuals used the VR job interview simulation tool. Eleven received the gaze analysis and support stage and participated in a second VR simulation session. Preliminary results were positive, in that participant scores on average eye contact duration, average time without eye contact, and percentage of eye contact while listening and while speaking generally approached the corresponding medians of the non-autistic reference dataset. Participants were surveyed about the utility of the tool after the second feedback session and two months later. All survey respondents perceived the tool to be useful and the provided feedback to be helpful in daily social interactions. Conclusions: This study provides insights towards the development of a VR job interview simulation and feedback framework that can enable solo situational practice of gaze and common interview questions. © 2024 Elsevier Ltd",Autism spectrum; Gaze behavior; Job interview practice; Social interaction practice; Social modulation; Virtual reality,adult; Article; association; autism; clinical article; data visualization; dizziness; eye tracking; feedback system; female; gaze; human; job interview; male; motion sickness; nausea; outlier detection; pilot study; positive reinforcement; role playing; social interaction; social vulnerability; socialization; speech; speech language pathologist; virtual reality,Article,Final,,Scopus,2-s2.0-85206882902,Gaming / VR
Ruiz A.J.; Albaladejo-García C.; Reina R.; Moreno F.J.,"Ruiz, Antonio J. (58475261900); Albaladejo-García, Carlos (57222628493); Reina, Raúl (7003903418); Moreno, Francisco J. (25228166700)",58475261900; 57222628493; 7003903418; 25228166700,Perceptual-Cognitive Skills of Basketball Referees: On-The-Court Visual Search Behavior,2024,Perceptual and Motor Skills,131,5,,1873,1893,20.0,2,10.1177/00315125241278532,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85204180912&doi=10.1177%2f00315125241278532&partnerID=40&md5=f1a597e18b9e8c22bbbf513cab3f0dc4,"Perceptual-cognitive skills are crucial in successfully managing information and decision-making in sports, particularly in high-pressure environments. We examined 16 basketball referees’ on-the-court visual search behavior by comparing referees of different experience levels (experienced, n = 8; and novice, n = 8) and different court positions. Participants’ visual search behavior was analyzed during 20 live gameplay situations using eye-tracking technology. Dependent variables were the number of eye fixations, mean fixation time, and total fixation time on selected areas of interest; and independent variables were the referees’ experience and visual angles (lead and trail referee positions). Experienced referees exhibited significantly lower total fixation time than novice referees (p =.009). Referees in the trail position showed more fixations of shorter duration and a greater focus on the basket than those in the lead position. Our findings suggest that the visual search behavior of basketball referees varies with their court position and experience. These data provide valuable insights into referees’ complex visual search patterns in the real-game context, and they highlight the importance of considering viewing angle and experience in future research. © The Author(s) 2024.",officiating; perception; perceptual-cognitive ability; umpire; vision; visual search strategies,"Adult; Attention; Basketball; Cognition; Decision Making; Eye-Tracking Technology; Female; Fixation, Ocular; Humans; Male; Visual Perception; Young Adult; adult; article; basketball; behavior; cognition; court; decision making; duration; eye fixation; human; hyperbaric pressure; normal human; perception; skill; vision; attention; cognition; eye-tracking technology; female; male; physiology; psychology; young adult",Article,Final,,Scopus,2-s2.0-85204180912,Gaming / VR
Chiossi F.; Gruenefeld U.; Hou B.J.; Newn J.; Ou C.; Liao R.; Welsch R.; Mayer S.,"Chiossi, Francesco (57196946809); Gruenefeld, Uwe (56426574900); Hou, Baosheng James (57217113933); Newn, Joshua (57188820341); Ou, Changkun (57211271298); Liao, Rulu (59196546300); Welsch, Robin (57195622102); Mayer, Sven (57014349100)",57196946809; 56426574900; 57217113933; 57188820341; 57211271298; 59196546300; 57195622102; 57014349100,Understanding the Impact of the Reality-Virtuality Continuum on Visual Search Using Fixation-Related Potentials and Eye Tracking Features,2024,Proceedings of the ACM on Human-Computer Interaction,8,MHCI,281,,,,8,10.1145/3676528,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85204232880&doi=10.1145%2f3676528&partnerID=40&md5=77c91812272ad9dcd016ca45ee5d64bb,"While Mixed Reality allows the seamless blending of digital content in users' surroundings, it is unclear if its fusion with physical information impacts users' perceptual and cognitive resources differently. While the fusion of digital and physical objects provides numerous opportunities to present additional information, it also introduces undesirable side effects, such as split attention and increased visual complexity. We conducted a visual search study in three manifestations of mixed reality (Augmented Reality, Augmented Virtuality, Virtual Reality) to understand the effects of the environment on visual search behavior. We conducted a multimodal evaluation measuring Fixation-Related Potentials (FRPs), alongside eye tracking to assess search efficiency, attention allocation, and behavioral measures. Our findings indicate distinct patterns in FRPs and eye-Tracking data that reflect varying cognitive demands across environments. Specifically, AR environments were associated with increased workload, as indicated by decreased FRP-P3 amplitudes and more scattered eye movement patterns, impairing users' ability to identify target information efficiently. Participants reported AR as the most demanding and distracting environment. These insights inform design implications for MR adaptive systems, emphasizing the need for interfaces that dynamically respond to user cognitive load based on physiological inputs.  © 2024 Owner/Author.",EEG; eye tracking; fixation-related potentials; mixed reality; physiological computing; visual search,Augmented reality; Eye movements; Fracture fixation; Mixed reality; Physiological models; Cognitive resources; Digital contents; Eye-tracking; Fixation-related potential; Mixed reality; Physical information; Physiological computing; Tracking feature; Virtuality continuum; Visual search; Virtual environments,Article,Final,,Scopus,2-s2.0-85204232880,Gaming / VR
Ben Chikha H.; Mguidich H.; Zoudji B.; Khacharem A.,"Ben Chikha, Houssem (58108865200); Mguidich, Hajer (58740365700); Zoudji, Bachir (6504620730); Khacharem, Aïmen (55628220800)",58108865200; 58740365700; 6504620730; 55628220800,Eye-Tracking Analyses of a Coach’s Pointing Gestures Timed With Speech: Implications for Players' Recall of Basketball Tactical Instructions,2024,Perceptual and Motor Skills,131,5,,1894,1915,21.0,1,10.1177/00315125241266645,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85199794127&doi=10.1177%2f00315125241266645&partnerID=40&md5=0c19dbfc69dac4ce4578d9c1f4beb833,"Coaches often use pointing gestures alongside their speech to reinforce their message and emphasize important concepts during instructional communications, but the impact of simultaneous pointing gestures and speech on learners’ recall remains unclear. We used eye-tracking and recalled performance to investigate the impact of a coach’s variously timed pointing gestures and speech on two groups of learners’ (novices and experts) visual attention and recall of tactical instructions. Participants were 96 basketball players (48 novice and 48 expert) who attempted to recall instructions about the evolution of a basketball game system under two teaching conditions: speech accompanied by gestures and speech followed by gestures. Overall, the results showed that novice players benefited more from instructional speech accompanied by gestures than from speech followed by gestures alone. This was evidenced by their greater visual attention to the diagrams, demonstrated through a higher fixation count and decreased saccadic shifts between the coach and the diagrams. Additionally, they exhibited improved recall and experienced reduced mental effort, despite having the same fixation time on the diagrams and equivalent recall time. Conversely, experts benefited more from instructional speech followed by gestures, indicating an expertise reversal effect. These results suggest that coaches and educators may improve their tactical instructions by timing the pairing of their hand gestures and speech in relation to the learner’s level of expertise. © The Author(s) 2024.",eye-tracking; pointing gesture; speech; sport psychology; tactical learning; temporal contiguity principle,Adult; Athletic Performance; Attention; Basketball; Eye-Tracking Technology; Female; Gestures; Humans; Male; Mental Recall; Speech; Young Adult; article; basketball; basketball player; eye tracking; eye-tracking technology; female; game; gesture; human; learning; male; mental effort; recall; speech; sports psychology; teaching; visual attention; adult; athletic performance; attention; physiology; psychology; young adult,Article,Final,,Scopus,2-s2.0-85199794127,Gaming / VR
Skok K.; Waszkiewicz N.,"Skok, Katarzyna (59322891500); Waszkiewicz, Napoleon (14036379800)",59322891500; 14036379800,Biomarkers of Internet Gaming Disorder—A Narrative Review,2024,Journal of Clinical Medicine,13,17,5110,,,,0,10.3390/jcm13175110,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85203665941&doi=10.3390%2fjcm13175110&partnerID=40&md5=31bb17dbc10fefbe47470565d3b1518a,"Since game mechanics and their visual aspects have become more and more addictive, there is concern about the growing prevalence of Internet gaming disorder (IGD). In the current narrative review, we searched PubMed and Google Scholar databases for the keywords “igd biomarker gaming” and terms related to biomarker modalities. The biomarkers we found are grouped into several categories based on a measurement method and are discussed in the light of theoretical addiction models (tripartite neurocognitive model, I-PACE). Both theories point to gaming-related problems with salience and inhibition. The first dysfunction makes an individual more susceptible to game stimuli (raised reward seeking), and the second negatively impacts resistance to these stimuli (decreased cognitive control). The IGD patients’ hypersensitivity to reward manifests mostly in ventral striatum (VS) measurements. However, there is also empirical support for a ventral-to-dorsal striatal shift and transition from goal-directed to habitual behaviors. The deficits in executive control are demonstrated in parameters related to the prefrontal cortex (PFC), especially the dorsolateral prefrontal cortex (DLPFC). In general, the connection of PFC with reward under cortex nuclei seems to be dysregulated. Other biomarkers include reduced P3 amplitudes, high-frequency heart rate variability (HRV), and the number of eye blinks and saccadic eye movements during the non-resting state. A few studies propose a diagnostic (multimodal) model of IGD. The current review also comments on inconsistencies in findings in the nucleus accumbens (NAcc), anterior cingulate cortex (ACC), and precuneus and makes suggestions for future IGD studies. © 2024 by the authors.",addiction; biomarkers; gaming; IGD; Internet gaming disorder; video games,dopamine; glucose; n acetylaspartic acid; alpha rhythm; anterior cingulate; arterial pressure; beta rhythm; blood sampling; brain region; brain size; cognitive defect; cortical thickness (brain); delta rhythm; disease marker; dopamine release; dorsal striatum; dorsolateral prefrontal cortex; electroencephalography; executive function; eyelid reflex; functional connectivity; game addiction; genetic analysis; glucose metabolism; gray matter; habit; heart rate variability; human; inhibition (psychology); inhibitory control; Medline; neuroimaging; nucleus accumbens; precuneus; prefrontal cortex; psychometry; Review; reward; reward seeking behavior; saccadic eye movement; search engine; synapse; theoretical model; theta rhythm; tripartite neurocognitive model; ventral striatum; video game,Review,Final,All Open Access; Gold Open Access; Green Open Access,Scopus,2-s2.0-85203665941,Gaming / VR
Ben Chikha H.; Zoudji B.; Khacharem A.,"Ben Chikha, Houssem (58108865200); Zoudji, Bachir (6504620730); Khacharem, Aïmen (55628220800)",58108865200; 6504620730; 55628220800,The role of coach’s gaze guidance on memorization of tactical movements in basketball: an eye tracking study,2024,German Journal of Exercise and Sport Research,54,3,,374,382,8.0,7,10.1007/s12662-023-00907-5,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85169667212&doi=10.1007%2fs12662-023-00907-5&partnerID=40&md5=84be7fad396820cdf0b6cf904d0d4671,"Even though coach behavior is known to affect learning, it is unclear which specific nonverbal behavior might optimize the teaching of a tactical content in basketball. Using eye-tracking technology, recall construction paradigm, and subjective ratings of mental effort, the present study investigated the question of whether the coach’s eye gaze would affect players’ visual attention and recall performance. Expert (N = 72) and novice (N = 72) players watched one of three types of video lecture in which the coach either (i) gazed at the camera while talking (ii) shifted his gaze between the camera and the whiteboard (guided gaze condition), or (iii) gazed at the whiteboard (fixed gaze condition). The results showed that the coach’s guided gaze not only made the novices focus their visual attention more on the corresponding elements of the game system, but also increased their recall performance and decreased their mental effort. However, the performance of the expert players remained the same regardless of the experimental condition, indicating an expertise reversal effect. The findings suggest that the effectiveness of the coach’s gaze guidance is strongly dependent on expertise levels. © The Author(s), under exclusive licence to Springer-Verlag GmbH Deutschland and Bundesinstitut für Sportwissenschaft, Deutscher Olympischer Sportbund, Deutsche Vereinigung für Sportwissenschaft 2023. korrigierte Publikation 2023.",Basketball; Expertise; Eye-tracking; Gaze guidance; Redundancy; Tactics,,Article,Final,,Scopus,2-s2.0-85169667212,Gaming / VR
Vogt A.Z.; Woodland M.B.; Carter M.J.; Lee A.G.,"Vogt, Ashtyn Z. (58639214300); Woodland, Matthew Brent (57221967365); Carter, Michael J. (57921600300); Lee, Andrew G. (7405628392)",58639214300; 57221967365; 57921600300; 7405628392,Curriculum in Neuro-Ophthalmic Principles for National Football League Game Officials: Comparison of Pretraining and Posttraining Ratings of Knowledge,2024,Journal of Neuro-Ophthalmology,44,3,,376,379,3.0,2,10.1097/WNO.0000000000001926,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85197294932&doi=10.1097%2fWNO.0000000000001926&partnerID=40&md5=283e2a6da3de41f7a5eb5d1157067a3b,"Background: We hypothesize that creation of a structured curriculum in neuro-ophthalmology principles might improve self-rated learner satisfaction and knowledge base of National Football League (NFL) game officials. Our initial objective is to create the said curriculum in coordination with game official experts and staff at the NFL to increase levels of understanding of neuro-ophthalmology principles. We reviewed the prior published literature on applicable neuro-ophthalmic principles in professional sports. Major neuro-ophthalmic principles reviewed include both the efferent (e.g., saccadic and pursuit eye movements and vestibulo-ocular reflex) and afferent (visual field, dynamic visual acuity during body movement, and selective attention deficits). Methods: A 6-question survey pertaining to levels of understanding, future applicability, relevance, satisfaction, and interest in additional training was then given to 26 individuals before and after a lecture given by Dr. Andrew Lee in Plano, TX. The primary outcome measure was the creation of the curriculum followed by real-world testing for face and content validity and ending with a self-rated assessment. Results: Twenty-one individuals completed the prelecture and postlecture survey out of 26 individuals who attended. Prelecture means for the level of understanding of oculomotor terms and the likelihood of using said terms were 3.4 and 3.2, respectively. Postlecture means were 8.9 and 8.8, respectively. The lecture was rated 9.2 of 10 for relevance to coaching and teaching officials, and individuals rated their interest in further content as 9.4 of 10. Conclusions: This study found that NFL game officials are interested in learning more about the science behind play-calling in terms of neuro-ophthalmology principles and practices. In addition, from our pilot survey, it is evident that even one lecture can improve participants’ level of understanding and likelihood of learning more about neuro-ophthalmic principles. © 2023 by North American Neuro-Ophthalmology Society.",,"Curriculum; Football; Health Knowledge, Attitudes, Practice; Humans; Male; Neurology; Ophthalmology; Surveys and Questionnaires; United States; Article; athlete; body movement; content validity; curriculum; e-learning; eye movement; football; game; head movement; human; knowledge; learning; neuroophthalmology; pilot study; selective attention; semicircular canal; teaching; training; vision; visual acuity; visual field; attitude to health; comparative study; education; injury; male; neurology; ophthalmology; questionnaire; United States",Article,Final,,Scopus,2-s2.0-85197294932,Gaming / VR
Ouyang Y.; Luo X.,"Ouyang, Yewei (57195718282); Luo, Xiaowei (35099137200)",57195718282; 35099137200,Effects of Physical Fatigue on Construction Workers' Visual Search Patterns during Hazard Identification,2024,Journal of Construction Engineering and Management,150,9,04024120-1,,,,1,10.1061/JCEMD4.COENG-14304,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85199261837&doi=10.1061%2fJCEMD4.COENG-14304&partnerID=40&md5=225d09a49f5cbbd5884eb8813846c714,"Physical fatigue is a frequent factor in construction workplaces that threatens workers' safety. This study examines whether fatigue alters visual search patterns adopted by workers during hazard identification, thereby affecting identification performance. Subjects (n=24) were exposed to four physical activity levels (nonfatigue, low fatigue, medium fatigue, and high fatigue) and then performed hazard identification tasks in a panoramic virtual reality context, with their search patterns being reflected by eye-movement metrics. The results show that physical fatigue significantly impacted the visual search patterns, with workers allocating less attention to hazardous areas and being less engaged in searching. These visual characteristics correspond to poorer hazard identification accuracy. Further, the degree of influence increased with fatigue levels. The findings could extend the knowledge of the relationship between physical fatigue and safety. They could also uncover the reasons inherent in workers' inability to identify hazards due to physical fatigue, which could provide insight for the industry to cope with the disruptions to safety posed by physical fatigue.  © 2024 American Society of Civil Engineers.",Eye-tracking; Hazard identification; Physical fatigue; Visual search patterns,Accident prevention; Eye movements; Hazards; Occupational risks; Safety factor; Virtual reality; Construction workers; Eye-tracking; Hazard identification; Performance; Physical fatigues; Search patterns; Visual search; Visual search pattern; Worker safety; Workers'; Eye tracking,Article,Final,,Scopus,2-s2.0-85199261837,Gaming / VR
Márquez I.; Treviño M.,"Márquez, Inmaculada (57224601227); Treviño, Mario (56000920500)",57224601227; 56000920500,Visuomotor predictors of interception,2024,PLoS ONE,19,9-Sep,e0308642,,,,3,10.1371/journal.pone.0308642,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85204417569&doi=10.1371%2fjournal.pone.0308642&partnerID=40&md5=9afe4c90acc87707fd10a3f6e6784563,"Intercepting moving targets is a fundamental skill in human behavior, influencing various domains such as sports, gaming, and other activities. In these contexts, precise visual processing and motor control are crucial for adapting and navigating effectively. Nevertheless, there are still some gaps in our understanding of how these elements interact while intercepting a moving target. This study explored the dynamic interplay among eye movements, pupil size, and interceptive hand movements, with visual and motion uncertainty factors. We developed a simple visuomotor task in which participants used a joystick to interact with a computer-controlled dot that moved along two-dimensional trajectories. This virtual system provided the flexibility to manipulate the target’s speed and directional uncertainty during chase trials. We then conducted a geometric analysis based on optimal angles for each behavior, enabling us to distinguish between simple tracking and predictive trajectories that anticipate future positions of the moving target. Our results revealed the adoption of a strong interception strategy as participants approached the target. Notably, the onset and amount of optimal interception strategy depended on task parameters, such as the target’s speed and frequency of directional changes. Furthermore, eye-tracking data showed that participants continually adjusted their gaze speed and position, continuously adapting to the target’s movements. Finally, in successful trials, pupillary responses predicted the amount of optimal interception strategy while exhibiting an inverse relationship in trials without collisions. These findings reveal key interactions among visuomotor parameters that are crucial for solving complex interception tasks. Copyright: © 2024 Márquez, Treviño. This is an open access article distributed under the terms of the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.",,Adult; Eye Movements; Eye-Tracking Technology; Female; Hand; Humans; Male; Motion Perception; Movement; Psychomotor Performance; Pupil; Young Adult; adult; area under the curve; Article; behavior; cognition; eye movement; eye tracking; female; frequency; geometry; human; human experiment; interception; linear regression analysis; male; movement (physiology); nervous system development; neuromuscular function; oculomotor system; psychometry; pupil reflex; pupillometry; questionnaire; sensitivity analysis; visual attention; visuomotor; eye movement; eye-tracking technology; hand; movement (physiology); movement perception; physiology; psychomotor performance; pupil; young adult,Article,Final,All Open Access; Gold Open Access; Green Open Access,Scopus,2-s2.0-85204417569,Gaming / VR
Xiao Y.; Li Q.; Zhang Z.; Zhang Y.,"Xiao, Yingzhe (35754207300); Li, Qianxi (59322612900); Zhang, Zhen (59322716000); Zhang, Yanyue (59322613000)",35754207300; 59322612900; 59322716000; 59322613000,Research and Evaluation of Multi-Sensory Design of Product Packaging Based on VR Technology in Online Shopping Environment,2024,Applied Sciences (Switzerland),14,17,7736,,,,0,10.3390/app14177736,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85203663672&doi=10.3390%2fapp14177736&partnerID=40&md5=d5ea1715d5df3d6deeabdbdd03979bfe,"The development and application of virtual reality (VR) technology significantly enhances consumer immersion. Exploring a multi-sensory evaluation model for virtual packaging is valuable for integrating VR technology with packaging. This study developed a multi-sensory evaluation model for virtual packaging using the analytic hierarchy process (AHP). Eye-tracker experimentation was conducted to identify consumer attention indicators when interacting with virtual packaging. These indicators were quantified using Saaty’s nine-level importance scale and expert input, resulting in a comprehensive multi-sensory evaluation model. Subsequently, a VR shopping system focused on potato chips and cola as packaging design objects. This system was evaluated using the established model, and the results were analyzed. Based on the findings, improvements were made, and the system was re-evaluated using the modified model. The post-improvement evaluation demonstrated significantly enhanced sensory experiences. These results validate that the developed evaluation model effectively guides multi-sensory design approaches for packaging within a VR environment. © 2024 by the authors.",multi-sensory experience; packaging design; packaging materials; virtual reality technology,Chip scale packages; Design of product; Evaluation models; Multi-Sensory; Multi-sensory experience; Packaging designs; Product packaging; Research and evaluation; Sensory designs; Sensory experiences; Virtual reality technology; Packaging materials,Article,Final,All Open Access; Gold Open Access,Scopus,2-s2.0-85203663672,Gaming / VR
Kapgate D.D.,"Kapgate, Deepak D. (57210174579)",57210174579,Application of hybrid SSVEP + P300 brain computer interface to control avatar movement in mobile virtual reality gaming environment,2024,Behavioural Brain Research,472,,115154,,,,2,10.1016/j.bbr.2024.115154,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85199470147&doi=10.1016%2fj.bbr.2024.115154&partnerID=40&md5=88c29d997895d24be565e993d3e9ecb8,"Introduction: This research evaluated the feasibility of a hybrid SSVEP + P300 brain computer interface (BCI) for controlling the movement of an avatar in a virtual reality (VR) gaming environment (VR + BCI). Existing VR + BCI gaming environments have limitations, such as visual fatigue, a lower communication rate, minimum accuracy, and poor system comfort. Hence, there is a need for an optimized hybrid BCI system that can simultaneously evoke the strongest P300 and SSVEP potentials in the cortex. Methods: A BCI headset was coupled with a VR headset to generate a VR + BCI environment. The author developed a VR game in which the avatar's movement is controlled using the user's cortical responses with the help of a BCI headset. Specifically designed visual stimuli were used in the proposed system to elicit the strongest possible responses from the user's brain. The proposed system also includes an auditory feedback mechanism to facilitate precise avatar movement. Results and conclusions: Conventional P300 BCI and SSVEP BCI were also used to control the movements of the avatar, and their performance metrics were compared to those of the proposed system. The results demonstrated that the hybrid SSVEP + P300 BCI system was superior to the other systems for controlling avatar movement. © 2024 Elsevier B.V.",Avatar movement control; Hybrid SSVEP + P300 BCI; Video game; VR+BCI system,"Adult; Avatar; Brain-Computer Interfaces; Electroencephalography; Event-Related Potentials, P300; Female; Humans; Male; Movement; Psychomotor Performance; User-Computer Interface; Video Games; Virtual Reality; Young Adult; accuracy; adult; Article; auditory feedback; avatar; calibration; discriminant analysis; electroencephalography; event related potential; executive function; eye tracking; fatigue; feedback system; female; game; head movement; human; human experiment; hybrid; imagery; male; measurement accuracy; middle aged; normal human; robot assisted surgery; robotics; stimulus; video game; virtual reality; visual evoked potential; visual feedback; visual stimulation; avatar; brain computer interface; computer interface; electroencephalography; event related potential; movement (physiology); physiology; psychomotor performance; young adult",Article,Final,,Scopus,2-s2.0-85199470147,Gaming / VR
Zhao Y.; Liu Z.; Xiao J.; Liu T.; Xu G.; Wang Y.,"Zhao, Yumeng (57722946200); Liu, Zhen (56263344500); Xiao, Jiangjian (7402564461); Liu, Tingting (55727711100); Xu, Gen (57271639700); Wang, Yuanyi (57348135300)",57722946200; 56263344500; 7402564461; 55727711100; 57271639700; 57348135300,Research on emotion modeling of intelligent agents in earthquake evacuation simulation,2024,Cognitive Systems Research,87,,101242,,,,1,10.1016/j.cogsys.2024.101242,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85195486909&doi=10.1016%2fj.cogsys.2024.101242&partnerID=40&md5=4e3e1bbfbd93a4a3a76b13278c0f49b7,"In recent years, virtual reality (VR) has been widely used in emergency drills, skills training and other fields of science education. However, existing VR emergency training platforms lack intelligent agents endowed with emotional and cognitive abilities, making them challenging to evoke user emotions and achieve immersion. Moreover, existing emotion contagion methods of virtual agents lack analysis on whether the emotions of virtual agents can infect users in the real world. Therefore, we proposed an emotional cognitive model (ECM) that simulates the emotional contagion of intelligent agents in a VR earthquake emergency training platform. To evaluate the proposed model, we conducted a user study requiring the user to control an avatar for evacuation training during a simulated earthquake. The user's brain signals are measured using EEG, fNIRS, and eye-tracking devices to analyze whether the user was affected by the emotions of other intelligent agents. The results show that intelligent agents with emotional cognition can evoke user's emotions in earthquake emergency training. © 2024 Elsevier B.V.",Emergency drills; Emotional cognition; Emotional modeling; Intelligent agent,Drills; Eye tracking; Infill drilling; Intelligent agents; Intelligent virtual agents; Emergency drills; Emergency training; Emotion models; Emotional cognition; Emotional models; Evacuation simulation; Skill training; Training platform; User emotions; Virtual agent; adult; Article; artificial intelligence; brain function; clinical research; cognition; comparative study; controlled study; decision making; earthquake; earthquake evacuation; electroencephalogram; emergency evacuation; emotion; functional near-infrared spectroscopy; health education; human; intelligence; process model; reaction time; risk factor; virtual reality; visual field; Earthquakes,Article,Final,,Scopus,2-s2.0-85195486909,Gaming / VR
Andrich A.; Weidner F.; Broll W.,"Andrich, Aliya (58108436000); Weidner, Florian (57194045099); Broll, Wolfgang (6602590663)",58108436000; 57194045099; 6602590663,Tick-tock: Revisiting the Influence of Zeitgebers and Cognitive Load on Time Judgments during and after VR Immersion,2024,ACM International Conference Proceeding Series,,,,360,369,9.0,0,10.1145/3670653.3670667,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85203670017&doi=10.1145%2f3670653.3670667&partnerID=40&md5=18d8a9b6cfdc66a473fc20c7c03a5afd,"Prior research has explored the impact of virtual reality (VR) on human time perception without definitive conclusions. To enhance understanding, we replicated a seminal study, refining it and introducing novel variables. Building upon the original study, we investigated the influence of virtual sun speed and cognitive workload on time perception in a VR environment. Our experiment involved 70 participants estimating time intervals under varying cognitive demands. In addition to assessing time perception during immersion, we examined post-VR time estimations. Contrary to the original study, virtual sun movements did not affect time judgments in VR. However, cognitive workload had a consistent effect, which is consistent with previous findings. Notably, VR immersion affected post-VR time perception of short intervals, a previously overlooked aspect. We contribute to the field by deepening the understanding of time perception dynamics during and after VR experiences and refining earlier findings through replication.  © 2024 Owner/Author.",replication.; Time perception; user study; virtual reality,Depth perception; Cognitive demands; Cognitive loads; Cognitive workloads; Replication.; Sun movement; Time estimation; Time interval; Time perception; User study; Virtual-reality environment; Virtual environments,Conference paper,Final,,Scopus,2-s2.0-85203670017,Gaming / VR
Goudé I.; Bruckert A.; Olivier A.-H.; Pettré J.; Cozot R.; Bouatouch K.; Christie M.; Hoyet L.,"Goudé, Ific (57209414536); Bruckert, Alexandre (57212481599); Olivier, Anne-Hélène (15761037000); Pettré, Julien (57203479876); Cozot, Rémi (6507374802); Bouatouch, Kadi (6602684291); Christie, Marc (57219918829); Hoyet, Ludovic (25928501800)",57209414536; 57212481599; 15761037000; 57203479876; 6507374802; 6602684291; 57219918829; 25928501800,Real-Time Multi-Map Saliency-Driven Gaze Behavior for Non-Conversational Characters,2024,IEEE Transactions on Visualization and Computer Graphics,30,7,,3871,3883,12.0,4,10.1109/TVCG.2023.3244679,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85149380115&doi=10.1109%2fTVCG.2023.3244679&partnerID=40&md5=8f79e539bebfc4fef0f5d0e2d1164fa8,"—Gaze behavior of virtual characters in video games and virtual reality experiences is a key factor of realism and immersion. Indeed, gaze plays many roles when interacting with the environment; not only does it indicate what characters are looking at, but it also plays an important role in verbal and non-verbal behaviors and in making virtual characters alive. Automated computing of gaze behaviors is however a challenging problem, and to date none of the existing methods are capable of producing close-to-real results in an interactive context. We therefore propose a novel method that leverages recent advances in several distinct areas related to visual saliency, attention mechanisms, saccadic behavior modelling, and head-gaze animation techniques. Our approach articulates these advances to converge on a multi-map saliency-driven model which offers real-time realistic gaze behaviors for non-conversational characters, together with additional user-control over customizable features to compose a wide variety of results. We first evaluate the benefits of our approach through an objective evaluation that confronts our gaze simulation with ground truth data using an eye-tracking dataset specifically acquired for this purpose. We then rely on subjective evaluation to measure the level of realism of gaze animations generated by our method, in comparison with gaze animations captured from real actors. Our results show that our method generates gaze behaviors that cannot be distinguished from captured gaze animations. Overall, we believe that these results will open the way for more natural and intuitive design of realistic and coherent gaze animations for real-time applications. © 2023 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission.",Animation dataset eye-tracking data gaze behavior; neural networks; simulation,"Adult; Attention; Computer Graphics; Eye Movements; Eye-Tracking Technology; Female; Fixation, Ocular; Humans; Male; Video Games; Virtual Reality; Animation; Behavioral research; Biological systems; Data visualization; Eye tracking; Interactive computer graphics; Interactive computer systems; Real time systems; Virtual reality; Visualization; Behavioral science; Biological system modeling; Dataset; Eye-tracking; Eye-tracking data; Gaze behaviours; Head; Neural-networks; Real - Time system; Simulation; Solid modelling; Tracking data; adult; attention; computer graphics; eye fixation; eye movement; eye-tracking technology; female; human; male; physiology; video game; virtual reality; Neural networks",Article,Final,All Open Access; Green Open Access,Scopus,2-s2.0-85149380115,Gaming / VR
Terzioğlu B.; Celikcan U.; Capin T.K.,"Terzioğlu, Berkay (59146013200); Celikcan, Ufuk (27867506800); Capin, Tolga Kurtulus (6603846240)",59146013200; 27867506800; 6603846240,Gaze-contingent adaptation of VR stereo parameters for cybersickness prevention,2024,Visual Computer,40,7,,5017,5028,11.0,1,10.1007/s00371-024-03505-0,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85195475859&doi=10.1007%2fs00371-024-03505-0&partnerID=40&md5=2a56943cd14bc20814b2bc19a15f5471,"Extended exposure to virtual reality displays has been linked to the emergence of cybersickness, characterized by symptoms such as nausea, dizziness, fatigue, and disruptions in eye movements. The main objective of our study is to examine the effects of real-time fine-tuning of stereo parameters and blurriness in virtual reality on the discomfort level of users who are experiencing motion sickness triggered by the display. Our hypothesis proposes that by dynamically correcting the rendering settings, the symptoms of motion sickness can be relieved and the overall VR user experience can be improved. Our methodology commences with a prediction model for the comfort level of the viewer based on their gaze parameters, such as pupil diameter, blink count, gaze position, and fixation duration. We then propose a method to dynamically adapt the stereoscopic rendering parameters by considering the predicted comfort level of the viewer. © The Author(s) 2024.",Cyber-sickness; Eye tracking; Gaze-contingent rendering; Stereo parameters; Virtual reality,Diseases; Eye movements; Stereo image processing; Virtual reality; Comfort level; Cyber sickness; Cybersickness; Eye-tracking; Gaze-contingent; Gaze-contingent rendering; Motion sickness; Real- time; Stereo parameter; Virtual-reality display; Eye tracking,Article,Final,,Scopus,2-s2.0-85195475859,Gaming / VR
Luo Y.; Yang Q.; Seo J.; Ahn S.,"Luo, Yanfang (57938022600); Yang, Qiang (57203386217); Seo, JoonOh (55924450700); Ahn, Seungjun (55276780900)",57938022600; 57203386217; 55924450700; 55276780900,Theoretical Framework for Utilizing Eye-Tracking Data to Understand the Cognitive Mechanism of Situational Awareness in Construction Hazard Recognition,2024,Journal of Management in Engineering,40,4,4024027,,,,10,10.1061/JMENEA.MEENG-5905,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85193204121&doi=10.1061%2fJMENEA.MEENG-5905&partnerID=40&md5=908ea7a0dc322cad37ee8b753ba631fb,"Comprehending the cognitive processes underlying hazard identification is crucial for enhancing worker safety behavior in construction. Recent studies have explored eye-tracking technology’s potential in understanding human cognition across contexts. However, limited research delves into the intricate cognitive processes linking eye movements and hazard recognition, particularly in the context of situational awareness (SA). Thus, this study investigates the relationship between eye movement data and SA’s cognitive processes in hazard recognition virtual reality (VR) scenarios at construction sites. The study employed experiments with 36 participants identifying construction hazards across six VR scenarios, yielding 216 trials. Eye movement data were collected via the VR headset’s eye-tracking device, concurrently recording hazard recognition performances. The results uncovered valuable insights into the correlation between eye movement patterns and global and local SA. In the context of global SA, time to and after first fixation elucidated the distinct variations among individuals in terms of perception (Global Level 1 SA) and comprehension times (Global Level 2 and Level 3 SA) across various hazard scenarios. In the realm of local SA, more fixations and saccades (Local Level 1 SA) were observed during the first dwell, underscoring the significance of the first encounter with a hazard. Additionally, pupil dilation, indicative of increased mental workload, occurred upon successful hazard recognition (Local Level 2 and Level 3 SA). These findings highlight the explanatory potential of various eye movement data types for diverse SA levels. They can serve as effective SA indicators in hazard recognition contexts, enhancing understanding of cognitive processes and refining assessment and training for SA in hazardous settings. © 2024 American Society of Civil Engineers.",Cognitive process; Eye movement; Global SA; Hazard recognition; Local SA; Situation awareness (SA),Cognitive systems; Eye tracking; Hazards; Virtual reality; Cognitive process; Eye movement datum; Global situation awareness; Hazard recognition; Local situation awareness; Situation awareness; Situational awareness; Eye movements,Article,Final,,Scopus,2-s2.0-85193204121,Gaming / VR
McAnally K.; Grove P.; Wallis G.,"McAnally, Ken (7004247671); Grove, Philip (6701491981); Wallis, Guy (7102196682)",7004247671; 6701491981; 7102196682,Vergence eye movements in virtual reality,2024,Displays,83,,102683,,,,3,10.1016/j.displa.2024.102683,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85188540145&doi=10.1016%2fj.displa.2024.102683&partnerID=40&md5=263b8f65566d0cd7e1e94edd05fa682b,"There is increasing interest in incorporating eye tracking in virtual reality (VR) systems to infer gaze in 3 dimensions. Estimation of gaze in depth may be limited by errors of measurement of the small eye vergence angles involved. It may also be limited by true errors of vergence in VR. We found that observers commonly made errors of vergence when viewing in VR, such that the dominant eye was more accurately aligned than the non-dominant eye. Errors of vergence were often larger than Panum's fusional range. The absence of reported diplopia suggests that some of the visual input from one eye was supressed or the double images escaped attention. Errors of vergence could not be explained by contamination by variations in pupil diameter. Errors of vergence were not reduced when the vergence-accommodation conflict was eliminated by optical correction. These results suggest that the ability to correctly infer gaze distance in VR may be limited by true errors of eye vergence, rather than by measurement errors imposed by limitations of eye tracking. © 2024 The Author(s)",Distance; Eye tracking; Virtual reality; Vision,Errors; Eye movements; Virtual reality; 3-dimension; Distance; Eye-tracking; Fusional range; Measurements of; Small eye; True error; Vergence eye movements; Vergences; Virtual reality system; Eye tracking,Article,Final,All Open Access; Hybrid Gold Open Access,Scopus,2-s2.0-85188540145,Gaming / VR
Koh D.H.; Xu Z.; Wang J.; Burgess A.N.; Seccia A.; Schneps M.; Pomplun M.; Lamb R.; Keil A.; Dawson K.; Antonenko P.,"Koh, Do Hyong (59431571800); Xu, Zhen (57216278287); Wang, Jiahui (57191491117); Burgess, Andrea N. (57544736200); Seccia, Amanda (57191573120); Schneps, Matthew (28568110000); Pomplun, Marc (7004222183); Lamb, Richard (36634407300); Keil, Andreas (7005544991); Dawson, Kara (16238356400); Antonenko, Pavlo (23090326200)",59431571800; 57216278287; 57191491117; 57544736200; 57191573120; 28568110000; 7004222183; 36634407300; 7005544991; 16238356400; 23090326200,Fixation Disparity: A Possible Index of Visuospatial Cognition during Authentic Learning Tasks,2024,"Mind, Brain, and Education",18,3,,346,359,13.0,2,10.1111/mbe.12424,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85199895038&doi=10.1111%2fmbe.12424&partnerID=40&md5=dbaf8ced64fef8129fb8267cae5d1c37,"This article describes a novel method for quantifying fixation disparity and evaluates its role in visuospatial cognition during an authentic learning task, specifically, the determination of molecule chirality in organic chemistry involving mental rotation and pattern comparison. The first study examined the influence of molecular model dimensionality (2D vs. 3D) on chirality determination performance and visual attention of 55 participants. The second study explored how the sustained playing of the tile-matching game Mahjong, a pattern comparison game, can affect visual attention and visuospatial performance during the chirality determination task of 59 participants. Fixation disparity was one of the eye tracking variables explored. Both studies revealed that (1) individuals with higher fixation disparity underperformed on the chirality task, which involves mental rotation and pattern comparison, and (2) fixation disparity improved over time in participants who played Mahjong. This work provides important implications for using fixation disparity as a possible biomarker of visuospatial performance. © 2024 International Mind, Brain, and Education Society and Wiley Periodicals LLC.",,,Article,Final,,Scopus,2-s2.0-85199895038,Gaming / VR
Kalantari S.; Mostafavi A.; Xu T.B.; Lee A.S.; Yang Q.,"Kalantari, Saleh (56472546100); Mostafavi, Armin (57222313283); Xu, Tong Bill (57417357000); Lee, Anne Seoyoung (58609374400); Yang, Qi (57828070600)",56472546100; 57222313283; 57417357000; 58609374400; 57828070600,Comparing spatial navigation in a virtual environment vs. an identical real environment across the adult lifespan,2024,Computers in Human Behavior,157,,108210,,,,6,10.1016/j.chb.2024.108210,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85190277665&doi=10.1016%2fj.chb.2024.108210&partnerID=40&md5=c29c1d31f446728c572df6a49ba74a5f,"Virtual environments (VEs) are increasingly being used as a research platform for investigating human responses to environmental variables. While VEs provide tremendous advantages in terms of variable isolation and manipulation, and ease of data-collection, some researchers have expressed concerns about the ecological validity of VE-based findings; that is, their transferability to real-world contexts. In the current study we created a virtual replica of a real-world, multi-level educational facility, and compared data collected in the VE vs. the real-world environment as participants completed identical navigational tasks. We found significant differences in all of the measures used, including distance covered, number of mistakes made, time for task completion, spatial memory, extent of backtracking, observation of directional signs, perceived uncertainty levels, perceived cognitive workload, and perceived task difficulty. We also analyzed potential age-related effects to look for heightened VE/real-world response discrepancies among older adult participants (age 55 or over) compared to younger adults (18–30 years of age). This analysis yielded no significant age-by-condition interaction effects. Finally, we examined the spatial distribution of self-reported wayfinding uncertainty across the building floorplan, finding that areas in which uncertainty was most pronounced were similar between the real-world and VE settings. Overall, these findings indicate that while virtual reality can be a useful design research tool, caution is needed when interpreting and generalizing its results. © 2024 Elsevier Ltd",Spatial learning; Spatial navigation; Virtual reality; Wayfinding,E-learning; Navigation; Environmental variables; Human response; Lifespans; Real environments; Real-world; Research platforms; Spatial learning; Spatial navigation; Uncertainty; Way finding; adult; article; cognition; controlled study; ecological validity; female; human; lifespan; male; spatial learning; spatial memory; spatial orientation; virtual reality; workload; Virtual reality,Article,Final,,Scopus,2-s2.0-85190277665,Gaming / VR
Fu Q.-W.; Liu Q.-H.; Hu T.,"Fu, Qian-Wen (58498630700); Liu, Qing-Hua (57226866877); Hu, Tao (57759847700)",58498630700; 57226866877; 57759847700,MULTI-OBJECTIVE OPTIMIZATION RESEARCH ON VR TASK SCENARIO DESIGN BASED ON COGNITIVE LOAD,2024,"Facta Universitatis, Series: Mechanical Engineering",22,2,,293,313,20.0,4,10.22190/FUME240122029F,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85201379824&doi=10.22190%2fFUME240122029F&partnerID=40&md5=9325277e9ffc12d1433ec7917b3e6701,"In order to improve the efficiency of information acquisition and task selection in Virtual Reality (VR) systems, enhance the interactive experience, and reduce cognitive load for users, it is crucial to effectively organize and leverage user cognitive psychology and design elements during the VR scene design phase. This paper focuses on analyzing the low cognitive load requirements of users and the need for a satisfactory user perceptual experience based on the cognitive resource theory. We propose a method for optimizing the design of VR system scenario tasks under low cognitive load requirements. By utilizing human-computer hybrid intelligent assistance for predicting user cognitive load and incorporating intelligent optimization genetic algorithms into the optimization of VR system design elements, we aim to minimize cognitive load as the objective function based on the principle of low cognitive load. Important knowledge granularity nodes are used as fitness functions in the optimization process of VR system design resource elements. An application study is conducted, combining the multi-channel cognition in a smart city VR system task information interface, to optimize the system resource features. The study validates and compares the solutions obtained through traditional design processes and the solutions optimized by the method proposed in this paper, using virtual reality eye-tracking experiments for the same design task requirements in VR systems. The results demonstrate that users experience lower cognitive load and better task operation experience when interacting with the optimized solutions proposed in this paper. Therefore, the optimization method studied in this paper can serve as a reference for the construction of virtual reality systems. © 2024 by University of Niš, Serbia.",Cognitive load; Human-computer hybrid intelligence; Multi-objective optimization; Task scenarios; Virtual reality,Cognitive systems; Cognitive loads; Design elements; Human-computer hybrid intelligence; Hybrid intelligence; Multi-objectives optimization; Optimisations; Optimization researches; Task scenario; Virtual reality system; Virtual reality system designs; Virtual environments,Article,Final,All Open Access; Gold Open Access,Scopus,2-s2.0-85201379824,Gaming / VR
Wechsler T.F.; Kocur M.; Schumacher S.; Rubenbauer M.; Ruider A.; Brockelmann M.; Lankes M.; Wolff C.; Mühlberger A.,"Wechsler, Theresa F (57210105561); Kocur, Martin (57197792311); Schumacher, Sandra (58554581500); Rubenbauer, Mirjam (58555406400); Ruider, Andreas (58554442300); Brockelmann, Martin (7801531982); Lankes, Michael (18935743600); Wolff, Christian (57203714801); Mühlberger, Andreas (6507375232)",57210105561; 57197792311; 58554581500; 58555406400; 58554442300; 7801531982; 18935743600; 57203714801; 6507375232,Looking fear in the eye: Gamified virtual reality exposure towards spiders for children using attention based feedback,2024,Clinical Child Psychology and Psychiatry,29,3,,1121,1136,15.0,2,10.1177/13591045231194103,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85169110070&doi=10.1177%2f13591045231194103&partnerID=40&md5=2ca2c2bc9d7edc4aa1c59250d2d82c79,"Many children around the globe suffer from spider phobia. Virtual reality exposure therapy is an effective phobia treatment, but so far predominantly tailored for adults. A gamified approach utilizing gaze interaction would allow for a more child-friendly and engaging experience, and provide the possibility to foster working mechanisms of exposure therapy. We developed an application in which children make spiders change in positively connoted ways (e.g., make them dance or shrink) if sufficient visual attention towards them is captured via eye tracking. Thereby, motivation for and positive affects during exposure towards spiders are aspired. In this pilot study on 21 children without (n = 11) and with fear of spiders (n = 10), we examined positive and negative affect during exposure to a virtual spider and to different gaze-related transformations of the spider within a quasi-experimental design. Within a one-group design, we additionally examined fear of spiders in spider fearful children before and one week after the intervention. We found that significantly more positive than negative affect was induced by the spiders’ transformations in children without and with fear of spiders. Fear of spiders was furthermore significantly reduced in spider-fearful children, showing large effect sizes (d >.80). Findings indicate eligibility for future clinical use and evaluation in children with spider phobia. © The Author(s) 2023.",anxiety disorder; eye tracking; gamified virtual reality exposure therapy; gaze interaction; human computer interaction; positive affect; serious game; specific phobia; Spider phobia; visual attention,"Adolescent; Animals; Attention; Child; Eye-Tracking Technology; Fear; Female; Humans; Implosive Therapy; Male; Phobia, Specific; Phobic Disorders; Pilot Projects; Spiders; Virtual Reality; Virtual Reality Exposure Therapy; adolescent; animal; attention; child; eye-tracking technology; fear; female; human; implosive therapy; male; phobia; pilot study; procedures; psychology; spider; therapy; virtual reality; virtual reality exposure therapy",Article,Final,All Open Access; Green Open Access; Hybrid Gold Open Access,Scopus,2-s2.0-85169110070,Gaming / VR
Liu B.; Moyle B.; Kralj A.; Li Y.,"Liu, Biqiang (57204877680); Moyle, Brent (36440163600); Kralj, Anna (35762888600); Li, Yaoqi (56288887900)",57204877680; 36440163600; 35762888600; 56288887900,EYE-TRACKING IN TOURISM,2024,Tourism Social Science Series,27,,,259,272,13.0,1,10.1108/S1571-504320240000027020,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85195611169&doi=10.1108%2fS1571-504320240000027020&partnerID=40&md5=833fae797140715a0976d75909b1d625,"Visual stimuli are integral for the destination selection process, as well as for the delivery of unique, novel and compelling tourist experiences. Emerging techniques, such as eye-tracking, are effective for mapping tourists’ visual interests and paths, presenting an opportunity to identify patterns of visual attention, which provide insights into the underlying cognitive processes which underpin experiences. Building on a systematic review of the progress and development of eye-tracking in tourism field, this chapter summarises five main current research contexts for application and five future research directions. It also narrows the gap between eye-tracking and cognitive psychology by critically examining bottom-up and top-down attentional mechanisms. © 2024 Noel Scott, Brent Moyle, Ana Cláudia Campos, Liubov Skavronskaya and Biqiang Liu.",attention; cognitive psychology; experimental design; eye movement; Eye-tracking; heat map; literature review,,Book chapter,Final,,Scopus,2-s2.0-85195611169,Gaming / VR
Yan J.; Lv J.; Hou Y.; Mo X.,"Yan, Jiahao (59178056100); Lv, Jian (57208089799); Hou, Yukang (57712305300); Mo, Xinzhu (59177603800)",59178056100; 57208089799; 57712305300; 59177603800,Research on the influence of eye movement interaction frequency on visual fatigue in virtual reality; [虚拟现实中眼动交互频率对视觉疲劳影响的研究],2024,Journal of Graphics,45,3,,528,538,10.0,0,10.11996/JG.j.2095-302X.2024030528,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85196307854&doi=10.11996%2fJG.j.2095-302X.2024030528&partnerID=40&md5=4ef2248ac3f42e6eb74995ed2266fcb1,"To investigate whether the frequency of eye movement interaction in virtual reality (VR) causes visual fatigue in users, 25 participants were selected for a visual fatigue experiment in a VR room. Eight interaction frequencies were set within the range of 0.2 to 1.6 Hz, and each participant underwent a 20-minute experiment at each frequency. Data on pupil diameter and blink frequency were captured using the built-in eye tracker in the head-mounted display (HMD) and were then subjected to linear interpolation and noise reduction processing. Firstly, subjective evaluations of the participants were recorded using a five-level fatigue scale, and the relative change rate in pupil diameter was utilized to reflect the degree of fatigue. Then, Spearman correlation analysis was employed to explore the relationship between subjective comfort ratings and blink frequency. Additionally, Kruskal-Wallis tests were conducted to analyze the relationship between blink frequency and interaction frequency. Finally, the proposed method was validated using the FAST disassembly and assembly robot digital twin system in VR. The experimental results demonstrated that the pupil diameter variation rate was minimal at 0.6 Hz, ranging from −1.86% to 2.26%, indicating a relatively comfortable interaction frequency. The highest blink frequency was 52 blinks per minute, with the highest level of visual fatigue. Within the frequency range of 0.2 to 0.6 Hz, the participants’ visual fatigue increased as the interaction frequency decreased. However, within the range of 0.8 to 1.6 Hz, the visual fatigue increased with the increase in frequency, with the lowest blink frequency observed at 0.6 Hz. Thus, appropriate eye movement interaction frequencies can reduce the degree of visual fatigue. © 2024 Editorial of Board of Journal of Graphics. All rights reserved.",blinking frequency; eye movement interaction; pupil diameter; virtual reality; visual fatigue,,Article,Final,,Scopus,2-s2.0-85196307854,Gaming / VR
MacArthur C.; Morayko K.; Luz A.; Hancock M.,"MacArthur, Cayley (57192383339); Morayko, Kateryna (57224006288); Luz, Alessandra (57703906000); Hancock, Mark (12239975800)",57192383339; 57224006288; 57703906000; 12239975800,Not All Those Who (Mind-)Wander Are Lost: Exploring Game-Unrelated Thoughts,2024,"Proceedings of the 2024 ACM Designing Interactive Systems Conference, DIS 2024",,,,2125,2142,17.0,0,10.1145/3643834.3661590,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85200384297&doi=10.1145%2f3643834.3661590&partnerID=40&md5=7ecabb1f7b76e6708bf856521bb4678f,"Task-unrelated thoughts (TUTs), colloquially referred to as mindwandering or daydreaming, are phenomena that can interfere with attention and focus, but are also associated with mental health, creativity, and learning. In digital games, it is unclear how players experience game-unrelated thoughts (GUTs), whether GUTs should be encouraged by game designers, or how it may impact player experience. We ran an initial study to confrm whether GUTs are common (50 of 100 participants reported experiencing them). We then collected 840 minutes of gameplay data from 12 participants playing games they: (1) found relaxing, (2) lost track of time in, and (3) spent most hours playing. Eye-tracking data and experience sampling were used to contextualize a phenomenological analysis of gameplay data. We identifed four themes encompassing gameplay, GUTs, and gaze behaviour: these provide a foundation for future research and game design incorporating GUTs. © 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.",daydreaming; games; mind wandering; play experience,Game design; Interactive computer graphics; Software design; Daydreaming; Digital games; Eye-tracking; Game; Gameplay; Games designers; Mental health; Mind wandering; Play experience; Player experience; Eye tracking,Conference paper,Final,,Scopus,2-s2.0-85200384297,Gaming / VR
Gao H.; Kasneci E.,"Gao, Hong (57217014766); Kasneci, Enkelejda (56059892600)",57217014766; 56059892600,Exploring Eye Tracking as a Measure for Cognitive Load Detection in VR Locomotion,2024,Eye Tracking Research and Applications Symposium (ETRA),,,38,,,,3,10.1145/3649902.3655644,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85196484293&doi=10.1145%2f3649902.3655644&partnerID=40&md5=73a6eb70506e1b32450fff865abb11ab,"Eye tracking data has long been recognized as a reliable indicator of user cognitive load levels during human-computer interaction (HCI) tasks. However, its potential in the context of virtual reality (VR) remains relatively unexplored. Here, we present an ongoing study aimed at investigating the feasibility of detecting cognitive load in VR, particularly during VR locomotion, using an eye-tracking-based machine-learning approach. Data were collected using a within-subjects design, with participants performing VR locomotion tasks using five locomotion techniques. Our preliminary analyses validate the effectiveness of leveraging eye-tracking data as informative features in uncovering cognitive load in VR locomotion contexts, which motivates our further explorations.  © 2024 Owner/Author.",Cognitive Load; Eye Tracking; Locomotion.; Virtual Reality,Human computer interaction; User interfaces; Virtual reality; Cognitive loads; Eye-tracking; Load detection; Load levels; Locomotion technique; Locomotion.; Machine learning approaches; Preliminary analysis; Tracking data; Eye tracking,Conference paper,Final,,Scopus,2-s2.0-85196484293,Gaming / VR
Lunkova E.; McCabe S.; Chen J.-K.; Saluja R.S.; Ptito A.,"Lunkova, Ekaterina (57223900990); McCabe, Sarah (59188976700); Chen, Jen-Kai (13410781700); Saluja, Rajeet Singh (36675807500); Ptito, Alain (7003719167)",57223900990; 59188976700; 13410781700; 36675807500; 7003719167,Exploring oculomotor functions in a pilot study with healthy controls: Insights from eye-tracking and fMRI,2024,PLoS ONE,19,6-Jun,e0303596,,,,0,10.1371/journal.pone.0303596,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85196884814&doi=10.1371%2fjournal.pone.0303596&partnerID=40&md5=d25e9ae19f039348039129af4f690032,"Eye-tracking techniques have gained widespread application in various fields including research on the visual system, neurosciences, psychology, and human-computer interaction, with emerging clinical implications. In this preliminary phase of our study, we introduce a pilot test of innovative virtual reality technology designed for tracking head and eye movements among healthy individuals. This tool was developed to assess the presence of mild traumatic brain injury (mTBI), given the frequent association of oculomotor function deficits with such injuries. Alongside eye-tracking, we also integrated fMRI due to the complementary nature of these techniques, offering insights into both neural activation patterns and behavioural responses, thereby providing a comprehensive understanding of oculomotor function. We used fMRI with tasks evaluating oculomotor functions: Smooth Pursuit (SP), Saccades, Anti-Saccades, and Optokinetic Nystagmus (OKN). Prior to the scanning, the testing with a system of VR goggles with integrated eye and head tracking was used where subjects performed the same tasks as those used in fMRI. 31 healthy adult controls (HCs) were tested with the purpose of identifying brain regions associated with these tasks and collecting preliminary norms for later comparison with concussed subjects. HCs' fMRI results showed following peak activation regions: SP-cuneus, superior parietal lobule, paracentral lobule, inferior parietal lobule (IPL), cerebellartonsil (CT); Saccades-middle frontal gyrus (MFG), postcentral gyrus, medial frontal gyrus; Anti-saccades-precuneus, IPL, MFG; OKN-middle temporal gyrus, ACC, postcentral gyrus, MFG, CT. These results demonstrated brain regions associated with the performance on oculomotor tasks in healthy controls and most of the highlighted areas are corresponding with those affected in concussion. This suggests that the involvement of brain areas susceptible to mTBI in implementing oculomotor evaluation, taken together with commonly reported oculomotor difficulties post-concussion, may lead to finding objective biomarkers using eye-tracking tasks. Copyright: © 2024 Lunkova et al. This is an open access article distributed under the terms of the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.",,"Adult; Brain; Eye Movements; Eye-Tracking Technology; Female; Humans; Magnetic Resonance Imaging; Male; Middle Aged; Nystagmus, Optokinetic; Pilot Projects; Pursuit, Smooth; Saccades; Young Adult; biological marker; adult; Article; attention deficit hyperactivity disorder; BOLD signal; brain analysis; cerebrospinal fluid; controlled study; executive function; eye movement control; eye tracking; female; functional magnetic resonance imaging; gray matter; head injury; head movement; human; male; middle aged; neurologic disease; neuroscience; normal human; nuclear magnetic resonance imaging; pilot study; postcentral gyrus; psychology; saccadic eye movement; smooth pursuit eye movement; superior parietal lobule; traumatic brain injury; virtual reality; visual system; white matter; brain; diagnostic imaging; eye movement; eye-tracking technology; nuclear magnetic resonance imaging; pathophysiology; physiologic nystagmus; physiology; procedures; young adult",Article,Final,All Open Access; Gold Open Access; Green Open Access,Scopus,2-s2.0-85196884814,Gaming / VR
Harris D.; Donaldson R.; Bray M.; Arthur T.; Wilson M.; Vine S.,"Harris, David (58424093800); Donaldson, Ross (58794492800); Bray, Max (58794244200); Arthur, Tom (57208675523); Wilson, Mark (55574207642); Vine, Sam (58459702500)",58424093800; 58794492800; 58794244200; 57208675523; 55574207642; 58459702500,Attention computing for enhanced visuomotor skill performance: Testing the effectiveness of gaze-adaptive cues in virtual reality golf putting,2024,Multimedia Tools and Applications,83,21,,60861,60879,18.0,10,10.1007/s11042-023-17973-4,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85181465178&doi=10.1007%2fs11042-023-17973-4&partnerID=40&md5=c9f62ab5976e73264c9c0acefad1d93d,"This work explored how immersive technologies like virtual reality can be exploited for improved motor learning. While virtual reality is becoming a practical replacement for training that is otherwise expensive, dangerous, or inconvenient to deliver, virtual simulations can also enhance the learning process. Based on the concept of ‘attention computing’, we developed and tested a novel ‘gaze-adaptive’ training method within a virtual putting environment augmented with eye and motion tracking. To our knowledge, this work is the first application of attention computing and adaptive virtual reality to sports skill training. Novice golfers were randomly assigned to either standard putting practice in virtual reality (control) or gaze-adaptive training conditions. For gaze-adaptive training, the golf ball was sensitive to the participant’s gaze and illuminated when fixated upon, to prompt longer and more stable pre-shot fixations. We recorded the effect of these training conditions on task performance, gaze control, and putting kinematics. Gaze-adaptive training was successful in generating more expert-like gaze control and putting kinematics, although this did not transfer to improved performance outcomes within the abbreviated training paradigm. These findings suggest that gaze-adaptive environments can enhance visuomotor learning and may be a promising method for augmenting virtual training environments. © The Author(s) 2024.",Adaptive VR; Attention; Attention computing; Eye movement modelling; Eye movement training,E-learning; Golf; Kinematics; Learning systems; Virtual reality; Adaptive training; Adaptive VR; Attention; Attention computing; Eye movement model; Eye movement training; Gaze control; Performance testing; Training conditions; Visuomotors; Eye movements,Article,Final,All Open Access; Hybrid Gold Open Access,Scopus,2-s2.0-85181465178,Gaming / VR
Dæhlen A.; Heldal I.; Rehman A.; Ali Q.; Katona J.; Kövári A.; Stefanut T.; Ferreira P.D.C.; Costescu C.,"Dæhlen, Are (58026837400); Heldal, Ilona (6506576998); Rehman, Abdul (59181221500); Ali, Qasim (57219972381); Katona, Jozsef (56208740200); Kövári, Attila (57204630743); Stefanut, Teodor (22035938400); Ferreira, Paula Da Costa (56293098000); Costescu, Cristina (57026457500)",58026837400; 6506576998; 59181221500; 57219972381; 56208740200; 57204630743; 22035938400; 56293098000; 57026457500,Towards More Accurate Help: Informing Teachers how to Support NDD Children by Serious Games and Eye Tracking Technologies,2024,Eye Tracking Research and Applications Symposium (ETRA),,,69,,,,2,10.1145/3649902.3653943,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85196561554&doi=10.1145%2f3649902.3653943&partnerID=40&md5=fdbb54a4353d146eaf145b577e7e225c,"The vision behind this research is to develop a platform that supports children with neurodevelopmental disorders (NDD) in handling their difficulties. This will be done by informing their teachers about each child's NDD specificity, personal ability, and learning progress through standardized tasks that allow tailoring the tasks to personalized requirements. The aim of this work in progress is to 1) examine the role of using eye tracking (ET) technologies to inform teachers to support NDD children through a platform, 2) provide an example for using ET data, and 3) show how ET data can be combined with Serious Games (SG) output in a pipeline. The results show the necessary requirements for the experimental setup with a focus on informing the teachers, the influence of inherent limitations of the participant pool, and illustrate how the ET and SG results can be used to communicate status for sustained attention.  © 2024 Owner/Author.",eye tracking; neurodevelopmental disorder; serious games; teaching,Eye tracking; Eye tracking technologies; Eye-tracking; Inherent limitations; Learning progress; Neurodevelopmental disorder; Sustained attention; Teachers'; Tracking data; Serious games,Conference paper,Final,,Scopus,2-s2.0-85196561554,Gaming / VR
Luz A.; Marcher F.; Nacke L.E.; Vogel D.,"Luz, Alessandra (57703906000); Marcher, Florian (59162944900); Nacke, Lennart E. (25723492300); Vogel, Daniel (8435582600)",57703906000; 59162944900; 25723492300; 8435582600,Encouraging Disengagement: Using Eye Tracking to Examine Attention with Different Levels of Juicy Design,2024,ACM International Conference Proceeding Series,,,8,,,,3,10.1145/3656650.3656694,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85195424987&doi=10.1145%2f3656650.3656694&partnerID=40&md5=87c094312826ac81e8ed77f47922b4cd,"Juicy design, typically used in games, involves adding non-functional visual embellishments to increase engagement. We investigate if too much or too little juicy design can lower attention. A controlled experiment examines the application of four levels of juicy elements to a target stimulus for saccade and smooth pursuit eye tracking tasks. Pupil size, blink rate, and questionnaires are used to estimate levels of attention. The results suggest that highly stimulating juicy elements paired with an engaging task, followed by a non-task rest break employing less stimulation, indicated decreased levels of attention and engagement. We discuss the implications of these findings and provide use cases for health, games, and ethics. © 2024 ACM.",attention; design; disengagement; psychophysiology,Eye movements; Game design; Attention; Blink rates; Controlled experiment; Disengagement; Eye-tracking; Non-functional; Pupil size; Smooth pursuit; Target stimulus; Visual embellishments; Eye tracking,Conference paper,Final,,Scopus,2-s2.0-85195424987,Gaming / VR
Caruana N.; Nalepka P.; Perez G.A.; Inkley C.; Munro C.; Rapaport H.; Brett S.; Kaplan D.M.; Richardson M.J.; Pellicano E.,"Caruana, Nathan (56251828400); Nalepka, Patrick (57021812300); Perez, Glicyr A (58734878200); Inkley, Christine (57211633714); Munro, Courtney (58734512500); Rapaport, Hannah (57208555067); Brett, Simon (57204534211); Kaplan, David M (40761453600); Richardson, Michael J (8244310700); Pellicano, Elizabeth (6507658484)",56251828400; 57021812300; 58734878200; 57211633714; 58734512500; 57208555067; 57204534211; 40761453600; 8244310700; 6507658484,Autistic young people adaptively use gaze to facilitate joint attention during multi-gestural dyadic interactions,2024,Autism,28,6,,1565,1581,16.0,3,10.1177/13623613231211967,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85178310994&doi=10.1177%2f13623613231211967&partnerID=40&md5=879d765f6537a16f2995670172bc844d,"Autistic people often experience difficulties navigating face-to-face social interactions. Historically, the empirical literature has characterised these difficulties as cognitive ‘deficits’ in social information processing. However, the empirical basis for such claims is lacking, with most studies failing to capture the complexity of social interactions, often distilling them into singular communicative modalities (e.g. gaze-based communication) that are rarely used in isolation in daily interactions. The current study examined how gaze was used in concert with communicative hand gestures during joint attention interactions. We employed an immersive virtual reality paradigm, where autistic (n = 22) and non-autistic (n = 22) young people completed a collaborative task with a non-autistic confederate. Integrated eye-, head- and hand-motion-tracking enabled dyads to communicate naturally with each other while offering objective measures of attention and behaviour. Autistic people in our sample were similarly, if not more, effective in responding to hand-cued joint attention bids compared with non-autistic people. Moreover, both autistic and non-autistic people demonstrated an ability to adaptively use gaze information to aid coordination. Our findings suggest that the intersecting fields of autism and social neuroscience research may have overstated the role of eye gaze during coordinated social interactions. Lay abstract: Autistic people have been said to have ‘problems’ with joint attention, that is, looking where someone else is looking. Past studies of joint attention have used tasks that require autistic people to continuously look at and respond to eye-gaze cues. But joint attention can also be done using other social cues, like pointing. This study looked at whether autistic and non-autistic young people use another person’s eye gaze during joint attention in a task that did not require them to look at their partner’s face. In the task, each participant worked together with their partner to find a computer-generated object in virtual reality. Sometimes the participant had to help guide their partner to the object, and other times, they followed their partner’s lead. Participants were told to point to guide one another but were not told to use eye gaze. Both autistic and non-autistic participants often looked at their partner’s face during joint attention interactions and were faster to respond to their partner’s hand-pointing when the partner also looked at the object before pointing. This shows that autistic people can and do use information from another person’s eyes, even when they don’t have to. It is possible that, by not forcing autistic young people to look at their partner’s face and eyes, they were better able to gather information from their partner’s face when needed, without being overwhelmed. This shows how important it is to design tasks that provide autistic people with opportunities to show what they can do. © The Author(s) 2023.",eye contact; gaze; non-verbal communication; social interaction; virtual reality,"Adolescent; Attention; Autistic Disorder; Case-Control Studies; Cues; Female; Fixation, Ocular; Gestures; Humans; Interpersonal Relations; Male; Social Interaction; Young Adult; 10-item Autism Quotient; adolescent; adult; Article; attention; autism; Autism Diagnostic Observation Schedule 2 score; Brief 6-item state anxiety scale; child; clinical article; cognition; eye movement; eye tracking; female; Full-Scale IQ using all four subtests; gaze; gesture; human; human experiment; joint attention; male; multi-gestural dyadic interactions; Perceptual Reasoning Index; scoring system; self report; social cognition; social interaction; social network; Verbal  Comprehension Index; virtual reality; Wechsler intelligence scale; association; case control study; eye fixation; gesture; human relation; psychology; young adult",Article,Final,All Open Access; Green Open Access; Hybrid Gold Open Access,Scopus,2-s2.0-85178310994,Gaming / VR
Severitt B.R.; Lenhart P.; Hosp B.W.; Castner N.J.; Wahl S.,"Severitt, Björn Rene (57892229100); Lenhart, Patrizia (58305494700); Hosp, Benedikt Werner (57202890167); Castner, Nora Jane (57193611337); Wahl, Siegfried (56556181900)",57892229100; 58305494700; 57202890167; 57193611337; 56556181900,Communication breakdown: Gaze-based prediction of system error for AI-assisted robotic arm simulated in VR,2024,Eye Tracking Research and Applications Symposium (ETRA),,,5,,,,1,10.1145/3649902.3653339,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85196550234&doi=10.1145%2f3649902.3653339&partnerID=40&md5=c3d3bc906968343655ec3934a76751dd,"Neurological degenerative conditions can affect motor functions, making mobility daunting. Recent configurations of mobility devices that leverage artificial intelligence (AI) show its ability to handle complex information like user input. We create a virtual reality environment to measure participants' reactions to correct and incorrect feedback from an AI-assistance system. Using gaze to evaluate these reactions, we investigate whether we can automatically predict an upcoming system error. Our results show that gaze reactions occur within 300 ms when the system highlights user input, but the delay extends to 1 second without highlighting. Subject dependent gaze behavior proved complicated for developing a generalizable model based on previous work using TCNs for online recognition of upcoming errors. Therefore, more adaptable models for individuals may be a better alternative for gaze-based accessibility systems.  © 2024 Owner/Author.",Accessibility; Error Prediction; Eye movements and cognition; Gaze Interaction; Machine Learning,Errors; Forecasting; Machine learning; Virtual reality; Accessibility; Communication breakdowns; Condition; Error prediction; Eye movement and cognition; Gaze interaction; Machine-learning; Motor function; System errors; User input; Eye movements,Conference paper,Final,,Scopus,2-s2.0-85196550234,Gaming / VR
Szczepaniak D.; Harvey M.; Deligianni F.,"Szczepaniak, Dominik (57472957100); Harvey, Monika (7202675460); Deligianni, Fani (6506096408)",57472957100; 7202675460; 6506096408,Predictive Modelling of Cognitive Workload in VR: An Eye-Tracking Approach,2024,Eye Tracking Research and Applications Symposium (ETRA),,,46,,,,3,10.1145/3649902.3655642,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85196499218&doi=10.1145%2f3649902.3655642&partnerID=40&md5=5b69a203585f711b0799242af4d1555c,"Cognitive training can boost and sharpen the brain's abilities to remember, focus, and switch between different tasks. One of the key elements of cognitive training is cognitive load. It allows a manipulation of the intensity of the intervention to suit the participant's ability level and keep the session enjoyable, i.e. neither too frustrating/hard nor too boring/easy). However, measuring cognitive workload in an objective way is still under-researched and difficult. Here, we have developed a novel sustained attention Virtual Reality (VR) task, using Unity, that aims to predict load in a controlled manner. We demonstrate promising results in that machine learning algorithms can identify perceived as well as objective difficulty of the game accurately, using a combination of eye-tracking and physiological data obtained directly within the VR environment.  © 2024 Owner/Author.",Cognitive Load; Cognitive Training; Eye-Tracking; Physiological Data; Virtual Reality,Cognitive systems; E-learning; Learning algorithms; Machine learning; Psychophysiology; Virtual reality; Cognitive loads; Cognitive training; Cognitive workloads; Eye-tracking; Key elements; Machine learning algorithms; Physiological data; Predictive models; Sustained attention; Tracking approaches; Eye tracking,Conference paper,Final,,Scopus,2-s2.0-85196499218,Gaming / VR
Li X.; Ren X.; Suzuki X.; Yamaji N.; Fung J.; Gondo Y.,"Li, Xiaoxuan (57219776494); Ren, Xiangshi (7401875870); Suzuki, Xin (58775118500); Yamaji, Naoaki (58774973300); Fung, Johnny (58775164600); Gondo, Yasuyuki (55800810400)",57219776494; 7401875870; 58775118500; 58774973300; 58775164600; 55800810400,Designing a Multisensory VR Game Prototype for Older Adults - the Acceptability and Design Implications,2024,Conference on Human Factors in Computing Systems - Proceedings,,,20,,,,8,10.1145/3613904.3642948,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85194839374&doi=10.1145%2f3613904.3642948&partnerID=40&md5=9cc817959908c2366a546a6d43c55c13,"Simultaneous declines in visual function (e.g., dynamic visual acuity), cognitive ability (e.g., cognitive control/multitasking), and physical function (e.g., balance) are major symptoms of aging. Integrating stimulation for those sensory channels into a game could be a suitable way for older adults to engage in long-term health interventions. However, existing game design has not considered the relationship and synergistic impact of multisensory channels of dynamic visual acuity, cognitive ability, and physical function for older adults. We therefore developed the first multisensory VR game system prototype based on cognitive psychology paradigms (e.g., multitasking and Go/No-Go tasks), full-body movement (limb movement), and dynamic visual acuity exercises (horizontal, vertical and forward-backward eye movements) in the VR system environment. We then conducted an experiment to measure the acceptability (in terms of e.g., cybersickness, mental workload, etc.) of our VR game for older adults. The young adults and a PC task were included for comparisons. Qualitative and quantitative results showed that older adults did not experience cybersickness in either sitting or standing postures during the VR gameplay; they well-accepted the workload of the VR game compared to the PC task. Our findings revealed that the design combination of three sensory channels shows synergistic benefits for older adults. Our game encourages older adults to engage in extensive body movement in sitting and standing postures, this is particularly important to people with disabilities who cannot stand. Design implications are provided for the future development and implementation of VR game design for older adults. Our work provides empirical support for the acceptability of multisensory VR systems in older adults, and contributes to the future design of VR games for older adults. © 2024 Copyright held by the owner/author(s)",Acceptability; Cognitive ability; Cybersickness; Dynamic visual acuity; Older adults; Physical ability; VR Game; Workload,Design; Dynamics; Eye movements; Acceptability; Cognitive ability; Cybersickness; Design implications; Dynamic visual acuities; Multisensory; Older adults; Physical ability; VR game; Workload; Game design,Conference paper,Final,All Open Access; Bronze Open Access,Scopus,2-s2.0-85194839374,Gaming / VR
Kim J.; Kim T.; Lee J.,"Kim, Jinwook (57391842900); Kim, Taesu (57226036172); Lee, Jeongmi (37034338500)",57391842900; 57226036172; 37034338500,VR-SSVEPeripheral: Designing Virtual Reality Friendly SSVEP Stimuli using Peripheral Vision Area for Immersive and Comfortable Experience,2024,Conference on Human Factors in Computing Systems - Proceedings,,,364,,,,0,10.1145/3613905.3651084,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85194149416&doi=10.1145%2f3613905.3651084&partnerID=40&md5=a35845e17fab45b5e564a397b743aedb,"Recent VR HMDs embed various bio-sensors (e.g., EEG, eye-tracker) to expand the interaction space. Steady-state visual evoked potential (SSVEP) is one of the most utilized methods in BCI, and recent studies are attempting to design novel VR interactions with it. However, most of them suffer from usability issues, as SSVEP uses flickering stimuli to detect target brain signals that could cause eye fatigue. Also, conventional SSVEP stimuli are not tailored to VR, taking the same form as in a 2D environment. Thus, we propose VR-friendly SSVEP stimuli that utilize the peripheral, instead of the central, vision area in HMD. We conducted an offline experiment to verify our design (n=20). The results indicated that VR-SSVEPeripheral was more comfortable than the conventional one (Central) and functional for augmenting synchronized brain signals for SSVEP detection. This study provides a foundation for designing a VRsuitable SSVEP system and guidelines for utilizing it. © 2024 Association for Computing Machinery. All rights reserved.",Brain-Computer Interface; Electroencephalography (EEG); Immersion; Steady State Visually Evoked Potentials (SSVEP); Usability; Virtual Reality,Brain computer interface; Electroencephalography; Electrophysiology; Eye tracking; Interface states; Signal detection; Bio sensor; Brain signals; Electroencephalography; Eye trackers; Immersion; Immersive; Peripheral vision; Steady state visually evoked potential; Steady-state visually evoked potential; Usability; Virtual reality,Conference paper,Final,All Open Access; Bronze Open Access,Scopus,2-s2.0-85194149416,Gaming / VR
Wang P.; Miller M.R.; Queiroz A.C.M.; Bailenson J.N.,"Wang, Portia (58068816300); Miller, Mark R. (57200524331); Queiroz, Anna C.M. (57202706514); Bailenson, Jeremy N. (6602840468)",58068816300; 57200524331; 57202706514; 6602840468,"Socially Late, Virtually Present: The Effects of Transforming Asynchronous Social Interactions in Virtual Reality",2024,Conference on Human Factors in Computing Systems - Proceedings,,,794,,,,8,10.1145/3613904.3642244,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85194853716&doi=10.1145%2f3613904.3642244&partnerID=40&md5=5a70f2e4dea48e1494689096a4162a22,"Social Virtual Reality (VR) typically entails users interacting in real time. However, asynchronous Social VR presents the possibility of combining the convenience of asynchronous communication with the high presence of VR. Because the tools to easily record and replay VR social interactions are fairly new, scholars have not yet examined how users perceive asynchronous VR social interactions, and how nonverbal transformations of recorded interactions influence user behavior. In this work, we study nonverbal transformations of group interactions around proxemics and gaze and present results from an exploratory user study (N=128) investigating their effects. We found that the combination of spatial accommodation and added gaze increases social presence, perceived attention, and mutual gaze. Results also showed an inverse relationship between interpersonal distance and perceived levels of dominance and threat of the recorded group. Finally, we outline implications for educators and virtual meeting organizers to incorporate these transformations into real-world scenarios. © 2024 Copyright held by the owner/author(s)",Eye Gaze; Proxemics; Social Interaction; Transformed Social Interaction; Virtual Reality,Behavioral research; Asynchronous communication; Eye-gaze; Nonverbals; Proxemic; Real- time; Record-and-replay; Social interactions; Transformed social interaction; User behaviors; Virtual reality,Conference paper,Final,All Open Access; Bronze Open Access,Scopus,2-s2.0-85194853716,Gaming / VR
Mikawa Y.; Fukiage T.,"Mikawa, Yuri (57205558783); Fukiage, Taiki (36188180600)",57205558783; 36188180600,Low-Latency Ocular Parallax Rendering and Investigation of Its Effect on Depth Perception in Virtual Reality,2024,IEEE Transactions on Visualization and Computer Graphics,30,5,,2228,2238,10.0,0,10.1109/TVCG.2024.3372078,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85187360115&doi=10.1109%2fTVCG.2024.3372078&partnerID=40&md5=57dd44ed51e2a82c2bb6f91e57f07fe4,"With a demand for an immersive experience in virtual/augmented reality (VR/AR) displays, recent efforts have incorporated eye states, such as focus and fixation, into display graphics. Among these, ocular parallax, a small parallax generated by eye rotation, has received considerable attention for its impact on depth perception. However, the substantial latency of head-mounted displays (HMDs) has made it challenging to accurately assess its true effect during free eye movements. To address this issue, we propose a high-speed (360 Hz) and low-latency (4.8 ms) ocular parallax rendering system with a custom-built eye tracker. Using this proposed system, we conducted an investigation to determine the latency requirements necessary for achieving perceptually stable ocular parallax rendering. Our findings indicate that, in binocular viewing, ocular parallax rendering is perceived as significantly less stable than conventional rendering when the latency exceeds 43.72 ms at 1.3 D and 21.50 ms at 2.0 D. We also evaluated the effects of ocular parallax rendering on binocular fusion and monocular depth perception under free viewing conditions. The results demonstrated that ocular parallax rendering can enhance binocular fusion but has a limited impact on depth perception under monocular viewing conditions when latency is minimized. © 1995-2012 IEEE.",binocular fusion; depth perception; eye's front-nodal-point tracking; low-latency feedback system; Ocular parallax,Computer Graphics; Depth Perception; Motion Perception; Virtual Reality; Vision Disparity; Augmented reality; Binoculars; Eye movements; Eye tracking; Geometrical optics; Helmet mounted displays; Rendering (computer graphics); Stereo image processing; Three dimensional computer graphics; Three dimensional displays; Virtual reality; Binocular fusion; Eye front-nodal-point tracking; Feedback systems; Gaze-tracking; Low latency; Low-latency communication; Low-latency feedback system; Nodal points; Ocular parallax; Point-tracking; Pupil; Rendering (computer graphic); Three-dimensional display; computer graphics; depth perception; movement perception; virtual reality; Depth perception,Article,Final,,Scopus,2-s2.0-85187360115,Gaming / VR
Narkar A.S.; Michalak J.J.; Peacock C.E.; David-John B.,"Narkar, Anish S. (59098058000); Michalak, Jan J. (57984811800); Peacock, Candace E. (57200309816); David-John, Brendan (57205639875)",59098058000; 57984811800; 57200309816; 57205639875,GazeIntent: Adapting dwell-time selection in VR interaction with real-time intent modeling,2024,Proceedings of the ACM on Human-Computer Interaction,8,ETRA,226,,,,5,10.1145/3655600,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85195032800&doi=10.1145%2f3655600&partnerID=40&md5=58b7765ec9eed06eaee73a5a31726f8e,"The use of ML models to predict a user’s cognitive state from behavioral data has been studied for various applications which includes predicting the intent to perform selections in VR. We developed a novel technique that uses gaze-based intent models to adapt dwell-time thresholds to aid gaze-only selection. A dataset of users performing selection in arithmetic tasks was used to develop intent prediction models (F1 = 0.94). We developed GazeIntent to adapt selection dwell times based on intent model outputs and conducted an end-user study with returning and new users performing additional tasks with varied selection frequencies. Personalized models for returning users effectively accounted for prior experience and were preferred by 63% of users. Our work provides the field with methods to adapt dwell-based selection to users, account for experience over time, and consider tasks that vary by selection frequency. © 2024 Copyright held by the owner/author(s).",algorithms; Eye movements and cognition; Gaze-controlled and hands-free interfaces; Gaze-input in augmented or mixed reality systems; Machine-learning methods; Novel systems; Predictive models; Task-specific evaluations,Forecasting; Learning systems; Mixed reality; Real time systems; Augmented reality systems; Eye movement and cognition; Free interface; Gaze-controled and hand-free interface; Gaze-input in augmented or mixed reality system; Hands-free; Machine learning methods; Mixed reality systems; Novel system; Predictive models; Task-specific evaluation; Eye movements,Article,Final,All Open Access; Green Open Access; Hybrid Gold Open Access,Scopus,2-s2.0-85195032800,Gaming / VR
P. Salgado D.; Fallon S.; Qiao Y.; L. M. Naves E.,"P. Salgado, Debora (57203144244); Fallon, Sheila (24767808000); Qiao, Yuansong (22036417400); L. M. Naves, Eduardo (59153101900)",57203144244; 24767808000; 22036417400; 59153101900,WheelSimPhysio-2023 dataset: Physiological and questionnaire-based dataset of immersive multisensory wheelchair simulator from 58 participants,2024,Data in Brief,54,,110535,,,,1,10.1016/j.dib.2024.110535,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85194902298&doi=10.1016%2fj.dib.2024.110535&partnerID=40&md5=69a4539f334cae80916fbda017e1ccf7,"This data paper presents a unique multimodal dataset collected from a comprehensive experiment using a wheelchair training simulator. The dataset consists of quantitative and qualitative data that represents the user's experience and performance. Participants engaged in a series of navigational tasks in a simulated environment under two distinct system configuration conditions: a. a conventional monitor display and b. a virtual reality (VR) headset. The monitor group has a total of 24 participants data while using the simulator with a standard display and then other two groups of 18 and 16 respectively using the VR headset with a different wheelchair's speed profile. It was collected data from total of 58 participants. The dataset includes physiological data - Heart Rate Variability (HRV), Electrodermal Activity (EDA), Acceleration (ACC), Skin Temperature, Heart Rate (HR), and Blood Volume Pulse (BVP) - collected during both experiments. Additionally, for the standard display condition, more detailed data comprising Electroencephalography (EEG) and eye-tracking metrics were recorded to provide insights into cognitive load and visual attention patterns. System metrics captured from the simulator provide an objective performance report, including task completion times, error rates (collision of the virtual wheelchair), number of joystick commands. Also, the navigation efficiency data is complemented by post-experiment questionnaires, which gathered subjective responses on user experience, perceived difficulty, the user immersive levels, arousal, and simulator sickness symptoms. This dataset is valuable for researchers and practitioners in the fields of assistive technology, human-computer interaction, and rehabilitation. It offers metrics to a comprehensive view of how different display technologies influence the user experience in wheelchair simulation training. The data allows for in-depth analysis of physiological responses, cognitive engagement, and subjective perceptions, providing a foundation for future research on effective wheelchair training methodologies and the potential benefits of VR in rehabilitation settings. © 2024 The Authors",Arousal; Assistive technology; Cognitive load; EDA; EEG; HRV; XR,Behavioral research; Display devices; Electrophysiology; Eye tracking; Heart; Human computer interaction; Physiological models; Virtual reality; Wheelchairs; Arousal; Assistive technology; Cognitive loads; Electrodermal activity; Heart rate variability; Immersive; Multisensory; Users' experiences; Virtual-reality headsets; XR; Electroencephalography,Data paper,Final,All Open Access; Gold Open Access; Green Open Access,Scopus,2-s2.0-85194902298,Gaming / VR
Bigne E.; Simonetti A.; Guixeres J.; Alcaniz M.,"Bigne, Enrique (55132662600); Simonetti, Aline (57211819253); Guixeres, Jaime (26423690300); Alcaniz, Mariano (36921902100)",55132662600; 57211819253; 26423690300; 36921902100,Visual attention and product interaction: a neuroscientific study on purchase across two product categories in a virtual store,2024,International Journal of Retail and Distribution Management,52,4,,389,406,17.0,7,10.1108/IJRDM-02-2023-0067,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85186244966&doi=10.1108%2fIJRDM-02-2023-0067&partnerID=40&md5=6c68e32c67b07e17b697097d3700cc24,"Purpose: This research analyses the searching, interacting and purchasing behavior of shoppers seeking semidurable and fast-moving consumer goods in an immersive virtual reality (VR) store, showing how physical examinations and visual inspections relate to purchases. Design/methodology/approach: Around 60 participants completed two forced-purchase tasks using a head-mounted display with visual and motor-tracking systems. A second study using a pictorial display of the products complemented the VR study. Findings: The findings indicate differences in shopping behavior for the two product categories, with semidurable goods requiring greater inspection and deliberation than fast-moving consumer goods. In addition, visual inspection of the shelf and products was greater than a physical examination through virtual handling for both product categories. The paper also presents relationships between visual inspections and product interactions during the searching stage of purchase decisions. Originality/value: The research consists of two types of implicit measures in this study: eye-tracking and hand-product interactions. This study reveals the suitability of implicit measures for evaluating consumer behavior in VR stores. © 2024, Emerald Publishing Limited.",Eye-tracking; Hand-tracking; Head-mounted displays; Retailers; Virtual reality,,Article,Final,,Scopus,2-s2.0-85186244966,Gaming / VR
Cardoso L.; Fraga C.,"Cardoso, Lucília (57204575057); Fraga, Carla (57189096146)",57204575057; 57189096146,Shaping the Future of Destinations: New Clues to Smart Tourism Research from a Neuroscience Methods Approach,2024,Administrative Sciences,14,6,106,,,,7,10.3390/admsci14060106,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85197177379&doi=10.3390%2fadmsci14060106&partnerID=40&md5=64f5ca3098656e5f9a1e6b56654d9afb,"In the context of the technological era, the smart tourism construct serves as a bridge between human and the artificial worlds, combining social sciences and neurosciences. This study aims to explore smart tourism through neuroscientific methods in order to shape the future of tourism destinations, using a hybrid methodology combining bibliometric techniques and content analysis. The findings reveal the integration of diverse scientific domains, highlighting a transdisciplinary approach. They offer clear evidence that neuroscientific methods in smart tourism integrate multiple areas of scientific knowledge, surpassing disciplinary boundaries. “Destination” stands out alongside “emotion”, “visual attention” and eye tracking (ET). The collaboration network reveals the emergence of a new school, called neurotourism in the 21st century, formed mainly by actors and organizations from the Global North, evoking the need to include the Global South in the research scenario. The predominant methods include ET, heart rate (HR), and electroencephalography (EEG), suggesting triangulation with traditional methods for robust results. Virtual reality emerges as the primary immersive technology, promising insights when integrated with neurosciences. This study’s practical and theoretical contributions guide smart tourism strategies and enhance destination experiences through neuroscientific methods, addressing a gap in the scientific literature while advancing ontological and epistemological understanding. © 2024 by the authors.",emotions in tourism experience; neuroscientific methods; shaping destinations; smart tourism; transdisciplinary approach,,Article,Final,All Open Access; Gold Open Access,Scopus,2-s2.0-85197177379,Gaming / VR
Turkmen R.; Gelmez Z.E.; Batmaz A.U.; Stuerzlinger W.; Asente P.; Sarac M.; Pfeuffer K.; Machuca M.D.B.,"Turkmen, Rumeysa (57673608500); Gelmez, Zeynep Ecem (59003835400); Batmaz, Anil Ufuk (57193638803); Stuerzlinger, Wolfgang (55902405400); Asente, Paul (18433774900); Sarac, Mine (55807561700); Pfeuffer, Ken (36141954200); Machuca, Mayra Donaji Barrera (56452760200)",57673608500; 59003835400; 57193638803; 55902405400; 18433774900; 55807561700; 36141954200; 56452760200,EyeGuide & EyeConGuide: Gaze-based Visual Guides to Improve 3D Sketching Systems,2024,Conference on Human Factors in Computing Systems - Proceedings,,,178,,,,4,10.1145/3613904.3641947,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85194841143&doi=10.1145%2f3613904.3641947&partnerID=40&md5=5cdd5c85ecb894525363dd6b3b28be1f,"Visual guides help to align strokes and raise accuracy in Virtual Reality (VR) sketching tools. Automatic guides that appear at relevant sketching areas are convenient to have for a seamless sketching with a guide. We explore guides that exploit eye-tracking to render them adaptive to the user's visual attention. EyeGuide and EyeConGuide cause visual grid fragments to appear spatially close to the user's intended sketches, based on the information of the user's eye-gaze direction and the 3D position of the hand. Here we evaluated the techniques in two user studies across simple and complex sketching objectives in VR. The results show that gaze-based guides have a positive effect on sketching accuracy, perceived usability and preference over manual activation in the tested tasks. Our research contributes to integrating gaze-contingent techniques for assistive guides and presents important insights into multimodal design applications in VR. © 2024 Copyright held by the owner/author(s)",3D Sketching; 3D User Interface; Eye-Gaze; VR,Behavioral research; Eye tracking; User interfaces; 3D positions; 3D sketching; 3D user interface; Eye-gaze; Eye-tracking; Gaze direction; Sketching tools; Sketchings; Visual Attention; Visual grids; Virtual reality,Conference paper,Final,All Open Access; Bronze Open Access,Scopus,2-s2.0-85194841143,Gaming / VR
Mu M.; Dohan M.; Goodyear A.; Hill G.; Johns C.; Mauthe A.,"Mu, Mu (36928091400); Dohan, Murtada (57205418759); Goodyear, Alison (58578665900); Hill, Gary (57219689547); Johns, Cleyon (57219687068); Mauthe, Andreas (6603558753)",36928091400; 57205418759; 58578665900; 57219689547; 57219687068; 6603558753,User attention and behaviour in virtual reality art encounter,2024,Multimedia Tools and Applications,83,15,,46595,46624,29.0,11,10.1007/s11042-022-13365-2,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85133478963&doi=10.1007%2fs11042-022-13365-2&partnerID=40&md5=4ff3d4d2a0cb24909ecca84c6289eaca,"With the proliferation of consumer virtual reality (VR) headsets and creative tools, content creators are experimenting with new forms of interactive audience experience using immersive media. Understanding user attention and behaviours in virtual environment can greatly inform the creative processes in VR. We developed an abstract VR painting and an experimentation system to study audience art encounters through eye gaze and movement tracking. The data from a user experiment with 35 participants reveal a range of user activity patterns in art exploration. Deep learning models are used to study the connections between the behavioural data and the audience’s background. The work also introduced new integrated methods to visualise user attention for content creators. © The Author(s) 2022.",Eye-tracking; Machine learning; User attention; Virtual reality; VR abstract painting,Arts computing; Behavioral research; Deep learning; E-learning; Eye movements; Learning systems; Virtual reality; Abstract paintings; Content creators; Creative tools; Eye-tracking; Machine-learning; User attention; User behaviors; Virtual reality abstract painting; Virtual reality art; Virtual-reality headsets; Eye tracking,Article,Final,,Scopus,2-s2.0-85133478963,Gaming / VR
Zhao C.; Liu N.; Li S.,"Zhao, Chunzhou (58647033200); Liu, Na (58647111000); Li, Sunnan (57835779300)",58647033200; 58647111000; 57835779300,Study on the relationship between fixation characteristics and hit rate in psychological procedure training of free throw,2024,PLoS ONE,19,5-May,e0293436,,,,1,10.1371/journal.pone.0293436,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85192856329&doi=10.1371%2fjournal.pone.0293436&partnerID=40&md5=eec3579218507ec483c06a09f8ef59ff,"Background Free throw is an important means of scoring in basketball games. With the improvement of basketball competition level and the enhancement of confrontation degree, the number of free throws in the game gradually increases, so the score of free throw will have an important impact on the result of the game. The purpose of this study is to explore the relationship between visual attention characteristics and hit rate of basketball players in free throw psychological procedure training, so as to provide scientific basis for basketball teaching and training. Methods Forty players with similar free throw abilities were randomly assigned to the experimental group (10 males, 10 females) and control group (10 males, 10 females). The experimental group was free throw psychological procedure training, while the control group was trained with routine training, Eye movement indices (number of fixations, fixation duration, and pupil dilation) and the free throw hit rate and analyzed before and after the experiment. Group differences were examined using t-tests, while paired sample t-tests were conducted to compare pre- and post-test results within each group. The training time and training times of the two groups were the same. Results There were significant differences in fixation duration, number of fixations, pupil diameter and free throw hit rate between pre-test and post-test in the experimental group (P < 0.05). Posttest, there were significant differences in number of fixations, fixation duration, pupil diameter and free throw hit rate between the two groups (P < 0.05). There was a significant positive correlation between number of fixations and free throw hit rate in top (P < 0.01), and there was a significant positive correlation between fixation duration and hit rate in front (P < 0.01). Conclusions The psychological procedure training can improve the visual information search strategy and information processing ability of free throw, and significantly improve the free throw hit rate. There was a positive correlation between the front fixation time and the free throw hit rate, and there was a positive correlation between the top number of fixations and the free throw hit rate.  © 2024 Zhao et al. This is an open access article distributed under the terms of the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.",,"Adult; Athletic Performance; Attention; Basketball; Eye Movements; Female; Fixation, Ocular; Humans; Male; Young Adult; adult; Article; basketball player; clinical article; controlled study; data analysis; eye fixation; eye movement; feedback system; female; free throw; human; male; physical education; psychological and psychiatric procedures; pupil diameter; teaching; training; visual information; young adult; athletic performance; attention; basketball; eye fixation; physiology; psychology; randomized controlled trial",Article,Final,All Open Access; Gold Open Access,Scopus,2-s2.0-85192856329,Gaming / VR
Chiappini M.; Dei C.; Micheletti E.; Biffi E.; Storm F.A.,"Chiappini, Mattia (57571201200); Dei, Carla (57571827000); Micheletti, Ettore (58688967900); Biffi, Emilia (26653960400); Storm, Fabio Alexander (56572448700)",57571201200; 57571827000; 58688967900; 26653960400; 56572448700,High-Functioning Autism and Virtual Reality Applications: A Scoping Review,2024,Applied Sciences (Switzerland),14,7,3132,,,,7,10.3390/app14073132,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85192527369&doi=10.3390%2fapp14073132&partnerID=40&md5=8a35af0ec91d5335be4e0de77ff36da5,"In recent years, the number of applications of virtual reality (VR) for the Autism spectrum disorder (ASD) population has increased and has become one of the most suitable tools to address the psychological needs of these individuals. The present scoping review aims to provide a literature mapping of experimental studies that have used immersive and semi-immersive VR for assessments or interventions specifically addressing high-functioning autism. A total of 23 papers were included and analyzed following PRISMA guidelines. The identified studies concerned social skills (11 papers), eye gaze and joint attention (3 papers), motor learning (3 papers), job training (2 papers), and other aims or rationales (4 papers). The evidence shows that, despite the intellectual potential of high-functioning ASD individuals, little research has been conducted to provide interventions that offer concrete training to improve their adaptive functioning. In addition, the percentage of individuals below 18 years of age is representative of half of the included studies, so aiming future studies at the early stages of development might be an asset in preparing the next generation of young adults to cope with age-related challenges, as early assessments and interventions are more likely to produce major long-term effects. © 2024 by the authors.",autism spectrum disorder; high-functioning autism; virtual reality,,Review,Final,,Scopus,2-s2.0-85192527369,Gaming / VR
Fu M.; Liu R.; Liu Q.,"Fu, Meiqing (57197871462); Liu, Rui (36045152400); Liu, Qipeng (58636695900)",57197871462; 36045152400; 58636695900,Why individuals do not use emergency exit doors during evacuations: A virtual reality and eye-tracking experimental study,2024,Advanced Engineering Informatics,60,,102396,,,,10,10.1016/j.aei.2024.102396,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85185328837&doi=10.1016%2fj.aei.2024.102396&partnerID=40&md5=9f4e24ff6ebf8d889c6371134defbf28,"Despite being purposefully designed and strategically placed in buildings to facilitate efficient evacuation, emergency exit doors are frequently underutilized during evacuations. To address this issue, this study explores why individuals often fail to use emergency exit doors during evacuations via a virtual reality and eye-tracking experimental study and investigates human emergency wayfinding behaviors through the lens of perceptions and attention. The results show that individuals’ attention to wayfinding clues is positively related to their exit choices. Failure to notice an emergency exit door is a key contributor to explaining how familiarity affects the usage of the emergency exit door. Participants in the familiarity group were less likely to pay attention to the exit door and thus rarely use it. However, when participants had fixations on the door, their exit choices were not significantly different between the familiarity and unfamiliarity groups. In the present experiment, door color and type did not have a significant impact on participants’ visual attention and exit choices. Specifically, the effect of door type on participants’ exit choices was impaired by the presence of the stairwell sign. The experimental results reveal the conspicuity and visibility of exit doors and explicit and clear wayfinding information are crucial to increasing their usage during indoor evacuations. © 2024 Elsevier Ltd",,Doors; Eye tracking; Virtual addresses; Virtual reality; Emergency exit; Eye-tracking; In-buildings; Through the lens; Visual Attention; Way finding; Behavioral research,Article,Final,,Scopus,2-s2.0-85185328837,Gaming / VR
Ren X.; He J.; Han T.; Liu S.; Lv M.; Zhou R.,"Ren, Xiaofei (58514755900); He, Jian (57193485633); Han, Teng (37072348400); Liu, Songxian (58514756000); Lv, Mengfei (58199270700); Zhou, Rui (58514297900)",58514755900; 57193485633; 37072348400; 58514756000; 58199270700; 58514297900,Exploring the effect of fingertip aero-haptic feedforward cues in directing eyes-free target acquisition in VR,2024,Virtual Reality and Intelligent Hardware,6,2,,113,131,18.0,3,10.1016/j.vrih.2023.12.001,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85192240045&doi=10.1016%2fj.vrih.2023.12.001&partnerID=40&md5=60fea49e816d1e0dfbaf12b884efda40,"Background: The sense of touch plays a crucial role in interactive behavior within virtual spaces, particularly when visual attention is absent. Although haptic feedback has been widely used to compensate for the lack of visual cues, the use of tactile information as a predictive feedforward cue to guide hand movements remains unexplored and lacks theoretical understanding. Methods: This study introduces a fingertip aero-haptic rendering method to investigate its effectiveness in directing hand movements during eyes-free spatial interactions. The wearable device incorporates a multichannel micro-airflow chamber to deliver adjustable tactile effects on the fingertips. Results: The first study verified that tactile directional feedforward cues significantly improve user capabilities in eyes-free target acquisition and that users rely heavily on haptic indications rather than spatial memory to control their hands. A subsequent study examined the impact of enriched tactile feedforward cues on assisting users in determining precise target positions during eyes-free interactions, and assessed the required learning efforts. Conclusions: The haptic feedforward effect holds great practical promise in eyeless design for virtual reality. We aim to integrate cognitive models and tactile feedforward cues in the future, and apply richer tactile feedforward information to alleviate users' perceptual deficiencies. © 2023 Beijing Zhongke Journal Publishing Co. Ltd",Aero-haptic; Feedforward; Haptic; Virtual reality,Behavioral research; Eye movements; Aero-haptic; Feed forward; Hands movement; Haptics; Interactive behavior; Sense of touch; Target acquisition; Virtual spaces; Visual Attention; Virtual reality,Article,Final,All Open Access; Gold Open Access,Scopus,2-s2.0-85192240045,Gaming / VR
Brand J.; Yeum D.; Stewart T.; Emond J.A.; Gilbert-Diamond D.,"Brand, John (55258376100); Yeum, Dabin (57210746044); Stewart, Tessa (58992998100); Emond, Jennifer A. (56402794100); Gilbert-Diamond, Diane (34969411400)",55258376100; 57210746044; 58992998100; 56402794100; 34969411400,"The associations between attentional bias to food cues, parent-report appetitive traits, and concurrent adiposity among adolescents",2024,Eating Behaviors,53,,101874,,,,0,10.1016/j.eatbeh.2024.101874,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85190809133&doi=10.1016%2fj.eatbeh.2024.101874&partnerID=40&md5=bbb717824cd74cc8ab0bf0782fc9d82a,"Objective: To assess whether attentional bias to food cues and appetitive traits are independently and interactively associated with adiposity in adolescents. Method: Eighty-five adolescents, 14–17-years had their attentional bias to food images measured in a sated state by computing eye tracking measures of attention (first fixation duration, cumulative fixation duration) to food and control distractor images that bordered a computer game. Parents reported adolescent appetitive traits including the food approach domains of enjoyment of food, food responsiveness, emotional overeating, and the food avoidance domains of satiety responsiveness and emotional overeating through the Children's Eating Behavior Questionnaire. Results: First fixation bias to food cues was positively associated with enjoyment of food, and negatively associated with satiety responsiveness. In a series of regression models adjusted for relevant covariates, first fixation bias to food cues (β = 0.83, p = 0.007), higher food responsiveness (β = 0.74, p < 0.001), higher emotional overeating (β = 0.51, p = 0.002), and a composite appetite score (β = 1.42, p < 0.001) were each significantly associated with greater BMI z-scores. In models assessing the interactive effects between attentional bias and appetitive traits, higher first fixation bias to food cues interacted synergistically with food responsiveness and emotional overeating in relation to BMI z-score. A synergistic interaction between first fixation bias to food cues and the composite appetite score in relation to BMI z-score was also observed. Conclusion: Individuals with high attentional bias to food cues and obesogenic appetitive traits may be particularly susceptible to weight gain. © 2024 Elsevier Ltd",Appetite of traits; Attentional bias to food cues; CEBQ,Adiposity; Adolescent; Appetite; Attentional Bias; Body Mass Index; Cues; Emotions; Feeding Behavior; Female; Food; Humans; Hyperphagia; Male; Parents; Surveys and Questionnaires; adolescent; adolescent obesity; appetite; appetitive trait; Article; attention; attentional bias; attentional bias to food cues; body mass; Child Eating Behavior Questionnaire; child parent relation; composite appetite score; controlled study; demography; desire to drink; emotional overeating; enjoyment of food; eye tracking; feeding behavior; female; food fussiness; human; male; sample size; satiety responsiveness; scoring system; slowness in eating; video game; appetite; association; emotion; food; hyperphagia; obesity; parent; physiology; psychology; questionnaire,Article,Final,,Scopus,2-s2.0-85190809133,Gaming / VR
Lin P.-H.; Chen H.-J.; Su K.-W.; Chou Y.-J.,"Lin, Po-Hung (37665372500); Chen, Hung-Jen (16038680500); Su, Kuo-Wei (7101635789); Chou, Yen-Ju (58862019500)",37665372500; 16038680500; 7101635789; 58862019500,"Effects of display technique, background complexity, and target size on visual performance evaluation–A case study using the “Spot The Difference” game",2024,International Journal of Industrial Ergonomics,100,,103555,,,,1,10.1016/j.ergon.2024.103555,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85183957942&doi=10.1016%2fj.ergon.2024.103555&partnerID=40&md5=f6c4a7be38870020ef3b6de064e73cc7,"The study explores the effects of display technique, target size, and complexity of the background on searching accuracy, eye tracking performance, visual fatigue, and presence. Twenty-five participants were recruited in the experiment, where two types of display technique (2D, 3D), two types of background complexity (low, high), and two types of target size (200 × 200 pixels, 60 × 60 pixels) were examined on accuracy, fixation time, numbers of fixations, Simulator Sickness Questionnaire (SSQ), and iGroup Presence Questionnaire (IPQ). The results indicated that display technique was significant on accuracy, fixation time, number of fixations, and SSQ. Though having inferior performance in accuracy and SSQ than 2D, 3D demonstrated better performance in the number of fixations and fixation time. Background complexity was significant on accuracy and IPQ, while low complexity has a better performance than that in accuracy and IPQ. Target size was significant in accuracy, where the accuracy of 200 × 200 pixels was better than that of 60 × 60 pixels. From the analysis of area of interest (AOI), participants tended to fix on revised pictures as compared with original pictures. The results of this study could serve as a reference for game manufacturers to design a 3D “Spot The Difference” game in the future. © 2024 Elsevier B.V.",3D; Background complexity; Eye tracking performance; Spot and difference; Target size,Eye tracking; Target tracking; Three dimensional displays; 3d; Background complexity; Eye tracking performance; Eye-tracking; Fixation time; Number of fixations; Simulator sickness; Spot and difference; Target size; Tracking performance; accuracy; adult; Article; background complexity; comparative study; eye tracking; human; iGroup Presence Questionnaire; image display; image display technique; questionnaire; Simulator Sickness Questionnaire; target size; three-dimensional imaging; two-dimensional imaging; vision; visual disorder; visual fatigue; young adult; Pixels,Article,Final,,Scopus,2-s2.0-85183957942,Gaming / VR
Wang Z.; Ng J.T.D.; Que Y.; Hu X.,"Wang, Zuo (57222735437); Ng, Jeremy Tzi Dong (57201810212); Que, Ying (57215926050); Hu, Xiao (55496358400)",57222735437; 57201810212; 57215926050; 55496358400,Unveiling Synchrony of Learners' Multimodal Data in Collaborative Maker Activities,2024,ACM International Conference Proceeding Series,,,,922,928,6.0,4,10.1145/3636555.3636935,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85187549301&doi=10.1145%2f3636555.3636935&partnerID=40&md5=f7bd9c30fc97661f2d8c20c04a9b11c7,"While current evaluation of maker activities has rarely explored students' learning processes, the multi-perspective and multi-level nature of collaboration adds complexity to learning processes of collaborative maker activities. In terms of group dynamics as an important indicator of collaboration quality, extant studies have shown the benefits of synchrony between learners' actions during collaborative learning processes. However, synchrony of learners' cognitive processes and visual attention in collaborative maker activities remains under-explored. Leveraging the multimodal learning analytics (MMLA) approach, this pilot study examines learners' synchrony patterns from multiple modalities of data in the collaborative maker activity of virtual reality (VR) content creation. We conducted a user experiment with five pairs of students, and collected and analyzed their electroencephalography (EEG) signals, eye movement and system log data. Results showed that the five pairs of collaborators demonstrated diverse synchrony patterns. We also discovered that, while some groups exhibited synchrony in one modality of data before becoming not synchronized in another modality, other groups started with a lack of synchrony followed by maintaining synchrony. This study is expected to make methodological and practical contributions to MMLA research and assessment of collaborative maker activities. © 2024 ACM.",Computer-supported collaborative learning; Maker activities; Multimodal learning analytics,Behavioral research; E-learning; Electroencephalography; Electrophysiology; Virtual reality; 'current; Computer Supported Collaborative Learning; Learning process; Maker activity; Multi-modal data; Multi-modal learning; Multi-perspective; Multilevels; Multimodal learning analytic; Student learning process; Eye movements,Conference paper,Final,,Scopus,2-s2.0-85187549301,Gaming / VR
Haskins A.J.; Mentch J.; Van Wicklin C.; Choi Y.B.; Robertson C.E.,"Haskins, Amanda J. (57201212494); Mentch, Jeff (57210840182); Van Wicklin, Caitlin (57211324253); Choi, Yeo Bi (57204476962); Robertson, Caroline E. (57196949630)",57201212494; 57210840182; 57211324253; 57204476962; 57196949630,Brief Report: Differences in Naturalistic Attention to Real-World Scenes in Adolescents with 16p.11.2 Deletion,2024,Journal of Autism and Developmental Disorders,54,3,,1078,1087,9.0,2,10.1007/s10803-022-05850-2,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85144047695&doi=10.1007%2fs10803-022-05850-2&partnerID=40&md5=8f2046c80824423ce9713012a43372b6,"Sensory differences are nearly universal in autism, but their genetic origins are poorly understood. Here, we tested how individuals with an autism-linked genotype, 16p.11.2 deletion (“16p”), attend to visual information in immersive, real-world photospheres. We monitored participants’ (N = 44) gaze while they actively explored 360° scenes via headmounted virtual reality. We modeled the visually salient and semantically meaningful information in scenes and quantified the relative bottom-up vs. top-down influences on attentional deployment. We found, when compared to typically developed control (TD) participants, 16p participants’ attention was less dominantly predicted by semantically meaningful scene regions, relative to visually salient regions. These results suggest that a reduction in top-down relative to bottom-up attention characterizes how individuals with 16p.11.2 deletions engage with naturalistic visual environments. © The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature 2022.",16p.11.2 deletion; Autism; Eyetracking; Visual attention,"Adolescent; Autism Spectrum Disorder; Autistic Disorder; Child; Child Development Disorders, Pervasive; Environment; Genotype; Humans; 16p.11.2 deletion; adolescent; Article; autism; autism-spectrum quotient; chromosome deletion; clinical article; controlled study; DSM-5; DSM-IV; eye-tracking technology; female; gaze; genetic database; genotype; human; intelligence quotient; male; psychometry; self report; virtual reality; visual attention; visual information; child; environment",Article,Final,All Open Access; Green Open Access,Scopus,2-s2.0-85144047695,Gaming / VR
Guo Y.; Elhai J.D.; Montag C.; Wang Y.; Yang H.,"Guo, Yawen (57232270300); Elhai, Jon D. (7003678279); Montag, Christian (23009620300); Wang, Yang (58155112600); Yang, Haibo (36618479600)",57232270300; 7003678279; 23009620300; 58155112600; 36618479600,Problematic mobile gamers have attention bias toward game social information,2024,Computers in Human Behavior,152,,108074,,,,4,10.1016/j.chb.2023.108074,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85179886377&doi=10.1016%2fj.chb.2023.108074&partnerID=40&md5=1d7ac12da88ff4d672b1c7f7937640bd,"Attention bias towards game information influences players' problematic mobile game usage (PMGU). Social experience is an important part of games. This study aimed to explore attention bias mechanisms of problematic mobile gamers for game social information. Experiments 1 and 2 recruited 68 participants (19.82 ± 1.38 years), and used the dot-probe task to investigate attention bias among problematic mobile gamers. Results showed that reaction time and trial-level bias scores (TL-BS) of socially anxious problematic mobile gamers toward game social information were not significantly different from those toward game non-social information. Experiment 3 recruited 35 participants (19.71 ± 1.18 years), and combined eye-tracking technology with the dot-probe task to investigate problematic mobile gamers' attention bias and dynamic visual processing. Results of this last experiment showed that socially anxious problematic mobile gamers’ first fixation latency for game social information was significantly shorter than for game non-social information, and their gaze duration and total fixation duration were significantly longer for social than game non-social information. In summary, the eye tracking experiments give support for the idea that socially anxious problematic mobile gamers show attention bias towards game social information, which is presented as the vigilance-maintenance pattern. © 2023 Elsevier Ltd",Attention bias; Dot-probe task; Eye-tracking; Mobile games; Social anxiety; Social compensation,Eye tracking; Social aspects; Attention bias; Bias mechanisms; Dot-probe task; Eye tracking technologies; Eye-tracking; Mobile games; Social anxieties; Social compensations; Social information; Visual-processing; adolescent; adult; alertness; article; attentional bias; compensation; controlled study; diagnosis; eye tracking; female; game; gaze; human; human experiment; male; normal human; reaction time; social anxiety; young adult; Probes,Article,Final,,Scopus,2-s2.0-85179886377,Gaming / VR
Gao B.; Zhou J.; Yu S.,"Gao, Biao (57212102486); Zhou, Jinhao (58975643000); Yu, Shuangshuang (58700976500)",57212102486; 58975643000; 58700976500,A Study on the Visual Appeal of Eco- Friendly Natural Elements in Metaverse 3D Game Environments,2024,Frontiers in Artificial Intelligence and Applications ,384,,,435,444,9.0,0,10.3233/FAIA240046,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85189760807&doi=10.3233%2fFAIA240046&partnerID=40&md5=8406db2b46fe599a837362680e5ceaaa,"This research endeavors to delve deeply into how various eco-friendly natural elements captivate players' visual attention within the 3D game environments of the Metaverse. We selected still frames from 'theHunter: Call of the Wild,' a game rich with natural elements, and conducted empirical studies on specific demographics using eye-tracking technology. The results reveal that players' visual focus is significantly drawn more to certain natural elements like trees and mountains compared to others such as skies and lakes within the game. This insight is crucial for game designers, offering practical guidance on utilizing natural elements effectively to optimize visual design, enhance the game's allure and educational value, and spur innovation and advancement in the field of Metaverse 3D gaming. By tailoring the incorporation of these elements, designers can create more engaging and meaningful experiences within the expansive realm of the Metaverse. © 2024 The Authors.",ecological design; Eye-tracking experiments; metaverse games; natural elements,Behavioral research; Environmental protection; Game design; 3D games; Eco-friendly; Eye-tracking; Eye-tracking experiment; Game environment; Metaverse game; Metaverses; Natural elements; Visual appeals; Visual Attention; Eye tracking,Conference paper,Final,All Open Access; Hybrid Gold Open Access,Scopus,2-s2.0-85189760807,Gaming / VR
Sim G.; Read J.C.,"Sim, Gavin (11839440300); Read, Janet C. (15760807600)",11839440300; 15760807600,Using Eye-Tracking to Demonstrate Children’s Attention to Detail When Evaluating Low-Fidelity Prototypes,2024,Interacting with Computers,36,2,,100,112,12.0,1,10.1093/iwc/iwad052,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85198981756&doi=10.1093%2fiwc%2fiwad052&partnerID=40&md5=23a95899d8b357404a2002658e60e6fa,"This study used eye-tracking glasses to better understand how children explore low-fidelity prototypes in the context of user experience studies and to explore the potential of eye tracking in this context. The main research question that was being explored was whether the aesthetic refinement, either wireframe or high-resolution images, would affect children’s self-report and if so, or if not, what could be learned from knowing where children looked when exploring the prototypes. The results showed that the aesthetic refinement had little influence over the children’s overall ratings of the game. The eye-tracking data demonstrated that there were no differences in the time spent viewing the prototypes and most of the children focused on both the visuals and text on all the pages. However, there were a higher number of fixations recorded in the wireframe prototype compared to the photo-realistic version. This paper contributes to the design of prototypes through an understanding of how children interact with prototypes, demonstrating the importance of the text along with the visuals when evaluating game concepts with children. Further research is required to understand the differences and whether similar results are replicated with different games. © The Author(s) 2024.",children; eye tracking; games; prototype; user experience,User experience; Attention to details; Child; Eye-tracking; Game; High-resolution images; Low fidelities; Prototype; Research questions; Tracking data; Users' experiences; Eye tracking,Article,Final,All Open Access; Green Open Access,Scopus,2-s2.0-85198981756,Gaming / VR
Miyaguchi T.; Tazawa M.; Kanaya T.; Ibe Y.; Arii H.; Yajima K.; Nakao Y.; Wada N.,"Miyaguchi, Takuma (58897872700); Tazawa, Masayuki (55372504900); Kanaya, Takafumi (58898024300); Ibe, Yoko (25931529000); Arii, Hironori (57222260076); Yajima, Kenji (58898482600); Nakao, Yumiko (57741658400); Wada, Naoki (36846584200)",58897872700; 55372504900; 58898024300; 25931529000; 57222260076; 58898482600; 57741658400; 36846584200,An Initial Assessment of the Correlation Between Virtual Reality and Paper and Pencil Line Bisection Test Results,2024,Archives of Rehabilitation Research and Clinical Translation,6,1,100322,,,,0,10.1016/j.arrct.2024.100322,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85185574586&doi=10.1016%2fj.arrct.2024.100322&partnerID=40&md5=44e76b825e18b5714ee95c3009d5fc0c,"Objective: To make an initial assessment of the correlation between immersive virtual reality–based (ILBT) line bisection testing and paper-and-pencil–based line bisection (PLBT) testing in healthy subjects. Design: Diagnostic study. Setting: Research laboratory. Participants: Twenty healthy adults (51.5 [11.0] years old, 55% women; N=20). Interventions: Participants underwent an ILBT and a conventional PLBT in near space (NS) and more distant space (MDS). Correlations between the ILBT and PLBT, deviation rates in the NS and MDS, horizontal gaze distribution, and presence of virtual reality sickness (VRS) were evaluated. Main Outcome Measures: Correlation between the deviation rates of the PLBT and ILBT. Results: There was no significant correlation between the ILBT and PLBT for evaluating the deviation rate of the line bisection test (LBT). There was no significant difference in the deviation rate of the LBTs between the NS and MDS, but there was a significant difference in the horizontal line-of-sight distribution. VRS was not observed as an adverse event. Conclusions: In healthy adult subjects, our results suggested that there was no significant correlation between the deviation rates of the ILBT and PLBT. We also found that the ILBT is a useful and safe method for evaluating the horizontal line-of-sight distribution and percentage deviation of line segments from the center in the NS and MDS without inducing VRS. © 2024 The Authors",Assessment; Eye-tracking technology; Hemispatial neglect; Rehabilitation; Virtual reality,adult; Article; Barthel index; cognition assessment; controlled study; correlation analysis; cybersickness; daily life activity; dizziness; evaluation study; female; gaze; headache; horizontal gaze distribution; human; human experiment; male; middle aged; nausea; normal human; outcome assessment; paper and pencil based line bisection test; virtual reality; virtual reality based line bisection test,Article,Final,All Open Access; Gold Open Access; Green Open Access,Scopus,2-s2.0-85185574586,Gaming / VR
Lukka L.; Karhulahti V.-M.; Bergman V.-R.; Palva J.M.,"Lukka, Lauri (58303535300); Karhulahti, Veli-Matti (55209873600); Bergman, Vilma-Reetta (58630848900); Palva, J. Matias (6602980742)",58303535300; 55209873600; 58630848900; 6602980742,"Measuring digital intervention user experience with a novel ecological momentary assessment (EMA) method, CORTO",2024,Internet Interventions,35,,100706,,,,4,10.1016/j.invent.2023.100706,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85182908457&doi=10.1016%2fj.invent.2023.100706&partnerID=40&md5=8680d46f741e27f64ebbfc2bc8b1bf71,"Digital interventions often suffer from low usage, which may reflect insufficient attention to user experience. Moreover, the existing evaluation methods have limited applicability in the remote study of user experience of complex interventions that have expansive content and that are used over an extensive period of time. To alleviate these challenges, we describe here a novel qualitative Ecological Momentary Assessment (EMA) method: the CORTO method (Contextual, One-item, Repeated, Timely, Open-ended). We used it to gather digital intervention user experience data from Finnish adults (n = 184) who lived with interview-confirmed major depressive disorder (MDD) and took part in a randomized controlled trial (RCT) that studied the efficacy of a novel 12-week game-based digital intervention for depression. A second dataset on user experience was gathered with retrospective interviews (n = 22). We inductively coded the CORTO method and retrospective interview data, which led to four user experience categories: (1) contextual use, (2) interaction-elicited emotional experience, (3) usability, and (4) technical issues. Then, we used the created user experience categories and Template Analysis to analyze both datasets together, and reported the results qualitatively. Finally, we compared the two datasets with each other. We found that the data generated with the CORTO method offered more insights into usability and technical categories than the interview data that particularly illustrated the contextual use. The emotional valence of the interview data was more positive compared with the CORTO data. Both the CORTO and interview data detected 55 % of the micro-level categories; 20 % of micro-level categories were only detected by the CORTO data and 25 % only by the interview data. We found that the during-intervention user experience measurement with the CORTO method can provide intervention-specific insights, and thereby further the iterative user-centered intervention development. Overall, these findings highlight the impact of evaluation methods on the categories and qualities of insights acquired in intervention research. © 2024 The Authors",Digital interventions; Ecological momentary assessment; Engagement; Evaluation methods; Formative evaluation; Interviewing; Mental health; Methodology; Mixed methods; Qualitative study; Questionnaire; Remote study; Serious games; User experience,adult; aged; anxiety; Article; artificial intelligence; behavior change; cognitive behavioral therapy; cognitive rehabilitation; depression; ecological momentary assessment; emotion; European Medicines Agency; eye tracking; feasibility study; female; follow up; human; learning; major clinical study; male; mental health; Patient Health Questionnaire 9; perception; psychotherapy; questionnaire; retrospective study; semi structured interview; thematic analysis; valence (emotion); working memory,Article,Final,All Open Access; Gold Open Access; Green Open Access,Scopus,2-s2.0-85182908457,Gaming / VR
Rubin M.; Muller K.; Hayhoe M.M.; Telch M.J.,"Rubin, Mikael (56459512900); Muller, Karl (57204293789); Hayhoe, Mary M. (7004513666); Telch, Michael J. (7006365823)",56459512900; 57204293789; 7004513666; 7006365823,Attentional heterogeneity in social anxiety disorder: Evidence from Hidden Markov Models,2024,Behaviour Research and Therapy,173,,104461,,,,2,10.1016/j.brat.2023.104461,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85180597935&doi=10.1016%2fj.brat.2023.104461&partnerID=40&md5=e16eafd1b76dae8aa07f44512cda1dd7,"There is some evidence for heterogeneity in attentional processes among individuals with social anxiety. However, there is limited work considering how attentional processes may differ as a mechanism in a naturalistic task-based context (e.g., public speaking). In this secondary analysis we tested attentional heterogeneity among individuals diagnosed with social anxiety disorder (N = 21) in the context of a virtual reality exposure treatment study. Participants completed a public speaking challenge in an immersive 360°-video virtual reality environment with eye tracking at pre-treatment, post-treatment, and at 1-week follow-up. Using a Hidden Markov Model (HMM) approach with clustering we tested whether there were distinct profiles of attention pre-treatment and whether there were changes following the intervention. As a secondary aim we tested whether the distinct attentional profiles at pre-treatment predicted differential treatment outcomes. We found two distinct attentional profiles pre-treatment that we characterized as audience-focused and audience-avoidant. However, by the 1-week follow-up the two profiles were no longer meaningfully different. We found a meaningful difference between HMM groups for fear of public speaking at post-treatment b = −8.54, 95% Highest Density Interval (HDI) [-16.00, −0.90], Bayes Factor (BF) = 8.31 but not at one-week follow-up b = −5.83, 95% HDI [-13.25, 1.81], BF = 2.28. These findings provide support for heterogeneity in attentional processes among socially anxious individuals, but our findings indicate that this may change following treatment. Moreover, our results offer preliminary mechanistic evidence that patterns of avoidance may be specifically related to poorer treatment outcomes for virtual reality exposure therapy. © 2023 Elsevier Ltd",Eye tracking; Hidden markov models; Social anxiety; Virtual reality,"Anxiety; Attention; Bayes Theorem; Humans; Phobia, Social; Phobic Disorders; adult; Article; audience avoidant therapy; audience focused therapy; clinical article; controlled study; exposure variable; female; follow up; hidden Markov model; human; male; prediction; psychotherapy; social phobia; treatment outcome; virtual reality; young adult; anxiety; attention; Bayes theorem; phobia",Article,Final,All Open Access; Bronze Open Access; Green Open Access,Scopus,2-s2.0-85180597935,Gaming / VR
Aliagas I.; Privado J.; Merino M.D.,"Aliagas, Irene (58415939000); Privado, Jesús (8546902200); Merino, Mª Dolores (56937973800)",58415939000; 8546902200; 56937973800,Unravelling cognitive processing of in-game brands using eye tracking and electroencephalography: incongruence fosters it,2024,Current Psychology,43,8,,7628,7642,14.0,2,10.1007/s12144-023-04970-x,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85164798869&doi=10.1007%2fs12144-023-04970-x&partnerID=40&md5=55958d2ae23465538fb1e837e572615c,"Videogames have become the world’s favorite pastime, which is why brands integrate their logos on them, a technique called product placement. While most research has focused on analyzing whether brands are memorized based on their presentation within the videogame, little is known about how they are perceived and cognitively processed. This article analyzes whether attention and cognitive load from the placement and the videogame mediate the relationship between prominence, congruence, and familiarity of the brands and their memorization. A preliminary study made it possible to find the brands to be placed (n = 180), while the main study was experimental (n = 160), with prominence and congruence as between-subject factors and familiarity as a within-subject factor. Applying eye tracking and electroencephalogram, the results indicate that when the brand inserted via product placement is incongruent with the videogame genre, it will draw attention, demand cognitive resources, and will be more likely to be remembered and recognized. This would happen with both familiar and unfamiliar brands, and all of this without prominence influencing the process. These findings suggest that incongruent in-game brands would be the first game elements to be cognitively processed, even when they are irrelevant to game achievement. © The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature 2023.",Attention; Brand; Cognitive load; Electroencephalography; Memory; Videogame,,Article,Final,,Scopus,2-s2.0-85164798869,Gaming / VR
Woodworth J.W.; Borst C.W.,"Woodworth, Jason W. (57192676048); Borst, Christoph W. (9736479200)",57192676048; 9736479200,Visual cues in VR for guiding attention vs. restoring attention after a short distraction,2024,Computers and Graphics (Pergamon),118,,,194,209,15.0,7,10.1016/j.cag.2023.12.008,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85183465304&doi=10.1016%2fj.cag.2023.12.008&partnerID=40&md5=8333bd7db6c24522e5a3e59cb15386c6,"Distraction in VR training environments may be mitigated with a visual cue intended to guide user attention to a target. A survey of related literature suggests a past focus on “search and selection” tasks to evaluate a cue's capability for guidance. We investigate the capability of 9 eye-tracked cues with a new type of task that focuses on how to restore attention when a short distraction (e.g., a notification) shifts focus away from a target. Our study includes a guidance task in which subjects gaze at objects in a randomized order and a restoration task in which gaze sequences are interrupted by distraction events after which gaze must be returned to an object. We consider a wider variety of factors and metrics than previous studies, varying object spacing, gaze dwell time, and distraction distance and duration, and breaking down guidance time into subcomponents. Results show a general positive trend for cues that directly connect the user's gaze to the target rather than indirectly suggesting direction. Results further reveal different patterns of cue effectiveness for the restoration task than for conventional guidance. This may be attributed to knowledge that subjects have about the location of the object from which they were distracted. An implication for more complex distraction tasks is that we expect them to be between the short distraction and regular guidance in terms of memory of object position. So, we speculate cue performance for other tasks would vary between the short distraction and guidance results. For restoration, some cues add complexity that reduces, rather than improves, performance. © 2023 Elsevier Ltd",Attention guidance; Attention restoration; Virtual reality; Visual cues,Eye tracking; Virtual reality; Attention guidance; Attention restorations; Breakings; Dwell time; Improve performance; Object positions; Performance; User attention; Visual cues; Restoration,Article,Final,All Open Access,Scopus,2-s2.0-85183465304,Gaming / VR
Wilf M.; Korakin A.; Bahat Y.; Koren O.; Galor N.; Dagan O.; Wright W.G.; Friedman J.; Plotnik M.,"Wilf, Meytal (36515832500); Korakin, Alona (58820577800); Bahat, Yotam (25926895000); Koren, Or (57487834900); Galor, Noam (57934610100); Dagan, Or (58834132100); Wright, W. Geoffrey (57212443990); Friedman, Jason (9337239400); Plotnik, Meir (6603202801)",36515832500; 58820577800; 25926895000; 57487834900; 57934610100; 58834132100; 57212443990; 9337239400; 6603202801,Using virtual reality-based neurocognitive testing and eye tracking to study naturalistic cognitive-motor performance,2024,Neuropsychologia,194,,108744,,,,1,10.1016/j.neuropsychologia.2023.108744,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85182674805&doi=10.1016%2fj.neuropsychologia.2023.108744&partnerID=40&md5=63558d9cff419dc4888074be91ae526c,"Natural human behavior arises from continuous interactions between the cognitive and motor domains. However, assessments of cognitive abilities are typically conducted using pen and paper tests, i.e., in isolation from “real life” cognitive-motor behavior and in artificial contexts. In the current study, we aimed to assess cognitive-motor task performance in a more naturalistic setting while recording multiple motor and eye tracking signals. Specifically, we aimed to (i) delineate the contribution of cognitive and motor components to overall task performance and (ii) probe for a link between cognitive-motor performance and pupil size. To that end, we used a virtual reality (VR) adaptation of a well-established neurocognitive test for executive functions, the ‘Color Trails Test’ (CTT). The VR-CTT involves performing 3D reaching movements to follow a trail of numbered targets. To tease apart the cognitive and motor components of task performance, we included two additional conditions: a condition where participants only used their eyes to perform the CTT task (using an eye tracking device), incurring reduced motor demands, and a condition where participants manually tracked visually-cued targets without numbers on them, incurring reduced cognitive demands. Our results from a group of 30 older adults (>65) showed that reducing cognitive demands shortened completion times more extensively than reducing motor demands. Conditions with higher cognitive demands had longer target search time, as well as decreased movement execution velocity and head-hand coordination. We found larger pupil sizes in the more cognitively demanding conditions, and an inverse correlation between pupil size and completion times across individuals in all task conditions. Lastly, we found a possible link between VR-CTT performance measures and clinical signatures of participants (fallers versus non-fallers). In summary, performance and pupil parameters were mainly dependent on task cognitive load, while maintaining systematic interindividual differences. We suggest that this paradigm opens the possibility for more detailed profiling of individual cognitive-motor performance capabilities in older adults and other at-risk populations. © 2023 Elsevier Ltd",Aging; Color trails test; Fall risk; Hand kinematics; Pupil; Virtual reality,Aged; Cognition; Executive Function; Eye-Tracking Technology; Humans; Virtual Reality; aged; Article; cognition assessment; executive function; eye tracking; female; human; human experiment; kinematics; male; motor coordination; motor performance; normal human; pupil diameter; questionnaire; task performance; virtual reality; cognition; eye-tracking technology,Article,Final,,Scopus,2-s2.0-85182674805,Gaming / VR
Soret R.; Prea N.; Peysakhovich V.,"Soret, Rébaï (57210112083); Prea, Noemie (58613515000); Peysakhovich, Vsevolod (56644829800)",57210112083; 58613515000; 56644829800,Exploring the Impact of Body Position on Attentional Orienting,2024,Information (Switzerland),15,2,111,,,,2,10.3390/info15020111,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85185726037&doi=10.3390%2finfo15020111&partnerID=40&md5=ef1efb0f451ead373969f7a550ef79be,"Attentional orienting is a crucial process in perceiving our environment and guiding human behavior. Recent studies have suggested a forward attentional bias, where faster reactions are observed to spatial cues indicating information appearing in the forward rather than the rear direction. This study investigated how the body position affects attentional orienting, using a modified version of the Posner cueing task within a virtual reality environment. Participants, seated at a 90° angle or reclined at 45°, followed arrows directing their attention to one of four spatial positions where a spaceship will appear, visible either through transparent windows (front space) or in mirrors (rear space). Their task was to promptly identify the spaceship’s color as red or blue. The results indicate that participants reacted more swiftly when the cue correctly indicated the target’s location (valid cues) and when targets appeared in the front rather than the rear. Moreover, the “validity effect”—the advantage of valid over invalid cues—on early eye movements, varied based on both the participant’s body position and the target’s location (front or rear). These findings suggest that the body position may modulate the forward attentional bias, highlighting its relevance in attentional orienting. This study’s implications are further discussed within contexts like aviation and space exploration, emphasizing the necessity for precise and swift responses to stimuli across diverse spatial environments. © 2024 by the authors.",attentional bias; attentional orienting; discrimination task; eye tracking; front space; Posner paradigm; rear space; spatial cognition; virtual reality,Behavioral research; Eye movements; Space research; Virtual reality; Attentional bias; Attentional orienting; Body positions; Discrimination tasks; Eye-tracking; Front space; Human behaviors; Posn paradigm; Rear space; Spatial cognition; Eye tracking,Article,Final,All Open Access; Gold Open Access,Scopus,2-s2.0-85185726037,Gaming / VR
Taupin M.L.; Ruffault A.; Slawinski J.; Bayle D.,"Taupin, Mildred Loiseau (57451039900); Ruffault, Alexis (56925052800); Slawinski, Jean (25422871500); Bayle, Dimitri (16444180600)",57451039900; 56925052800; 25422871500; 16444180600,Effects of Acute Physical Fatigue on Gaze Behavior in Expert Badminton Players,2024,Journal of Sport and Exercise Psychology,46,1,,1,10,9.0,1,10.1123/jsep.2023-0075,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85184345147&doi=10.1123%2fjsep.2023-0075&partnerID=40&md5=03dc2605a7dbd7e57fda49a4376cd9bf,"Perceptual cognitive skills in real game settings, under conditions of fatigue, such as the ability to gather relevant visual information, are key factors in achieving motor goals in sports. The objectives were to evaluate the effects of acute physical fatigue on gaze behavior during a badminton game (Study 1) and in an unfavorable force ratio situation (Study 2). Six international-level badminton players played two sets and unfavorable force ratio situations while wearing eye-tracking glasses before and after a fatiguing task. During the set, fatiguing physical exercise led to fewer fixations per exchange and more fixations on one area of interest. During unfavorable force ratio situations, fatiguing physical exercise led to shorter fixation durations per exchange, shorter fixation durations on two areas of interest, and longer fixation durations on one area of interest. The results showed that gaze behaviors were adapted in acute physical fatigue conditions to maintain performance. © 2024 Human Kinetics, Inc.",exertion; performance; physical load; racket sports; visual search strategy,"Fatigue; Fixation, Ocular; Humans; Racquet Sports; eye fixation; fatigue; human; psychology; racquet sport",Article,Final,,Scopus,2-s2.0-85184345147,Gaming / VR
Megalingam R.K.; Kuttankulungara Manoharan S.; Riju G.,"Megalingam, Rajesh Kannan (35119069500); Kuttankulungara Manoharan, Sakthiprasad (57201722892); Riju, Gokul (59133464200)",35119069500; 57201722892; 59133464200,Accurate Gaze Estimation: An Innovative Eye Movement Prediction Architecture,2024,"Proceedings of International Conference on Circuit Power and Computing Technologies, ICCPCT 2024",,,,1677,1682,5.0,0,10.1109/ICCPCT61902.2024.10673363,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85205600656&doi=10.1109%2fICCPCT61902.2024.10673363&partnerID=40&md5=da3e6554d5b71e736480ffd03fccb731,"Eye movement prediction, a budding technology, peers into the future of human interaction. Tracking our eye movement unlocks exciting possibilities: interfaces anticipating our needs, immersive virtual worlds mirroring our natural focus, and a deeper understanding of cognitive processes. However, the current landscape of eye movement prediction is fraught with challenges. Despite advancements in image processing and machine learning, existing models often struggle with accuracy, the ability to adapt to individual differences, and delivering real-time predictions. Researchers are exploring new avenues, such as gaze-based interaction, personalized eye movement models, and faster algorithms to overcome these challenges. Incorporating these elements into eye movement prediction systems can lead to interfaces that seamlessly respond to the user's gaze, virtual environments replicating natural exploration, and insights into cognitive function based on gaze patterns. Moreover, developing eye movement prediction algorithms that require fewer resources, like the one presented in this paper, is crucial for making this technology more accessible and widely applicable. By minimizing resource usage in terms of CPU and processing time, these algorithms can be integrated into various devices and applications, from smartphones to virtual reality headsets, enabling a more intuitive and immersive user experience. In summary, the future of eye movement prediction lies in addressing the challenges of accuracy, personalization, and real-time processing, while also focusing on minimizing resource usage and increasing accessibility. Achieving these goals will unlock the full potential of eye movement prediction, transforming how we interact with technology and understand human cognition. © 2024 IEEE.",eye localization; eye motion tracking; eye movement prediction; Eye tracking; facial landmarking; facial recognition; gaze tracking; image processing; iris detection; pupil,Eye movements; Face recognition; Motion estimation; Motion tracking; Prediction models; Virtual environments; Eye motion tracking; Eye movement prediction; Eye-tracking; Eyes localization; Facial landmarking; Facial recognition; Gaze-tracking; Images processing; Iris detection; Motion tracking; Movement prediction; Pupil; Virtual reality,Conference paper,Final,,Scopus,2-s2.0-85205600656,Gaming / VR
Wang C.; Zhang T.,"Wang, Chengbao (59738560100); Zhang, Tianyang (59738675600)",59738560100; 59738675600,Design and Validation of Attention Training Game Based on Eye Tracking Interaction,2024,"Proceedings - 2024 International Conference on Information Technology, Communication Ecosystem and Management, ITCEM 2024",,,,107,113,6.0,0,10.1109/ITCEM65710.2024.00028,https://www.scopus.com/inward/record.uri?eid=2-s2.0-105002701349&doi=10.1109%2fITCEM65710.2024.00028&partnerID=40&md5=4dd4eafabeb10ab82279dff80ce866c1,"In recent years, with the rapid development of digital technology and the explosion of information, individuals are facing increasingly severe problems of distraction. However, most attention-related training adopts structured and standardized tasks and pays little attention to the design of the training content itself, which lacks interest and attractiveness and makes it difficult to maintain user engagement and persistence. In this paper, we propose the design principles and application framework of an eye-movement interaction attention training system for the characteristics of attention processing mechanism and eye-movement interaction mode, and based on this, we design an eye-movement interaction attention training game application. The training effect of attention was evaluated and tested by visual search task and training experience questionnaire. The results showed that eye-movement interaction training improved the subjects' attention level. © 2024 IEEE.",attention assessment; Attention training; eye tracking interaction,Game design; Application frameworks; Attention assessment; Attention training; Design applications; Design Principles; Digital technologies; Eye tracking interaction; Eye-tracking; Game-Based; User engagement; Eye movements,Conference paper,Final,,Scopus,2-s2.0-105002701349,Gaming / VR
Lavoie P.; Lapierre A.; Maheu-Cadotte M.-A.; Brien L.-A.; Ledoux I.; Gosselin É.,"Lavoie, Patrick (56131881700); Lapierre, Alexandra (57205615543); Maheu-Cadotte, Marc-André (57200104220); Brien, Louise-Andrée (25026607600); Ledoux, Isabelle (57215482397); Gosselin, Émilie (56072962300)",56131881700; 57205615543; 57200104220; 25026607600; 57215482397; 56072962300,Nursing Students' Engagement in Virtual Reality and Hybrid Simulations: A Quasi-Experimental Study,2024,Clinical Simulation in Nursing,87,,101496,,,,8,10.1016/j.ecns.2023.101496,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85181821662&doi=10.1016%2fj.ecns.2023.101496&partnerID=40&md5=1f94b52315343636b0c814355676ca1d,"Background: Virtual reality enables reproducing scenarios that are difficult to access in clinical education. Its immersive qualities and ability to isolate students from external auditory and visual distractions may enhance engagement over other simulation modalities. Methods: This quasi-experimental study compared the effects of virtual reality and hybrid simulations (i.e., actor and manikin) on nursing students' engagement. Data on satisfaction, confidence in learning, cognitive load, mental effort, and clinical reasoning were also collected. The study included 179 nursing students who completed a post-simulation survey. Results: Participants in the hybrid group reported higher engagement, satisfaction, and mental effort than the virtual reality group. No significant differences regarding confidence, cognitive load, and clinical reasoning were found. Conclusion: Even if virtual reality emerges as a promising tool for simulation-based education, certain inherent aspects of traditional simulation modalities may contribute to enhanced engagement, such as human presence and opportunities for natural interactions. © 2023",clinical judgment; clinical simulation; cognitive workload; engagement; experiential learning; healthcare education; immersive simulation; nursing education,,Article,Final,,Scopus,2-s2.0-85181821662,Gaming / VR
Prawake R.; Srisawasdi N.; Chaipidech P.,"Prawake, Rapheephan (59129535600); Srisawasdi, Niwat (55203537900); Chaipidech, Pawat (57194160708)",59129535600; 55203537900; 57194160708,Analyzing Computational Processes in Tracking Visual Attention and Assessing Scientific Understanding within Interactive Learning Content,2024,"2024 IEEE 7th Eurasian Conference on Educational Innovation: Educational Innovations and Emerging Technologies, ECEI 2024",,,,75,80,5.0,0,10.1109/ECEI60433.2024.10510865,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85193287567&doi=10.1109%2fECEI60433.2024.10510865&partnerID=40&md5=911e97c53da27fd9fb7718aee939fd72,"Understanding phenomena scientifically is fundamental. Educators constantly implement diverse learning approaches and tools to enhance students' effective learning processes in science. Thus, we investigated the impact of integrating interactive learning content and game-transformed inquiry-based pedagogy on secondary school students' scientific understanding of the human circulatory system. Moreover, cognitive visual attention was also explored using computational process analysis of eye movement. The participants were 130 eleventh-grade students divided into experimental (full and semi-interactive content settings) and control (no digital content setting) groups. The data were analyzed using a t-test and One-Way ANOVA. The results indicated that the full interactive content with a game-transformed inquiry-based learning approach outperformed students' scientific understanding of biological concepts. Notably, the post-test scores for both gamed-transform inquiry-based learning approaches were significantly higher than the conventional inquiry class without digital content. Two volunteers from the full and semi-interactive content classes were involved in an analysis of visual attention and cognitive processes. An eye-tracking instrument was used to monitor their eye movements and analyze visual attention. The eye movement metrics revealed distinct behavioral patterns for both conditions. The interactive video in the full interactive content setting captured students' attention more quickly than the game. However, the game maintained prolonged fixation time more effectively and drew students' attention to relevant content. The processing of visual pattern analysis indicated that guidance for linking interactivity and information presented through interactive learning contents is essential for a better understanding of scientific concepts and the effectiveness of the learning approach in science. © 2024 IEEE.",cognitive computing; digital content; eye movement behavior; scientific understanding; visual attention,Air navigation; Behavioral research; Cardiovascular system; E-learning; Eye tracking; Learning systems; Students; Cognitive Computing; Digital contents; Eye movement behavior; Interactive contents; Interactive learning; Learning approach; Learning contents; Movement behaviour; Scientific understanding; Visual Attention; Eye movements,Conference paper,Final,,Scopus,2-s2.0-85193287567,Gaming / VR
Montolio-Vila A.; Argilés M.; Quevedo L.; Sunyer-Grau B.; Erickson G.,"Montolio-Vila, Anna (59366393200); Argilés, Marc (56946819500); Quevedo, Lluïsa (8289904400); Sunyer-Grau, Bernat (57909941700); Erickson, Graham (7102189289)",59366393200; 56946819500; 8289904400; 57909941700; 7102189289,Effect of Action Video Games in Eye Movement Behavior: A Systematic Review,2024,Journal of Eye Movement Research,17,3,6,1,22,21.0,3,10.16910/JEMR.17.3.6,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85206290444&doi=10.16910%2fJEMR.17.3.6&partnerID=40&md5=f493635957c0dea12fff7ffe7217ee82,"Previous research shows that playing action video games seems to modify the behavior of eye movements such as eye fixations and saccades. The aim of the current work was to determine the effect of playing action video games on eye movements behavior such as fixations, saccades and pursuits. A systematic research review in PubMed and Scopus databases was conducted to identify articles published between 2010 and 2022 which referred to action video games and eye movements, including fixations, saccades and pursuits. We included those that were experimental and quasi-experimental, comparing at least two groups between action vs. non-action video games players. All the studies included used an eye tracker to study eye movements. A total of 97 scientific articles were found in the databases. After inclusion criteria, thirteen articles (N=13) were analyzed for the present work, of which ten (n=10) had a cross-sectional design, and three (n=3) were randomized intervention studies. Playing regularly or training with action video games is not likely to produce changes in eye movements, based on the literature research analyzed. For future research, more interventional studies, with less gender bias, more sample participants and general consensus on the distinction between the action and non-action video games is needed. © (2024), (International Group for Eye Movement Research). All rights reserved.",action video games; attention; Eye movements; eye tracking; fixations; saccades; smooth pursuit; systematic review,,Article,Final,All Open Access; Gold Open Access,Scopus,2-s2.0-85206290444,Gaming / VR
Li W.-A.; Chiu F.-Y.,"Li, Wen-Ai (59704914700); Chiu, Fu-Yuan (15070146400)",59704914700; 15070146400,Using VR Eye-Tracking Technology to Explore the Perceptiveness of Preschool Teachers in Observing Students with Special Needs in Teaching Environments,2024,"Proceedings - 2024 6th International Workshop on Artificial Intelligence and Education, WAIE 2024",,,,177,181,4.0,0,10.1109/WAIE63876.2024.00039,https://www.scopus.com/inward/record.uri?eid=2-s2.0-105000707362&doi=10.1109%2fWAIE63876.2024.00039&partnerID=40&md5=645c94ee74c56fd7d3dad42fb5a1ce44,"This study utilized virtual reality (VR) eye tracking technology to compare pre-service and in-service preschool teachers' classroom observations regarding the identification of students with special needs. By analyzing eye movement data collected through VR eye trackers and conducting semistructured interviews, the research demonstrated that pre-service teachers hold positive attitudes toward integrating VR eye trackers into their education and training. Additionally, the study examined the eye-tracking acuity of both pre-service and inservice preschool teachers using virtual reality technology, observing their attention to special needs children visual cues and the utilization of technological aids in classroom teaching and counseling. Overall, the results suggest that VR eye tracking technology holds promise for enhancing teacher training and classroom practices in preschool education.  © 2024 IEEE.",preschool teachers; special education; special needs; virtual reality eye-tracking,Eye movements; Personnel training; Students; Teaching; Virtual reality; Eye trackers; Eye tracking technologies; Eye-tracking; Preschool teacher; Preservice teachers; Semi structured interviews; Special education; Special needs; Teachers'; Virtual reality eye-tracking; Virtual environments,Conference paper,Final,,Scopus,2-s2.0-105000707362,Gaming / VR
Wang H.; Yang J.; Hu M.; Tang J.; Yu W.,"Wang, Haoyue (58751290100); Yang, Jian (57255188400); Hu, Menghan (55818700700); Tang, Jingyu (57326047800); Yu, Wangyang (57224880889)",58751290100; 57255188400; 55818700700; 57326047800; 57224880889,A comparative analysis for eye movement characteristics between professional and non-professional players in FIFA eSports game,2024,Displays,81,,102599,,,,7,10.1016/j.displa.2023.102599,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85179005344&doi=10.1016%2fj.displa.2023.102599&partnerID=40&md5=408e8fdbb1d1f824255d1006d09aa539,"Eye-tracking technology has been widely applied to various kinds of traditional sports research, such as football, basketball and table tennis. eSports is an emerging digital sport, which is currently enjoying rapid development. The previous study primarily focuses on the practical applications of eye movement in eSports, such as game design, visual attention, and performance analysis. However, there is a scientific gap for the specific differences between expert and novice players, the potential of eye-tracking as a biological marker for eSports performance, and the transfer of insights from real sports to eSports through eye-tracking studies. This study aims to uncover disparities in eye movement characteristics between professional players (N = 14, 24.1 ± 2.5 years) and non-professional players (N = 14, 25.2 ± 2.3 years) by employing the expert-novice paradigm. We extracted several key eye movement features, including average fixation counts, average fixation duration, first fixation duration, eye movement trajectory, and visits to areas of interest. The results indicate that professional players exhibit higher fixation counts, shorter first fixation duration, reduced average fixation duration, and employ a more effective visual search strategy. Professional players’ fixation trajectories are characterized by clarity and focus, with a greater number of fixation points concentrated on key areas of interest. In contrast, non-professional players demonstrate relatively random fixation trajectories, lacking distinct focal points. This study not only advances our understanding of eye-tracking in eSports but also provides technical guidance and practical significance for further research in this burgeoning field. © 2023",eSports; Eye-tracking; FIFA 21; Fixation; Professional-player,Behavioral research; Eye movements; Professional aspects; Sports; Trajectories; Area of interest; Average fixation durations; Comparative analyzes; E-sports; Eye-tracking; FIFA 21; Fixation; Fixation duration; Movement characteristics; Professional-player; Eye tracking,Article,Final,All Open Access; Bronze Open Access,Scopus,2-s2.0-85179005344,Gaming / VR
Ball L.J.; Richardson B.H.,"Ball, Linden J. (7102282029); Richardson, Beth H. (46661351000)",7102282029; 46661351000,Eye-Tracking and Physiological Measurements for UX Evaluation,2024,User Experience Methods and Tools in Human-Computer Interaction,,,,224,245,21.0,3,10.1201/9781003495161-9,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85204849895&doi=10.1201%2f9781003495161-9&partnerID=40&md5=5201401dd026bbf2794ef91228bcdd5d,"Eye-tracking and physiological measurements are central to research concerned with evaluating user experience in the context of human–computer interaction. Eye-tracking is especially valuable for understanding where people’s attention is being deployed during interface search and interaction,thereby providing insights into factors that hinder the usability of computer-based technologies. Physiological measurements,in contrast,are attuned to indexing the affective aspects of user experience (e.g.,levels of arousal) as well as the cognitive workload associated with an interaction task. This chapter enables readers to gain a detailed understanding of a range of eye-tracking and physiological measurements and how they are interpreted when evaluating user experience. The value of these measurement techniques will be illustrated through studies drawn from both pioneering and recent research. This chapter also considers the limitations of eye-tracking and physiological measurements in usability research as well as ways to mitigate such limitations. The final section of this chapter discusses key trends and directions in the use of such measurements in user experience studies,including the use of eye-movement traces to elicit retrospective reports of interface problems and the potential for automated identification and categorization of eye-movement and physiological patterns that are diagnostic of interaction difficulties. © 2025 selection and editorial matter,Constantine Stephanidis and Gavriel Salvendy; individual chapters,the contributors.",,,Book chapter,Final,,Scopus,2-s2.0-85204849895,Gaming / VR
Kuvar V.K.; Bailenson J.N.; Mills C.,"Kuvar, Vishal Kiran (57202091142); Bailenson, Jeremy N. (6602840468); Mills, Caitlin (40661587800)",57202091142; 6602840468; 40661587800,A novel quantitative assessment of engagement in virtual reality: Task-unrelated thought is reduced compared to 2D videos.,2024,Computers and Education,209,,104959,,,,4,10.1016/j.compedu.2023.104959,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85177182880&doi=10.1016%2fj.compedu.2023.104959&partnerID=40&md5=55a2e743e2e30096f9d6d14a94d4d2f0,"Recent meta-analytic evidence suggests that students’ minds are likely to wander off-task frequently, regardless of the learning modality; yet virtual reality (VR) has been notably unexplored in this space. VR may present an opportunity to mitigate task-unrelated thought (TUT; the most common operationalization of mind wandering) because it minimizes audio-visual distractions and increases feelings of immersion. The current study tested this possibility by analyzing TUT frequency reports from 118 participants as they learned about climate change in one of two conditions: a 360° video in VR versus a traditional video on a 2D monitor. Participants answered momentary thought probes at pseudo-random intervals throughout the video and eye-gaze was recorded in both modalities. Results indicated that participants were less likely to experience TUT in the VR condition compared to non-VR (B = 0.49; p = 0.02). Consistent with prior research, TUT was also negatively related to posttest performance (B = −0.05; p = 0.01). Finally, TUT mediated the effect between learning modality on posttest performance, such that participants in VR experienced lower TUT and subsequently scored higher on the posttest (B = 0.19; p = 0.03). We also present exploratory analyses on how gaze patterns differed across modalities as well as how gaze was related to instances of TUT. © 2023 Elsevier Ltd",,Climate change; 'current; 2D video; Audio-visual; Condition; Learning modalities; Performance; Pseudo-random; Quantitative assessments; Random interval; Visual distractions; Virtual reality,Article,Final,,Scopus,2-s2.0-85177182880,Gaming / VR
Huang J.; Zhang Y.; He X.; Zhang C.,"Huang, Jinni (58515885300); Zhang, Yongtao (59567368200); He, Xingyi (59567368300); Zhang, Cheng (57871816100)",58515885300; 59567368200; 59567368300; 57871816100,Application of 360° Panoramic VR and Eye Tracking in the Study of Spatial Visual Perception in Traditional Chinese Gardens: A Case Study of the Humble Administrator's Garden,2024,"2024 10th International Conference on Virtual Reality, ICVR 2024",,,,332,337,5.0,0,10.1109/ICVR62393.2024.10869247,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85218413385&doi=10.1109%2fICVR62393.2024.10869247&partnerID=40&md5=6183a158d27a6e3939ad459f9718c3a1,"As a model of human spatial visual art, it is necessary for traditional Chinese gardens to reveal their visual perception patterns through scientific quantitative methods in order to promote the contemporary translation of garden space in the field of contemporary architecture. In this paper, we take the Humble Administrator's Garden as an example and use 360° panoramic VR combined with eye movement experiments to analyze the visual attention and perception patterns of visitors in the space of the Humble Administrator's Garden. In addition, during the experiment, we found that there are many problems in the current combination of eye-tracking technology and virtual reality immersive scene, and the integration of the two technologies needs to be further strengthened. Therefore, this study reveals the limitations of the combination of 360° panoramic VR and eye-tracking technology as well as the direction of future development from three aspects: sample presentation, data collection, data processing and analysis, in order to provide relevant insights for the combination of VR technology and eye-tracking technology at present. © 2024 IEEE.",360° panoramic VR; Eye tracking technology; Humble Administrator's Garden; Spatial perception,Data acquisition; Network security; Spatio-temporal data; 360° panoramic VR; Case-studies; Eye tracking technologies; Eye-tracking; Humble administrator garden; Perception patterns; Quantitative method; Spatial perception; Visual arts; Visual perception; Eye movements,Conference paper,Final,,Scopus,2-s2.0-85218413385,Gaming / VR
Latini A.; Marcelli L.; Di Giuseppe E.; D’Orazio M.,"Latini, Arianna (57289600800); Marcelli, Ludovica (58923659400); Di Giuseppe, Elisa (57210410589); D’Orazio, Marco (15826296900)",57289600800; 58923659400; 57210410589; 15826296900,Potential of Biophilic Design in Workplaces: A Pilot Study with Eye Tracking in Immersive Virtual Environments,2024,"Smart Innovation, Systems and Technologies",378,,,355,365,10.0,1,10.1007/978-981-99-8501-2_32,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85188753402&doi=10.1007%2f978-981-99-8501-2_32&partnerID=40&md5=b1add3c4aac452b87aaf02814f0af2db,"Biophilic design (BD) approach promotes the integration of nature-based systems into the built environment, with a positive impact on human comfort, health, well‐being, and cognitive functions. In this pilot study, a virtual model of an office room was integrated with nature’s patterns (living wall and potted plants) to create an immersive biophilic environment (IBE). In the IBE, 25 participants performed three cognitive tasks, while their eyes movements were detected through an eye-tracking technology integrated within the head-mounted display for virtual reality (VR). The authors focused on two goals: verifying the ecological validity of the virtual biophilic model and evaluating the potential effect of introducing green elements in terms of visual attention, interest, and distraction. Findings revealed that the IBE created an excellent level of presence and immersivity. The preliminary results show that visual attention could be positively triggered by the proximity of users to the natural element and the possibility of freely exploring the biophilic environment, while visual distraction from operative tasks might be negatively influenced by the spatial location and dimension. Hence, the results of this pilot study support the potentiality of adopting VR in extensive research studies to support a proper biophilic design promoting emotional attachment and work efficiency. © The Author(s), under exclusive license to Springer Nature Singapore Pte Ltd. 2024.",Biophilic design; Eye tracking; Immersive virtual environment,Behavioral research; Cognitive systems; Eye movements; Helmet mounted displays; Virtual reality; Biophilic design; Built environment; Design approaches; Eye-tracking; Human comforts; Immersive; Immersive virtual environments; Pilot studies; Visual Attention; Visual distractions; Eye tracking,Conference paper,Final,,Scopus,2-s2.0-85188753402,Gaming / VR
Singh J.; Modi N.,"Singh, Jaiteg (26422494700); Modi, Nandini (57212387785)",26422494700; 57212387785,"A robust, real-time camera-based eye gaze tracking system to analyze users’ visual attention using deep learning",2024,Interactive Learning Environments,32,2,,409,430,21.0,10,10.1080/10494820.2022.2088561,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85132719571&doi=10.1080%2f10494820.2022.2088561&partnerID=40&md5=c544c635818230337b7c920c22869616,"Eye gaze tracking has recently become indispensable for domains like virtual reality, augmented reality, human–computer interaction and advertisement. The commercial eye gaze tracking equipment is too expensive to be used by the masses. In this manuscript, a non-invasive, low-resolution ordinary camera-based system has been proposed for tracking eye gaze. The proposed system attained an accuracy of 84% without requiring any costly equipment or infrared light like other commercial tools. Eye gaze fixations data of 37 participants were recorded on a webpage and gaze patterns were plotted using heat maps. Further, a case analysis was done to validate the performance of the proposed system, where the user interface has been controlled using eye movements. © 2022 Informa UK Limited, trading as Taylor & Francis Group.",consumer attention; consumer behavior; Eye gaze tracking; user interface; video-oculography; vision research,,Article,Final,,Scopus,2-s2.0-85132719571,Gaming / VR
Tang M.; Liu X.; Dong Y.; Tang Z.; Huo H.; Fan L.; Qiao X.; Chen D.; Wang J.; Du X.; Guo J.; Fan Y.,"Tang, Min (57226893595); Liu, Xiaoyu (57218655037); Dong, Ying (57222476946); Tang, Zhili (57226890312); Huo, Hongqiang (57202436822); Fan, Linyuan (57704040700); Qiao, Xiaofeng (57704039100); Chen, Duo (57216855150); Wang, Jinghui (57699015300); Du, Xin (57704726900); Guo, Jieyi (58155870300); Fan, Yubo (55648008700)",57226893595; 57218655037; 57222476946; 57226890312; 57202436822; 57704040700; 57704039100; 57216855150; 57699015300; 57704726900; 58155870300; 55648008700,Absence of Inertial Load on Hand Decreases Task Performance in Virtual Reality Interaction,2024,International Journal of Human-Computer Interaction,40,12,,3219,3233,14.0,2,10.1080/10447318.2023.2184960,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85150821369&doi=10.1080%2f10447318.2023.2184960&partnerID=40&md5=0b7d4d7dc7c180ee6e20cd00c9ffa82e,"The inertia of manipulated objects contributes to natural human performance, but its effects on virtual reality (VR) interactions have rarely been investigated. Here, we designed a virtual goal-directed task, in which virtual objects with different masses were moved into a target hole. Based on synchronized kinematic and eye-tracking data, we examined the effects of inertia on participants’ performance during the virtual task in a virtual environment. Our results indicated that hand movements presented greater spatial variability and more discontinuities when the inertial load was removed. It suggested a decline in the ability of motor control and feedback regulation, since the absence of an inertial load weakened the proprioception for sensing limb movements. Eye-movement evidence indicated that increased preferential allocations of visual attention contribute to compensating the weakened proprioceptive cues, supporting the kinematic results. These findings reveal the importance and mechanism of inertial effects on human behaviors in VR interactions. © 2023 Taylor & Francis Group, LLC.",eye movements; eye-hand coordination; goal-directed task; inertial load; motor control; Virtual reality,Behavioral research; Eye tracking; Kinematics; Virtual reality; Eye-hand coordination; Goal-directed; Goal-directed task; Human performance; Inertial loads; Manipulated objects; Motor control; Task performance; Virtual objects; Virtual reality interactions; Eye movements,Article,Final,,Scopus,2-s2.0-85150821369,Gaming / VR
Ouyang Y.; Huang G.; He S.,"Ouyang, Yewei (57195718282); Huang, Guoqing (59136674900); He, Shiyi (56779333400)",57195718282; 59136674900; 56779333400,Impacts of adverse environmental factors on construction workers' attention allocation during hazard identification: a study of noise and heat exposure,2024,"Engineering, Construction and Architectural Management",,,,,,,4,10.1108/ECAM-04-2024-0438,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85203552766&doi=10.1108%2fECAM-04-2024-0438&partnerID=40&md5=f3dd8b132812de337c5422c239adff61,"Purpose: There are many safety hazards in construction workplaces, and inattention to the hazards is the main reason why construction workers failed to identify the hazards. Reasonably allocating attention during hazard identification is critical for construction workers’ safety. However, adverse working environments in job sites may undermine workers’ attention. Previous studies failed to investigate the impacts of environmental factors on attention allocation, which hinders taking appropriate measures to eliminate safety incidents when encountering adverse working environments. This study aims to examine the effects of workplace noise and heat exposure on workers’ attention allocation during construction hazard identification to fill the research gap. Design/methodology/approach: This study applied an experimental study where a within-subject experiment was designed. Fifteen construction workers were invited to perform hazard identification tasks in panoramic virtual reality. They were exposed to three noise levels (60, 85 and 100 dBA) in four thermal conditions (26°C, 50% RH; 33°C, 50% RH; 30°C, 70% RH; 33°C, 70% RH). Their eye movements were recorded to indicate their attention allocation under each condition. Findings: The results show that noise exposure reduced workers’ attention to hazardous areas and the impacts increased with the noise level. Heat exposure also reduced the attention, but it did not increase with the heat stress but with subjects’ thermal discomfort. The attention was impacted more by noise than heat exposure. Noise exposure in the hot climate should be more noteworthy because lower levels of noise would lead to significant changes. These visual characteristics led to poorer identification accuracy. Originality/value: This study could extend the understanding of the relationship between adverse environmental factors and construction safety. Understanding the intrinsic reasons for workers' failed identification may also provide insights for the industry to enhance construction safety under adverse environments. © 2024, Emerald Publishing Limited.",Attention allocation; Eye-tracking; Hazard identification; Heat exposure; Workplace noise,Construction industry; Eye movements; Hazards; Attention allocation; Construction workers; Environmental factors; Eye-tracking; Hazard identification; Heat exposure; Noise exposure; Workers'; Working environment; Workplace noise,Article,Article in press,,Scopus,2-s2.0-85203552766,Gaming / VR
Luis-del Campo V.; Morenas Martín J.; León Llamas J.L.; Ortega Morán J.F.; Díaz-García J.; García-Calvo T.,"Luis-del Campo, Vicente (6603193056); Morenas Martín, Jesús (55934512500); León Llamas, Juan Luis (57209417921); Ortega Morán, Juan Francisco (57195558964); Díaz-García, Jesús (57210716066); García-Calvo, Tomás (58262725600)",6603193056; 55934512500; 57209417921; 57195558964; 57210716066; 58262725600,Influence of the time-task constraint on ocular metrics of semi-elite soccer players,2024,Science and Medicine in Football,8,2,,179,186,7.0,5,10.1080/24733938.2023.2172203,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85147340279&doi=10.1080%2f24733938.2023.2172203&partnerID=40&md5=8c30999aa8f7e2bd74c5021ea2cde011,"This study novelty aimed to investigate the influence of manipulating the available time to perform the training tasks on soccer players´ ocular metrics, following training. Specifically, pupillary response (pupil diameter) and saccadic features (latency, accuracy, velocity, and number) were measured with a portable eye tracker following training to reflect the mental load accumulated by players during the training sessions. Nine semi-elite soccer players performed two training sessions, based on large-sided games, on an artificial grass field. These two sessions were composed of the same tasks but varying the required time to complete the task goals (Session 1: No time limitations to perform the tasks; Session 2: Limited time to perform the tasks). The participants performed, before (pre-test) and after (post-test) each training session, a prosaccade task in a room near the playing field. Findings revealed a differentiated effect of the available time to complete the training tasks on ocular metrics because significant differences were found in all variables after training (p <.001 for pupil diameter; p <.01 for saccade accuracy and number of saccades; p <.05 for saccade velocity and latency). Ocular metrics could be a promising tool to evaluate mental load following practice because they were sensitive to the time-task constraint, providing researchers a valuable information for a better planning of the mental workload when designed training tasks. © 2023 Informa UK Limited, trading as Taylor & Francis Group.",cognitive load; eye tracking; Informational constraint; soccer,Eye; Face; Humans; Saccades; Soccer; Workload; eye; face; human; saccadic eye movement; soccer; workload,Article,Final,,Scopus,2-s2.0-85147340279,Gaming / VR
Tsai H.-J.; Wu H.-K.; Chen C.-C.; Yeh C.-H.; Chu T.-Y.; Yeh S.-C.,"Tsai, Ho-Jung (58109725100); Wu, Hsiao-Kuang (55366211300); Chen, Chun-Chuan (8942129700); Yeh, Chen-Hsin (59659453400); Chu, Tzu-Yueh (59659453500); Yeh, Shih-Ching (14619911100)",58109725100; 55366211300; 8942129700; 59659453400; 59659453500; 14619911100,Intelligent ADHD Detection Using Explainable Multimodal Fusion Model,2024,"Proceedings of 2024 IEEE 4th International Conference on Information Technology, Big Data and Artificial Intelligence, ICIBA 2024",,,,1713,1718,5.0,0,10.1109/ICIBA62489.2024.10868017,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85219152390&doi=10.1109%2fICIBA62489.2024.10868017&partnerID=40&md5=b5c45ea21e4049f7fe5b056f9e3ef0a6,"Deep learning has been proved to diagnose Attention Deficit/Hyperactivity Disorder (ADHD) accurately, but it has raised concerns about trustworthiness because of the lack of explainability. Fortunately, the development of explainable artificial intelligence (XAI) offers a solution to this problem. In this study, we employed a VR-based GO/NOGO task with distractions, capturing participants' eye movement, head movement, and electroencephalography (EEG) data. We used the collected data to train an explainable multimodal fusion model. Besides classifying between ADHD and normal children, the proposed model also generates explanation heatmaps. The heatmaps provide the importance of specific variables and timestamps in the EEG data to help us analyze the patterns captured by the model. According to our observations, the model identified specific time intervals that related to specific event-related potentials (ERPs) components. The heatmaps also demonstrate that the impacts of distractions vary between not only the GO and NOGO events but also ADHD and normal children. © 2024 IEEE.",Attention Deficit/Hyperactivity Disorder (ADHD); Explainable Artificial Intelligence (XAI); multimodal fusion; neurobehavioral data; Virtual Reality (VR),Data fusion; Virtual reality; Attention deficit hyperactivity disorder; Attention deficit/hyperactivity disorder; Explainable artificial intelligence (XAI); Fusion model; Head movements; Heatmaps; Multi-modal fusion; Neurobehavioral data; Neurobehavioural; Virtual reality; Eye movements,Conference paper,Final,,Scopus,2-s2.0-85219152390,Gaming / VR
Huang Y.-T.; Gong A.-D.,"Huang, Yi-Ting (57221274536); Gong, An-Di (57914727700)",57221274536; 57914727700,The Role of Game Involvement on Attention to Ads: Exploring Influencing Factors of Visual Attention to Game Ads on Instagram Stories,2024,Human Behavior and Emerging Technologies,2024,,3706590,,,,1,10.1155/2024/3706590,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85201752776&doi=10.1155%2f2024%2f3706590&partnerID=40&md5=2ee7288b52bcb235f3eed88e9061bc09,"Instagram Stories advertising has the advantages of precise targeting, diverse advertising formats, and a wide range of users; it has, thus, become an important medium for mobile game marketing. To find the most effective ad designs for mobile games on Instagram Stories, this study applied the stimulus-organism-response (S-O-R) theory to investigate how game involvement and ad designs affect users' visual attention. Visual attention is an important physiological indicator of advertising effectiveness, and eye-tracking technology can accurately assess consumers' visual attention. This study applied eye-tracking technology to 80 participants, grouped as follows: high-involvement gamers, low-involvement gamers, and nongamers. This study used the eye movement indicators for first fixation duration (FFD) and total fixation duration (TFD) to, respectively, evaluate the attentional salience and hold of mobile game ads. The ads were presented in three formats (5 s video, 15 s video, and 5 s image) with two types of content (gameplay and game characters). Results showed that the mobile game ads on Stories exerted the highest attentional hold on high-involvement gamers. In terms of ad format, video ads offered a higher attentional hold and attentional salience. In terms of ad content, ads that introduced game characters resulted in better attentional salience; however, ads that introduced gameplay exerted better attentional hold for nongamers. This study examined both individual differences in media users and ad design to provide recommendations for the personalization of mobile game ads for social media. For example, ads designed for a high-involvement gamer should incorporate more diverse and complex information. In addition, we found that when the marketing goal is the promotion of game characters, image ads are the most appropriate format.  © 2024 Yi-Ting Huang and An-Di Gong.",,,Article,Final,All Open Access; Gold Open Access,Scopus,2-s2.0-85201752776,Gaming / VR
Prummer F.; Sidenmark L.; Gellersen H.,"Prummer, Franziska (57819924300); Sidenmark, Ludwig (57210111157); Gellersen, Hans (6701531333)",57819924300; 57210111157; 6701531333,Dynamics of Eye Dominance Behavior in Virtual Reality,2024,Journal of Eye Movement Research,17,3,2,,,,4,10.16910/JEMR.17.3.2,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85192953165&doi=10.16910%2fJEMR.17.3.2&partnerID=40&md5=10d7135e0b110f3e7e86924720c74d21,"Prior research has shown that sighting eye dominance is a dynamic behavior and dependent on horizontal viewing angle. Virtual reality (VR) offers high flexibility and control for studying eye movement and human behavior, yet eye dominance has not been given significant attention within this domain. In this work, we replicate Khan and Crawford’s (2001) original study in VR to confirm their findings within this specific context. Additionally, this study extends its scope to study alignment with objects presented at greater depth in the visual field. Our results align with previous results, remaining consistent when targets are presented at greater distances in the virtual scene. Using greater target distances presents opportunities to investigate alignment with objects at varying depths, providing greater flexibility for the design of methods that infer eye dominance from interaction in VR. Copyright © 2024, Prummer, F., Sidenmark, L. & Gellersen, H. This article is licensed under a Creative Commons Attribution 4.0 International license.",distance in VR; Dominant eye; eye tracking; virtual reality,,Article,Final,All Open Access; Gold Open Access; Green Open Access,Scopus,2-s2.0-85192953165,Gaming / VR
Chao W.; Xuxi Y.; Zhe W.,"Chao, Wang (58928803800); Xuxi, Yin (57610012200); Zhe, Wang (59434153000)",58928803800; 57610012200; 59434153000,Research on the Application of Artificial Intelligence and Virtual Reality Technology in Intelligent Education Interactive System,2024,"Proceedings - 2024 2nd International Conference on Mechatronics, IoT and Industrial Informatics, ICMIII 2024",,,,517,521,4.0,0,10.1109/ICMIII62623.2024.00102,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85204775086&doi=10.1109%2fICMIII62623.2024.00102&partnerID=40&md5=2e59f764f8a215b2b05c4ecc85eb0ce2,"In this paper, eye tracking technology is used to obtain human line of sight information, real-time analysis and display. The visual effect shows the observation range and the movement of the eyes of the students, so that the teacher can make instant evaluation of the class. This fully plays the role of guiding and supervising students in VR teaching. A global user-oriented attention range visualization algorithm is studied, which is realized by using spatial clustering and statistics. Then according to the design of user registration module, projection selection module, multi-channel edge fusion module, in this paper, an effective single-user track visualization method is proposed. An online clustering algorithm based on density is designed to filter users' invalid concern data. The experiment proves immediate and direct response to video content. The system runs stably, the performance is stable, and the realization of each function module has reached the expected goal. © 2024 IEEE.",data visualization; J2EE technology; online education; Virtual reality,Data visualization; Eye movements; Students; Teaching; Virtual reality; Visualization; Eye tracking technologies; Intelligent educations; Interactive system; J2EE technology; Line of Sight; Lines-of-sight; On-line education; Real time analysis; Real time display; Virtual reality technology; Virtual environments,Conference paper,Final,,Scopus,2-s2.0-85204775086,Gaming / VR
Rikhtehgar D.J.; Usta B.; Wang S.,"Rikhtehgar, Delaram Javdani (57810567500); Usta, Batuhan (59921268500); Wang, Shenghui (57191711697)",57810567500; 59921268500; 57191711697,Ontology-Based Modeling for Object Segmentation and Eye Gaze Data in VR Art Exhibitions,2024,CEUR Workshop Proceedings,3967,,,,,,0,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-105006929994&partnerID=40&md5=1e04b1daf55ae2013bb32f94824489fa,"Virtual reality (VR) art exhibitions provide an immersive platform for experiencing art, yet understanding user interaction and perception within these environments remains a challenge. Traditional VR approaches often fail to offer personalized insights into how users engage with specific elements of an artwork. Object detection and segmentation techniques hold potential by enabling the identification and localization of objects within paintings, thereby adding semantic meaning to user eye-gaze data. In this work, we propose an ontology-based modeling framework that links segmented objects in artworks with user gaze data. This framework allows for the integration of numerical data, such as image segments, and measurement data, including user engagement metrics (e.g., eye-gaze patterns), time series (tracking eye movements across exhibits), and qualitative assessments of user experience. By semantically enriching gaze data through associations with specific objects and concepts within the artwork, we are able to generate insights into gaze-based behaviors, such as individual users' visual attention and navigation. These insights enable the exploration of higher-order interpretations, such as visitor behavior and engagement trends, which are critical for improving user experience and optimising exhibition design. Our study demonstrates how this ontology was populated with real-world data from a user study and present further analysis based on recorded numerical data from the virtual environment. Finally, we discuss our findings, limitations, and potential directions for future work. Copyright © 2024 for this paper by its authors.",Cultural Heritage; Eye Tracking; Object Detection; Ontology-based modeling; Segmentation; Virtual Reality,Engineering exhibitions; Object detection; Semantic Segmentation; Virtual environments; Cultural heritages; Eye-gaze; Eye-tracking; Numerical data; Objects detection; Objects segmentation; Ontology based modeling; Segmentation; Users' experiences; Virtual reality art; Virtualization,Conference paper,Final,,Scopus,2-s2.0-105006929994,Gaming / VR
Kovalev A.; Nefeld E.; Anton G.,"Kovalev, Artem (56102634700); Nefeld, Ekaterina (59258841300); Anton, Gasimov (58354029300)",56102634700; 59258841300; 58354029300,The Optokinetic Nystagmus Indicates Functional State Changes: VR Study,2024,"Proceedings - 6th International Conference ""Neurotechnologies and Neurointerfaces"", CNN 2024",,,,87,89,2.0,0,10.1109/CNN63506.2024.10705838,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85207834148&doi=10.1109%2fCNN63506.2024.10705838&partnerID=40&md5=bc08355247863b8df856dbd86d51a201,"The purpose of this work was to study the relationship between the features of optokinetic nystagmus and changes in the functional state of a person during observation of rotation in virtual reality. The following hypothesis has been proposed: during the changes in functional state under optokinetic stimulation, slow phases of nystagmus will be longer than during moments of functional state stability. The experiment involved 10 subjects. A virtual optokinetic drum rotating around vertical axis in a horizontal plane at a speed of 30, 45 and 60 degrees per second was used for stimulation. The stimulation was demonstrated using virtual reality system HTC Vive Pro Eye. The analyzed parameters included the relative duration of the slow phase of OKN, the overall score of the ""Simulator Sickness Questionnaire”, and the subjective perception of the own state. It was found that with an increase in rotation speed, the overall score of the questionnaire and the subjective perception of the changes in functional state also increased. It was also found that the optokinetic nystagmus slow phases duration increases during the functional state instability, which can be explained by the phenomenon of ""nystagmus fading,"" which occurs due to eye muscle fatigue. ©2024 IEEE.",eye movements; functional state; optokinetic nystagmus; virtual reality,Eye movements; Virtualization; Functional state; Optokinetic nystagmus; Rotation speed; Simulator sickness; Slow phase; State stability; States change; Subjective perceptions; Vertical axis; Virtual reality system; Virtual environments,Conference paper,Final,,Scopus,2-s2.0-85207834148,Gaming / VR
Hugo L.; Sean H.,"Hugo, Loydell (59673828300); Sean, Hanna (59673674800)",59673828300; 59673674800,Beyond Depth Cues Lighting and visual complexity as factors in navigation,2024,"14th International Space Syntax Symposium, SSS 2024",,,,,,,0,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-86000248018&partnerID=40&md5=35616ef7cffbaac443aedf29f3a21033,"Spatial configuration is central to Space Syntax modelling, either implicitly in methods such as axial line analysis (Hillier & Hanson 1984; Hillier 1996), or explicitly in visibility graph analysis (Turner et al., 2001) and visual agents (Penn and Turner, 2001), and in fundamental spatial principles on which these are based (Benedikt, 1979). Its effect on movement via individual navigation is mediated by the capacity of human vision, as demonstrated in observations of the link between visual fixation and route selection (Emo, 2014). Vision, however, is frequently unable to perceive spatial configuration accurately (McElhinney et al., 2022), and may be affected by other aspects of the environment with relevant influence on movement, which are not currently accommodated by Space Syntax models. This paper investigates two variables distinct from spatial configuration, light intensity and surface complexity, for their effect on route choice. A 3D game-like environment, implemented through Grasshopper within Rhino3D, was used to record the behaviour of human navigators exploring an irregular pattern of orthogonally placed, intersecting corridors, for which both light and complexity were varied. Routes were recorded for each journey, and gaze monitored using an eye-tracking headset developed for this experiment. Results reveal relationships between each of the variables and gaze, and between gaze and subsequent path choice. Compared with a baseline of all possible isovists within the environment, the gaze distribution of participants for all experiments has more distant mean and peak values, and this is most distant when light is varied. Movement, as assessed by path choice at corridor junctions, shows an expected overall correlation with path angle, but the relationship with other spatial variables, such as visible distance, varies significantly across the experiment when light and complexity variables are changed. Both variables are seen to correlate positively with paths chosen, with the effect of surface complexity being stronger when both are varied simultaneously. A causal chain can be inferred that suggests higher relative light levels draw the visual attention, and one or both of these then positively influence the choice of route in that direction. © 14th International Space Syntax Symposium, SSS 2024.",complexity; eye-tracking; lighting; navigation; perception; Visible distance,Air navigation; Depth perception; Eye movements; Complexity; Depth cue; Eye-tracking; Line analysis; Path choice; Space syntax models; Spatial configuration; Surface complexity; Visible distance; Visual complexity; Syntactics,Conference paper,Final,,Scopus,2-s2.0-86000248018,Gaming / VR
Negri Y.; Panceri J.; Freitas É.; Schreider S.; Caldeira E.; Bastos-Filho T.,"Negri, Yuri (58799677300); Panceri, João (57371491900); Freitas, Éberte (57371644800); Schreider, Sheila (57643522600); Caldeira, Eliete (6506632422); Bastos-Filho, Teodiano (6602380721)",58799677300; 57371491900; 57371644800; 57643522600; 6506632422; 6602380721,Proposal of Serious Games for a Socially Assistive Robot Based on Eye Contact and Visual Attention of Children with Autism Spectrum Disorder,2024,IFMBE Proceedings,100,,,538,549,11.0,0,10.1007/978-3-031-49407-9_54,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85181768145&doi=10.1007%2f978-3-031-49407-9_54&partnerID=40&md5=0b838fc745555133f3396e51f88d7d1d,"Eye contact is one of the main human skills, and a prerequisite of verbal language. However, children with Autism Spectrum Disorder (ASD) often have an important deficit in this skill, compromising their entire cognitive and social development. This work shows the development of two Serious Games (SGs), based on the concept of Child-Robot Interaction (CRI), for the exercise and improvement of eye contact and visual attention, as well as concepts such as imitation and emotion recognition. For face detection and eye movement monitoring, an open source development framework of machine learning named MediaPipe was used. Tests with children with ASD will be conducted, and it is expected these SGs have a positive impact regarding the improvement of both eye contact and visual attention for these children. © 2024, The Author(s), under exclusive license to Springer Nature Switzerland AG.",Autism Spectrum Disorder; Eye contact; Social assistive robot; Social interaction; Visual attention,Behavioral research; Diseases; Emotion Recognition; Eye movements; Face recognition; Robots; Assistive robots; Autism spectrum disorders; Children with autisms; Cognitive development; Eye-contact; Human skills; Social assistive robot; Social interactions; Socially assistive robots; Visual Attention; Serious games,Conference paper,Final,,Scopus,2-s2.0-85181768145,Gaming / VR
Artiran S.; Bedmutha P.S.; Cosman P.,"Artiran, Saygin (57419665000); Bedmutha, Poorva S. (57409247900); Cosman, Pamela (7003359562)",57419665000; 57409247900; 7003359562,"Analysis of Gaze, Head Orientation, and Joint Attention in Autism With Triadic VR Interviews",2024,IEEE Transactions on Neural Systems and Rehabilitation Engineering,32,,,759,769,10.0,4,10.1109/TNSRE.2024.3363728,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85185221358&doi=10.1109%2fTNSRE.2024.3363728&partnerID=40&md5=6ab17df73891c98e5c8723203c2a9939,"Effective use of gaze and head orientation can strengthen the sense of inclusion in multi-party interactions, including job interviews. Not making significant eye contact with the interlocutors, or not turning towards them, may be interpreted as disinterest, which could worsen job interview outcomes. This study aims to support the situational solo practice of gaze behavior and head orientation using a triadic (three-way) virtual reality (VR) job interview simulation. The system lets users encounter common interview questions and see how they share attention among the interviewers based on their conversational role (speaking or listening). Given the yaw and position readings of the VR headset, we use a machine learning-based approach to analyze head orientations relative to the interviewers in the virtual environment, and achieve low angular error in a low complexity way. We examine the degree to which interviewer backchannels trigger attention shifts or behavioral mirroring and investigate the social modulation of gaze and head orientation for autistic and non-autistic individuals. In both speaking and listening roles, the autistic participants gazed at, and oriented towards the two virtual interviewers less often, and they displayed less behavioral mirroring (mirroring the head turn of one avatar towards another) compared to the non-autistic participants. © 2001-2011 IEEE.",Autism; job interview practice; machine learning; social modulation of gaze and head orientation; virtual reality,Attention; Autistic Disorder; Avatar; Communication; Humans; Virtual Reality; Artificial intelligence; Behavioral research; Diseases; Learning systems; Magnetic heads; Autism; Behavioral science; Interview; Job interview practice; Job interviews; Joint attention; Machine-learning; Oral communication; Social modulation of gaze and head orientation; adult; Article; autism; Autism spectrum quotient score; backchannel timing; Behavioral Mirroring; eye movement; eye tracking; face-to-face interview; female; gaze; Head Orientation; human; job interview; joint attention; Kolmogorov Smirnov test; machine learning; male; Participatory Design; questionnaire; social behavior; Social modulation; system pipeline; triadic conversation; virtual reality; voice activity; voice activity detection; attention; interpersonal communication; Virtual reality,Article,Final,All Open Access; Gold Open Access,Scopus,2-s2.0-85185221358,Gaming / VR
Guo L.; Babekuhl D.; Orr R.; Kavehei O.,"Guo, Luyao (59384889300); Babekuhl, Daniel (58972913400); Orr, Rhonda (8510056600); Kavehei, Omid (56011177700)",59384889300; 58972913400; 8510056600; 56011177700,A wearable non-contact optical system based on muscle tracking for ultra-long-term and indirect eye-tracking,2024,"International Conference on Electrical, Computer, and Energy Technologies, ICECET 2024",,,,,,,0,10.1109/ICECET61485.2024.10698669,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85207391085&doi=10.1109%2fICECET61485.2024.10698669&partnerID=40&md5=0acea7ccf427544053a06d7feba9563d,"Eye signals have become popular worldwide due to their potential in multiple fields. Such data can be regarded as indications of diseases such as epilepsy, a sign of fatigue, expression of potential interests during shopping and so on. In this paper, we demonstrate a novel optical eye-tracking system, NIRSense, which is able to track eye activities based on muscle movements, without any touching with the cornea. The NIRSense sensors, six pairs of near-infrared (NIR) light-emitting diodes (LEDs) and photodiodes are emitting 940 nm lights on skins below eyes in a contactless distance, where lights are able to penetrate and be reflected back to photodiodes. These signals are then transmitted into electric currents and processed to build relationships between signals and pupil locations. It is proved that the NIRSense system can track eye activities not only accurately (1.05 degrees) and precisely (0.6 degrees), but also in a safe, long-term used way. Moreover, the hardware design of the NIRSense system allows the assembling easily on diverse near-eye frames, involving Virtual reality (VR) goggles, Augmented reality (AR) headsets and normal glasses frames, via attaching sensors near the bottoms of lenses. In this way, this system has enriched the possibilities of exploring human reactions to virtual or real environments. In conclusion, We propose the NIRSense eye tracking system with functions of accurate and precise eye trace prediction, pupil tracking with eyelids closure, comparison of performances with a commercial eye tracker Tobii Pro Glasses 3, robustness to ambient light, and future clinical application for concussion assessment. © 2024 IEEE.",Eye muscles detection; Human-centered computing; Optical eye tracking techniques; Pupil trace mapping,Breath controlled devices; Contact lenses; Electrotherapeutics; Eye protection; Eyeglasses; Goggles; Integrated circuit design; Eye activities; Eye muscle detection; Eye tracking systems; Eye-tracking; Human-centered computing; Non-contact; Optical eye tracking technique; Optical-; Pupil trace mapping; Tracking techniques; Virtualization,Conference paper,Final,,Scopus,2-s2.0-85207391085,Gaming / VR
Bonneterre S.; Zerhouni O.; Boffo M.,"Bonneterre, Solenne (57218326742); Zerhouni, Oulmann (55644350500); Boffo, Marilisa (37074132900)",57218326742; 55644350500; 37074132900,"The Influence of Billboard-Based Tobacco Prevention Posters on Memorization, Attitudes, and Craving: Immersive Virtual Reality Study",2024,Journal of Medical Internet Research,26,1,e49344,,,,2,10.2196/49344,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85198322713&doi=10.2196%2f49344&partnerID=40&md5=213e357a03ab046cf6c33e6bf3e1c208,"Background: Health prevention campaigns often face challenges in reaching their target audience and achieving the desired impact on health behaviors. These campaigns, particularly those aimed at reducing tobacco use, require rigorous evaluation methods to assess their effectiveness. Objective: This study aims to use immersive virtual reality (iVR) to systematically evaluate recall, attitudinal, and craving responses to antitobacco prevention messages when presented in a realistic virtual environment, thereby exploring the potential of iVR as a novel tool to improve the effectiveness of public health campaigns. Methods: A total of 121 undergraduate students (mean age 19.6, SD 3.7 years), mostly female (n=99, 82.5%), were invited to take a guided walk in the virtual environment, where they were randomly exposed to a different ratio of prevention and general advertising posters (80/20 or 20/80) depending on the experimental condition. Participants’ gaze was tracked throughout the procedure, and outcomes were assessed after the iVR exposure. Results: Incidental exposure to antitobacco prevention and general advertising posters did not significantly alter attitudes toward tobacco. Memorization of prevention posters was unexpectedly better in the condition where advertising was more frequent (β=–6.15; P<.001), and high contrast between poster types led to a better memorization of the less frequent type. Despite a nonsignificant trend, directing attention to prevention posters slightly improved their memorization (β=.02; P=.07). In addition, the duration of exposure to prevention posters relative to advertisements negatively affected memorization of advertising posters (β=–2.30; P=.01). Conclusions: Although this study did not find significant changes in attitudes toward tobacco after exposure to prevention campaigns using iVR, the technology does show promise as an evaluation tool. To fully evaluate the use of iVR in public health prevention strategies, future research should examine different types of content, longer exposure durations, and different contexts. © 2024 JMIR Publications Inc.. All rights reserved.",advertising; health prevention campaigns; health promotion; immersive virtual reality; incidental exposure; smoking; tobacco,Adolescent; Adult; Advertising; Craving; Female; Health Promotion; Humans; Male; Posters as Topic; Smoking Prevention; Virtual Reality; Young Adult; adult; advertising; Article; craving; cybersickness; demographics; eye fixation; eye tracking; female; gaze; health promotion; human; human experiment; male; normal human; public health campaign; saccadic eye movement; smoking; tobacco use; undergraduate student; virtual reality; adolescent; craving; procedures; publication; smoking prevention; young adult,Article,Final,All Open Access; Gold Open Access; Green Open Access,Scopus,2-s2.0-85198322713,Gaming / VR
Stær K.; Iranzo A.; Terkelsen M.H.; Stokholm M.G.; Danielsen E.H.; Østergaard K.; Serradell M.; Otto M.; Svendsen K.B.; Garrido A.; Vilas D.; Santamaria J.; Møller A.; Gaig C.; Brooks D.J.; Borghammer P.; Tolosa E.; Pavese N.,"Stær, Kristian (57216506824); Iranzo, Alex (7004231237); Terkelsen, Miriam Højholt (57900406800); Stokholm, Morten Gersel (56439916100); Danielsen, Erik Hvid (7006532802); Østergaard, Karen (7005767794); Serradell, Mónica (17346855800); Otto, Marit (7201539857); Svendsen, Kristina B. (55197674700); Garrido, Alicia (57189043160); Vilas, Dolores (35094644300); Santamaria, Joan (7202364451); Møller, Arne (7401764527); Gaig, Carles (8610914900); Brooks, David J. (35373001000); Borghammer, Per (8950557400); Tolosa, Eduardo (35392145900); Pavese, Nicola (6701818479)",57216506824; 7004231237; 57900406800; 56439916100; 7006532802; 7005767794; 17346855800; 7201539857; 55197674700; 57189043160; 35094644300; 7202364451; 7401764527; 8610914900; 35373001000; 8950557400; 35392145900; 6701818479,Progression of brain cholinergic dysfunction in patients with isolated rapid eye movement sleep behavior disorder,2024,European Journal of Neurology,31,1,e16101,,,,11,10.1111/ene.16101,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85174289804&doi=10.1111%2fene.16101&partnerID=40&md5=57153246092a70d8cf631551f2348792,"Background: Reduced cortical acetylcholinesterase activity, as measured by 11C-donepezil positron emission tomography (PET), has been reported in patients with isolated rapid eye movement (REM) sleep behavior disorder (iRBD). However, its progression and clinical implications have not been fully investigated. Here, we explored the relationship between longitudinal changes in brain acetylcholinesterase activity and cognitive function in iRBD. Methods: Twelve iRBD patients underwent 11C-donepezil PET at baseline and after 3 years. PET images were interrogated with statistical parametric mapping (SPM) and a regions of interest (ROI) approach. Clinical progression was assessed with the Movement Disorder Society-Unified Parkinson's Disease Rating Scale-Part III (MDS-UPDRS-III). Cognitive function was rated using the Mini-Mental State Examination (MMSE) and the Montreal Cognitive Assessment (MoCA). Results: From baseline to follow-up, the mean 11C-donepezil distribution volume ratio (DVR) decreased in the cortex (p = 0.006), thalamus (p = 0.013), and caudate (p = 0.013) ROI. Despite no significant changes in the group mean MMSE or MoCA scores being observed, individually, seven patients showed a decline in their scores on these cognitive tests. Subgroup analysis showed that only the subgroup of patients with a decline in cognitive scores had a significant reduction in mean cortical 11C-donepezil DVR. Conclusions: Our results show that severity of brain cholinergic dysfunction in iRBD patients increases significantly over 3 years, and those changes are more severe in those with a decline in cognitive test scores. © 2023 The Authors. European Journal of Neurology published by John Wiley & Sons Ltd on behalf of European Academy of Neurology.",biomarkers; cognition; Parkinson's disease; positron emission tomography; REM sleep behavior disorder,Acetylcholinesterase; Brain; Donepezil; Humans; Parkinson Disease; REM Sleep Behavior Disorder; acetylcholinesterase; carbon 11; donepezil; acetylcholinesterase; donepezil; aged; Article; brain cortex; caudate nucleus; cholinergic transmission; clinical article; clinical assessment; cognition; cohort analysis; dementia; enzyme activity; female; follow up; frontal lobe; human; male; MDS-Unified Parkinson Disease Rating Scale; mild cognitive impairment; Mini Mental State Examination; Montreal cognitive assessment; occipital lobe; parietal lobe; positron emission tomography; putamen; REM sleep behavior disorder; substantia innominata; temporal lobe; thalamus; volume of distribution ratio; brain; diagnostic imaging; Parkinson disease; psychology,Article,Final,All Open Access; Green Open Access; Hybrid Gold Open Access,Scopus,2-s2.0-85174289804,Gaming / VR
Zhao P.,"Zhao, Pengfei (58927979800)",58927979800,Exploring the influence of body movements on spatial perception in landscape and interior design,2024,MCB Molecular and Cellular Biomechanics,21,3,434,,,,1,10.62617/mcb434,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85208810930&doi=10.62617%2fmcb434&partnerID=40&md5=13e9586705f9530f7548f2b141f68ee8,"This study investigates the influence of body movements on spatial perception in both landscape and interior design environments, focusing on how physical interactions shape spatial understanding beyond visual perception alone. Grounded in the theory of embodied cognition, the research examines how gait, posture, and movement dynamics affect spatial awareness. The study captures detailed data on movement patterns and visual engagement across different spatial contexts using a combination of real-world observations and Virtual Reality (VR) simulations, motion-tracking systems, wearable sensors, and eye-tracking technology. A total of 157 participants, aged 20 to 65, navigated both outdoor landscapes and indoor environments, with key variables such as surface materials, spatial layout, and lighting conditions manipulated to assess their effects on spatial perception. The study measured gait speed, step frequency, path deviations, time to destination, visual attention, and subjective ratings of perceived openness, ease of movement, and emotional response. Key findings include that surface materials significantly influenced gait speed and step frequency. For example, participants walking on concrete had a significantly faster gait speed (mean difference = 0.5220, p = 0.001) than those walking on gravel. In terms of spatial layout, the two-way Analysis of variance (ANOVA) results showed that winding paths led to more path deviations (F-statistic = 350.00, p = 3.19 ×10−8) and longer times to destination (F-statistic = 1744.00, p = 2.39 x 10−11) compared to straight paths. The environment type (landscape vs. interior) also significantly affected navigation, with landscape participants showing a more significant deviation from direct paths (F-statistic = 19.60, p = 2.37 ×10−3). Visual engagement data, analyzed through a chi-square test, indicated that vertical elements like walls approached significance in attracting visual attention (Chi-square = 2.88, p = 0.0896), while other elements like trees and benches had less impact. The Wilcoxon signed-rank test results showed significant differences between real-world and VR experiences in perceived openness (W-statistic = 0.0, p = 0.001953), ease of movement (W-statistic = 0.0, p = 0.001953), and comfort (W-statistic = 0.0, p = 0.001953), highlighting VR’s limitations in replicating the full embodied experience of physical spaces. Copyright © 2024 by author(s).",body movements; embodied cognition; motion tracking; spatial perception; virtual reality,Error statistics; Eye movements; Mortar; Population statistics; Stereo vision; Subjective testing; Body movements; Embodied cognition; Gait speed; Interior designs; Landscape design; Motion tracking; Real-world; Spatial layout; Spatial perception; Surface materials; Interiors (building),Article,Final,All Open Access; Gold Open Access,Scopus,2-s2.0-85208810930,Gaming / VR
Yuan S.; Chen D.; Ding J.; He B.,"Yuan, Sinan (57984976900); Chen, Duhaoyu (59673256300); Ding, Jiayu (59673565800); He, Beijie (57195942342)",57984976900; 59673256300; 59673565800; 57195942342,Cognition-Relevant Vertical Spatial Morphological Characteristics in Traditional Mountain Villages Validated through a Virtual Reality Experiment: The case of Wang Lu village in Taihang Mountain Region in Northern China,2024,"14th International Space Syntax Symposium, SSS 2024",,,,,,,0,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-86000217940&partnerID=40&md5=03f378d0abe230e10819f22b2ed44e4f,"In northern China, existing traditional villages are predominantly situated in Taihang Mountain Region and are characterized by typical mountainous features. While relevant research has recognized certain characteristics of them, the precise indicators affecting individuals’ cognitive experiences remain ambiguous. Despite the spatial analytical and predictive modeling offered by Space Syntax for spatial cognition-relevant morphology, it focuses on planar geometric order, thus selectively ignores the vertical part. However, the vertical morphological features assume non-negligible significance in mountain village studies due to their obvious undulating terrain, and this aspect remains notably under-explored in international literature. From a cognitive perspective, this study delves into the vertical morphological characteristics of a representative sample of the traditional mountain villages in Taihang Mountain Region. Based on a virtual reality (VR) platform, a cognitive experiment was conducted as two parts: an overall roaming contrast experiment and a street segment experiencing comparative experiment, to validate different morphological indicators. A total of over 120 subjects participated the experiment and freely explored the virtual village environment, with real-time movement trajectory, eye-gazing data, and physiological indicators such as electroencephalogram (EEG) collected. Subsequently, participants were invited to complete perceptual evaluations and cognitive maps. The findings revealed that several vertical morphological indicators significantly impact individuals’ cognition and spatial memory of the mountain village to varying degrees. Through a representative case study, this paper elucidated the interrelations between vertical morphological features and cognitive characteristics, providing a foundational understanding of how the three-dimensional morphology of mountain villages shapes human cognition. This insight into cognition also furnished a theoretical underpinning for pertinent space-behavior models, including Space Syntax. © 14th International Space Syntax Symposium, SSS 2024.",Human Factors Research; Mountain Villages; Spatial Cognition; Vertical Characteristics; Virtual Reality Experiment,Eye movements; Physiological models; Rural areas; Syntactics; Virtual reality; Human factors research; Morphological characteristic; Mountain regions; Mountain village; Northern China; Space syntax; Spatial cognition; Taihang Mountains; Vertical characteristics; Virtual reality experiment; Virtual environments,Conference paper,Final,,Scopus,2-s2.0-86000217940,Gaming / VR
Shen M.; Xie Y.; Fujii Y.; Furuya N.,"Shen, Muhan (57657945200); Xie, Yunfei (57218995262); Fujii, Yuri (57219120384); Furuya, Nobuaki (55842422100)",57657945200; 57218995262; 57219120384; 55842422100,Analyzing spatial visual characteristics in Japanese stroll gardens based on eye-tracking technology: case study of Saihō-ji Garden,2024,Journal of Asian Architecture and Building Engineering,23,1,,140,156,16.0,5,10.1080/13467581.2023.2229413,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85165579716&doi=10.1080%2f13467581.2023.2229413&partnerID=40&md5=0984f8647082548152e33503da356e43,"The Japanese garden Saihō-ji is used as a case study to evaluate the spatial-visual characteristics of sequential landscapes using eye-tracking technology. We present objective data and quantitative analysis of subjective behavior, addressing the lack of research in landscape perception. In this study, we combine virtual reality and eye-tracking technologies to collect statistical data for an entire garden through static stimulus images. The data produced are objective and enable the direct assessment of landscape elements and objects. The correlation between spatial compositions indicators and the eye-tracking data reveals that different spatial compositions have varying effects on distinct gaze elements. While this research provides a preliminary validation, it still has certain limitations. Factors such as visual fatigue and demographic differences should be considered in future studies. Further, our findings emphasize the viability of eye-tracking technology for investigating the spatial visual characteristics of sequential landscapes, prompting further research in this field. © 2023 The Author(s). Published by Informa UK Limited, trading as Taylor & Francis Group on behalf of the Architectural Institute of Japan, Architectural Institute of Korea and Architectural Society of China.",Eye-tracking; Landscape perception; Sequential landscapes; VR,Virtual reality; Case-studies; Direct assessment; Eye tracking technologies; Eye-tracking; Landscape elements; Landscape perceptions; Sequential landscape; Spatial composition; Statistical datas; VR; Eye tracking,Article,Final,All Open Access; Gold Open Access,Scopus,2-s2.0-85165579716,Gaming / VR
Nasri M.; Kosa M.; Chukoskie L.; Moghaddam M.; Harteveld C.,"Nasri, Mahsa (58778417900); Kosa, Mehmet (57027872500); Chukoskie, Leanne (6507740817); Moghaddam, Mohsen (57191058842); Harteveld, Casper (23392508000)",58778417900; 57027872500; 6507740817; 57191058842; 23392508000,Exploring Eye Tracking to Detect Cognitive Load in Complex Virtual Reality Training,2024,"Proceedings - 2024 IEEE International Symposium on Mixed and Augmented Reality Adjunct, ISMAR-Adjunct 2024",,,,51,54,3.0,4,10.1109/ISMAR-Adjunct64951.2024.00022,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85214403661&doi=10.1109%2fISMAR-Adjunct64951.2024.00022&partnerID=40&md5=a4ca85e1b5ddda73ee163067626a83af,"Virtual Reality (VR) has been a beneficial training tool in fields like advanced manufacturing. However, users could experience a high cognitive load due to various factors, such as using VR hardware or tasks within the VR environment. Various studies have shown that eye-tracking has the potential to detect cognitive load, but in the context of VR and complex spatiotemporal tasks (e.g., assembly, disassembly), it is relatively unexplored. Here, we present an ongoing study to detect users' cognitive load using an eye-tracking-based machine learning approach. We developed a VR training system for cold spray and tested it with 22 participants, obtaining 19 valid eye-tracking datasets and NASA-TLX scores. We applied Multi-Layer Perceptron (MLP) and Random Forest (RF) models to compare the accuracy of predicting cognitive load (i.e., NASA-TLX) with pupil dilation and fixation duration. Our preliminary analysis demonstrates the possibility of using eye tracking to detect CL in complex spatiotemporal VR experiences and motivates our further explorations.  © 2024 IEEE.",cognitive load; machine learning; training; Virtual reality,Adversarial machine learning; Eye tracking; NASA; Virtual reality; Advanced manufacturing; Assembly-disassembly; Cognitive loads; Eye-tracking; In-field; Machine-learning; NASA-TLX; Training tools; Virtual reality training; Virtual-reality environment; Virtual environments,Conference paper,Final,All Open Access; Green Open Access,Scopus,2-s2.0-85214403661,Gaming / VR
Xinru W.,"Xinru, Wu (58573304400)",58573304400,A novel jigsaw game with eye-tracking: A multimodel interaction based on psycholinguistics for ADHD therapeutic,2024,Computer Animation and Virtual Worlds,35,1,e2214,,,,4,10.1002/cav.2214,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85170689289&doi=10.1002%2fcav.2214&partnerID=40&md5=7cff6415b3755acb208d7dae1d4f1bac,"Attention-deficit hyperactivity disorder (ADHD) causes impulsive or hyperactive behaviors and emotional outbursts or trouble focusing. Simple psychotherapy has difficulty achieving desired therapeutic effects, and ADHD diagnoses in adults and children are increasing. However, e-health games may mitigate the limitations of traditional methods by providing an ecologically relevant experience. In this paper, inspired by psycholinguistics, multimodal interaction, and he Pupil-CR method, we have developed a narrative game therapy with eye-tracking to enhance dyslexia and attention deficit manifestations. Using the situated discourses in the game's interactive narration, ADHD patients can concentrate and reduce anxiety and impulsivity. To evaluate the efficacy in improving attention a controlled trial (N = 48) randomly assigned volunteers in 1:1:1 to an eye-tracking jigsaw game or two control groups. The principal measure was the average difference in attention comparison score (ACS) of the test of variables of attention (TOVA) preintervention and post-intervention periods. Based on significant differences observed between the groups, we concluded the eye-tracking jigsaw game could serve as a viable ADHD intervention for children. © 2023 John Wiley & Sons, Ltd.",ADHD; digital therapy; eye-tracking; graphical human-computer interaction; user experience design,Diagnosis; Human computer interaction; Linguistics; Attention deficit hyperactivity disorder; Digital therapy; E health; Ehealth; Eye-tracking; Graphical human-computer interaction; Multi-modelling; Simple++; Therapeutic effects; User experience design; Eye tracking,Conference paper,Final,,Scopus,2-s2.0-85170689289,Gaming / VR
Schalbetter L.; Keller N.; Evans D.; Chamberlain B.; Hayek U.W.; Grêt-Regamey A.,"Schalbetter, Laura (57742649700); Keller, Nicolas (59166517000); Evans, David (58059078600); Chamberlain, Brent (7006697205); Hayek, Ulrike Wissen (36894532200); Grêt-Regamey, Adrienne (16024264700)",57742649700; 59166517000; 58059078600; 7006697205; 36894532200; 16024264700,Eye Tracking in VR to Analyse Physiological Responses to Peri-urban Landscape Elements,2024,Journal of Digital Landscape Architecture,2024,9,,482,489,7.0,2,10.14627/537752044,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85195590388&doi=10.14627%2f537752044&partnerID=40&md5=8e9b2c37cc1e47a466c2d489ab169cd6,"This research addresses the challenges of peri-urbanisation, which often leads to a loss of landscape character and residents' sense of place. To actively develop peri-urban landscapes as identity-creating places, we employ high-fidelity virtual reality scenes and use advanced sensing techniques, including electrodermal activity, heart rate variability, and eye-tracking. Unlike previous studies that have focused on whole urban scenes, our study delves into individual landscape elements to understand their impact on perception. Our analysis reveals correlations between participants' preferences, physiological responses and eye-tracking indices, shedding light on the complex relationship between subjective experience and visual attention patterns in peri-urban environments. © Wichmann Verlag, VDE VERLAG GMBH.",3D point cloud simulation; electrodermal activity (EDA); eye tracking in VR; heart rate variability (HRV); Virtual reality (VR),,Article,Final,,Scopus,2-s2.0-85195590388,Gaming / VR
Xenos M.; Mallas A.; Minas D.,"Xenos, Michalis (55185679800); Mallas, Andreas (57211256509); Minas, Dimosthenis (58949546700)",55185679800; 57211256509; 58949546700,Using Eye-Tracking for Adaptive Human-Machine Interfaces for Pilots: A Literature Review and Sample Cases,2024,Journal of Physics: Conference Series,2716,1,12072,,,,2,10.1088/1742-6596/2716/1/012072,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85188331759&doi=10.1088%2f1742-6596%2f2716%2f1%2f012072&partnerID=40&md5=49343e591f7acfa2da0f6c182381b288,"This paper explores the potential of eye-tracking technology in adaptive human-machine interfaces for pilots in aviation. We argue that an interface able to adjust its layout and elements based on pilots' real-time eye-tracking data can prevent errors and enhance their performance. The study presents a literature review on the use of eye-tracking for various pilot cases, including flight simulator games, drone pilots, and cockpit pilots. Results in most cases showed that eye-tracking has been employed to improve interactions, enhance spatial awareness, guide pilots' gaze to relevant areas, and provide insights into pilots' information processing and task load. The paper discusses two sample cases demonstrating the potential of eye-tracking in adaptive human-machine interfaces. In the first case, during challenging drone simulations, eye-tracking identified areas where an adaptive human-machine interface could aid navigation and reduce cognitive load. In the second one, based on real drone flights, when signal loss incidents occurred, eye-tracking data showed that the interface should adapt to pilots' needs by providing critical information to help them to improve situational awareness. The paper concludes that eye-tracking technology has significant potential in adaptive human-machine interfaces for aviation, emphasising the importance of refining these technologies to meet pilots' specific needs and enhance flight safety. © Published under licence by IOP Publishing Ltd.",,Drones; Flight simulators; Man machine systems; Element-based; Eye tracking technologies; Eye-tracking; Human Machine Interface; Literature reviews; Performance; Pilot information; Real-time eye tracking; Spatial awareness; Tracking data; Eye tracking,Conference paper,Final,All Open Access; Gold Open Access,Scopus,2-s2.0-85188331759,Gaming / VR
Avoyan A.; Ribeiro M.; Schotter A.; Schotter E.R.; Vaziri M.; Zou M.,"Avoyan, Ala (57215828704); Ribeiro, Mauricio (59146547600); Schotter, Andrew (6602431557); Schotter, Elizabeth R. (36440275400); Vaziri, Mehrdad (57203918285); Zou, Minghao (59146547700)",57215828704; 59146547600; 6602431557; 36440275400; 57203918285; 59146547700,Planned vs. Actual Attention,2024,Management Science,70,5,,2912,2933,21.0,1,10.1287/mnsc.2023.4834,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85194364820&doi=10.1287%2fmnsc.2023.4834&partnerID=40&md5=ba2879e612e3bdadbfe64cc2f4de1694,"People often need to plan how to allocate their attention across different tasks. In this paper, we run two experiments to study a stylized version of this attention-allocation problem between strategic tasks. More specifically, we present subjects with pairs of 2 × 2 games, and for each pair, we give them 10 seconds to decide how they would split a fixed time budget between the two games. Then, subjects play both games without time constraints, and we use eye-tracking to estimate the fraction of time they spend on each game. We find that subjects’ planned and actual attention allocation differ and identify the determinants of this mismatch. Further, we argue that misallocations can be relevant in games in which a player’s strategy choice is sensitive to the time taken to reach a decision. © 2024 INFORMS Inst.for Operations Res.and the Management Sciences. All rights reserved.",attention allocation; eye tracking; planned attention; time and choice,Behavioral research; Budget control; Allocation problems; Attention allocation; Eye-tracking; Fixed time; Planned attention; Strategy choices; Time and choice; Time budget; Time constraints; Eye tracking,Article,Final,All Open Access; Green Open Access,Scopus,2-s2.0-85194364820,Gaming / VR
Xu N.; Fan J.; Wen Z.,"Xu, Ning (58900120300); Fan, Jiluan (58900749100); Wen, Zikai (57847212900)",58900120300; 58900749100; 57847212900,Email Reading Behavior-Informed Machine Learning Model to Predict Phishing Susceptibility,2024,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),14509 LNCS,,,579,592,13.0,1,10.1007/978-981-99-9785-5_40,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85185709245&doi=10.1007%2f978-981-99-9785-5_40&partnerID=40&md5=8b49c33407def42fae8d160eb4e74e17,"As phishing threats intensify, incidents like the “COVID-19 vaccination form” phishing website underscore the limitations of relying solely on traditional firewall-based defenses. Consequently, there is a growing inclination towards user-centered anti-phishing solutions, exemplified by training games such as What.Hack. But could we proactively notify users in real time when they are on the brink of a scam or when their attention wanes? Our research explores machine learning and eye-tracking to identify email-reading weak spots and gauge a user’s risk of succumbing to phishing lures. We put forth innovative hybrid models, TransMLP Link and TransMLP Hybrid, melding the strengths of both Transformer and MLP. Our method also facilitates consistent interpretation of eye-tracking data across varied email interfaces and displays. Our TransMLP Hybrid model boasts an 88.75% accuracy rate, outperforming the standard Transformer model. Our research points to the future of anti-phishing tools that elegantly combine technological advancements with insights into human behavior.  © The Author(s), under exclusive license to Springer Nature Singapore Pte Ltd. 2024.",Anti-Phishing; Machine Learning; User Modeling,Behavioral research; Computer crime; COVID-19; Electronic mail; Machine learning; User profile; Anti-phishing; Eye-tracking; Hybrid model; Machine learning models; Machine-learning; Phishing; Phishing websites; Real- time; User Modelling; User-centred; Eye tracking,Conference paper,Final,,Scopus,2-s2.0-85185709245,Gaming / VR
Yüce A.,"Yüce, Arif (57199421030)",57199421030,Eye tracking,2024,"Encyclopedia of Sport Management, Second Edition",,,,353,354,1.0,0,10.4337/9781035317189.ch205,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85213188253&doi=10.4337%2f9781035317189.ch205&partnerID=40&md5=e68f57d538f05651ec813a870edd2656,"Eye-tracking technology is a pivotal tool for analyzing human visual behavior, allowing researchers to gain insights into attentional processes, cognitive performance, and decision-making. This technology uses devices like eye trackers and VR headsets to precisely monitor where and how individuals look at stimuli. Its application spans various fields, from marketing, where it helps understand consumer attention in ads, to psychology and human-computer interaction, improving interfaces and systems. Additionally, it's increasingly used in educational research to customize teaching strategies for better learning outcomes and in sports science to enhance athletes' performance and decision-making. Studies in sports marketing also leverage eye-tracking to optimize advertising strategies by analyzing fans' visual engagement. This wide-ranging use underlines eye-tracking's versatility and its growing importance in both academic and practical contexts, continuously expanding innovation horizons and revolutionizing learning and professional practices. © The Editor and Contributing Authors Severally 2024. All rights reserved.",Eye tracking; Human-computer interaction; Neuromarketing; Visual attention,,Book chapter,Final,,Scopus,2-s2.0-85213188253,Gaming / VR
Hall C.D.; Flynn S.; Clendaniel R.A.; Roberts D.C.; Stressman K.D.; Pu W.; Mershon D.; Schubert M.C.,"Hall, Courtney D. (57203516655); Flynn, Sheryl (7007046039); Clendaniel, Richard A. (6602782631); Roberts, Dale C. (7404254613); Stressman, Kara D. (58681704200); Pu, William (59168476600); Mershon, David (59167711200); Schubert, Michael C. (55708219000)",57203516655; 7007046039; 6602782631; 7404254613; 58681704200; 59168476600; 59167711200; 55708219000,"Remote assessment and management of patients with dizziness: development, validation, and feasibility of a gamified vestibular rehabilitation therapy platform",2024,Frontiers in Neurology ,15,,1367582,,,,4,10.3389/fneur.2024.1367582,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85195652063&doi=10.3389%2ffneur.2024.1367582&partnerID=40&md5=2e5e706116c5276928c6f8686513202b,"Introduction: Dizziness is a growing public health concern with as many as 95 million adults in Europe and the United States experiencing vestibular hypofunction, which is associated with reduced quality of life, poorer health, and falls. Vestibular rehabilitation therapy (VRT) is effective in reducing symptoms and improving balance; however, limited access to qualified clinicians and poor patient adherence impedes optimal delivery. The goal of this study was to develop and evaluate the feasibility of a remote therapeutic monitoring VRT Platform application (APP) for the assessment and treatment of vestibular dysfunction. Methods: User-centered iterative design process was used to gather and integrate the needs of users (clinicians and patients) into the design at each stage of development. Commonly used vestibular patient-reported outcome measures (PROs) were integrated into the APP and adults with chronic dizziness were enrolled to evaluate validity and reliability of the APP compared to standard clinical measures (CLIN). Gaze stabilization exercises were gamified to provide an engaging experience and an off-the-shelf sensor captured eye and head movement to provide feedback on accuracy of performance. A prospective, pilot study design with pre-and post-treatment assessment assessed feasibility of the APP compared to standard VRT (CLIN). Results: Participants with dizziness wanted a summary rehabilitation report shared with their clinicians, felt that an app could help with accountability, and believed that a gaming format might help with exercise adherence. Clinicians felt that the app should include features to record and track eye and head movement, monitor symptoms, score accuracy of task performance, and measure adherence. Validity and reliability of the digital PROs (APP) were compared to scores from CLIN across two sessions and found to have good validity, good to excellent test-retest reliability, and excellent usability (≥88%ile). The pilot study demonstrated feasibility for use of the APP compared to CLIN for treatment of vestibular hypofunction. The mean standard system usability score of the APP was 82.5 indicating excellent usability. Discussion: Both adult patients with chronic dizziness and VRT clinicians were receptive to the use of technology for VRT. The HiM-V APP is a feasible alternative to clinical management of adults with chronic peripheral vestibular hypofunction. Copyright © 2024 Hall, Flynn, Clendaniel, Roberts, Stressman, Pu, Mershon and Schubert.",digital health; dizziness; feasibility; remote assessment; remote patient monitoring; usability; vestibular rehabilitation,adult; aged; Article; clinical article; cognition assessment; controlled study; dizziness; eye movement; eye position; feasibility study; female; gait; head movement; human; male; middle aged; Mini Mental State Examination; patient monitoring; patient-reported outcome; physical activity; pilot study; quality control; quality of life; range of motion; reliability; remote assessment; unsteadiness; validation study; vestibular disorder; Vestibular rehabilitation therapy; visual acuity; visual field,Article,Final,All Open Access; Gold Open Access; Green Open Access,Scopus,2-s2.0-85195652063,Gaming / VR
Ramirez G.N.; Spivack J.; David-John B.,"Ramirez, G. Nikki (58260824400); Spivack, Jameson (59166102700); David-John, Brendan (57205639875)",58260824400; 59166102700; 57205639875,Deceptive Patterns and Perceptual Risks in an Eye-Tracked Virtual Reality,2024,"Proceedings - 2024 IEEE Conference on Virtual Reality and 3D User Interfaces Abstracts and Workshops, VRW 2024",,,,341,344,3.0,1,10.1109/VRW62533.2024.00068,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85192590044&doi=10.1109%2fVRW62533.2024.00068&partnerID=40&md5=7855e09b0068cd5ceba1bbeeb4e5756c,"In the context of targeted advertisements and design, the line between nudging and manipulation is difficult to define, measure, and enforce. The discussion of what crosses the line between nudging users towards content they may find more enjoyable and what could manipulate them towards specific behaviors is common when defining deceptive or dark patterns. Dark patterns are increasingly present in both web and mobile contexts: producing interface designs that make it difficult to cancel subscriptions or make informed decisions that differ from default settings. In this position paper, we discuss behavior manipulation via eye tracking and the ethical implications of attention guidance in the context of virtual reality (VR) to highlight key challenges for an emerging technology in immersive spaces. © 2024 IEEE.",,Ethical technology; Eye tracking; User interfaces; Default setting; Emerging technologies; Ethical implications; Eye-tracking; Immersive; Informed decision; Interface designs; Mobile context; Position papers; Targeted advertisements; Virtual reality,Conference paper,Final,,Scopus,2-s2.0-85192590044,Gaming / VR
Rehman A.; Heldal I.; Stilwell D.; Lin J.C.-W.,"Rehman, Abdul (59181221500); Heldal, Ilona (6506576998); Stilwell, Diana (59560923000); Lin, Jerry Chun-Wei (56825962600)",59181221500; 6506576998; 59560923000; 56825962600,"Towards a Supporting Framework for Neuro-Developmental Disorder: Considering Artificial Intelligence, Serious Games and Eye Tracking",2024,"Proceedings - 2024 IEEE International Conference on Big Data, BigData 2024",,,,8238,8240,2.0,0,10.1109/BigData62323.2024.10825158,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85218009125&doi=10.1109%2fBigData62323.2024.10825158&partnerID=40&md5=c7a77c4d509add8e393cc7ad76bf45af,"This paper focuses on developing a framework for uncovering insights about NDD children's performance (e.g., raw gaze cluster analysis, duration analysis & area of interest for sustained attention, stimuli expectancy, loss of focus/motivation, inhibitory control) and informing their teachers. The hypothesis behind this work is that self-adaptation of games can contribute to improving students' well-being and performance by suggesting personalized activities (e.g., highlighting stimuli to increase attention or choosing a difficulty level that matches students' abilities). The aim is to examine how AI can be used to help solve this problem. The results would not only contribute to a better understanding of the problems of NDD children and their teachers but also help psychologists to validate the results against their clinical knowledge, improve communication with patients and identify areas for further investigation, e.g., by explaining the decision made and preserving the children's private data in the learning process. © 2024 IEEE.",Artificial Intelligence; Eye Tracking; Neurodevelopmental Disorder; Serious Games,Ambient intelligence; Cluster analysis; Teaching; Area of interest; Developmental disorders; Eye-tracking; Neurodevelopmental disorder; Performance; Self- adaptations; Sustained attention; Teachers'; Well being; Well performance; Students,Conference paper,Final,,Scopus,2-s2.0-85218009125,Gaming / VR
Qu Z.; Byrne R.; Gorlatova M.,"Qu, Zhehan (59166362200); Byrne, Ryleigh (59166493300); Gorlatova, Maria (24775927000)",59166362200; 59166493300; 24775927000,'Looking' into Attention Patterns in Extended Reality: An Eye Tracking-Based Study,2024,"Proceedings - 2024 IEEE International Symposium on Mixed and Augmented Reality, ISMAR 2024",,,,855,864,9.0,3,10.1109/ISMAR62088.2024.00101,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85213499331&doi=10.1109%2fISMAR62088.2024.00101&partnerID=40&md5=d987beb03521e79a5dab56e6eabb6c2f,"Virtual reality (VR) simulations have been adopted to provide controllable environments for running augmented reality (AR) experiments in diverse scenarios. However, insufficient research has explored the impact of AR applications on users, especially their attention patterns, and whether VR simulations accurately replicate these effects. In this work, we propose to analyze user attention patterns via eye tracking during XR usage. To represent applications that provide both helpful guidance and irrelevant information, we built a Sudoku Helper app that includes visual hints and potential distractions during the puzzle-solving period. We conducted two user studies with 19 different users each in AR and VR, in which we collected eye tracking data, conducted gaze-based analysis, and trained machine learning (ML) models to predict user attentional states and attention control ability. Our results show that the AR app had a statistically significant impact on enhancing attention by increasing the fixated proportion of time, while the VR app reduced fixated time and made the users less focused. Results indicate that there is a discrepancy between VR simulations and the AR experience. Our ML models achieve 99.3% and 96.3% accuracy in predicting user attention control ability in AR and VR, respectively. A noticeable performance drop when transferring models trained on one medium to the other further highlights the gap between the AR experience and the VR simulation of it. © 2024 IEEE.",Human computer interaction (HCI); Human-centered computing; Interaction paradigms; Mixed / augmented reality; Virtual reality,Augmented reality; Mixed reality; Attention control; Computer interaction; Eye-tracking; Human computer interaction; Human-centered computing; Interaction paradigm; Machine learning models; Mixed / augmented reality; User attention; Virtual reality simulations; Virtual environments,Conference paper,Final,,Scopus,2-s2.0-85213499331,Gaming / VR
Heinemann B.; Schroeder U.,"Heinemann, Birte (57204048897); Schroeder, Ulrik (24434093700)",57204048897; 24434093700,Learning Analytics and Classroom Management in Specialized Environments: Enhancing the VR Classroom for CS Teacher Education,2024,Communications in Computer and Information Science,1904 CCIS,,,37,52,15.0,3,10.1007/978-3-031-47328-9_3,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85176930509&doi=10.1007%2f978-3-031-47328-9_3&partnerID=40&md5=5330c3de1d41c53ff11898f84d6065eb,"The teachers’ attention is an important factor in managing a classroom effectively. It affects students’ learning outcomes and is different for novice and experienced teachers. However, teaching classroom management, especially for specialized subjects like computer science where specific equipment is necessary, can be challenging. Moreover, it is difficult to provide real-life training opportunities as lessons cannot be repeated and experimentation time is very limited. This is particularly problematic for novice teachers and teacher students. To address this and support this issue, virtual reality and immersive learning can be used to simulate classroom scenarios and provide (data-driven and personal) feedback to pre-service teachers. In this paper, the subject-specific dimensions of computer science classroom management are discussed and a virtual computer lab for training purposes is proposed. Additionally, the potential of learning analytics to help learners reflect on their experiences is theory-led derived. The developments presented can be used to examine subject-specific differences between computer science and other subjects, as well as cultural differences between teachers around the world and differences between novices and more experienced teachers. © 2024, The Author(s), under exclusive license to Springer Nature Switzerland AG.",Classroom management; Computer science education; Eye-tracking; Learning analytics; Teacher training; Virtual reality,E-learning; Education computing; Engineering education; Eye tracking; Personnel training; Students; Teaching; Virtual addresses; Classroom management; Computer Science Education; Eye-tracking; Learning analytic; Reality learning; Student learning outcomes; Subject-specific; Teacher education; Teacher training; Teachers'; Virtual reality,Conference paper,Final,,Scopus,2-s2.0-85176930509,Gaming / VR
Szczepaniak D.; Harvey M.; Deligianni F.,"Szczepaniak, Dominik (57472957100); Harvey, Monika (7202675460); Deligianni, Fani (6506096408)",57472957100; 7202675460; 6506096408,ML-Driven Cognitive Workload Estimation in a VR-based Sustained Attention Task,2024,"Proceedings - 2024 IEEE International Symposium on Mixed and Augmented Reality Adjunct, ISMAR-Adjunct 2024",,,,557,560,3.0,0,10.1109/ISMAR-Adjunct64951.2024.00160,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85214432648&doi=10.1109%2fISMAR-Adjunct64951.2024.00160&partnerID=40&md5=39190bccaa3e8b9e44b1c3c45f7ca40f,"Cognitive training can boost and sharpen the brain's abilities to remember, focus, and switch between different tasks. One of the key elements of cognitive training is that it manipulates 'cognitive load', by adjusting the intensity of the intervention to suit the participant's ability level and keep the session enjoyable. This study introduces a novel sustained attention task in Virtual Reality (VR) to predict cognitive load dynamically. Unlike previous research, which often used non-VR settings, simpler tasks, or performance metrics as predictors, our approach aims to measure cognitive load objectively in a more ecologically valid and gamified VR environment. We employed machine learning techniques to enable real-time, personalized cognitive training. This work contributes to the development of more effective cognitive training interventions that can adapt to individual differences and maintain optimal engagement levels.  © 2024 IEEE.",cognitive load; cognitive training; eye-tracking; physiological data; Virtual reality,Adversarial machine learning; Virtual reality; Cognitive loads; Cognitive training; Cognitive workloads; Eye-tracking; Key elements; Performance metrices; Physiological data; Simple++; Sustained attention; Virtual-reality environment; Virtual environments,Conference paper,Final,,Scopus,2-s2.0-85214432648,Gaming / VR
Washington J.; Pande P.; Ramasamy P.; Moeller M.E.; Mojsoska B.,"Washington, Julianna (58044993600); Pande, Prajakt (56534799500); Ramasamy, Praveen (59821780400); Moeller, Morten Erik (57221702500); Mojsoska, Biljana (55650165100)",58044993600; 56534799500; 59821780400; 57221702500; 55650165100,Enacting Molecular Interactions in VR: Preliminary Relationships Between Visual Navigation and Learning Outcomes,2024,"Proceedings - 2024 IEEE Conference on Virtual Reality and 3D User Interfaces Abstracts and Workshops, VRW 2024",,,,945,946,1.0,1,10.1109/VRW62533.2024.00269,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85195548402&doi=10.1109%2fVRW62533.2024.00269&partnerID=40&md5=b0d8cc313052f6e4bd58841dea03bf9a,"Twenty-three undergraduates participated in a pre-post quasi-experimental single-group study involving an immersive VR simulation which allowed them to embody (i.e. become) a biomolecule and enact/experience its molecular interactions at a microscopic level using actions and gestures. Based on initial data analyses from this study, the present poster reports preliminary findings on the relationships between the participants' visual navigation, and conceptual as well as affective learning outcomes. © 2024 IEEE.",embodied cognition; enactive; eye tracking; science education; Virtual reality,E-learning; Eye tracking; Molecular interactions; Molecular structure; Embodied cognition; Enactive; Eye-tracking; Group study; Immersive VR; Learning outcome; Science education; Visual learning; Visual Navigation; VR simulation; Virtual reality,Conference paper,Final,,Scopus,2-s2.0-85195548402,Gaming / VR
Woodworth J.W.; Khokhar A.; Rahman Y.; Kulshreshth A.; Borst C.W.,"Woodworth, Jason W. (57192676048); Khokhar, Adil (57210910170); Rahman, Yitoshee (57215358246); Kulshreshth, Arun (55315352400); Borst, Christoph W. (9736479200)",57192676048; 57210910170; 57215358246; 55315352400; 9736479200,Attention Guidance In The Wild: An Experiment Testing Visual Guidance Cues for VR Field Trips at High Schools,2024,"Proceedings - 2024 IEEE Conference on Virtual Reality and 3D User Interfaces Abstracts and Workshops, VRW 2024",,,,173,179,6.0,0,10.1109/VRW62533.2024.00035,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85195545518&doi=10.1109%2fVRW62533.2024.00035&partnerID=40&md5=9958f9d750fd4227aa25b35800424673,"We assess the effectiveness of attention guidance cues in a virtual field trip in high schools with real students. Three eye-tracked visual cues, previously considered in lab settings for their ability to guide and restore attention, are compared against a baseline of no cue in a VR field trip of a virtual solar plant. Such cues are commonly studied in abstract lab tasks and not in the context of maintaining attention in an educational environment or in such real-world deployments. Results suggest a difference in effectiveness between lab subjects and high school students. © 2024 IEEE.",Applied Computing; E-learning; Human-centered computing; Education; Field studies Human-centered computing; HCI design and evaluation methods; Human computer interaction (HCI); Visualization; Visualization design and evaluation methods,E-learning; Eye tracking; Students; Virtual reality; Visualization; Applied computing; Design and evaluation methods; E - learning; E-learning;; Field studies; Field study human-centered computing; Human computer interaction; Human computer interaction design and evaluation method; Human-centered computing; Human-computer-interaction designs; Interaction design methods; Interaction evaluations; Visualization design and evaluation method; Visualization designs; Human computer interaction,Conference paper,Final,,Scopus,2-s2.0-85195545518,Gaming / VR
Kim N.; Gero J.S.,"Kim, Nayeon (57216898685); Gero, John S. (7006434401)",57216898685; 7006434401,Visual Attention to Biophilic Elements in Virtual Classroom Design: A VR Eye-Tracking Study,2024,Design Computing and Cognition’24: Volume 2,2,,,20,36,16.0,2,10.1007/978-3-031-71922-6_2,https://www.scopus.com/inward/record.uri?eid=2-s2.0-105004166502&doi=10.1007%2f978-3-031-71922-6_2&partnerID=40&md5=b0f9f311729be174d84f9f6d2896b7e0,"Recently, interest in indoor nature-based solutions through biophilic design interventions has grown across various spatial contexts. This study examines the influence of biophilic elements in virtual classroom design on university students’ visual attention, utilizing eye-tracking integrated with VR technology. This study focuses on how eye-tracking can provide insights into attention processes in relation to biophilic design, and whether two different biophilic treatments are superposable. The experiment involved 35 participants who interacted with seven different classroom cases, ranging from a non-biophilic control to various biophilic interventions. Analysis of fixations and saccades revealed significant differences in visual attention, with biophilic designs eliciting greater engagement than non-biophilic environments. The results show that applying two separate biophilic treatments are not superposable. The study’s findings suggest that biophilic design in VR enhances engagement, impacting the development of effective virtual learning spaces and informing future classroom designs. © The Editor(s) (if applicable) and The Author(s), under exclusive license to Springer Nature Switzerland AG 2025, corrected publication 2025.",,Biophilic elements; Eye-tracking; Eye-tracking studies; Spatial context; University students; Virtual Classroom; Virtual learning; Visual Attention; VR technology; Teaching,Book chapter,Final,,Scopus,2-s2.0-105004166502,Gaming / VR
Asish S.M.; Kulshreshth A.K.; Borst C.W.; Sutradhar S.,"Asish, Sarker M (57215357842); Kulshreshth, Arun K (55315352400); Borst, Christoph W (9736479200); Sutradhar, Shaon (57210735685)",57215357842; 55315352400; 9736479200; 57210735685,Classification of Internal and External Distractions in an Educational VR Environment Using Multimodal Features,2024,IEEE Transactions on Visualization and Computer Graphics,30,11,,7332,7342,10.0,1,10.1109/TVCG.2024.3456207,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85203974363&doi=10.1109%2fTVCG.2024.3456207&partnerID=40&md5=01ac392956f64e6260a0c7f8d36b9f9e,"Virtual reality (VR) can potentially enhance student engagement and memory retention in the classroom. However, distraction among participants in a VR-based classroom is a significant concern. Several factors, including mind wandering, external noise, stress, etc., can cause students to become internally and/or externally distracted while learning. To detect distractions, single or multi-modal features can be used. A single modality is found to be insufficient to detect both internal and external distractions, mainly because of individual variability. In this work, we investigated multi-modal features: eye tracking and EEG data, to classify the internal and external distractions in an educational VR environment. We set up our educational VR environment and equipped it for multi-modal data collection. We implemented different machine learning (ML) methods, including k-nearest-neighbors (kNN), Random Forest (RF), one-dimensional convolutional neural network - long short-term memory (1 D-CNN-LSTM), and two-dimensional convolutional neural networks (2D-CNN) to classify participants' internal and external distraction states using the multi-modal features. We performed cross-subject, cross-session, and gender-based grouping tests to evaluate our models. We found that the RF classifier achieves the highest accuracy over 83% in the cross-subject test, around 68% to 78% in the cross-session test, and around 90% in the gender-based grouping test compared to other models. SHAP analysis of the extracted features illustrated greater contributions from the occipital and prefrontal regions of the brain, as well as gaze angle, gaze origin, and head rotation features from the eye tracking data. © 1995-2012 IEEE.",EEG; Eye-tracking; Human-centered computing; Machine Learning,"Adult; Attention; Brain; Computer Graphics; Electroencephalography; Eye-Tracking Technology; Female; Humans; Machine Learning; Male; Neural Networks, Computer; Virtual Reality; Young Adult; Adversarial machine learning; Contrastive Learning; Convolutional neural networks; Federated learning; Long short-term memory; Nearest neighbor search; Convolutional neural network; Eye-tracking; Grouping tests; Human-centered computing; Machine-learning; Memory retention; Multi-modal; Multimodal features; Student engagement; Virtual-reality environment; adult; artificial neural network; attention; brain; computer graphics; diagnostic imaging; electroencephalography; eye-tracking technology; female; human; machine learning; male; physiology; procedures; virtual reality; young adult; Decision trees",Article,Final,,Scopus,2-s2.0-85203974363,Gaming / VR
Nakamura A.; Ohya M.; Furhashi H.,"Nakamura, Asuka (58999285000); Ohya, Masaki (59505257000); Furhashi, Hideo (7004819030)",58999285000; 59505257000; 7004819030,Autofocus control in the visual system of an avatar robot,2024,"International Conference on Control, Automation and Systems",,,,1560,1564,4.0,0,10.23919/ICCAS63016.2024.10773060,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85214428635&doi=10.23919%2fICCAS63016.2024.10773060&partnerID=40&md5=c4e942d51fe14a8c259891f1289361f9,"This study develops a visual system for an avatar robot. The system uses eye-tracking data obtained from a head-mounted display to perform autofocus control on images captured by a stereo camera. The distance between the two cameras of the eyes of the avatar robot is automatically controlled according to the interpupillary distance of the operator measured by the head-mounted display (HMD). The distance to the point of fixation is determined through distance estimation using eye tracking of the stereo camera, and the focus is adjusted to that point. Distance estimation was performed using geometric calculations. In the experiment, subjects wore the HMD, and virtual reality (VR) sickness was evaluated using subjective assessment methods; simulator sickness questionnaire (SSQ), virtual reality sickness questionnaire (VRSQ), and fast motion sickness scale (FMS). The superiority of autofocus was evaluated using the Wilcoxon signed rank test. Most values were less than 0.05, indicating that autofocus control is effective in reducing visual fatigue and mitigating VR sickness. This advancement enhances depth perception, reduces VR sickness, and improves immersion during operation. © 2024 ICROS.",autofocus; eye tracking; visual system,Anthropomorphic robots; Depth perception; Diseases; Stereo image processing; Subjective testing; Time difference of arrival; Virtualization; Auto focus; Distance estimation; Eye-tracking; Geometric calculations; Head-mounted-displays; Point of fixation; Stereo cameras; System use; Tracking data; Visual systems; Helmet mounted displays,Conference paper,Final,,Scopus,2-s2.0-85214428635,Gaming / VR
Nenna F.; Zanardi D.; Pluchino P.; Gamberini L.,"Nenna, Federica (57218995768); Zanardi, Davide (57817751700); Pluchino, Patrik (18038486400); Gamberini, Luciano (6603650210)",57218995768; 57817751700; 18038486400; 6603650210,Exploring age-related phenomena in VR-based teleoperations: a human-centered perspective for industry 5.0,2024,Behaviour and Information Technology,43,15,,3922,3938,16.0,2,10.1080/0144929X.2023.2291678,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85179982511&doi=10.1080%2f0144929X.2023.2291678&partnerID=40&md5=c6ec18c60e1c3e30acc5007d3f0e68bb,"The increasingly aging workforce is bringing particular attention to senior individuals in production sectors. While the interest in Virtual Reality (VR) applications for industrial robotics grows, the question of whether and how senior workers can withstand VR-based repetitive tasks arises. We here aimed to answer such questions by systematically assessing young and senior users’ experiential, behavioural, and cognitive factors during simulated robotic teleoperations in VR. Two control systems for VR telerobotics, button- and action-based controls, were employed. Human performance, vigilance, and workload were measured through self-reports and a VR-integrated eye-tracker. Additionally, age-dependent differences in individual cultural and experiential factors were explored via self-report measures. Despite being slower and experiencing increased fatigue under specific conditions, as suggested by the eye-tracking measures, senior users demonstrated comparable precision in operating the robotic arm to their younger counterparts. Notably, both age groups reported similar levels of perceived fatigue. The paper provides an in-depth analysis of the advantages and challenges of adopting advanced telerobotics control systems across different age groups, consistently emphasising the human-centered dimension. © 2023 Informa UK Limited, trading as Taylor & Francis Group.",aging; eye-tracking; human-centered IT; Human-Robot Interaction; Virtual reality,Control systems; Eye tracking; Human robot interaction; Industrial robots; Man machine systems; Remote control; Age groups; Age-related; Aging workforce; Eye-tracking; Human-centered IT; Humans-robot interactions; Industrial robotics; Production sector; Senior user; Senior worker; Virtual reality,Article,Final,,Scopus,2-s2.0-85179982511,Gaming / VR
Zhang C.; Kunze K.,"Zhang, Chuyang (59166273400); Kunze, Kai (21743317500)",59166273400; 21743317500,Cross-Reality Attention Guidance on the Light Field Display,2024,"Proceedings - 2024 IEEE Conference on Virtual Reality and 3D User Interfaces Abstracts and Workshops, VRW 2024",,,,809,810,1.0,0,10.1109/VRW62533.2024.00201,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85195592304&doi=10.1109%2fVRW62533.2024.00201&partnerID=40&md5=81b20786ce214e47e43dbba7bd58a448,"In this work we present a cross-reality collaboration system with visual attention guidance that allows a user in virtual reality (VR) to share his view with the users in the real world. The VR user can remotely select the display content of the light field display oriented to multiple real-world users by manipulating the camera in the virtual world. Using a head-mounted display (HMD) integrated with an eye tracker, the VR user's gaze focus depth is estimated and then used to adjust the focal plane of the light field display. The dioptric blur on the areas out of the focal plane is used as a stimulus to naturally guide real-world viewers to where the VR user is currently focusing on in real time. In a high-quality and low-cost way, our system can improve collaboration in multi-user scenarios especially in which one host user is delivering information to other users such as in classrooms and museums. © 2024 IEEE.",Virtual Reality,Behavioral research; Eye tracking; Field emission displays; Focusing; Helmet mounted displays; User interfaces; Collaboration systems; Eye trackers; Focal Plane; Head-mounted-displays; High-low; Light field displays; Real- time; Real-world; Virtual worlds; Visual Attention; Virtual reality,Conference paper,Final,,Scopus,2-s2.0-85195592304,Gaming / VR
Panjaburee P.; Hwang G.-J.; Intarakamhang U.; Srisawasdi N.; Chaipidech P.,"Panjaburee, Patcharin (35746909100); Hwang, Gwo-Jen (7202677655); Intarakamhang, Ungsinun (55090096100); Srisawasdi, Niwat (55203537900); Chaipidech, Pawat (57194160708)",35746909100; 7202677655; 55090096100; 55203537900; 57194160708,Effects of a personalized game on students’ outcomes and visual attention during digital citizenship learning,2024,Cogent Education,11,1,2351275,,,,0,10.1080/2331186X.2024.2351275,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85193069830&doi=10.1080%2f2331186X.2024.2351275&partnerID=40&md5=420f018c8ffef18ab4fc0fa5f29a65bb,"Previous studies have designed educational methods to cultivate digital citizenship behavior and support the construction of knowledge. However, these methods have not well incorporated personalized feedback mechanisms for enhancing digital citizenship knowledge. Therefore, this study proposed an algorithm that combines concept-effect propagation, fuzzy logic, and decision tree methods to address this drawback and create a personalized, contextual gaming experience. This personalization ensures an engaging and contextually relevant learning experience, addressing learning challenges related to digital citizenship scales. The game was tailored to individual learning experiences and decision-making patterns, with fuzzy logic interpreting nuanced student responses and decision trees guiding learning paths. A digital citizenship knowledge test and an affection questionnaire measured the game’s impact. Moreover, eye tracking was used to ensure attention in the experimental group. Therefore, a quasi-experimental design was conducted to evaluate the influence of a digital citizenship game on 110 students. ANCOVA and the Chi-square tests were performed to analyze students’ knowledge of digital citizenship. Moreover, eye-tracking metrics were used to gain deeper insights into students’ visual attention and engagement. The experimental results reveal that the proposed game enhanced the students’ digital citizenship achievement and promoted their perceptions. Additionally, eye-tracking data showed that the proposed gaming environment positively influenced students’ engagement. Findings indicate that using fuzzy logic and decision trees in educational games significantly promotes affection and alters attention in learning digital citizenship. This study contributes to educational technology by showcasing the potential benefits of personalized educational experiences. The insights gained are valuable for educators and educational game developers focused on digital citizenship education. © 2024 The Author(s). Published by Informa UK Limited, trading as Taylor & Francis Group.","Classroom Practice; digital game; Education–Social Sciences; essential skill; Michael William Dunn, Washington State University Vancouver, United States; Personalized learning; Secondary Education; student engagement; Teaching & Learning; visual attention",,Article,Final,All Open Access; Gold Open Access,Scopus,2-s2.0-85193069830,Gaming / VR
Kuznetsov A.A.; Petrenko T.S.; Gorbunov A.V.; Borisov V.I.,"Kuznetsov, Artyom A. (58774811800); Petrenko, Timur S. (56439264500); Gorbunov, Anatoly V. (59296550400); Borisov, Vasilii I. (55352323900)",58774811800; 56439264500; 59296550400; 55352323900,Neurophysiology Data of Cognitive Training in Immersive Environment,2024,"International Conference of Young Specialists on Micro/Nanotechnologies and Electron Devices, EDM",,,,2220,2225,5.0,2,10.1109/EDM61683.2024.10615177,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85201932512&doi=10.1109%2fEDM61683.2024.10615177&partnerID=40&md5=94bd466bca24531bfb74efa88e7f9879,"A study of the cognitive status of relatively healthy young volunteers was presented in this paper. Cognitive status assessment is based on the study of electrophysiology and eye-tracking (ET) signals during cognitive training in an immersive environment (virtual reality). The methodology of the experiment, the process of dataset formation and methods of data analysis are described. Several parameters of biological signals (EEG, ECG, ET) related to cognitive functions (attention concentration, working memory, reaction speed, motor control, hand-eye coordination) were identified. EEG and ECG signals were obtained using a 19-channel electroencephalograph ""Neurosoft"". While eye tracking data (26 parameters) was extracted from the built-in eye tracker in virtual reality glasses HTC Vive Pro Eye. An open database on the conducted research was developed and published. A comparison of similar published studies is provided in the discussion section of the paper. The project is intended to involve more subjects and to apply machine learning algorithms to the analysis of biosignals. Consequently, an objective system will be developed to assess cognitive status based on biological signals. © 2024 IEEE.",cognitive; electrophysiology processing; eye tracking; functions; impairment; training; virtual reality,Brain mapping; Eye controlled devices; Network security; Spatio-temporal data; Biological signals; Cognitive; Cognitive functions; Cognitive training; Electrophysiology processing; Eye-tracking; Immersive environment; Impairment; Methods of data analysis; Tracking signals; Virtualization,Conference paper,Final,,Scopus,2-s2.0-85201932512,Gaming / VR
Mañas-Viniegra L.; Veloso A.I.; Núñez-Gómez P.,"Mañas-Viniegra, Luis (57197747570); Veloso, Ana I. (35422995200); Núñez-Gómez, Patricia (55580401900)",57197747570; 35422995200; 55580401900,The influence of football celebrity endorsements on the attention paid to advertising: eye tracker research related to amateur online poker players in Portugal,2024,Soccer and Society,25,7,,851,871,20.0,0,10.1080/14660970.2023.2284891,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85178017437&doi=10.1080%2f14660970.2023.2284891&partnerID=40&md5=060512e1d43fbe292076826c51463a51,"The aim of this research is to determine how football stars influence the attention paid to online poker advertising and whether a corresponding nationality might influence the perception of audiences between 18 and 24 years of age, which is the adult age group most vulnerable to this type of game. Using an eye tracking technique, a sample of 60 Portuguese young people who are amateur online poker players has been studied. The results suggest that affinity with a sports star increases the level of attention paid to the image of the star. Otherwise, the poker hand registers the greatest amount of attention. Sports stars who are currently playing attract more attention than those who are retired, and smiles make it possible to attract even more attention from the audience. These stars facilitate advertising brands in associating online gaming with a form of leisure. © 2023 Informa UK Limited, trading as Taylor & Francis Group.",,,Article,Final,,Scopus,2-s2.0-85178017437,Gaming / VR
Jamil Uddin S.M.; Tabassum N.; Ovid A.; Alsharef A.; Albert A.,"Jamil Uddin, S.M. (57219706665); Tabassum, Nafisa (58958188000); Ovid, Anto (58258359600); Alsharef, Abdullah (57215429443); Albert, Alex (54894250700)",57219706665; 58958188000; 58258359600; 57215429443; 54894250700,Measuring Mental Fatigue in Construction: State of the Science and Future Opportunities,2024,"Construction Research Congress 2024, CRC 2024",4,,,688,698,10.0,1,10.1061/9780784485293.069,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85188826178&doi=10.1061%2f9780784485293.069&partnerID=40&md5=ffffdadcfe0b917a0f30703e3bd63b65,"Mental fatigue in construction is a growing concern as it can negatively impact worker productivity, safety, and overall job performance. Factors such as long work hours, monotonous tasks, and high stress levels can contribute to mental fatigue. This can lead to decreased decision-making capabilities, increased errors, and a higher risk of accidents on the jobsite. To combat this challenge, studies have focused on measuring mental fatigue in construction using technologies such as EEG, eye-tracking, and virtual reality, among others. This study focuses on capturing the state of the science related to the utilization of technology to measure mental fatigue of construction workers. To achieve the objectives, relevant literature was searched on one of the largest scientific databases, Web of Science. Upon extracting the relevant research articles, the articles were investigated thoroughly to identify the technology adopted to measure mental fatigue, the targeted construction activity, and future opportunities to tackle mental fatigue in construction. The findings of this study will contribute to the body of existing knowledge by paving the pathway for future research on measuring and managing mental fatigue. © CRC 2024. All rights reserved.",,Construction industry; Decision making; Diseases; Virtual reality; Decisions makings; High stress; Job performance; Job sites; Mental fatigue; Risk of accidents; Stress levels; Task stress; Work hours; Worker productivities; Eye tracking,Conference paper,Final,,Scopus,2-s2.0-85188826178,Gaming / VR
Dillen A.; Omidi M.; Díaz M.A.; Ghaffari F.; Roelands B.; Vanderborght B.; Romain O.; De Pauw K.,"Dillen, Arnau (57472017200); Omidi, Mohsen (58066093300); Díaz, María Alejandra (57643496600); Ghaffari, Fakhreddine (24779594100); Roelands, Bart (8590460900); Vanderborght, Bram (57191490554); Romain, Olivier (7004279535); De Pauw, Kevin (36476057400)",57472017200; 58066093300; 57643496600; 24779594100; 8590460900; 57191490554; 7004279535; 36476057400,Evaluating the real-world usability of BCI control systems with augmented reality: a user study protocol,2024,Frontiers in Human Neuroscience,18,,1448584,,,,2,10.3389/fnhum.2024.1448584,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85201400089&doi=10.3389%2ffnhum.2024.1448584&partnerID=40&md5=6a9db88104e8c8db18d3cd380b727ae4,"Brain-computer interfaces (BCI) enable users to control devices through their brain activity. Motor imagery (MI), the neural activity resulting from an individual imagining performing a movement, is a common control paradigm. This study introduces a user-centric evaluation protocol for assessing the performance and user experience of an MI-based BCI control system utilizing augmented reality. Augmented reality is employed to enhance user interaction by displaying environment-aware actions, and guiding users on the necessary imagined movements for specific device commands. One of the major gaps in existing research is the lack of comprehensive evaluation methodologies, particularly in real-world conditions. To address this gap, our protocol combines quantitative and qualitative assessments across three phases. In the initial phase, the BCI prototype's technical robustness is validated. Subsequently, the second phase involves a performance assessment of the control system. The third phase introduces a comparative analysis between the prototype and an alternative approach, incorporating detailed user experience evaluations through questionnaires and comparisons with non-BCI control methods. Participants engage in various tasks, such as object sorting, picking and placing, and playing a board game using the BCI control system. The evaluation procedure is designed for versatility, intending applicability beyond the specific use case presented. Its adaptability enables easy customization to meet the specific user requirements of the investigated BCI control application. This user-centric evaluation protocol offers a comprehensive framework for iterative improvements to the BCI prototype, ensuring technical validation, performance assessment, and user experience evaluation in a systematic and user-focused manner. Copyright © 2024 Dillen, Omidi, Díaz, Ghaffari, Roelands, Vanderborght, Romain and De Pauw.",augmented reality; brain-computer interface; eye tracking; human-robot interaction; robot control; user evaluation; user experience,Article; augmented reality; clinical outcome; computer language; control system; cross validation; data analysis; drinking; electroencephalogram; evaluation study; evoked response; eye tracking; eye-tracking technology; fatigue; game; human; imagery; information processing; mental fatigue; mental performance; movement (physiology); outcome assessment; performance; qualitative analysis; quantitative analysis; questionnaire; robotics; software; validation process; visual analog scale,Article,Final,All Open Access; Gold Open Access,Scopus,2-s2.0-85201400089,Gaming / VR
Bjørnsten T.B.; Wade T.J.; Nellemann C.; Christiansen L.,"Bjørnsten, Thomas Bøgevald (55443439700); Wade, Tine Juhl (59249473900); Nellemann, Camilla (57709321900); Christiansen, Lasse (57226748758)",55443439700; 59249473900; 57709321900; 57226748758,Extending the Learning Factory Through Virtual Reality,2024,Lecture Notes in Networks and Systems,1059 LNNS,,,79,86,7.0,0,10.1007/978-3-031-65411-4_10,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85200595402&doi=10.1007%2f978-3-031-65411-4_10&partnerID=40&md5=c599447b582dac95b0e241cf7cea20d4,"The Learning Factory approach has offered authentic solutions to mimic industrial products and processes in higher education, industrial upskilling and primary education. However, the potential can be increased by adopting a virtual layer through the so-called virtually extended learning factories in simulation, augmented reality, and virtual reality.These dynamics have generated promising results, but the relation between the setup’s visuals, sound, haptics, and adaptability remains to be investigated. This paper aims to demonstrate the correlation between attention paid to a given object in a virtual reality simulation to critically evaluate selected functionalities that we consider crucial for a potential, later application of the knowledge. We assess the attention through eye-tracking within the virtual reality environment, which extends a physical learning factory into the virtual space. The Learning Factory-context pertains to enhancing operational efficiency, where participants are tasked with executing a sequence of procedures on a simulated Festo plant. The objective is to augment the Overall Equipment Effectiveness (OEE). The paper demonstrates how the virtual reality environment provides a flexible user interface and scenario that can potentially enhance the understanding of industrial procedures within a learning factory. Hence, leveraging virtual reality is a way to impart key operations management terminologies like overall equipment effectiveness, single-minute exchange of dies, and takt/cycle time while emphasizing their practical applications. However, this comes with a technological entry barrier, which reduces the potential for learners unfamiliar with virtual reality technologies. © The Author(s), under exclusive license to Springer Nature Switzerland AG 2024.",Adaptability; Eye-tracking; Multimodality; Technological entry barrier; Virtual Reality,Augmented reality; E-learning; Engineering education; Eye tracking; User interfaces; Adaptability; Entry barriers; Eye-tracking; Factory approaches; Industrial processs; Learning factory; Multi-modality; Overall equipment effectiveness; Technological entry barrier; Virtual-reality environment; Virtual reality,Conference paper,Final,,Scopus,2-s2.0-85200595402,Gaming / VR
,,,"Media Watermarking, Security, and Forensics 2024",2024,IS and T International Symposium on Electronic Imaging Science and Technology,36,4,,,,274.0,0,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85185382480&partnerID=40&md5=1045f6306c2c2b9e5c7425f9c2f9e0bb,The proceedings contain 34 papers. The topics discussed include: extending lidar depth range using stereo depth estimation on intensity data; transformers for microscopy slide image segmentation of invasive melanoma; intracell capacitances in pixel-level hybrid bond image sensor cells; self-attention enhanced recognition: a unified model for handwriting and scene-text recognition with improved inference; evaluation of subjective video quality of television displays; experimental study on reducing wavelength dependency of spatial resolution characteristics of a digital camera by MTF correction; comparative analysis of user experience in virtual reality (VR) and mixed reality (MR) systems using eye-tracking measurements; RAIV: researchable archives for interactive visualizations; mobile 3D mapping of Erdstall facilities; real-time stereoscopic image-parallel path tracing; automatic reading order sequencing: a novel reading order generator for text-based documents; and improving video deepfake detection: a DCT-based approach with patch-level analysis.,,,Conference review,Final,,Scopus,2-s2.0-85185382480,Gaming / VR
,,,Computational Imaging XXII,2024,IS and T International Symposium on Electronic Imaging Science and Technology,36,15,,,,274.0,0,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85185374144&partnerID=40&md5=fb54d382493129accd8f55549187b29c,The proceedings contain 34 papers. The topics discussed include: extending lidar depth range using stereo depth estimation on intensity data; transformers for microscopy slide image segmentation of invasive melanoma; intracell capacitances in pixel-level hybrid bond image sensor cells; self-attention enhanced recognition: a unified model for handwriting and scene-text recognition with improved inference; evaluation of subjective video quality of television displays; experimental study on reducing wavelength dependency of spatial resolution characteristics of a digital camera by MTF correction; comparative analysis of user experience in virtual reality (VR) and mixed reality (MR) systems using eye-tracking measurements; RAIV: researchable archives for interactive visualizations; mobile 3D mapping of Erdstall facilities; real-time stereoscopic image-parallel path tracing; automatic reading order sequencing: a novel reading order generator for text-based documents; and improving video deepfake detection: a DCT-based approach with patch-level analysis.,,,Conference review,Final,,Scopus,2-s2.0-85185374144,Gaming / VR
Bai X.; Zhang P.; Yu X.; Zheng J.; Hancock E.R.; Zhou J.; Gu L.,"Bai, Xiao (50861149800); Zhang, Pengcheng (57461501600); Yu, Xiaohan (56510536400); Zheng, Jin (50662210800); Hancock, Edwin R. (7202876234); Zhou, Jun (56926930600); Gu, Lin (57213713704)",50861149800; 57461501600; 56510536400; 50662210800; 7202876234; 56926930600; 57213713704,Learning From Human Attention for Attribute-Assisted Visual Recognition,2024,IEEE Transactions on Pattern Analysis and Machine Intelligence,46,12,,11152,11167,15.0,11,10.1109/TPAMI.2024.3458921,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85204112800&doi=10.1109%2fTPAMI.2024.3458921&partnerID=40&md5=f20d5af5698d6a447bd983bbff52cbe8,"With prior knowledge of seen objects, humans have a remarkable ability to recognize novel objects using shared and distinct local attributes. This is significant for the challenging tasks of zero-shot learning (ZSL) and fine-grained visual classification (FGVC), where the discriminative attributes of objects have played an important role. Inspired by human visual attention, neural networks have widely exploited the attention mechanism to learn the locally discriminative attributes for challenging tasks. Though greatly promoted the development of these fields, existing works mainly focus on learning the region embeddings of different attribute features and neglect the importance of discriminative attribute localization. It is also unclear whether the learned attention truly matches the real human attention. To tackle this problem, this paper proposes to employ real human gaze data for visual recognition networks to learn from human attention. Specifically, we design a unified Attribute Attention Network (A2Net) that learns from human attention for both ZSL and FGVC tasks. The overall model consists of an attribute attention branch and a baseline classification network. On top of the image feature maps provided by the baseline classification network, the attribute attention branch employs attribute prototypes to produce attribute attention maps and attribute features. The attribute attention maps are converted to gaze-like attentions to be aligned with real human gaze attention. To guarantee the effectiveness of attribute feature learning, we further align the extracted attribute features with attribute-defined class embeddings. To facilitate learning from human gaze attention for the visual recognition problems, we design a bird classification game to collect real human gaze data using the CUB dataset via an eye-tracker device. Experiments on ZSL and FGVC tasks without/with real human gaze data validate the benefits and accuracy of our proposed model. This work supports the promising benefits of collecting human gaze datasets and automatic gaze estimation algorithms learning from human attention for high-level computer vision tasks. © 1979-2012 IEEE.",Fine-Grained visual classification; gaze estimation; human gaze dataset; local attribute; zero-shot learning,"Algorithms; Attention; Eye-Tracking Technology; Humans; Image Processing, Computer-Assisted; Machine Learning; Neural Networks, Computer; Pattern Recognition, Automated; Pattern Recognition, Visual; Adversarial machine learning; Image classification; Network embeddings; Embeddings; Fine grained; Fine-grained visual classification; Gaze estimation; Human attention; Human gaze dataset; Learn+; Local attribute; Visual classification; Visual recognition; algorithm; artificial neural network; attention; automated pattern recognition; eye-tracking technology; human; image processing; machine learning; physiology; procedures; visual pattern recognition; Zero-shot learning",Article,Final,,Scopus,2-s2.0-85204112800,Gaming / VR
,,,Image Quality and System Performance XXI,2024,IS and T International Symposium on Electronic Imaging Science and Technology,36,9,,,,274.0,0,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85185381354&partnerID=40&md5=611d76cb6bbe3c5304e09534302fe7f3,The proceedings contain 34 papers. The topics discussed include: extending lidar depth range using stereo depth estimation on intensity data; transformers for microscopy slide image segmentation of invasive melanoma; intracell capacitances in pixel-level hybrid bond image sensor cells; self-attention enhanced recognition: a unified model for handwriting and scene-text recognition with improved inference; evaluation of subjective video quality of television displays; experimental study on reducing wavelength dependency of spatial resolution characteristics of a digital camera by MTF correction; comparative analysis of user experience in virtual reality (VR) and mixed reality (MR) systems using eye-tracking measurements; RAIV: researchable archives for interactive visualizations; mobile 3D mapping of Erdstall facilities; real-time stereoscopic image-parallel path tracing; automatic reading order sequencing: a novel reading order generator for text-based documents; and improving video deepfake detection: a DCT-based approach with patch-level analysis.,,,Conference review,Final,,Scopus,2-s2.0-85185381354,Gaming / VR
,,,Intelligent Robotics and Industrial Applications Using Computer Vision 2024,2024,IS and T International Symposium on Electronic Imaging Science and Technology,36,6,,,,274.0,0,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85185369060&partnerID=40&md5=0b80aa869e3d1f5cc72ae5a37ee855b0,The proceedings contain 34 papers. The topics discussed include: extending lidar depth range using stereo depth estimation on intensity data; transformers for microscopy slide image segmentation of invasive melanoma; intracell capacitances in pixel-level hybrid bond image sensor cells; self-attention enhanced recognition: a unified model for handwriting and scene-text recognition with improved inference; evaluation of subjective video quality of television displays; experimental study on reducing wavelength dependency of spatial resolution characteristics of a digital camera by MTF correction; comparative analysis of user experience in virtual reality (VR) and mixed reality (MR) systems using eye-tracking measurements; RAIV: researchable archives for interactive visualizations; mobile 3D mapping of Erdstall facilities; real-time stereoscopic image-parallel path tracing; automatic reading order sequencing: a novel reading order generator for text-based documents; and improving video deepfake detection: a DCT-based approach with patch-level analysis.,,,Conference review,Final,,Scopus,2-s2.0-85185369060,Gaming / VR
Aruna S.; Maturi S.; Kotha V.; Swarna K.,"Aruna, S. (59411318500); Maturi, Sripriya (57209539478); Kotha, Vaishnavi (59337501200); Swarna, K. (55899331200)",59411318500; 57209539478; 59337501200; 55899331200,Leveraging AI for Student Attention Estimation,2024,Multifaceted Approaches for Data Acquisition Processing and Communication,,,,15,21,6.0,1,10.1201/9781003470939-3,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85204536653&doi=10.1201%2f9781003470939-3&partnerID=40&md5=dd7eee22e0773313b9765f503da3f878,"In a physical classroom setting, teachers face the challenge of monitoring the attentiveness of each student. This is because there are several factors that can impact student engagement, including distractions, boredom, lack of interest, and varying attention spans. It can be difficult for teachers to identify and address these issues in real-time, as they may not have the ability to observe and track the engagement levels of every student at all times. Fortunately, automation can help to solve this problem by providing teachers with data-driven insights into student behavior and engagement. Using this data, teachers can gain a better understanding of how engaged students are during class, which can help them to identify areas where students may need more support or where they may be struggling. They can also use this information to adjust their teaching methods to better suit the needs of their students, ultimately leading to improved learning outcomes. The authors have proposed an AI-based framework for estimating the attention of students in physical classroom settings, which can be a game-changer for the education sector. The framework consists of five modules – Input preprocessing, face recognition, emotion recognition, gaze tracking, and attention estimation. The implementation of these modules leverages advanced techniques such as deep learning and computer vision to improve the accuracy of the attention estimation model. The collected data can be analyzed to identify patterns and trends in student behavior, which can help teachers identify areas that need improvement and modify their teaching strategies accordingly. © 2024 selection and editorial matter, Dr. Chinmay Chakraborty, Dr. Manisha Guduri, Dr. K. Shyamala, and Dr. B. Sandhya; individual chapters, the contributors.",Artificial intelligence; Attention estimation; Deep learning; Emotion recognition; Gaze tracking,,Book chapter,Final,,Scopus,2-s2.0-85204536653,Gaming / VR
Niknam S.; Botev J.,"Niknam, Sahar (55176239500); Botev, Jean (25927814800)",55176239500; 25927814800,Predicting Cognitive Failures in Virtual Reality Using Pupillometry,2024,"Proceedings - 2024 IEEE International Conference on Artificial Intelligence and eXtended and Virtual Reality, AIxVR 2024",,,,261,264,3.0,1,10.1109/AIxVR59861.2024.00043,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85187222731&doi=10.1109%2fAIxVR59861.2024.00043&partnerID=40&md5=36dffcae0fa0760a8d3ad9ce7f6044ff,"Pupil dilation has consistently been investigated and confirmed as a reliable measure of cognitive load. In this study, we aim to explore the possibility of predicting cognitive failures in Virtual Reality by monitoring variations in pupil dilation during cognitive processing. To this end, we collected eye-tracking data from an individual performing a mental arithmetic task over two months, totaling 700 minutes. We achieved promising prediction results by training a neural network on the collected data, particularly considering the dataset's imbalanced nature. The ability to predict impending cognitive failures generally holds significant implications across various domains, including education, delegating decision-making tasks to autonomous systems, or self-adaptive virtual environments and user interfaces. © 2024 IEEE.",cognitive load; neural networks; pupillometry; virtual reality,Cognitive systems; Decision making; Eye tracking; Forecasting; User interfaces; Arithmetic tasks; Cognitive failures; Cognitive loads; Cognitive processing; Eye-tracking; Mental arithmetic; Neural-networks; Pupil dilation; Pupillometry; Tracking data; Virtual reality,Conference paper,Final,All Open Access; Green Open Access,Scopus,2-s2.0-85187222731,Gaming / VR
Gulyas N.; Doicaru M.; Boode W.; Campos F.; van Gisbergen M.S.,"Gulyas, Napsugar (58864875500); Doicaru, Miruna (57210116576); Boode, Wilco (57205059809); Campos, Fabio (55190176400); van Gisbergen, Marnix S. (57194885005)",58864875500; 57210116576; 57205059809; 55190176400; 57194885005,‘What Are They Looking at?’ Testing the Capacity of Action Units to Direct Attention in a 360° Recorded Virtual Reality Narrative,2024,Springer Proceedings in Business and Economics,,,,140,151,11.0,0,10.1007/978-3-031-50559-1_11,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85184108323&doi=10.1007%2f978-3-031-50559-1_11&partnerID=40&md5=b206a6c0198cf4fcf1b7847f1f6da237,"Virtual Reality (VR) creates immersion, which makes the medium interesting for storytelling. A good VR narrative brings the audiences close to the stories, placing them inside the story world and in the shoes of the characters themselves. Focused attention is a key component of narrative engagement and a key determinant of enjoyment of the narrative. However, as the audience can freely look around in VR, it is a challenge to draw audience attention to key aspects of the narrative at the right moment. This paper compared two techniques of directing attention in a 360° narrative: diegetic (Action Units) versus non-diegetic (Pointing Arrows). A between-subjects experiment among 71 participants revealed an effect opposite to what was expected. Pointing Arrows provided more control to direct attention in VR, which created more enjoyment compared to Action Units. This effect was confirmed by subjective (self-report) and objective (eye-tracking data) measurements. © The Author(s), under exclusive license to Springer Nature Switzerland AG 2024.",360°; Action units; Attention; Eye-tracking; Immersion; Narrative; Pointing arrows; Suspense; Virtual reality,,Conference paper,Final,,Scopus,2-s2.0-85184108323,Gaming / VR
Zhang R.; Duan W.; Zheng Z.,"Zhang, Ruoshi (57789814400); Duan, Weiyue (58844293100); Zheng, Zhikai (58844293200)",57789814400; 58844293100; 58844293200,Multimodal Quantitative Research on the Emotional Attachment Characteristics between People and the Built Environment Based on the Immersive VR Eye-Tracking Experiment,2024,Land,13,1,52,,,,7,10.3390/land13010052,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85183120409&doi=10.3390%2fland13010052&partnerID=40&md5=02ddcac6bcd752de90cc702d65c82078,"The campus landscape contributes a lot to students’ mental and physical health. Students’ emotional attachment to landscape space is an important scientific basis for landscape design. This study used immersive virtual reality eye tracking supported by HTC Vivo Pro and an emotional attachment scale to investigate the relationship between different landscape elements and students’ visual behavior and emotional attachment. ErgoLab and SPSS were used to analyze the indicators. The results showed that: (1) Artificial elements were more likely to attract students’ visual attention and continuously enhance their interest in the landscape. (2) The waterscape space was more likely to attract students’ visual attention, while the attractiveness of arbors and shrubs was related to their color and spatial location. (3) The characteristics related to nature were generally conducive to the establishment of students’ emotional attachment, including both the natural elements and artificial structures that could reflect the natural texture and time traces. (4) Three-dimensional spatial sequence design of landscape elements significantly affected students’ visual focus and emotional experience. The results further contribute to providing a clearer understanding of how students’ preference for specific landscape elements can be obtained and used in decision making for the planning and management during campus renewal and design. © 2024 by the authors.",campus landscape element; emotional attachment; immersive VR eye-tracking; the Heart of Forest,experiment; landscape; psychology; research; student; urban planning; urban renewal; virtual reality,Article,Final,All Open Access; Gold Open Access,Scopus,2-s2.0-85183120409,Gaming / VR
,,,"Color Imaging XXIX: Displaying, Processing, Hardcopy, and Applications",2024,IS and T International Symposium on Electronic Imaging Science and Technology,36,16,,,,274.0,0,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85185386441&partnerID=40&md5=4c17cf44a88fc615de8e44896618ffe7,The proceedings contain 34 papers. The topics discussed include: extending lidar depth range using stereo depth estimation on intensity data; transformers for microscopy slide image segmentation of invasive melanoma; intracell capacitances in pixel-level hybrid bond image sensor cells; self-attention enhanced recognition: a unified model for handwriting and scene-text recognition with improved inference; evaluation of subjective video quality of television displays; experimental study on reducing wavelength dependency of spatial resolution characteristics of a digital camera by MTF correction; comparative analysis of user experience in virtual reality (VR) and mixed reality (MR) systems using eye-tracking measurements; RAIV: researchable archives for interactive visualizations; mobile 3D mapping of Erdstall facilities; real-time stereoscopic image-parallel path tracing; automatic reading order sequencing: a novel reading order generator for text-based documents; and improving video deepfake detection: a DCT-based approach with patch-level analysis.,,,Conference review,Final,,Scopus,2-s2.0-85185386441,Gaming / VR
,,,Stereoscopic Displays and Applications XXXV,2024,IS and T International Symposium on Electronic Imaging Science and Technology,36,2,,,,274.0,0,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85185389317&partnerID=40&md5=f6306d3c3fa66f2466afa85e5ef37a4f,The proceedings contain 34 papers. The topics discussed include: extending lidar depth range using stereo depth estimation on intensity data; transformers for microscopy slide image segmentation of invasive melanoma; intracell capacitances in pixel-level hybrid bond image sensor cells; self-attention enhanced recognition: a unified model for handwriting and scene-text recognition with improved inference; evaluation of subjective video quality of television displays; experimental study on reducing wavelength dependency of spatial resolution characteristics of a digital camera by MTF correction; comparative analysis of user experience in virtual reality (VR) and mixed reality (MR) systems using eye-tracking measurements; RAIV: researchable archives for interactive visualizations; mobile 3D mapping of Erdstall facilities; real-time stereoscopic image-parallel path tracing; automatic reading order sequencing: a novel reading order generator for text-based documents; and improving video deepfake detection: a DCT-based approach with patch-level analysis.,,,Conference review,Final,,Scopus,2-s2.0-85185389317,Gaming / VR
,,,3D Imaging and Applications 2024,2024,IS and T International Symposium on Electronic Imaging Science and Technology,36,18,,,,274.0,0,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85185585387&partnerID=40&md5=e4a11576b9d9793b8ed01c0a62a25a95,The proceedings contain 34 papers. The topics discussed include: extending lidar depth range using stereo depth estimation on intensity data; transformers for microscopy slide image segmentation of invasive melanoma; intracell capacitances in pixel-level hybrid bond image sensor cells; self-attention enhanced recognition: a unified model for handwriting and scene-text recognition with improved inference; evaluation of subjective video quality of television displays; experimental study on reducing wavelength dependency of spatial resolution characteristics of a digital camera by MTF correction; comparative analysis of user experience in virtual reality (VR) and mixed reality (MR) systems using eye-tracking measurements; RAIV: researchable archives for interactive visualizations; mobile 3D mapping of Erdstall facilities; real-time stereoscopic image-parallel path tracing; automatic reading order sequencing: a novel reading order generator for text-based documents; and improving video deepfake detection: a DCT-based approach with patch-level analysis.,,,Conference review,Final,,Scopus,2-s2.0-85185585387,Gaming / VR
Liu D.; Ham Y.,"Liu, Di (58294085600); Ham, Youngjib (55651299500)",58294085600; 55651299500,Investigating the Cognition-Control Pattern of Multi-Worker Human-Robot Collaboration in Construction,2024,"Computing in Civil Engineering 2023: Data, Sensing, and Analytics - Selected Papers from the ASCE International Conference on Computing in Civil Engineering 2023",,,,571,578,7.0,1,10.1061/9780784485224.069,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85184282937&doi=10.1061%2f9780784485224.069&partnerID=40&md5=a02b354bad94a344bf963c7a3814152b,"When operating a construction robot (e.g., excavator), the operator's unsafe behavior directly affects the safety risk (e.g., underground utility damage occurrence during excavation process). Operators' behavior is greatly influenced by the surrounding environment and further the communication with other coworkers (i.e., spotter), and thus there is a need for studying human factors during work by investigating how the operator-spotter interaction affects the operator when performing the task in a challenging work environment. In this paper, we investigate how the different levels of environmental complexities and operator-spotter communication channels affect operators' performance and the accident occurrence during work. A human-centered experiment is designed and conducted in environmentally realistic scenarios based on immersive virtual reality. The task of operating a virtual excavator as well as the interaction between the operator and a spotter are performed in realistic jobsites in which a series of environmental stimuli are modeled and simulated. Operators' cognitive responses and work performance are assessed by subjective evaluations and instrument-based measurements (i.e., eye-Tracking). This study can establish a better understanding about the effectiveness of between-worker communication and worker-To-robot interaction during robot operation as well as the influence of environmental visual and auditory stimuli on the teleoperator. © International Conference on Computing in Civil Engineering 2023.All rights reserved.",,Construction equipment; Excavation; Excavators; Robots; Virtual reality; Construction robots; Control patterns; During excavations; Excavation process; Human-robot collaboration; Safety risks; Surrounding environment; Underground utilities; Unsafe behaviors; Workers'; Eye tracking,Conference paper,Final,,Scopus,2-s2.0-85184282937,Gaming / VR
,,,Human Vision and Electronic Imaging 2024,2024,IS and T International Symposium on Electronic Imaging Science and Technology,36,11,,,,274.0,0,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85185587055&partnerID=40&md5=d398bec5039a30afde9ffa486868716f,The proceedings contain 34 papers. The topics discussed include: extending lidar depth range using stereo depth estimation on intensity data; transformers for microscopy slide image segmentation of invasive melanoma; intracell capacitances in pixel-level hybrid bond image sensor cells; self-attention enhanced recognition: a unified model for handwriting and scene-text recognition with improved inference; evaluation of subjective video quality of television displays; experimental study on reducing wavelength dependency of spatial resolution characteristics of a digital camera by MTF correction; comparative analysis of user experience in virtual reality (VR) and mixed reality (MR) systems using eye-tracking measurements; RAIV: researchable archives for interactive visualizations; mobile 3D mapping of Erdstall facilities; real-time stereoscopic image-parallel path tracing; automatic reading order sequencing: a novel reading order generator for text-based documents; and improving video deepfake detection: a DCT-based approach with patch-level analysis.,,,Conference review,Final,,Scopus,2-s2.0-85185587055,Gaming / VR
Inal Y.; Volden F.; Norgaard G.; Eline Thomt Roksvag A.; Forsberg Sommerfelt E.; Stenersen Saeth E.,"Inal, Yavuz (16238894800); Volden, Frode (56157130900); Norgaard, Gabrielle (59318861200); Eline Thomt Roksvag, Anna (59345503800); Forsberg Sommerfelt, Eilert (59345504100); Stenersen Saeth, Erlend (59345499300)",16238894800; 56157130900; 59318861200; 59345503800; 59345504100; 59345499300,Effects of Gameplay Dynamics on Visual Attention,2024,IEEE Access,12,,,126961,126969,8.0,1,10.1109/ACCESS.2024.3454756,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85203403900&doi=10.1109%2fACCESS.2024.3454756&partnerID=40&md5=d6198d98c26964523f9b6bddcf7523de,"This study aimed to explore the effects of gameplay dynamics on players' visual attention. We examined gameplay dynamics related to the game system, player, and context, which have the potential to influence visual attention. We divided the game interface into four areas of interest, namely avatar, obstacles, game score, and text explaining the game. We investigated how data from four designated areas of interest varied for different eye-gaze measures, including the total duration of fixations, the time to first fixation, and the number of fixations. Data were collected from 32 participants through the Tobii Pro Fusion eye tracker and questionnaires. Overall, the total fixation duration was highest for obstacles, followed by avatar, game score, and text area. This was similar to the time to first fixation metric, as the participants predominantly directed their attention to game elements in the same order. Gamers and high-in-game performers developed covert attention that was skewed and transitional, expanding from the avatar to the obstacles. In contrast, non-gamers, low-in-game performers, and participants with low-in-game experience focused their visual attention mostly on the game score and text area.  © 2013 IEEE.",browser games; Eye-gaze behavior; game experience; game performance; gameplay dynamics; player experience; visual attention,Browser games; Eye-gaze; Eye-gaze behavior; Game experience; Game performance; Gameplay; Gameplay dynamic; Gaze behaviours; Performance; Player experience; Visual Attention,Article,Final,All Open Access; Gold Open Access,Scopus,2-s2.0-85203403900,Gaming / VR
Landeck M.; Unruh F.; Lugrin J.-L.; Latoschik M.E.,"Landeck, Maximilian (56159756300); Unruh, Fabian (57210912143); Lugrin, Jean-Luc (10038974500); Latoschik, Marc Erich (6602976914)",56159756300; 57210912143; 10038974500; 6602976914,Object Motion Manipulation and time perception in virtual reality,2024,Frontiers in Virtual Reality,5,,1390703,,,,1,10.3389/frvir.2024.1390703,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85200222494&doi=10.3389%2ffrvir.2024.1390703&partnerID=40&md5=6f4075cdea8ba3b61ddc19c47e87b05d,"This paper presents a novel approach to altering how time is perceived in Virtual Reality (VR). It involves manipulating the speed and pattern of motion in objects associated with timekeeping, both directly (such as clocks) and indirectly (like pendulums). Objects influencing our perception of time are called ‘zeitgebers‘; for instance, observing a clock or pendulum tends to affect how we perceive the passage of time. The speed of motion of their internal parts (clock hands or pendulum rings) is explicitly or implicitly related to the perception of time. However, the perceptual effects of accelerating or decelerating the speed of a virtual clock or pendulum in VR is still an open question. We hypothesize that the acceleration of their internal motion will accelerate the passage of time and that the irregularity of the orbit pendulum’s motion will amplify this effect. We anticipate that the irregular movements of the pendulum will lower boredom and heighten attention, thereby making time seem to pass more quickly. Therefore, we conducted an experiment with 32 participants, exposing them to two types of virtual zeitgebers exhibiting both regular and irregular motions. These were a virtual clock and an orbit pendulum, each operating at slow, normal, and fast speeds. Our results revealed that time passed by faster when participants observed virtual zeitgebers in the fast speed condition than in the slow speed condition. The orbit pendulum significantly accelerated the perceived passage of time compared to the clock. We believe that the irregular motion requires a higher degree of attention, which is confirmed by the significantly longer gaze fixations of the participants. These findings are crucial for time perception manipulation in VR, offering potential for innovative treatments for conditions like depression and improving wellbeing. Yet, further clinical research is needed to confirm these applications. Copyright © 2024 Landeck, Unruh, Lugrin and Latoschik.",extended reality; eye tracking; mixed reality; object motion; time perception; virtual time; virtual zeitgeber,,Article,Final,All Open Access; Gold Open Access,Scopus,2-s2.0-85200222494,Gaming / VR
,,,Visualization and Data Analysis 2024,2024,IS and T International Symposium on Electronic Imaging Science and Technology,36,1,,,,274.0,0,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85185372889&partnerID=40&md5=ed9392505685fe10b4333e26b6a1f7dc,The proceedings contain 34 papers. The topics discussed include: extending lidar depth range using stereo depth estimation on intensity data; transformers for microscopy slide image segmentation of invasive melanoma; intracell capacitances in pixel-level hybrid bond image sensor cells; self-attention enhanced recognition: a unified model for handwriting and scene-text recognition with improved inference; evaluation of subjective video quality of television displays; experimental study on reducing wavelength dependency of spatial resolution characteristics of a digital camera by MTF correction; comparative analysis of user experience in virtual reality (VR) and mixed reality (MR) systems using eye-tracking measurements; RAIV: researchable archives for interactive visualizations; mobile 3D mapping of Erdstall facilities; real-time stereoscopic image-parallel path tracing; automatic reading order sequencing: a novel reading order generator for text-based documents; and improving video deepfake detection: a DCT-based approach with patch-level analysis.,,,Conference review,Final,,Scopus,2-s2.0-85185372889,Gaming / VR
Mascarenhas D.R.D.,"Mascarenhas, Duncan R. D. (8432020000)",8432020000,BUILDING KNOWLEDGE OF THE GAME TO MAKE EFFECTIVE DECISIONS,2024,Managing and Developing Sports Officials: Officiating Excellence,,,,127,138,11.0,2,10.4324/9781003370987-15,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85205612373&doi=10.4324%2f9781003370987-15&partnerID=40&md5=6370749b443203bbb6945c0348291566,"This chapter provides a synopsis of the research into decision-making, highlighting the different types of decisions that sports officials are confronted with. Focusing primarily on more subjective decisions, I discuss the importance of having an officiating philosophy, accounting for safety, fairness, accuracy, and entertainment. Decision accuracy and decision appropriateness are discussed from a naturalistic standpoint, and I argue the significance of context for making effective decisions. The terms declarative knowledge (knowing what) and procedural knowledge (knowing how) are introduced, to highlight the basis upon which sports officials' decisions are made, and think-aloud protocols are discussed as processes for revealing decision processes. I draw attention to the importance of situation awareness in (i) attending to the right cues and (ii) understanding what they mean, which creates the capacity to (iii) predict likely future events, allowing officials to effectively read the game and make appropriate decisions. An approach to decision training using experts in their real-world context, with point-of-view video recording and eye-tracking glasses, is used to reveal the knowledge that informs expert decision processes in the context of touch (rugby/football). This results in a decision tree as an aid to develop expertise in less-experienced officials. © 2025 selection and editorial matter, Tom Webb, David J. Hancock, Pamm Phillips, and Jacob K. Tingle; individual chapters, the contributors. All rights reserved.",,,Book chapter,Final,,Scopus,2-s2.0-85205612373,Gaming / VR
,,,Imaging Sensors and Systems 2024,2024,IS and T International Symposium on Electronic Imaging Science and Technology,36,7,,,,274.0,0,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85185387219&partnerID=40&md5=9bd65230a7307d85ab1a065c4010ccb2,The proceedings contain 34 papers. The topics discussed include: extending lidar depth range using stereo depth estimation on intensity data; transformers for microscopy slide image segmentation of invasive melanoma; intracell capacitances in pixel-level hybrid bond image sensor cells; self-attention enhanced recognition: a unified model for handwriting and scene-text recognition with improved inference; evaluation of subjective video quality of television displays; experimental study on reducing wavelength dependency of spatial resolution characteristics of a digital camera by MTF correction; comparative analysis of user experience in virtual reality (VR) and mixed reality (MR) systems using eye-tracking measurements; RAIV: researchable archives for interactive visualizations; mobile 3D mapping of Erdstall facilities; real-time stereoscopic image-parallel path tracing; automatic reading order sequencing: a novel reading order generator for text-based documents; and improving video deepfake detection: a DCT-based approach with patch-level analysis.,,,Conference review,Final,,Scopus,2-s2.0-85185387219,Gaming / VR
Mulry K.B.; Santos A.F.D.; López-Contreras Gonzalez E.; Roy R.N.; Verma M.K.; Peysakhovich V.; Rácková L.,"Mulry, Katherine B. (58568756500); Santos, André F.D. (59672698400); López-Contreras Gonzalez, Elena (58530841300); Roy, Raphaëlle N. (55904560000); Verma, Maneesh Kumar (57211338605); Peysakhovich, Vsevolod (56644829800); Rácková, Lucie (57224469063)",58568756500; 59672698400; 58530841300; 55904560000; 57211338605; 56644829800; 57224469063,Human-Machine Interaction for Rover Teleoperation During Lunar Analogue,2024,"Proceedings of the International Astronautical Congress, IAC",2,,,562,566,4.0,0,10.52202/078364-0063,https://www.scopus.com/inward/record.uri?eid=2-s2.0-86000274349&doi=10.52202%2f078364-0063&partnerID=40&md5=ec2f8a4dadd72484f3885907141ced1f,"The HUMANISE (Human Machine Interaction, Stress, and Performance) project investigates the difference in cognitive workload induced by teleoperation in interactions between analogue astronauts and rovers. Future astronauts on Lunar and Martian settlements may benefit from the advantage of remote control of robots. HUMANISE 2.0, conducted in the Asclepios IV lunar analogue mission in Switzerland, trials the experiment's research and data processing methodology prior to scaling up the project for the World's Biggest Analog. Analog astronauts will eventually control a rover using three control methods: Brain-Computer Interface (BCI) via mental imagery, virtual reality (VR), and joystick control; for HUMANISE 2.0, the experiment was trialled with joystick only. Biomarker metrics such as eye tracking, breathing rate, and heart rate were collected as the analogue astronaut remotely navigated the rover along a predetermined path. The astronaut did not have direct observation of the rover but instead utilized a real-time video feed to observe its progress. It is expected that this setup presents significant cognitive challenges for the astronaut. Subjective user feedback was collected via NASA Task Load Index (TLX) questionnaires completed by the astronauts after each trial and through post-task questionnaires about the system. The project focuses on enhancing human-robot interaction in space exploration by developing user-friendly remote-control interfaces for navigating rovers in Mars- or Moon-like conditions. Reducing the cognitive load on users could improve the effectiveness of future exploration missions. Through comprehensive analysis of user feedback, biometric data, and qualitative observations, the project seeks to deepen understanding of human-machine interaction in extreme environments. The HUMANISE 2.0 experiment demonstrates a field-tested concept for robotic long-distance teleoperation experiments in analogue missions. We hypothesize that differences in physiological signals will be measured depending on search versus navigation tasks. During Asclepios IV, the rover was physically located in the Netherlands and controlled by astronauts in isolation in Switzerland; this paper explores the implementation of such an experiment, the data analysis methods developed for the dataset, and plans for the next trials throughout 2025 during a mission in the Mars Desert Research Station in February, Asclepios V in July, and World's Biggest Analog in October. © 2024 International Astronautical Federation, IAF. All rights reserved.",analogue missions; human-machine interaction; teleoperation,Blood vessels; Brain; Data reduction; Eye tracking; Haptic interfaces; Human robot interaction; Martian surface analysis; Network security; Steganography; Analogue missions; Cognitive workloads; Control methods; Experiment data; Experiment research; Human machine interaction; Performance; Scaling-up; Switzerland; User feedback; NASA,Conference paper,Final,,Scopus,2-s2.0-86000274349,Gaming / VR
Ghiglino D.; De Tommaso D.; Lukomski A.W.; Wykowska A.,"Ghiglino, Davide (57195956118); De Tommaso, Davide (36727098400); Lukomski, Adam W. (59377495000); Wykowska, Agnieszka (15924284200)",57195956118; 36727098400; 59377495000; 15924284200,The Presence of a Humanoid Robot Can Be Detrimental to Human Performance in an Attentional Task,2024,"Technology, Mind, and Behavior",,,,,,,1,10.1037/tmb0000137,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85207016272&doi=10.1037%2ftmb0000137&partnerID=40&md5=6af6d0ad05ca13de953121bddce8b02c,"Being surrounded by others has enabled humans to optimize everyday life tasks, from cultivating fields to industrial assembly lines. The mere presence of others can increase an individual’s arousal, resulting in better performance for familiar tasks. However, the presence of an audience can also be detrimental to an individual’s performance, especially when the arousal becomes excessive. Still, it is unclear what happens when these “others” include artificial agents, such as humanoid robots. Literature has shown mixed results in understanding whether robots can be facilitators or distractors in joint tasks. Thus, to understand the impact that the presence of a robot might have on human attentional mechanisms, we designed a visual search-based game that participants could play alone, under the surveillance of a humanoid robot, or collaborating with it. Thirty-seven participants completed this experiment (age = 26.44 ± 6.35, 10 males). Attentional processes were assessed using metrics of performance (i.e., search times, accuracy) and eye-tracking (i.e., fixation duration, fixation count, and time to first fixation). Results showed that the presence of the robot negatively affected participants’ performance in the game, with longer search times and time to first fixation when the robot was observing them. We hypothesize that the robot acted as a distractor, delaying the allocation of attentional resources to the task, and potentially exerting pressure of monitoring. © 2024 The Author(s)",attention; human–robot interaction; monitoring pressure; social presence,,Article,Article in press,All Open Access; Gold Open Access; Green Open Access,Scopus,2-s2.0-85207016272,Gaming / VR
,,,Image Processing: Algorithms and Systems XXII,2024,IS and T International Symposium on Electronic Imaging Science and Technology,36,10,,,,274.0,0,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85185577016&partnerID=40&md5=160e0220ddb4e49038800a1bc49aaf6d,The proceedings contain 34 papers. The topics discussed include: extending lidar depth range using stereo depth estimation on intensity data; transformers for microscopy slide image segmentation of invasive melanoma; intracell capacitances in pixel-level hybrid bond image sensor cells; self-attention enhanced recognition: a unified model for handwriting and scene-text recognition with improved inference; evaluation of subjective video quality of television displays; experimental study on reducing wavelength dependency of spatial resolution characteristics of a digital camera by MTF correction; comparative analysis of user experience in virtual reality (VR) and mixed reality (MR) systems using eye-tracking measurements; RAIV: researchable archives for interactive visualizations; mobile 3D mapping of Erdstall facilities; real-time stereoscopic image-parallel path tracing; automatic reading order sequencing: a novel reading order generator for text-based documents; and improving video deepfake detection: a DCT-based approach with patch-level analysis.,,,Conference review,Final,,Scopus,2-s2.0-85185577016,Gaming / VR
,,,Engineering Reality of Virtual Reality 2024,2024,IS and T International Symposium on Electronic Imaging Science and Technology,36,13,,,,274.0,0,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85185379576&partnerID=40&md5=32fc085a97da345b5ab3493bf8c60c7b,The proceedings contain 34 papers. The topics discussed include: extending lidar depth range using stereo depth estimation on intensity data; transformers for microscopy slide image segmentation of invasive melanoma; intracell capacitances in pixel-level hybrid bond image sensor cells; self-attention enhanced recognition: a unified model for handwriting and scene-text recognition with improved inference; evaluation of subjective video quality of television displays; experimental study on reducing wavelength dependency of spatial resolution characteristics of a digital camera by MTF correction; comparative analysis of user experience in virtual reality (VR) and mixed reality (MR) systems using eye-tracking measurements; RAIV: researchable archives for interactive visualizations; mobile 3D mapping of Erdstall facilities; real-time stereoscopic image-parallel path tracing; automatic reading order sequencing: a novel reading order generator for text-based documents; and improving video deepfake detection: a DCT-based approach with patch-level analysis.,,,Conference review,Final,,Scopus,2-s2.0-85185379576,Gaming / VR
Obo T.; Sekiguchi T.; Matsuda T.; Kubota N.,"Obo, Takenori (35243395400); Sekiguchi, Takuro (58929824100); Matsuda, Tadamitsu (59117067700); Kubota, Naoyuki (7202158579)",35243395400; 58929824100; 59117067700; 7202158579,Cognitive Modeling Based on Perceiving-Acting Cycle in Unilateral Spatial Neglect,2024,Proceedings of the International Joint Conference on Neural Networks,,,,,,,0,10.1109/IJCNN60899.2024.10651058,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85205017885&doi=10.1109%2fIJCNN60899.2024.10651058&partnerID=40&md5=e07ad3f49336c1a59f9dd370dd8bfb8a,"|Unilateral Spatial Neglect (USN) is characterized by an attention deficit to one side of space, where individuals struggle to perceive stimuli on that side without a lack of sensation. Traditional paper-pencil tasks like line cancellation and copying tests are commonly used to assess USN, but they have limitations in evaluating neglect areas confined to a two-dimensional plane. Immersive VR systems and multimodal sensing systems offer a more sensitive approach for diagnosis and training. In related works, AR/VR systems and eye-tracking devices are utilized for measuring, evaluating, and creating assessment tasks for USN. However, these approaches can only analyze relationships between perception and movements in specific environments. In this study, we propose a method for cognitive modeling based on perceiving-acting cycle in USN, utilizing computational intelligence techniques to establish a structured coupling framework, aiming to contribute to a novel and effective approach for understanding and addressing USN. © 2024 IEEE.",cognitive modeling; cognitive rehabilitation; perceiving-acting cycle; spatial neglect; structured learning,Attention deficit; Cognitive model; Cognitive rehabilitation; Immersive VR; Model-based OPC; Perceiving-acting cycle; Spatial neglect; Structured learning; Two dimensional plane; VR systems,Conference paper,Final,,Scopus,2-s2.0-85205017885,Gaming / VR
Shao F.; Zhang T.; Gao S.; Sun Q.; Yang L.,"Shao, Fenghua (59473761300); Zhang, Tong (59474441000); Gao, Shang (59474441100); Sun, Qi (59474273700); Yang, Liuqingqing (57210158072)",59473761300; 59474441000; 59474441100; 59474273700; 57210158072,Computer Vision-Driven Gesture Recognition: Toward Natural and Intuitive Human-Computer Interfaces,2024,"2024 4th International Conference on Electronic Information Engineering and Computer Communication, EIECC 2024",,,,1313,1317,4.0,0,10.1109/EIECC64539.2024.10929472,https://www.scopus.com/inward/record.uri?eid=2-s2.0-105002305949&doi=10.1109%2fEIECC64539.2024.10929472&partnerID=40&md5=5bd7ae26c1af24261e0ef4b342b16b1b,"This study mainly explores the application of natural gesture recognition based on computer vision in human-computer interaction, aiming to improve the fluency and naturalness of human-computer interaction through gesture recognition technology. In the fields of virtual reality, augmented reality and smart home, traditional input methods have gradually failed to meet the needs of users for interactive experience. As an intuitive and convenient interaction method, gestures have received more and more attention. This paper proposes a gesture recognition method based on a three-dimensional hand skeleton model. By simulating the three-dimensional spatial distribution of hand joints, a simplified hand skeleton structure is constructed. By connecting the palm and each finger joint, a dynamic and static gesture model of the hand is formed, which further improves the accuracy and efficiency of gesture recognition. Experimental results show that this method can effectively recognize various gestures and maintain high recognition accuracy and real-time response capabilities in different environments. In addition, combined with multimodal technologies such as eye tracking, the intelligence level of the gesture recognition system can be further improved, bringing a richer and more intuitive user experience. In the future, with the continuous development of computer vision, deep learning and multimodal interaction technology, natural interaction based on gestures will play an important role in a wider range of application scenarios and promote revolutionary progress in human-computer interaction.  © 2024 IEEE.",3D skeleton model; computer vision; Gesture recognition; human-computer interaction,Augmented reality; Deep learning; Joints (anatomy); Machine vision; Palmprint recognition; Smart homes; 3D skeleton; 3d skeleton model; Computer interaction; Gesture recognition technologies; Gestures recognition; Human computer interfaces; Input methods; Interaction methods; Skeleton models; Smart homes; Gesture recognition,Conference paper,Final,,Scopus,2-s2.0-105002305949,Gaming / VR
Yan K.; Wang Z.; Ji L.; Wang Y.; Duan N.; Ma S.,"Yan, Kun (59741401000); Wang, Zeyu (59293188000); Ji, Lei (57193623189); Wang, Yuntao (53867328700); Duan, Nan (52163366000); Ma, Shuai (56611103800)",59741401000; 59293188000; 57193623189; 53867328700; 52163366000; 56611103800,Voila-A: Aligning Vision-Language Models with User's Gaze Attention,2024,Advances in Neural Information Processing Systems,37,,,,,,2,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-105000523827&partnerID=40&md5=be0e280491cdc9220c2d9e6379d67eb5,"In recent years, the integration of vision and language understanding has led to significant advancements in artificial intelligence, particularly through Vision-Language Models (VLMs). However, existing VLMs face challenges in handling real-world applications with complex scenes and multiple objects, as well as aligning their focus with the diverse attention patterns of human users. In this paper, we introduce gaze information, feasibly collected by AR or VR devices, as a proxy for human attention to guide VLMs and propose a novel approach, Voila-A, for gaze alignment to enhance the interpretability and effectiveness of these models in real-world applications. First, we collect hundreds of minutes of gaze data to demonstrate that we can mimic human gaze modalities using localized narratives. We then design an automatic data annotation pipeline utilizing GPT-4 to generate the VOILA-COCO dataset. Additionally, we innovate the Voila Perceiver modules to integrate gaze information into VLMs while preserving their pre-trained knowledge. We evaluate Voila-A using a hold-out validation set and a newly collected VOILA-GAZE test set, which features real-life scenarios captured with a gaze-tracking device. Our experimental results demonstrate that Voila-A significantly outperforms several baseline models. By aligning model attention with human gaze patterns, Voila-A paves the way for more intuitive, user-centric VLMs and fosters engaging human-AI interaction across a wide range of applications. Our code is available at https://github.com/naykun/Voila-A. © 2024 Neural information processing systems foundation. All rights reserved.",,,Conference paper,Final,,Scopus,2-s2.0-105000523827,Gaming / VR
Alharbi H.,"Alharbi, Hadeel (57223049298)",57223049298,Explainable feature selection and deep learning based emotion recognition in virtual reality using eye tracker and physiological data,2024,Frontiers in Medicine,11,,1438720,,,,0,10.3389/fmed.2024.1438720,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85205078373&doi=10.3389%2ffmed.2024.1438720&partnerID=40&md5=ed99ab2a71d6c965a7c25bae23b03ea5,"Emotional recognition is a way of detecting, evaluating, interpreting, and responding to others' emotional states and feelings, which might range from delight to fear to disgrace. There is increasing interest in the domains of psychological computing and human-computer interface (HCI), especially Emotion Recognition (ER) in Virtual Reality (VR). Human emotions and mental states are effectively captured using Electroencephalography (EEG), and there has been a growing need for analysis in VR situations. In this study, we investigated emotion recognition in a VR environment using explainable machine learning and deep learning techniques. Specifically, we employed Support Vector Classifiers (SVC), K-Nearest Neighbors (KNN), Logistic Regression (LR), Deep Neural Networks (DNN), DNN with flattened layer, Bi-directional Long-short Term Memory (Bi-LSTM), and Attention LSTM. This research utilized an effective multimodal dataset named VREED (VR Eyes: Emotions Dataset) for emotion recognition. The dataset was first reduced to binary and multi-class categories. We then processed the dataset to handle missing values and applied normalization techniques to enhance data consistency. Subsequently, explainable Machine Learning (ML) and Deep Learning (DL) classifiers were employed to predict emotions in VR. Experimental analysis and results indicate that the Attention LSTM model excelled in binary classification, while both DNN and Attention LSTM achieved outstanding performance in multi-class classification, with up to 99.99% accuracy. These findings underscore the efficacy of integrating VR with advanced, explainable ML and DL methods for emotion recognition. Copyright © 2024 Alharbi.",deep learning; EEG; emotion recognition; explainable artificial intelligence (XAI); machine learning; virtual reality,Article; binary classification; classifier; deep learning; deep neural network; electroencephalogram; electroencephalography; emotion; explainable artificial intelligence; explainable machine learning; fear; feature selection; human; k nearest neighbor; logistic regression analysis; long short term memory network; machine learning; multiclass classification; recognition; short term memory; support vector machine; virtual reality,Article,Final,All Open Access; Gold Open Access,Scopus,2-s2.0-85205078373,Gaming / VR
Zheng Y.-J.; Tsao H.-Y.,"Zheng, Yu-Jie (59491213000); Tsao, Hsaio-Yue (57903827300)",59491213000; 57903827300,Examining the emotional impact and visual attention of audiences in virtual reality environments through character configuration and spatial distance management,2024,GCCE 2024 - 2024 IEEE 13th Global Conference on Consumer Electronics,,,,1049,1050,1.0,0,10.1109/GCCE62371.2024.10760905,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85213330920&doi=10.1109%2fGCCE62371.2024.10760905&partnerID=40&md5=2388faf9e0d9908b9ed696d543ccde6d,"This study aims to explore audience experience in virtual reality environments through character configuration and spatial distance management. With the widespread application of VR technology in fields such as education, entertainment, and exhibitions, enhancing audience immersion and emotional engagement has become a critical challenge for designers. To address this, the study designed multiple scenarios with varying distances and character interactions. By setting up interactions between spatial distance and character interaction, and utilizing eye-tracking technology and the PANAS emotion scale, the study evaluates the impact of these factors on audience emotional response and attention distribution. The goal is to discover the best emotional induction strategies to enhance user experience. © 2024 IEEE.",Attention Distribution; Audience Experience; Character Configuration; Emotional Effects; Spatial Distance Management; Virtual Reality,Distance education; Virtual reality; Attention distribution; Audience experience; Character configuration; Emotional effect; In-field; Spatial distance; Spatial distance management; Virtual-reality environment; Visual Attention; VR technology; Virtual environments,Conference paper,Final,,Scopus,2-s2.0-85213330920,Gaming / VR
Lavoie E.; Hebert J.S.; Chapman C.S.,"Lavoie, Ewen (57197840075); Hebert, Jacqueline S. (8692858600); Chapman, Craig S. (24597673600)",57197840075; 8692858600; 24597673600,"Comparing eye–hand coordination between controllermediated virtual reality, and a real-world object interaction task",2024,Journal of Vision,24,2,,1,18,17.0,9,10.1167/JOV.24.2.9,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85185864351&doi=10.1167%2fJOV.24.2.9&partnerID=40&md5=3d00d5392539c71c8b3cf134a6fa4466,"Virtual reality (VR) technology has advanced significantly in recent years, with many potential applications. However, it is unclear how well VR simulations mimic real-world experiences, particularly in terms of eye–hand coordination. This study compares eye–hand coordination from a previously validated real-world object interaction task to the same task re-created in controller-mediated VR.We recorded eye and body movements and segmented participants’ gaze data using the movement data. In the real-world condition, participants wore a head-mounted eye tracker and motion capture markers and moved a pasta box into and out of a set of shelves. In the VR condition, participants wore a VR headset and moved a virtual box using handheld controllers. Unsurprisingly, VR participants took longer to complete the task. Before picking up or dropping off the box, participants in the real world visually fixated the box about half a second before their hand arrived at the area of action. This 500-ms minimum fixation time before the hand arrived was preserved in VR. Real-world participants disengaged their eyes from the box almost immediately after their hand initiated or terminated the interaction, but VR participants stayed fixated on the box for much longer after it was picked up or dropped off.We speculate that the limited haptic feedback during object interactions in VR forces users to maintain visual fixation on objects longer than in the real world, altering eye–hand coordination. These findings suggest that current VR technology does not replicate real-world experience in terms of eye–hand coordination. Copyright (2024) The Authors",eye tracking; eyeâ€“hand coordination; object interactions; virtual reality; visual attention,"Eye; Fixation, Ocular; Hand; Humans; Movement; Virtual Reality; eye; eye fixation; hand; human; movement (physiology); virtual reality",Article,Final,All Open Access; Gold Open Access; Green Open Access,Scopus,2-s2.0-85185864351,Gaming / VR
,,,Imaging and Multimedia Analytics at the Edge 2024,2024,IS and T International Symposium on Electronic Imaging Science and Technology,36,8,,,,274.0,0,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85185382078&partnerID=40&md5=0aed53bdad29b4eae95ea0fa1eea3839,The proceedings contain 34 papers. The topics discussed include: extending lidar depth range using stereo depth estimation on intensity data; transformers for microscopy slide image segmentation of invasive melanoma; intracell capacitances in pixel-level hybrid bond image sensor cells; self-attention enhanced recognition: a unified model for handwriting and scene-text recognition with improved inference; evaluation of subjective video quality of television displays; experimental study on reducing wavelength dependency of spatial resolution characteristics of a digital camera by MTF correction; comparative analysis of user experience in virtual reality (VR) and mixed reality (MR) systems using eye-tracking measurements; RAIV: researchable archives for interactive visualizations; mobile 3D mapping of Erdstall facilities; real-time stereoscopic image-parallel path tracing; automatic reading order sequencing: a novel reading order generator for text-based documents; and improving video deepfake detection: a DCT-based approach with patch-level analysis.,,,Conference review,Final,,Scopus,2-s2.0-85185382078,Gaming / VR
Huyghe T.; Calleja-González J.; Bird S.P.; Alcaraz P.E.,"Huyghe, Thomas (57215698566); Calleja-González, Julio (57573546000); Bird, Stephen P. (7201459416); Alcaraz, Pedro E. (55509074400)",57215698566; 57573546000; 7201459416; 55509074400,Pupillometry as a new window to player fatigue? A glimpse inside the eyes of a Euro Cup Women’s Basketball team,2024,Biology of Sport,41,1,,3,15,12.0,5,10.5114/biolsport.2024.125590,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85182582748&doi=10.5114%2fbiolsport.2024.125590&partnerID=40&md5=f6abacd6cc8ac1fcd2749d63fb1a182c,"A rapidly emerging area of interest in high-pressure environments is that of pupillometry, where handheld quantitative infrared pupillometers (HQIPs) are able to track psycho-physiological fatigue in a fast, objective, valid, reliable, and non-invasive manner. However, the application of HQIPs in the context of athlete monitoring is yet to be determined. Therefore, the main aim of this pilot study was to examine the potential usefulness of a HQIP to monitor game-induced fatigue inside a professional female basketball setting by determining its (1) test-retest repeatability, (2) relationship with other biomarkers of game-induced fatigue, and (3) time-course from rested to fatigued states. A non-ophthalmologic practitioner performed a standardized Pupil Light Reflex (PLR) test using a medically graded HQIP among 9 professional female basketball players (2020–2021 Euro Cup) at baseline, 24-h pre-game (GD-1), 24-h post-game (GD+1) and 48-h post-game (GD+2). This was repeated over four subsequent games, equalling a total of 351 observations per eye. Two out of seven pupillometrics displayed good ICCs (0.95–0.99) (MinD and MaxD). Strong significant relationships were found between MaxD, MinD, and all registered biomarkers of game-induced fatigue (r = 0.69–0.82, p < 0.05), as well as between CV, MCV, and cognitive, lower-extremity muscle, and physiological fatigue markers (r = 0.74–0.76, p < 0.05). Three pupillometrics were able to detect a significant difference between rested and fatigued states. In particular, PC (right) (F = 5.173, η2 = 0.115 p = 0.028) and MCV (right) (F = 3.976, η2 = 0.090 p = 0.049) significantly decreased from baseline to GD+2, and LAT (left) (F = 4.023, η2 = 0.109 p = 0.009) significantly increased from GD-1 to GD+2. HQIPs have opened a new window of opportunity for monitoring game-induced fatigue in professional female basketball players. However, future research initiatives across larger and heterogenous samples, and longer investigation periods, are required to expand upon these preliminary findings. © 2024 Institute of Sport. All rights reserved.",AMS; Athlete monitoring; Eye-tracking; Neuroimaging; Neurotechnology,,Article,Final,,Scopus,2-s2.0-85182582748,Gaming / VR
Ferreira M.E.C.; Lima-Junior D.; Faro H.; Roelands B.; Fortes L.S.,"Ferreira, Maria E.C. (23979925800); Lima-Junior, Dalton (57209201320); Faro, Heloiana (57201417997); Roelands, Bart (8590460900); Fortes, Leonardo S. (54986005700)",23979925800; 57209201320; 57201417997; 8590460900; 54986005700,Prolonged cognitive effort impairs inhibitory control and causes significant mental fatigue after an endurance session with an auditive distractor in professional soccer players,2024,Psychology of Sport and Exercise,70,,102533,,,,8,10.1016/j.psychsport.2023.102533,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85170421564&doi=10.1016%2fj.psychsport.2023.102533&partnerID=40&md5=535ae91018b9ff98e3eb2f695f9ef7c1,"Background: Throughout official soccer matches, the presence of cheer by the crowd could be considered a critical auditive distraction that could further impair the cognitive interference control system, multiple object tracking (MOT) skill, heart rate variability (HRV), and increase mental fatigue. As the resource is not immediately replenished, the impairment of the cognitive interference control system may be delayed following a soccer game. Then, evaluating the recovery time course of the cognitive interference control system, MOT skill, HRV, and mental fatigue after prolonged tasks combining physical, endurance, and cognitive effort are essential. Purpose: We aimed to analyze the acute effect of cognitive effort and auditive distractor with 24-h follow-up throughout a prolonged endurance session on inhibitory control, subjective mental fatigue, MOT skill, and HRV in professional soccer players. Methods: Twenty professional male soccer players were recruited (23.56 ± 3.8 years, 78.1 ± 6.9 kg, 1.77 ± 0.06 m, and 12.5 ± 5.3% body fat). The sessions were performed in a randomized and counterbalanced crossover design, divided into four experimental conditions: endurance, endurance + MOT, endurance + MOT + AD, and endurance + AD. The soccer players completed the incongruent Stroop task utilizing an eye-tracker to assess cognitive effort. MOT task, subjective mental fatigue, and HRV were evaluated before the endurance training (60%Δ of maximal aerobic velocity during 40-min) and after 30-min and 24-h of recovery. These sessions were designed to investigate the acute effect of prolonged cognitive effort (repeated MOT throughout the endurance task) and AD (constant crowd noise and coach's voice each 15–40 s, totalizing = 80 voices) on inhibitory control, MOT skills, HRV, and subjective mental fatigue after a fixed endurance training session. Results: There was no condition × time interaction for accuracy of inhibitory control (p > 0.05, ηp2 = 0.001). There was a significant condition × time interaction for inhibitory control response time (p < 0.05, ηp2 = 0.16). A higher response time of inhibitory control was found for the endurance + MOT + AD and endurance + MOT experimental sessions (p < 0.05). There was a significant condition × time interaction for subjective mental fatigue (p < 0.05, ηp2 = 0.46). A higher subjective mental fatigue was found for the endurance + MOT + AD and endurance + MOT experimental sessions (p < 0.05). There was no condition × time interaction for HRV (p > 0.05, ηp2 = 0.02). Conclusion: We concluded that cognitive effort throughout a prolonged endurance session impaired inhibitory control and increased mental fatigue without promoting greater MOT skill and HRV changes in professional soccer players. © 2023 Elsevier Ltd",Brain; Cognition; Endurance training; Executive function; Sports,,Article,Final,,Scopus,2-s2.0-85170421564,Gaming / VR
He M.; Xu L.-X.; Li C.-S.R.; Liu Z.; Hu J.; Guo X.; Liu H.; Zhang J.-T.,"He, Mengxin (57412353700); Xu, Lin-Xuan (57215192800); Li, Chiang-shan R. (8208349200); Liu, Zihan (57219782842); Hu, Jiaqi (57411409700); Guo, Xiangyi (57224951571); Liu, Hongyun (36918643000); Zhang, Jin-Tao (55720355700)",57412353700; 57215192800; 8208349200; 57219782842; 57411409700; 57224951571; 36918643000; 55720355700,Do Real-Time Strategy Video Gamers Have Better Attentional Control?,2024,Human Factors,66,1,,258,270,12.0,6,10.1177/00187208211064683,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122809127&doi=10.1177%2f00187208211064683&partnerID=40&md5=118cf6d4602f47155343280d55291697,"Objective: Do real-time strategy (RTS) video gamers have better attentional control? To examine this issue, we tested experienced versus inexperienced RTS video gamers on multi-object tracking tasks (MOT) and dual-MOT tasks with visual or auditory secondary tasks (dMOT). We employed a street-crossing task with a visual working memory task as a secondary task in a virtual reality (VR) environment to examine any generalized attentional advantage. Background: Similar to action video games, RTS video games require players to switch attention between multiple visual objects and views. However, whether the attentional control advantage is limited by sensory modalities or generalizes to real-life tasks remains unclear. Method: In study 1, 25 RTS video game players (SVGP) and 25 non-video game players (NVGP) completed the MOT task and two dMOT tasks. In study 2, a different sample with 25 SVGP and 25 NVGP completed a simulated street-crossing task with the visual dual task in a VR environment. Results: After controlling the effects of the speed-accuracy trade-off, SVGP showed better performance than NVGP in the MOT task and the visual dMOT task, but SVGP did not perform better in either the auditory dMOT task or the street-crossing task. Conclusion: RTS video gamers had better attentional control in visual computer tasks, but not in the auditory tasks and the VR tasks. Attentional control benefits associated with RTS video game experience may be limited by sensory modalities, and may not translate to performance benefits in real-life tasks. © 2022 Human Factors and Ergonomics Society.",attentional control; ecological validity; real-time strategy video game; virtual reality,"Attention; Humans; Memory, Short-Term; Video Games; Computer software; Economic and social effects; Human computer interaction; Interactive computer graphics; Real time systems; Attentional control; Ecological validity; Game players; Multi-object tracking; Real time strategies; Real-time strategy video game; Secondary tasks; Street crossing; Video-games; Virtual-reality environment; article; attention; ecological validity; eye tracking; human; velocity; video game; virtual reality; working memory; attention; short term memory; Virtual reality",Article,Final,,Scopus,2-s2.0-85122809127,Gaming / VR
Pettersson K.; Tervonen J.; Heininen J.; Mäntyjärvi J.,"Pettersson, K. (55931088700); Tervonen, J. (57218384086); Heininen, J. (59359397200); Mäntyjärvi, J. (6602500425)",55931088700; 57218384086; 59359397200; 6602500425,Head-area sensing in virtual reality: future visions for visual perception and cognitive state estimation,2024,Frontiers in Virtual Reality,5,,1423756,,,,0,10.3389/frvir.2024.1423756,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85205868746&doi=10.3389%2ffrvir.2024.1423756&partnerID=40&md5=58c2c9f8b4a5678c828316a3ab638e7c,"Biosensing techniques are progressing rapidly, promising the emergence of sophisticated virtual reality (VR) headsets with versatile biosensing enabling an objective, yet unobtrusive way to monitor the user’s physiology. Additionally, modern artificial intelligence (AI) methods provide interpretations of multimodal data to obtain personalised estimations of the users’ oculomotor behaviour, visual perception, and cognitive state, and their possibilities extend to controlling, adapting, and even creating the virtual audiovisual content in real-time. This article proposes a visionary approach for personalised virtual content adaptation via novel and precise oculomotor feature extraction from a freely moving user and sophisticated AI algorithms for cognitive state estimation. The approach is presented with an example use-case of a VR flight simulation session explaining in detail how cognitive workload, decrease in alertness level, and cybersickness symptoms could be modified in real-time by using the methods and embedded stimuli. We believe the envisioned approach will lead to significant cost savings and societal impact and will thus be a necessity in future VR setups. For instance, it will increase the efficiency of a VR training session by optimizing the task difficulty based on the user’s cognitive load and decrease the probability of human errors by guiding visual perception via content adaptation. Copyright © 2024 Pettersson, Tervonen, Heininen and Mäntyjärvi.",adaptive sampling; artificial intelligence; cognitive state estimation; oculomotor behavior; virtual reality; visual perception,,Article,Final,All Open Access; Gold Open Access,Scopus,2-s2.0-85205868746,Gaming / VR
Hulle N.; Aroca-Ouellette S.; Ries A.J.; Brawer J.; Von Der Wense K.; Roncone A.,"Hulle, Nikhil (59244084900); Aroca-Ouellette, Stéphane (57210727523); Ries, Anthony J. (36848887500); Brawer, Jake (57202049270); Von Der Wense, Katharina (58606838900); Roncone, Alessandro (56641523600)",59244084900; 57210727523; 36848887500; 57202049270; 58606838900; 56641523600,"Eyes on the Game: Deciphering Implicit Human Signals to Infer Human Proficiency, Trust, and Intent",2024,"IEEE International Workshop on Robot and Human Communication, RO-MAN",,,,453,460,7.0,1,10.1109/RO-MAN60168.2024.10731204,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85209820374&doi=10.1109%2fRO-MAN60168.2024.10731204&partnerID=40&md5=77ed4541d1d3c8a5d858e1ef8052a560,"Effective collaboration between humans and AIs hinges on transparent communication and alignment of mental models. However, explicit, verbal communication is not always feasible. Under such circumstances, human-human teams often depend on implicit, nonverbal cues to glean important information about their teammates such as intent and expertise, thereby bolstering team alignment and adaptability. Among these implicit cues, two of the most salient and fundamental are a human's actions in the environment and their visual attention. In this paper, we present a novel method to combine eye gaze data and behavioral data, and evaluate their respective predictive power for human proficiency, trust, and intent. We first collect a dataset of paired eye gaze and gameplay data in the fast-paced collaborative ""Overcooked""environment. We then train models on this dataset to compare how the predictive powers differ between gaze data, gameplay data, and their combination. We additionally compare our method to prior works that aggregate eye gaze data and demonstrate how these aggregation methods can substantially reduce the predictive ability of eye gaze. Our results indicate that, while eye gaze data and gameplay data excel in different situations, a model that integrates both types consistently outperforms all baselines. This work paves the way for developing intuitive and responsive agents that can efficiently adapt to new teammates.  © 2024 IEEE.",,Behavioral data; Eye-gaze; Gameplay; Human actions; Mental model; Nonverbal cues; Novel methods; Predictive power; Verbal communications; Visual Attention,Conference paper,Final,,Scopus,2-s2.0-85209820374,Gaming / VR
Guastello S.J.; McGuigan L.M.,"Guastello, Stephen J. (7004615891); McGuigan, Laura M. (57846388500)",7004615891; 57846388500,Cusp Catastrophe Models for Cognitive Workload and Fatigue for Teams Making Dynamic Decisions,2024,"Nonlinear Dynamics, Psychology, and Life Sciences",28,1,,71,109,38.0,4,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85181395780&partnerID=40&md5=ec40cba5003926a3b46a5d6405d955a1,"This study evaluated cusp models of workload and fatigue experienced by teams on a dynamic decision making task. Cognitive workload is the amount of information that a person is required to process in a given way in a fixed amount of time. Fatigue, which is captured by a work curve or a cubic polynomial function, is the loss of work capacity that is produced by an extended amount of time spent on a particular cognitive or physical task. In this experiment, 32 groups of three, four or five members (136 individuals) played two matches of a first-person shooter computer game, and completed subjective measures of workload and cognitive measures of elasticity versus rigidity. For the workload cusp models with elasticity-rigidity components, the bifurcation in performance levels occurred when teams expressed greater emotional intelligence, anxiety, levels of fluid intelligence, coping flexibility, cognitive flexibility, and were more decisive (R2=.54-.56, linear alternative,.09-.23). For workload cusp models assessing subjective ratings of workload, bifurcation occurred with groups who reported greater levels of performance demand and effort required (R2=.51, linear alternative,.20). For fatigue cusp models, bifurcation occurred for groups that played fewer rounds of the game before winning or losing the match, or came from the smaller-sized groups, which were supplemented by computer-generated agents (R2=.66-.67, linear alternative,.21-.68). Results supported the general-ization of the cusp models for workload and fatigue to situations requiring teamwork in dynamic decision making environments. The study also raised new questions about the role of autonomic synchrony in the workload or fatigue processes and similarity of the dynamics of human-autonomy teams compared to all-human teams. © 2024 Society for Chaos Theory in Psychology & Life Sciences",cusp catastrophe; dynamic decisions; group fatigue; group workload,Cognition; Emotional Intelligence; Fatigue; Humans; Task Performance and Analysis; Workload; cognition; emotional intelligence; fatigue; human; psychology; task performance; workload,Article,Final,,Scopus,2-s2.0-85181395780,Gaming / VR
Houzangbe S.; Lemay M.; Levac D.E.,"Houzangbe, Samory (57202609989); Lemay, Martin (7005435451); Levac, Danielle E. (25937323900)",57202609989; 7005435451; 25937323900,Toward Physiological Detection of a “Just-Right” Challenge Level for Motor Learning in Immersive Virtual Reality: Protocol for a Cross-Sectional Study,2024,JMIR Research Protocols,13,,e55730,,,,0,10.2196/55730,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85204783463&doi=10.2196%2f55730&partnerID=40&md5=5eba928f7582ea59b0af498cae89f6fd,"Background: Motor learning, a primary goal of pediatric rehabilitation, is facilitated when tasks are presented at a “just-right” challenge level—at the edge of the child’s current abilities, yet attainable enough to motivate the child in persistent efforts for success. Immersive virtual reality (VR) may be ideally suited for “just-right” task challenges because it enables precise adjustments of task parameters in motivating environments. Rehabilitation-specific VR tasks often use dynamic difficulty algorithms based on task performance to personalize task difficulty. However, these approaches do not consider relevant cognitive processes that could also impact “just-right” challenges, such as attention and engagement. Objective physiological measurement of these cognitive processes using wearable sensors could support their integration within “just-right” challenge detection and prediction algorithms. As a first step, it is important to explore relationships between objectively and subjectively measured psychophysiological states at progressively challenging task difficulty levels. Objective: This study aims to (1) evaluate the performance of wearable sensors in a novel movement-based motor learning immersive VR task; (2) evaluate changes in physiological data at 3 task difficulty levels; and (3) explore the relationship between physiological data, task performance, and self-reported cognitive processes at each task difficulty level. Methods: This study uses the within-participant experimental design. Typically developing children and youth aged 8-16 years will be recruited to take part in a single 90-minute data collection session. Physiological sensors include electrodermal activity, heart rate, electroencephalography, and eye-tracking. After collecting physiological data at rest, participants will play a seated unimanual immersive VR task involving bouncing a virtual ball on a virtual racket. They will first play for 3 minutes at a predefined medium level of difficulty to determine their baseline ability level and then at a personalized choice of 3 progressive difficulty levels of 3 minutes each. Following each 3-minute session, participants will complete a short Likert-scale questionnaire evaluating engagement, attention, cognitive workload, physical effort, self-efficacy, and motivation. Data loss and data quality will be calculated for each sensor. Repeated-measures ANOVAs will evaluate changes in physiological response at each difficulty level. Correlation analyses will determine individual relationships between task performance, physiological data, and self-reported data at each difficulty level. Results: Research ethics board approval has been obtained, and data collection is underway. Data collection was conducted on December 12, 2023, and April 12, 2024, with a total of 15 typically developing children. Data analysis has been completed, and results are expected to be published in the fall of 2024. Conclusions: Wearable sensors may provide insights into the physiological effects of immersive VR task interaction at progressive difficulty levels in children and youth. Understanding the relationship between physiological and self-reported cognitive processes is a first step in better identifying and predicting “just-right” task challenges during immersive VR motor learning interventions. ©Samory Houzangbe, Martin Lemay, Danielle E Levac.",engagement; just-right challenge; pediatric rehabilitation; physiological data; virtual reality,,Article,Final,All Open Access; Gold Open Access,Scopus,2-s2.0-85204783463,Gaming / VR
Mancone S.; Tosti B.; Corrado S.; Diotaiuti P.,"Mancone, Stefania (36337979300); Tosti, Beatrice (58076427300); Corrado, Stefano (57219947553); Diotaiuti, Pierluigi (56263490400)",36337979300; 58076427300; 57219947553; 56263490400,Effects of video game immersion and task interference on cognitive performance: a study on immediate and delayed recall and recognition accuracy,2024,PeerJ,12,10,e18195,,,,1,10.7717/peerj.18195,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85206438340&doi=10.7717%2fpeerj.18195&partnerID=40&md5=51e26af7c97bc68cf1108cb3796fb4b1,"This study investigates the cognitive impacts of video game immersion and task interference on immediate and delayed recall as well as recognition tasks. We enrolled 160 subjects aged 18 to 29, who were regular players of “shoot-em-up” video games for at least 3 years. Participants were assigned to one of three experimental groups or a control group. The experimental conditions varied in the timing and type of tasks: the first group performed a video game session between recall tasks, the second group multitasked with video games and recall tasks simultaneously, and the third group engaged in task switching from video games to recall tasks. Using the Rey Auditory Verbal Learning Test, we measured the effects of these conditions on cognitive performance, focusing on error types and recall accuracy. Results indicated that multitasking and task switching significantly affected the subjects’ performance, with notable decrements in recall and recognition accuracy in conditions of high task interference. The study highlights the cognitive costs associated with multitasking in immersive digital games and provides insights into how task similarity and interference might increase error rates and affect memory performance. Copyright 2024 Mancone et al.",Attentional resources; Cognitive interference; Cognitive load; Digital game immersion; Error analysis; Human-computer interaction; Memory recall; Memory recognition; Multitasking; Task switching,adult; Article; attention deficit hyperactivity disorder; cognition; cognitive load; controlled study; delayed recall; eye tracking; female; human; human computer interaction; human experiment; immediate recall; immersion; male; memory consolidation; mental performance; normal human; personal experience; recall; recognition; Rey auditory verbal learning test; task performance; task switching; video game; work environment; working memory,Article,Final,All Open Access; Gold Open Access,Scopus,2-s2.0-85206438340,Gaming / VR
Moharana B.; Keighrey C.; Murray N.,"Moharana, Bhagyabati (57830087500); Keighrey, Conor (57194121861); Murray, Niall (7201513152)",57830087500; 57194121861; 7201513152,Role-Specific Physiological Responses and Quality of Experience in Collaborative Virtual Reality Tasks: A Comparative Study of Leaders and Followers,2024,"2024 10th International Conference on Virtual Reality, ICVR 2024",,,,11,20,9.0,0,10.1109/ICVR62393.2024.10868723,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85218422316&doi=10.1109%2fICVR62393.2024.10868723&partnerID=40&md5=f4439574c24bd42c9f824cfd00a7bde8,"The emergence of Collaborative Virtual Reality (CVR) technology has transformed team-based interactions across diverse fields, offering immersive and interactive environments that enhance collaborative efforts. However, a critical gap remains in understanding the Quality of Experience (QoE) from a task role-specific perspective within these environments. This study addresses this gap by evaluating the QoE differences between leaders and followers in CVR settings, using both subjective assessments and objective physiological measures. The research employs advanced biosensors, including eye-tracking and physiological responses, to examine how distinct roles influence cognitive load, emotional states, and overall user satisfaction. Participants engaged in a collaborative 'pick and place' task within a VR environment, with data collected on electrodermal activity (EDA), heart rate (HR), blood volume pulse (BVP), inter-beat interval (IBI), skin temperature, and pupil dilation (PD). The NASA Task Load Index (NASA-TLX) was also used to measure perceived workload. Findings reveal significant differences in physiological responses between leaders and followers, with leaders experiencing higher cognitive and temporal demands. The results suggest that role-specific QoE evaluations can inform the design of more equitable and effective CVR systems, enhancing user satisfaction and collaboration outcomes. This study advances the field by integrating multimodal data acquisition and providing insights into CVR environments for diverse collaborative roles. © 2024 IEEE.",Collaborative Virtual Reality (CVR); Head Mounted Display (HMD); Physiological Sensors; Pupil Dilation; Quality of Experience (QoE),Electrotherapeutics; Physiological models; Collaborative virtual reality; Comparatives studies; Head mounted display; Head-mounted-displays; Physiological response; Physiological sensors; Pupil dilation; Quality of experience; Users' satisfactions; Helmet mounted displays,Conference paper,Final,,Scopus,2-s2.0-85218422316,Gaming / VR
Hussain G.F.; Heena A.,"Hussain, G. Fayaz (59307395700); Heena, A. (59778041700)",59307395700; 59778041700,"Exploring the Impact of Physical vs. Artificial Motion on Immersion, Usability, and Cybersickness in Virtual Environments",2024,"2024 2nd International Conference on Artificial Intelligence Trends and Pattern Recognition, ICAITPR 2024",,,,,,,0,10.1109/ICAITPR63242.2024.10960045,https://www.scopus.com/inward/record.uri?eid=2-s2.0-105004405709&doi=10.1109%2fICAITPR63242.2024.10960045&partnerID=40&md5=c71e5b164ee8037e9e8d870f6a78eb20,"Virtual reality (VR) technology has gained considerable attention in recent years due to its potential in providing immersive experiences. However, challenges remain in creating VR environments that balance immersion, usability, and minimizing cybersickness. This study examines the effects of physical and artificial motion on immersion, usability, and cybersickness in VR. Through a mixed-methods approach, participants are exposed to VR experiences incorporating different levels of physical and artificial motion. Immersion is assessed using subjective evaluations and physiological measurements, encompassing presence, engagement, and emotional responses. Usability is evaluated through user interaction, task performance, and cognitive workload measures. Cybersickness, a common adverse effect, is measured using self-report questionnaires and physiological indicators. Preliminary findings indicate that physical and artificial motion significantly influence immersion, usability, and cybersickness in VR. Increased physical motion enhances user immersion, fostering a heightened sense of presence and emotional engagement. However, excessive motion may lead to higher cybersickness levels and reduced usability due to discomfort and disorientation. Artificial motion techniques, such as redirected walking and dynamic field-of-view adaptation, show promise in mitigating cybersickness while maintaining immersion and usability. These findings have implications for VR developers, designers, and researchers seeking to create compelling and user-friendly experiences. Understanding the effects of physical and artificial motion on immersion, usability, and cybersickness can inform the optimization of VR environments, striking a balance between realism and user comfort. Further research is warranted to refine motion techniques and explore individual differences in cybersickness susceptibility, ultimately facilitating the widespread adoption of VR across various domains.  © 2024 IEEE.",Joystick-based locomotion; Kinesthetic cues; Proprioceptive cues,Brain; Sensory perception; Virtual reality; Virtualization; Cybersickness; Immersive; Joystick-based locomotion; Kinesthetic cue; Kinesthetics; Mixed method; Motion techniques; Proprioceptive cue; Virtual reality technology; Virtual-reality environment; Usability engineering,Conference paper,Final,,Scopus,2-s2.0-105004405709,Gaming / VR
